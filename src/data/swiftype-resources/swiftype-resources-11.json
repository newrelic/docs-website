{
  "/docs/integrations/amazon-integrations/get-started/connect-aws-new-relic-infrastructure-monitoring": [
    {
      "sections": [
        "Integrations and managed policies",
        "Recommended policy",
        "Important",
        "Optional policy",
        "Option 1: Use our CloudFormation template",
        "CloudFormation template",
        "Option 2: Manually add permissions",
        "Required by all integrations",
        "ALB permissions",
        "API Gateway permissions",
        "Auto Scaling permissions",
        "Billing permissions",
        "Cloudfront permissions",
        "CloudTrail permissions",
        "DynamoDB permissions",
        "EBS permissions",
        "EC2 permissions",
        "ECS/ECR permissions",
        "EFS permissions",
        "ElastiCache permissions",
        "ElasticSearch permissions",
        "Elastic Beanstalk permissions",
        "ELB permissions",
        "EMR permissions",
        "Health permissions",
        "IAM permissions",
        "IoT permissions",
        "Kinesis Firehose permissions",
        "Kinesis Streams permissions",
        "Lambda permissions",
        "RDS, RDS Enhanced Monitoring permissions",
        "Redshift permissions",
        "Route 53 permissions",
        "S3 permissions",
        "Simple Email Service (SES) permissions",
        "SNS permissions",
        "SQS permissions",
        "Trusted Advisor permissions",
        "VPC permissions",
        "X-Ray monitoring permissions"
      ],
      "title": "Integrations and managed policies",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Get started"
      ],
      "external_id": "80e215e7b2ba382de1b7ea758ee1b1f0a1e3c7df",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/get-started/integrations-managed-policies/",
      "published_at": "2021-05-04T18:30:29Z",
      "updated_at": "2021-05-04T18:30:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In order to use infrastructure integrations, you need to grant New Relic permission to read the relevant data from your account. Amazon Web Services (AWS) uses managed policies to grant these permissions. Recommended policy Important Recommendation: Grant an account-wide ReadOnlyAccess managed policy from AWS. AWS automatically updates this policy when new services are added or existing services are modified. New Relic infrastructure integrations have been designed to function with ReadOnlyAccess policies. For instructions, see Connect AWS integrations to infrastructure. Exception: The Trusted Advisor integration is not covered by the ReadOnlyAccess policy. It requires the additional AWSSupportAccess managed policy. This is also the only integration that requires full access permissions (support:*) in order to correctly operate. We notified Amazon about this limitation. Once it's resolved we'll update documentation with more specific permissions required for this integration. Optional policy If you cannot use the ReadOnlyAccess managed policy from AWS, you can create your own customized policy based on the list of permissions. This allows you to specify the optimal permissions required to fetch data from AWS for each integration. While this option is available, it is not recommended because it must be manually updated when you add or modify your integrations. Important New Relic has no way of identifying problems related to custom permissions. If you choose to create a custom policy, it is your responsibility to maintain it and ensure proper data is being collected. There are two ways to set up your customized policy: You can either use our CloudFormation template, or create own yourself by adding the permissions you need. Option 1: Use our CloudFormation template Our CloudFormation template contains all the permissions for all our AWS integrations. A user different than root can be used in the managed policy. CloudFormation template AWSTemplateFormatVersion: 2010-09-09 Outputs: NewRelicRoleArn: Description: NewRelicRole to monitor AWS Lambda Value: !GetAtt - NewRelicIntegrationsTemplate - Arn Parameters: NewRelicAccountNumber: Type: String Description: The Newrelic account number to send data AllowedPattern: '[0-9]+' Resources: NewRelicIntegrationsTemplate: Type: 'AWS::IAM::Role' Properties: RoleName: !Sub NewRelicTemplateTest AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: AWS: !Sub 'arn:aws:iam::754728514883:root' Action: 'sts:AssumeRole' Condition: StringEquals: 'sts:ExternalId': !Ref NewRelicAccountNumber Policies: - PolicyName: NewRelicIntegrations PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'elasticloadbalancing:DescribeLoadBalancers' - 'elasticloadbalancing:DescribeTargetGroups' - 'elasticloadbalancing:DescribeTags' - 'elasticloadbalancing:DescribeLoadBalancerAttributes' - 'elasticloadbalancing:DescribeListeners' - 'elasticloadbalancing:DescribeRules' - 'elasticloadbalancing:DescribeTargetGroupAttributes' - 'elasticloadbalancing:DescribeInstanceHealth' - 'elasticloadbalancing:DescribeLoadBalancerPolicies' - 'elasticloadbalancing:DescribeLoadBalancerPolicyTypes' - 'apigateway:GET' - 'apigateway:HEAD' - 'apigateway:OPTIONS' - 'autoscaling:DescribeLaunchConfigurations' - 'autoscaling:DescribeAutoScalingGroups' - 'autoscaling:DescribePolicies' - 'autoscaling:DescribeTags' - 'autoscaling:DescribeAccountLimits' - 'budgets:ViewBilling' - 'budgets:ViewBudget' - 'cloudfront:ListDistributions' - 'cloudfront:ListStreamingDistributions' - 'cloudfront:ListTagsForResource' - 'cloudtrail:LookupEvents' - 'config:BatchGetResourceConfig' - 'config:ListDiscoveredResources' - 'dynamodb:DescribeLimits' - 'dynamodb:ListTables' - 'dynamodb:DescribeTable' - 'dynamodb:ListGlobalTables' - 'dynamodb:DescribeGlobalTable' - 'dynamodb:ListTagsOfResource' - 'ec2:DescribeVolumeStatus' - 'ec2:DescribeVolumes' - 'ec2:DescribeVolumeAttribute' - 'ec2:DescribeInstanceStatus' - 'ec2:DescribeInstances' - 'ec2:DescribeVpnConnections' - 'ecs:ListServices' - 'ecs:DescribeServices' - 'ecs:DescribeClusters' - 'ecs:ListClusters' - 'ecs:ListTagsForResource' - 'ecs:ListContainerInstances' - 'ecs:DescribeContainerInstances' - 'elasticfilesystem:DescribeMountTargets' - 'elasticfilesystem:DescribeFileSystems' - 'elasticache:DescribeCacheClusters' - 'elasticache:ListTagsForResource' - 'es:ListDomainNames' - 'es:DescribeElasticsearchDomain' - 'es:DescribeElasticsearchDomains' - 'es:ListTags' - 'elasticbeanstalk:DescribeEnvironments' - 'elasticbeanstalk:DescribeInstancesHealth' - 'elasticbeanstalk:DescribeConfigurationSettings' - 'elasticloadbalancing:DescribeLoadBalancers' - 'elasticmapreduce:ListInstances' - 'elasticmapreduce:ListClusters' - 'elasticmapreduce:DescribeCluster' - 'elasticmapreduce:ListInstanceGroups' - 'health:DescribeAffectedEntities' - 'health:DescribeEventDetails' - 'health:DescribeEvents' - 'iam:ListSAMLProviders' - 'iam:ListOpenIDConnectProviders' - 'iam:ListServerCertificates' - 'iam:GetAccountAuthorizationDetails' - 'iam:ListVirtualMFADevices' - 'iam:GetAccountSummary' - 'iot:ListTopicRules' - 'iot:GetTopicRule' - 'iot:ListThings' - 'firehose:DescribeDeliveryStream' - 'firehose:ListDeliveryStreams' - 'kinesis:ListStreams' - 'kinesis:DescribeStream' - 'kinesis:ListTagsForStream' - 'rds:ListTagsForResource' - 'rds:DescribeDBInstances' - 'rds:DescribeDBClusters' - 'redshift:DescribeClusters' - 'redshift:DescribeClusterParameters' - 'route53:ListHealthChecks' - 'route53:GetHostedZone' - 'route53:ListHostedZones' - 'route53:ListResourceRecordSets' - 'route53:ListTagsForResources' - 's3:GetLifecycleConfiguration' - 's3:GetBucketTagging' - 's3:ListAllMyBuckets' - 's3:GetBucketWebsite' - 's3:GetBucketLogging' - 's3:GetBucketCORS' - 's3:GetBucketVersioning' - 's3:GetBucketAcl' - 's3:GetBucketNotification' - 's3:GetBucketPolicy' - 's3:GetReplicationConfiguration' - 's3:GetMetricsConfiguration' - 's3:GetAccelerateConfiguration' - 's3:GetAnalyticsConfiguration' - 's3:GetBucketLocation' - 's3:GetBucketRequestPayment' - 's3:GetEncryptionConfiguration' - 's3:GetInventoryConfiguration' - 's3:GetIpConfiguration' - 'ses:ListConfigurationSets' - 'ses:GetSendQuota' - 'ses:DescribeConfigurationSet' - 'ses:ListReceiptFilters' - 'ses:ListReceiptRuleSets' - 'ses:DescribeReceiptRule' - 'ses:DescribeReceiptRuleSet' - 'sns:GetTopicAttributes' - 'sns:ListTopics' - 'sqs:ListQueues' - 'sqs:ListQueueTags' - 'sqs:GetQueueAttributes' - 'tag:GetResources' - 'ec2:DescribeInternetGateways' - 'ec2:DescribeVpcs' - 'ec2:DescribeNatGateways' - 'ec2:DescribeVpcEndpoints' - 'ec2:DescribeSubnets' - 'ec2:DescribeNetworkAcls' - 'ec2:DescribeVpcAttribute' - 'ec2:DescribeRouteTables' - 'ec2:DescribeSecurityGroups' - 'ec2:DescribeVpcPeeringConnections' - 'ec2:DescribeNetworkInterfaces' - 'lambda:GetAccountSettings' - 'lambda:ListFunctions' - 'lambda:ListAliases' - 'lambda:ListTags' - 'lambda:ListEventSourceMappings' - 'cloudwatch:GetMetricStatistics' - 'cloudwatch:ListMetrics' - 'cloudwatch:GetMetricData' - 'support:*' Resource: '*' Copy Option 2: Manually add permissions To create your own policy using available permissions: Add the permissions for all integrations. Add permissions that are specific to the integrations you need The following permissions are used by New Relic to retrieve data for specific AWS integrations: Required by all integrations Important If an integration is not listed on this page, these permissions are all you need. All integrations Permissions CloudWatch cloudwatch:GetMetricStatistics cloudwatch:ListMetrics cloudwatch:GetMetricData Config API config:BatchGetResourceConfig config:ListDiscoveredResources Resource Tagging API tag:GetResources ALB permissions Additional ALB permissions: elasticloadbalancing:DescribeLoadBalancers elasticloadbalancing:DescribeTargetGroups elasticloadbalancing:DescribeTags elasticloadbalancing:DescribeLoadBalancerAttributes elasticloadbalancing:DescribeListeners elasticloadbalancing:DescribeRules elasticloadbalancing:DescribeTargetGroupAttributes elasticloadbalancing:DescribeInstanceHealth elasticloadbalancing:DescribeLoadBalancerPolicies elasticloadbalancing:DescribeLoadBalancerPolicyTypes API Gateway permissions Additional API Gateway permissions: apigateway:GET apigateway:HEAD apigateway:OPTIONS Auto Scaling permissions Additional Auto Scaling permissions: autoscaling:DescribeLaunchConfigurations autoscaling:DescribeAutoScalingGroups autoscaling:DescribePolicies autoscaling:DescribeTags autoscaling:DescribeAccountLimits Billing permissions Additional Billing permissions: budgets:ViewBilling budgets:ViewBudget Cloudfront permissions Additional Cloudfront permissions: cloudfront:ListDistributions cloudfront:ListStreamingDistributions cloudfront:ListTagsForResource CloudTrail permissions Additional CloudTrail permissions: cloudtrail:LookupEvents DynamoDB permissions Additional DynamoDB permissions: dynamodb:DescribeLimits dynamodb:ListTables dynamodb:DescribeTable dynamodb:ListGlobalTables dynamodb:DescribeGlobalTable dynamodb:ListTagsOfResource EBS permissions Additional EBS permissions: ec2:DescribeVolumeStatus ec2:DescribeVolumes ec2:DescribeVolumeAttribute EC2 permissions Additional EC2 permissions: ec2:DescribeInstanceStatus ec2:DescribeInstances ECS/ECR permissions Additional ECS/ECR permissions: ecs:ListServices ecs:DescribeServices ecs:DescribeClusters ecs:ListClusters ecs:ListTagsForResource ecs:ListContainerInstances ecs:DescribeContainerInstances EFS permissions Additional EFS permissions: elasticfilesystem:DescribeMountTargets elasticfilesystem:DescribeFileSystems ElastiCache permissions Additional ElastiCache permissions: elasticache:DescribeCacheClusters elasticache:ListTagsForResource ElasticSearch permissions Additional ElasticSearch permissions: es:ListDomainNames es:DescribeElasticsearchDomain es:DescribeElasticsearchDomains es:ListTags Elastic Beanstalk permissions Additional Elastic Beanstalk permissions: elasticbeanstalk:DescribeEnvironments elasticbeanstalk:DescribeInstancesHealth elasticbeanstalk:DescribeConfigurationSettings ELB permissions Additional ELB permissions: elasticloadbalancing:DescribeLoadBalancers EMR permissions Additional EMR permissions: elasticmapreduce:ListInstances elasticmapreduce:ListClusters elasticmapreduce:DescribeCluster elasticmapreduce:ListInstanceGroups elasticmapreduce:ListInstanceFleets Health permissions Additional Health permissions: health:DescribeAffectedEntities health:DescribeEventDetails health:DescribeEvents IAM permissions Additional IAM permissions: iam:ListSAMLProviders iam:ListOpenIDConnectProviders iam:ListServerCertificates iam:GetAccountAuthorizationDetails iam:ListVirtualMFADevices iam:GetAccountSummary IoT permissions Additional IoT permissions: iot:ListTopicRules iot:GetTopicRule iot:ListThings Kinesis Firehose permissions Additional Kinesis Firehose permissions: firehose:DescribeDeliveryStream firehose:ListDeliveryStreams Kinesis Streams permissions Additional Kinesis Streams permissions: kinesis:ListStreams kinesis:DescribeStream kinesis:ListTagsForStream Lambda permissions Additional Lambda permissions: lambda:GetAccountSettings lambda:ListFunctions lambda:ListAliases lambda:ListTags lambda:ListEventSourceMappings RDS, RDS Enhanced Monitoring permissions Additional RDS and RDS Enhanced Monitoring permissions: rds:ListTagsForResource rds:DescribeDBInstances rds:DescribeDBClusters Redshift permissions Additional Redshift permissions: redshift:DescribeClusters redshift:DescribeClusterParameters Route 53 permissions Additional Route 53 permissions: route53:ListHealthChecks route53:GetHostedZone route53:ListHostedZones route53:ListResourceRecordSets route53:ListTagsForResources S3 permissions Additional S3 permissions: s3:GetLifecycleConfiguration s3:GetBucketTagging s3:ListAllMyBuckets s3:GetBucketWebsite s3:GetBucketLogging s3:GetBucketCORS s3:GetBucketVersioning s3:GetBucketAcl s3:GetBucketNotification s3:GetBucketPolicy s3:GetReplicationConfiguration s3:GetMetricsConfiguration s3:GetAccelerateConfiguration s3:GetAnalyticsConfiguration s3:GetBucketLocation s3:GetBucketRequestPayment s3:GetEncryptionConfiguration s3:GetInventoryConfiguration s3:GetIpConfiguration Simple Email Service (SES) permissions Additional SES permissions: ses:ListConfigurationSets ses:GetSendQuota ses:DescribeConfigurationSet ses:ListReceiptFilters ses:ListReceiptRuleSets ses:DescribeReceiptRule ses:DescribeReceiptRuleSet SNS permissions Additional SNS permissions: sns:GetTopicAttributes sns:ListTopics SQS permissions Additional SQS permissions: sqs:ListQueues sqs:GetQueueAttributes sqs:ListQueueTags Trusted Advisor permissions Additional Trusted Advisor permissions: support:* See also the note about the Trusted Advisor integration and recommended policies. VPC permissions Additional VPC permissions: ec2:DescribeInternetGateways ec2:DescribeVpcs ec2:DescribeNatGateways ec2:DescribeVpcEndpoints ec2:DescribeSubnets ec2:DescribeNetworkAcls ec2:DescribeVpcAttribute ec2:DescribeRouteTables ec2:DescribeSecurityGroups ec2:DescribeVpcPeeringConnections ec2:DescribeNetworkInterfaces ec2:DescribeVpnConnections X-Ray monitoring permissions Additional X-ray monitoring permissions: xray:BatchGet* xray:Get*",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 212.06676,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Integrations</em> and managed policies",
        "sections": "<em>Integrations</em> and managed policies",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "In order to use infrastructure <em>integrations</em>, you need to grant New Relic permission to read the relevant data from your account. <em>Amazon</em> Web Services (AWS) uses managed policies to grant these permissions. Recommended policy Important Recommendation: Grant an account-wide ReadOnlyAccess managed"
      },
      "id": "6045079fe7b9d27db95799d9"
    },
    {
      "sections": [
        "Introduction to AWS integrations",
        "Tip",
        "Region availability",
        "Connect AWS and New Relic",
        "Integrations and AWS costs",
        "View your AWS data"
      ],
      "title": "Introduction to AWS integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Get started"
      ],
      "external_id": "26a36d0da0ba98b48ccaff2e574ec4e535e68844",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/get-started/introduction-aws-integrations/",
      "published_at": "2021-05-05T01:03:47Z",
      "updated_at": "2021-03-16T05:37:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Amazon integrations let you monitor your AWS data in several New Relic features. To see the features of specific integrations and the data you can collect, see the AWS integrations list. Tip To use Amazon integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Region availability Most AWS services offer regional endpoints to reduce data latency between cloud resources and applications. New Relic can obtain monitoring data from services and endpoints that are located in all AWS regions except from China regions. Connect AWS and New Relic In order to obtain AWS data, follow the procedure to connect AWS to New Relic, or learn more about the types of integration data that New Relic receives. The New Relic AWS integration also supports seamless deployments of your workloads using AWS Outposts. Integrations and AWS costs New Relic integrations use the Amazon CloudWatch API to obtain metrics from the AWS services you monitor. The number of calls to the CloudWatch API increases as you enable more integrations, add AWS resources to those integrations, or scale those integrations across more regions. This can cause requests to the CloudWatch API to exceed the 1 million free limits granted by AWS and increase your CloudWatch bill. View your AWS data Once you follow the configuration process, data from your Amazon Web Services will report directly to New Relic. AWS data will also be visible in the Infrastructure UI. However, unlike standard New Relic dashboards, pre-configured integrations dashboards can't be edited. To view your AWS data: Go to one.newrelic.com > Infrastructure > AWS. For any of the AWS integrations listed: Select an integration name to view data. OR Select the Explore data icon to view AWS data. You can view and reuse NRQL queries both in the pre-configured dashboards and in the Events explorer dashboards. This allows you to tailor queries to your specific needs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 127.94288,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to AWS <em>integrations</em>",
        "sections": "Introduction to AWS <em>integrations</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "<em>Amazon</em> <em>integrations</em> let you monitor your AWS data in several New Relic features. To see the features of specific <em>integrations</em> and the data you can collect, see the AWS <em>integrations</em> list. Tip To use <em>Amazon</em> <em>integrations</em> and the rest of our observability platform, join the New Relic family! Sign up"
      },
      "id": "603e84ec28ccbc9dffeba789"
    },
    {
      "sections": [
        "Connect AWS GovCloud to New Relic",
        "Requirements",
        "How to obtain GovCloud credentials for New Relic"
      ],
      "title": "Connect AWS GovCloud to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Get started"
      ],
      "external_id": "ab6a129f8c50643b9af1c135863572d1ab595e30",
      "image": "https://docs.newrelic.com/static/2700987e921c2d686abb7518317cc2e1/49217/AWS-add-user.png",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/get-started/connect-aws-govcloud-new-relic/",
      "published_at": "2021-05-05T15:13:38Z",
      "updated_at": "2021-03-16T05:36:20Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The AWS GovCloud (US) regions are designed to address the specific regulatory needs of United States (federal, state, and local agencies), education institutions, and the supporting ecosystem. It is an isolated AWS region designed to host sensitive data and regulated workloads in the cloud, helping customers support their US government compliance requirements. The available set of AWS services is a subset of the AWS ecosystem. New Relic provides you with the confidence to deploy your most critical services on GovCloud, allowing you to monitor and observe your entire ecosystem from New Relic One. Requirements Requirements include: You must have your AWS account connected to New Relic before connecting GovCloud. If you're using our AWS Lambda monitoring: our newrelic-log-ingestion is not deployed in the AWS Serverless Application Repository for AWS GovCloud; it must be installed manually. For instructions, see Enable Lambda monitoring. AWS integrations supported in GovCloud: ALB/NLB API Gateway Autoscaling CloudTrail DirectConnect DynamoDB EBS EC2 Elasticsearch ELB (Classic) EMR IAM Lambda RDS Redshift Route53 S3 SNS SQS Step Functions Connect AWS GovCloud to New Relic To start receiving Amazon data with New Relic AWS integrations, connect your Amazon account to New Relic. Obtain your credentials. Go to one.newrelic.com > Infrastructure > GovCloud. Click on Add AWS GovCloud account. Give your AWS account a name, provide the credentials to connect your account, and click Submit. Select the Amazon Web Services to be monitored with New Relic infrastructure integrations, then click Save. How to obtain GovCloud credentials for New Relic From the IAM console, click Add user. For the User name, type NewRelicInfrastructure-Integrations. For Select AWS access type, select as Programmatic access. AWS IAM console > Add user: add NewRelicInfrastructure-Integrations as a user. Attach the Policy: Search for ReadOnlyAccess, select the checkbox for the policy named ReadOnlyAccess, then click Next: Tags (adding tags is optional). Alternatively, you can create your own managed policy and limit the permissions you grant New Relic according to the AWS services you want to monitor. AWS IAM console > Add user > Set permissions: select ReadOnlyAccess. On the Tags page, click Next: Review. Review the user detail summary and click Create user. AWS IAM console > Add user > Set permissions > Tags > Review: verify that the new user information is accurate. Your user should be successfully created. Download the user security credentials by clicking on the Download .csv button and then click Close.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 125.06822,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": ". For instructions, see Enable Lambda monitoring. AWS <em>integrations</em> supported in GovCloud: ALB&#x2F;NLB API Gateway Autoscaling CloudTrail DirectConnect DynamoDB EBS EC2 Elasticsearch ELB (Classic) EMR IAM Lambda RDS Redshift Route53 S3 SNS SQS Step Functions Connect AWS GovCloud to New Relic To <em>start</em>"
      },
      "id": "603e85bc196a675469a83dcd"
    }
  ],
  "/docs/integrations/amazon-integrations/get-started/integrations-managed-policies": [
    {
      "sections": [
        "Introduction to AWS integrations",
        "Tip",
        "Region availability",
        "Connect AWS and New Relic",
        "Integrations and AWS costs",
        "View your AWS data"
      ],
      "title": "Introduction to AWS integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Get started"
      ],
      "external_id": "26a36d0da0ba98b48ccaff2e574ec4e535e68844",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/get-started/introduction-aws-integrations/",
      "published_at": "2021-05-05T01:03:47Z",
      "updated_at": "2021-03-16T05:37:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Amazon integrations let you monitor your AWS data in several New Relic features. To see the features of specific integrations and the data you can collect, see the AWS integrations list. Tip To use Amazon integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Region availability Most AWS services offer regional endpoints to reduce data latency between cloud resources and applications. New Relic can obtain monitoring data from services and endpoints that are located in all AWS regions except from China regions. Connect AWS and New Relic In order to obtain AWS data, follow the procedure to connect AWS to New Relic, or learn more about the types of integration data that New Relic receives. The New Relic AWS integration also supports seamless deployments of your workloads using AWS Outposts. Integrations and AWS costs New Relic integrations use the Amazon CloudWatch API to obtain metrics from the AWS services you monitor. The number of calls to the CloudWatch API increases as you enable more integrations, add AWS resources to those integrations, or scale those integrations across more regions. This can cause requests to the CloudWatch API to exceed the 1 million free limits granted by AWS and increase your CloudWatch bill. View your AWS data Once you follow the configuration process, data from your Amazon Web Services will report directly to New Relic. AWS data will also be visible in the Infrastructure UI. However, unlike standard New Relic dashboards, pre-configured integrations dashboards can't be edited. To view your AWS data: Go to one.newrelic.com > Infrastructure > AWS. For any of the AWS integrations listed: Select an integration name to view data. OR Select the Explore data icon to view AWS data. You can view and reuse NRQL queries both in the pre-configured dashboards and in the Events explorer dashboards. This allows you to tailor queries to your specific needs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 127.94287,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to AWS <em>integrations</em>",
        "sections": "Introduction to AWS <em>integrations</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "<em>Amazon</em> <em>integrations</em> let you monitor your AWS data in several New Relic features. To see the features of specific <em>integrations</em> and the data you can collect, see the AWS <em>integrations</em> list. Tip To use <em>Amazon</em> <em>integrations</em> and the rest of our observability platform, join the New Relic family! Sign up"
      },
      "id": "603e84ec28ccbc9dffeba789"
    },
    {
      "sections": [
        "Connect AWS to New Relic infrastructure monitoring",
        "Tip",
        "Connect AWS to New Relic",
        "Connect multiple AWS integrations",
        "Connect multiple AWS accounts",
        "Add or edit custom tags",
        "Disconnect your AWS integrations",
        "Regional support"
      ],
      "title": "Connect AWS to New Relic infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Get started"
      ],
      "external_id": "a00c91900961871b2c48d88bca610d5457473f11",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/get-started/connect-aws-new-relic-infrastructure-monitoring/",
      "published_at": "2021-05-05T15:16:17Z",
      "updated_at": "2021-03-13T03:47:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To start receiving Amazon data with New Relic AWS integrations, connect your Amazon account to New Relic. Tip To use Amazon integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Connect AWS to New Relic To connect your Amazon account to infrastructure monitoring in New Relic: Go to one.newrelic.com > Infrastructure > AWS. Click on one of the available service tiles. From the IAM console, click Create role, then click Another AWS account. For Account ID, use 754728514883. Check the Require external ID box. For External ID, enter your New Relic account ID. Do not enable the setting to Require MFA (multi-factor authentication). Attach the Policy: Search for ReadOnlyAccess, select the checkbox for the policy named ReadOnlyAccess, then click Next: Review. Alternatively, you can create your own managed policy and limit the permissions you grant New Relic according to the AWS services you want to monitor. For the Role name, enter NewRelicInfrastructure-Integrations, then click Create role. Select the newly created role from the listed roles. On the Role summary page, select and copy the entire Role ARN (required later in this procedure). Configure a Budgets policy: While viewing the Role summary for your new role, select Add inline policy. Create a Custom policy: Enter a policy name (for example, NewRelicBudget), add the following permission statement, and then select Apply policy. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"budgets:ViewBudget\" ], \"Resource\": \"*\" } ] } Copy Return to the New Relic UI to enter your AWS account name and the ARN for the new role. Select the Amazon Web Services to be monitored with New Relic infrastructure integrations, then Save. Connect multiple AWS integrations To connect multiple AWS integrations to a single New Relic account: If you previously set up an ARN with the more restrictive AmazonEC2ReadOnlyAccess policy, first unlink your existing integration, then create a new one with a broader policy. Follow the instructions to connect your Amazon account to New Relic . Provide the ARN that contains the ReadOnlyAccess policy. Once setup is complete, select the integrations you want to monitor: Go to one.newrelic.com > Infrastructure > AWS. Select the edit icon. Select the checkbox for each integration you want to monitor. Connect multiple AWS accounts By default, the Amazon EC2 AmazonEC2ReadOnlyAccess permission grants New Relic access to all EC2 instances in the individual Amazon account you specify during the setup steps. If you have multiple AWS accounts, follow the steps to connect an AWS account for each AWS account you want to associate with New Relic. Add or edit custom tags New Relic automatically imports any custom tags you have added or edited for your EC2 instances. Custom EC2 tags are labeled ec2Tag_TAG_NAME in the Infrastructure UI. If you do not see EC2 tags in the Add filter menu of the Filter sets sidebar within a few minutes, delete the integration and try again: Go to one.newrelic.com > Infrastructure > AWS. Select the edit icon. Remove individual integrations or the entire account linkage as needed. Disconnect your AWS integrations You can disable one or more integrations anytime and still keep your AWS account connected to New Relic. However, New Relic recommends that you do not disable EC2 or EBS monitoring. These two integrations add important metadata to your EC2 instances and EBS volumes in New Relic. To uninstall your services completely from New Relic infrastructure Integrations, unlink your AWS account. Regional support China regions are not supported.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 125.70712,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Connect multiple AWS <em>integrations</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "To <em>start</em> receiving <em>Amazon</em> data with New Relic AWS <em>integrations</em>, connect your <em>Amazon</em> account to New Relic. Tip To use <em>Amazon</em> <em>integrations</em> and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data"
      },
      "id": "6045079f196a679cc1960f2d"
    },
    {
      "sections": [
        "Connect AWS GovCloud to New Relic",
        "Requirements",
        "How to obtain GovCloud credentials for New Relic"
      ],
      "title": "Connect AWS GovCloud to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Get started"
      ],
      "external_id": "ab6a129f8c50643b9af1c135863572d1ab595e30",
      "image": "https://docs.newrelic.com/static/2700987e921c2d686abb7518317cc2e1/49217/AWS-add-user.png",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/get-started/connect-aws-govcloud-new-relic/",
      "published_at": "2021-05-05T15:13:38Z",
      "updated_at": "2021-03-16T05:36:20Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The AWS GovCloud (US) regions are designed to address the specific regulatory needs of United States (federal, state, and local agencies), education institutions, and the supporting ecosystem. It is an isolated AWS region designed to host sensitive data and regulated workloads in the cloud, helping customers support their US government compliance requirements. The available set of AWS services is a subset of the AWS ecosystem. New Relic provides you with the confidence to deploy your most critical services on GovCloud, allowing you to monitor and observe your entire ecosystem from New Relic One. Requirements Requirements include: You must have your AWS account connected to New Relic before connecting GovCloud. If you're using our AWS Lambda monitoring: our newrelic-log-ingestion is not deployed in the AWS Serverless Application Repository for AWS GovCloud; it must be installed manually. For instructions, see Enable Lambda monitoring. AWS integrations supported in GovCloud: ALB/NLB API Gateway Autoscaling CloudTrail DirectConnect DynamoDB EBS EC2 Elasticsearch ELB (Classic) EMR IAM Lambda RDS Redshift Route53 S3 SNS SQS Step Functions Connect AWS GovCloud to New Relic To start receiving Amazon data with New Relic AWS integrations, connect your Amazon account to New Relic. Obtain your credentials. Go to one.newrelic.com > Infrastructure > GovCloud. Click on Add AWS GovCloud account. Give your AWS account a name, provide the credentials to connect your account, and click Submit. Select the Amazon Web Services to be monitored with New Relic infrastructure integrations, then click Save. How to obtain GovCloud credentials for New Relic From the IAM console, click Add user. For the User name, type NewRelicInfrastructure-Integrations. For Select AWS access type, select as Programmatic access. AWS IAM console > Add user: add NewRelicInfrastructure-Integrations as a user. Attach the Policy: Search for ReadOnlyAccess, select the checkbox for the policy named ReadOnlyAccess, then click Next: Tags (adding tags is optional). Alternatively, you can create your own managed policy and limit the permissions you grant New Relic according to the AWS services you want to monitor. AWS IAM console > Add user > Set permissions: select ReadOnlyAccess. On the Tags page, click Next: Review. Review the user detail summary and click Create user. AWS IAM console > Add user > Set permissions > Tags > Review: verify that the new user information is accurate. Your user should be successfully created. Download the user security credentials by clicking on the Download .csv button and then click Close.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 125.06822,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": ". For instructions, see Enable Lambda monitoring. AWS <em>integrations</em> supported in GovCloud: ALB&#x2F;NLB API Gateway Autoscaling CloudTrail DirectConnect DynamoDB EBS EC2 Elasticsearch ELB (Classic) EMR IAM Lambda RDS Redshift Route53 S3 SNS SQS Step Functions Connect AWS GovCloud to New Relic To <em>start</em>"
      },
      "id": "603e85bc196a675469a83dcd"
    }
  ],
  "/docs/integrations/amazon-integrations/get-started/introduction-aws-integrations": [
    {
      "sections": [
        "Integrations and managed policies",
        "Recommended policy",
        "Important",
        "Optional policy",
        "Option 1: Use our CloudFormation template",
        "CloudFormation template",
        "Option 2: Manually add permissions",
        "Required by all integrations",
        "ALB permissions",
        "API Gateway permissions",
        "Auto Scaling permissions",
        "Billing permissions",
        "Cloudfront permissions",
        "CloudTrail permissions",
        "DynamoDB permissions",
        "EBS permissions",
        "EC2 permissions",
        "ECS/ECR permissions",
        "EFS permissions",
        "ElastiCache permissions",
        "ElasticSearch permissions",
        "Elastic Beanstalk permissions",
        "ELB permissions",
        "EMR permissions",
        "Health permissions",
        "IAM permissions",
        "IoT permissions",
        "Kinesis Firehose permissions",
        "Kinesis Streams permissions",
        "Lambda permissions",
        "RDS, RDS Enhanced Monitoring permissions",
        "Redshift permissions",
        "Route 53 permissions",
        "S3 permissions",
        "Simple Email Service (SES) permissions",
        "SNS permissions",
        "SQS permissions",
        "Trusted Advisor permissions",
        "VPC permissions",
        "X-Ray monitoring permissions"
      ],
      "title": "Integrations and managed policies",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Get started"
      ],
      "external_id": "80e215e7b2ba382de1b7ea758ee1b1f0a1e3c7df",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/get-started/integrations-managed-policies/",
      "published_at": "2021-05-04T18:30:29Z",
      "updated_at": "2021-05-04T18:30:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In order to use infrastructure integrations, you need to grant New Relic permission to read the relevant data from your account. Amazon Web Services (AWS) uses managed policies to grant these permissions. Recommended policy Important Recommendation: Grant an account-wide ReadOnlyAccess managed policy from AWS. AWS automatically updates this policy when new services are added or existing services are modified. New Relic infrastructure integrations have been designed to function with ReadOnlyAccess policies. For instructions, see Connect AWS integrations to infrastructure. Exception: The Trusted Advisor integration is not covered by the ReadOnlyAccess policy. It requires the additional AWSSupportAccess managed policy. This is also the only integration that requires full access permissions (support:*) in order to correctly operate. We notified Amazon about this limitation. Once it's resolved we'll update documentation with more specific permissions required for this integration. Optional policy If you cannot use the ReadOnlyAccess managed policy from AWS, you can create your own customized policy based on the list of permissions. This allows you to specify the optimal permissions required to fetch data from AWS for each integration. While this option is available, it is not recommended because it must be manually updated when you add or modify your integrations. Important New Relic has no way of identifying problems related to custom permissions. If you choose to create a custom policy, it is your responsibility to maintain it and ensure proper data is being collected. There are two ways to set up your customized policy: You can either use our CloudFormation template, or create own yourself by adding the permissions you need. Option 1: Use our CloudFormation template Our CloudFormation template contains all the permissions for all our AWS integrations. A user different than root can be used in the managed policy. CloudFormation template AWSTemplateFormatVersion: 2010-09-09 Outputs: NewRelicRoleArn: Description: NewRelicRole to monitor AWS Lambda Value: !GetAtt - NewRelicIntegrationsTemplate - Arn Parameters: NewRelicAccountNumber: Type: String Description: The Newrelic account number to send data AllowedPattern: '[0-9]+' Resources: NewRelicIntegrationsTemplate: Type: 'AWS::IAM::Role' Properties: RoleName: !Sub NewRelicTemplateTest AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: AWS: !Sub 'arn:aws:iam::754728514883:root' Action: 'sts:AssumeRole' Condition: StringEquals: 'sts:ExternalId': !Ref NewRelicAccountNumber Policies: - PolicyName: NewRelicIntegrations PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'elasticloadbalancing:DescribeLoadBalancers' - 'elasticloadbalancing:DescribeTargetGroups' - 'elasticloadbalancing:DescribeTags' - 'elasticloadbalancing:DescribeLoadBalancerAttributes' - 'elasticloadbalancing:DescribeListeners' - 'elasticloadbalancing:DescribeRules' - 'elasticloadbalancing:DescribeTargetGroupAttributes' - 'elasticloadbalancing:DescribeInstanceHealth' - 'elasticloadbalancing:DescribeLoadBalancerPolicies' - 'elasticloadbalancing:DescribeLoadBalancerPolicyTypes' - 'apigateway:GET' - 'apigateway:HEAD' - 'apigateway:OPTIONS' - 'autoscaling:DescribeLaunchConfigurations' - 'autoscaling:DescribeAutoScalingGroups' - 'autoscaling:DescribePolicies' - 'autoscaling:DescribeTags' - 'autoscaling:DescribeAccountLimits' - 'budgets:ViewBilling' - 'budgets:ViewBudget' - 'cloudfront:ListDistributions' - 'cloudfront:ListStreamingDistributions' - 'cloudfront:ListTagsForResource' - 'cloudtrail:LookupEvents' - 'config:BatchGetResourceConfig' - 'config:ListDiscoveredResources' - 'dynamodb:DescribeLimits' - 'dynamodb:ListTables' - 'dynamodb:DescribeTable' - 'dynamodb:ListGlobalTables' - 'dynamodb:DescribeGlobalTable' - 'dynamodb:ListTagsOfResource' - 'ec2:DescribeVolumeStatus' - 'ec2:DescribeVolumes' - 'ec2:DescribeVolumeAttribute' - 'ec2:DescribeInstanceStatus' - 'ec2:DescribeInstances' - 'ec2:DescribeVpnConnections' - 'ecs:ListServices' - 'ecs:DescribeServices' - 'ecs:DescribeClusters' - 'ecs:ListClusters' - 'ecs:ListTagsForResource' - 'ecs:ListContainerInstances' - 'ecs:DescribeContainerInstances' - 'elasticfilesystem:DescribeMountTargets' - 'elasticfilesystem:DescribeFileSystems' - 'elasticache:DescribeCacheClusters' - 'elasticache:ListTagsForResource' - 'es:ListDomainNames' - 'es:DescribeElasticsearchDomain' - 'es:DescribeElasticsearchDomains' - 'es:ListTags' - 'elasticbeanstalk:DescribeEnvironments' - 'elasticbeanstalk:DescribeInstancesHealth' - 'elasticbeanstalk:DescribeConfigurationSettings' - 'elasticloadbalancing:DescribeLoadBalancers' - 'elasticmapreduce:ListInstances' - 'elasticmapreduce:ListClusters' - 'elasticmapreduce:DescribeCluster' - 'elasticmapreduce:ListInstanceGroups' - 'health:DescribeAffectedEntities' - 'health:DescribeEventDetails' - 'health:DescribeEvents' - 'iam:ListSAMLProviders' - 'iam:ListOpenIDConnectProviders' - 'iam:ListServerCertificates' - 'iam:GetAccountAuthorizationDetails' - 'iam:ListVirtualMFADevices' - 'iam:GetAccountSummary' - 'iot:ListTopicRules' - 'iot:GetTopicRule' - 'iot:ListThings' - 'firehose:DescribeDeliveryStream' - 'firehose:ListDeliveryStreams' - 'kinesis:ListStreams' - 'kinesis:DescribeStream' - 'kinesis:ListTagsForStream' - 'rds:ListTagsForResource' - 'rds:DescribeDBInstances' - 'rds:DescribeDBClusters' - 'redshift:DescribeClusters' - 'redshift:DescribeClusterParameters' - 'route53:ListHealthChecks' - 'route53:GetHostedZone' - 'route53:ListHostedZones' - 'route53:ListResourceRecordSets' - 'route53:ListTagsForResources' - 's3:GetLifecycleConfiguration' - 's3:GetBucketTagging' - 's3:ListAllMyBuckets' - 's3:GetBucketWebsite' - 's3:GetBucketLogging' - 's3:GetBucketCORS' - 's3:GetBucketVersioning' - 's3:GetBucketAcl' - 's3:GetBucketNotification' - 's3:GetBucketPolicy' - 's3:GetReplicationConfiguration' - 's3:GetMetricsConfiguration' - 's3:GetAccelerateConfiguration' - 's3:GetAnalyticsConfiguration' - 's3:GetBucketLocation' - 's3:GetBucketRequestPayment' - 's3:GetEncryptionConfiguration' - 's3:GetInventoryConfiguration' - 's3:GetIpConfiguration' - 'ses:ListConfigurationSets' - 'ses:GetSendQuota' - 'ses:DescribeConfigurationSet' - 'ses:ListReceiptFilters' - 'ses:ListReceiptRuleSets' - 'ses:DescribeReceiptRule' - 'ses:DescribeReceiptRuleSet' - 'sns:GetTopicAttributes' - 'sns:ListTopics' - 'sqs:ListQueues' - 'sqs:ListQueueTags' - 'sqs:GetQueueAttributes' - 'tag:GetResources' - 'ec2:DescribeInternetGateways' - 'ec2:DescribeVpcs' - 'ec2:DescribeNatGateways' - 'ec2:DescribeVpcEndpoints' - 'ec2:DescribeSubnets' - 'ec2:DescribeNetworkAcls' - 'ec2:DescribeVpcAttribute' - 'ec2:DescribeRouteTables' - 'ec2:DescribeSecurityGroups' - 'ec2:DescribeVpcPeeringConnections' - 'ec2:DescribeNetworkInterfaces' - 'lambda:GetAccountSettings' - 'lambda:ListFunctions' - 'lambda:ListAliases' - 'lambda:ListTags' - 'lambda:ListEventSourceMappings' - 'cloudwatch:GetMetricStatistics' - 'cloudwatch:ListMetrics' - 'cloudwatch:GetMetricData' - 'support:*' Resource: '*' Copy Option 2: Manually add permissions To create your own policy using available permissions: Add the permissions for all integrations. Add permissions that are specific to the integrations you need The following permissions are used by New Relic to retrieve data for specific AWS integrations: Required by all integrations Important If an integration is not listed on this page, these permissions are all you need. All integrations Permissions CloudWatch cloudwatch:GetMetricStatistics cloudwatch:ListMetrics cloudwatch:GetMetricData Config API config:BatchGetResourceConfig config:ListDiscoveredResources Resource Tagging API tag:GetResources ALB permissions Additional ALB permissions: elasticloadbalancing:DescribeLoadBalancers elasticloadbalancing:DescribeTargetGroups elasticloadbalancing:DescribeTags elasticloadbalancing:DescribeLoadBalancerAttributes elasticloadbalancing:DescribeListeners elasticloadbalancing:DescribeRules elasticloadbalancing:DescribeTargetGroupAttributes elasticloadbalancing:DescribeInstanceHealth elasticloadbalancing:DescribeLoadBalancerPolicies elasticloadbalancing:DescribeLoadBalancerPolicyTypes API Gateway permissions Additional API Gateway permissions: apigateway:GET apigateway:HEAD apigateway:OPTIONS Auto Scaling permissions Additional Auto Scaling permissions: autoscaling:DescribeLaunchConfigurations autoscaling:DescribeAutoScalingGroups autoscaling:DescribePolicies autoscaling:DescribeTags autoscaling:DescribeAccountLimits Billing permissions Additional Billing permissions: budgets:ViewBilling budgets:ViewBudget Cloudfront permissions Additional Cloudfront permissions: cloudfront:ListDistributions cloudfront:ListStreamingDistributions cloudfront:ListTagsForResource CloudTrail permissions Additional CloudTrail permissions: cloudtrail:LookupEvents DynamoDB permissions Additional DynamoDB permissions: dynamodb:DescribeLimits dynamodb:ListTables dynamodb:DescribeTable dynamodb:ListGlobalTables dynamodb:DescribeGlobalTable dynamodb:ListTagsOfResource EBS permissions Additional EBS permissions: ec2:DescribeVolumeStatus ec2:DescribeVolumes ec2:DescribeVolumeAttribute EC2 permissions Additional EC2 permissions: ec2:DescribeInstanceStatus ec2:DescribeInstances ECS/ECR permissions Additional ECS/ECR permissions: ecs:ListServices ecs:DescribeServices ecs:DescribeClusters ecs:ListClusters ecs:ListTagsForResource ecs:ListContainerInstances ecs:DescribeContainerInstances EFS permissions Additional EFS permissions: elasticfilesystem:DescribeMountTargets elasticfilesystem:DescribeFileSystems ElastiCache permissions Additional ElastiCache permissions: elasticache:DescribeCacheClusters elasticache:ListTagsForResource ElasticSearch permissions Additional ElasticSearch permissions: es:ListDomainNames es:DescribeElasticsearchDomain es:DescribeElasticsearchDomains es:ListTags Elastic Beanstalk permissions Additional Elastic Beanstalk permissions: elasticbeanstalk:DescribeEnvironments elasticbeanstalk:DescribeInstancesHealth elasticbeanstalk:DescribeConfigurationSettings ELB permissions Additional ELB permissions: elasticloadbalancing:DescribeLoadBalancers EMR permissions Additional EMR permissions: elasticmapreduce:ListInstances elasticmapreduce:ListClusters elasticmapreduce:DescribeCluster elasticmapreduce:ListInstanceGroups elasticmapreduce:ListInstanceFleets Health permissions Additional Health permissions: health:DescribeAffectedEntities health:DescribeEventDetails health:DescribeEvents IAM permissions Additional IAM permissions: iam:ListSAMLProviders iam:ListOpenIDConnectProviders iam:ListServerCertificates iam:GetAccountAuthorizationDetails iam:ListVirtualMFADevices iam:GetAccountSummary IoT permissions Additional IoT permissions: iot:ListTopicRules iot:GetTopicRule iot:ListThings Kinesis Firehose permissions Additional Kinesis Firehose permissions: firehose:DescribeDeliveryStream firehose:ListDeliveryStreams Kinesis Streams permissions Additional Kinesis Streams permissions: kinesis:ListStreams kinesis:DescribeStream kinesis:ListTagsForStream Lambda permissions Additional Lambda permissions: lambda:GetAccountSettings lambda:ListFunctions lambda:ListAliases lambda:ListTags lambda:ListEventSourceMappings RDS, RDS Enhanced Monitoring permissions Additional RDS and RDS Enhanced Monitoring permissions: rds:ListTagsForResource rds:DescribeDBInstances rds:DescribeDBClusters Redshift permissions Additional Redshift permissions: redshift:DescribeClusters redshift:DescribeClusterParameters Route 53 permissions Additional Route 53 permissions: route53:ListHealthChecks route53:GetHostedZone route53:ListHostedZones route53:ListResourceRecordSets route53:ListTagsForResources S3 permissions Additional S3 permissions: s3:GetLifecycleConfiguration s3:GetBucketTagging s3:ListAllMyBuckets s3:GetBucketWebsite s3:GetBucketLogging s3:GetBucketCORS s3:GetBucketVersioning s3:GetBucketAcl s3:GetBucketNotification s3:GetBucketPolicy s3:GetReplicationConfiguration s3:GetMetricsConfiguration s3:GetAccelerateConfiguration s3:GetAnalyticsConfiguration s3:GetBucketLocation s3:GetBucketRequestPayment s3:GetEncryptionConfiguration s3:GetInventoryConfiguration s3:GetIpConfiguration Simple Email Service (SES) permissions Additional SES permissions: ses:ListConfigurationSets ses:GetSendQuota ses:DescribeConfigurationSet ses:ListReceiptFilters ses:ListReceiptRuleSets ses:DescribeReceiptRule ses:DescribeReceiptRuleSet SNS permissions Additional SNS permissions: sns:GetTopicAttributes sns:ListTopics SQS permissions Additional SQS permissions: sqs:ListQueues sqs:GetQueueAttributes sqs:ListQueueTags Trusted Advisor permissions Additional Trusted Advisor permissions: support:* See also the note about the Trusted Advisor integration and recommended policies. VPC permissions Additional VPC permissions: ec2:DescribeInternetGateways ec2:DescribeVpcs ec2:DescribeNatGateways ec2:DescribeVpcEndpoints ec2:DescribeSubnets ec2:DescribeNetworkAcls ec2:DescribeVpcAttribute ec2:DescribeRouteTables ec2:DescribeSecurityGroups ec2:DescribeVpcPeeringConnections ec2:DescribeNetworkInterfaces ec2:DescribeVpnConnections X-Ray monitoring permissions Additional X-ray monitoring permissions: xray:BatchGet* xray:Get*",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 212.06665,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Integrations</em> and managed policies",
        "sections": "<em>Integrations</em> and managed policies",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "In order to use infrastructure <em>integrations</em>, you need to grant New Relic permission to read the relevant data from your account. <em>Amazon</em> Web Services (AWS) uses managed policies to grant these permissions. Recommended policy Important Recommendation: Grant an account-wide ReadOnlyAccess managed"
      },
      "id": "6045079fe7b9d27db95799d9"
    },
    {
      "sections": [
        "Connect AWS to New Relic infrastructure monitoring",
        "Tip",
        "Connect AWS to New Relic",
        "Connect multiple AWS integrations",
        "Connect multiple AWS accounts",
        "Add or edit custom tags",
        "Disconnect your AWS integrations",
        "Regional support"
      ],
      "title": "Connect AWS to New Relic infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Get started"
      ],
      "external_id": "a00c91900961871b2c48d88bca610d5457473f11",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/get-started/connect-aws-new-relic-infrastructure-monitoring/",
      "published_at": "2021-05-05T15:16:17Z",
      "updated_at": "2021-03-13T03:47:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To start receiving Amazon data with New Relic AWS integrations, connect your Amazon account to New Relic. Tip To use Amazon integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Connect AWS to New Relic To connect your Amazon account to infrastructure monitoring in New Relic: Go to one.newrelic.com > Infrastructure > AWS. Click on one of the available service tiles. From the IAM console, click Create role, then click Another AWS account. For Account ID, use 754728514883. Check the Require external ID box. For External ID, enter your New Relic account ID. Do not enable the setting to Require MFA (multi-factor authentication). Attach the Policy: Search for ReadOnlyAccess, select the checkbox for the policy named ReadOnlyAccess, then click Next: Review. Alternatively, you can create your own managed policy and limit the permissions you grant New Relic according to the AWS services you want to monitor. For the Role name, enter NewRelicInfrastructure-Integrations, then click Create role. Select the newly created role from the listed roles. On the Role summary page, select and copy the entire Role ARN (required later in this procedure). Configure a Budgets policy: While viewing the Role summary for your new role, select Add inline policy. Create a Custom policy: Enter a policy name (for example, NewRelicBudget), add the following permission statement, and then select Apply policy. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"budgets:ViewBudget\" ], \"Resource\": \"*\" } ] } Copy Return to the New Relic UI to enter your AWS account name and the ARN for the new role. Select the Amazon Web Services to be monitored with New Relic infrastructure integrations, then Save. Connect multiple AWS integrations To connect multiple AWS integrations to a single New Relic account: If you previously set up an ARN with the more restrictive AmazonEC2ReadOnlyAccess policy, first unlink your existing integration, then create a new one with a broader policy. Follow the instructions to connect your Amazon account to New Relic . Provide the ARN that contains the ReadOnlyAccess policy. Once setup is complete, select the integrations you want to monitor: Go to one.newrelic.com > Infrastructure > AWS. Select the edit icon. Select the checkbox for each integration you want to monitor. Connect multiple AWS accounts By default, the Amazon EC2 AmazonEC2ReadOnlyAccess permission grants New Relic access to all EC2 instances in the individual Amazon account you specify during the setup steps. If you have multiple AWS accounts, follow the steps to connect an AWS account for each AWS account you want to associate with New Relic. Add or edit custom tags New Relic automatically imports any custom tags you have added or edited for your EC2 instances. Custom EC2 tags are labeled ec2Tag_TAG_NAME in the Infrastructure UI. If you do not see EC2 tags in the Add filter menu of the Filter sets sidebar within a few minutes, delete the integration and try again: Go to one.newrelic.com > Infrastructure > AWS. Select the edit icon. Remove individual integrations or the entire account linkage as needed. Disconnect your AWS integrations You can disable one or more integrations anytime and still keep your AWS account connected to New Relic. However, New Relic recommends that you do not disable EC2 or EBS monitoring. These two integrations add important metadata to your EC2 instances and EBS volumes in New Relic. To uninstall your services completely from New Relic infrastructure Integrations, unlink your AWS account. Regional support China regions are not supported.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 125.70712,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Connect multiple AWS <em>integrations</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "To <em>start</em> receiving <em>Amazon</em> data with New Relic AWS <em>integrations</em>, connect your <em>Amazon</em> account to New Relic. Tip To use <em>Amazon</em> <em>integrations</em> and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data"
      },
      "id": "6045079f196a679cc1960f2d"
    },
    {
      "sections": [
        "Connect AWS GovCloud to New Relic",
        "Requirements",
        "How to obtain GovCloud credentials for New Relic"
      ],
      "title": "Connect AWS GovCloud to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Get started"
      ],
      "external_id": "ab6a129f8c50643b9af1c135863572d1ab595e30",
      "image": "https://docs.newrelic.com/static/2700987e921c2d686abb7518317cc2e1/49217/AWS-add-user.png",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/get-started/connect-aws-govcloud-new-relic/",
      "published_at": "2021-05-05T15:13:38Z",
      "updated_at": "2021-03-16T05:36:20Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The AWS GovCloud (US) regions are designed to address the specific regulatory needs of United States (federal, state, and local agencies), education institutions, and the supporting ecosystem. It is an isolated AWS region designed to host sensitive data and regulated workloads in the cloud, helping customers support their US government compliance requirements. The available set of AWS services is a subset of the AWS ecosystem. New Relic provides you with the confidence to deploy your most critical services on GovCloud, allowing you to monitor and observe your entire ecosystem from New Relic One. Requirements Requirements include: You must have your AWS account connected to New Relic before connecting GovCloud. If you're using our AWS Lambda monitoring: our newrelic-log-ingestion is not deployed in the AWS Serverless Application Repository for AWS GovCloud; it must be installed manually. For instructions, see Enable Lambda monitoring. AWS integrations supported in GovCloud: ALB/NLB API Gateway Autoscaling CloudTrail DirectConnect DynamoDB EBS EC2 Elasticsearch ELB (Classic) EMR IAM Lambda RDS Redshift Route53 S3 SNS SQS Step Functions Connect AWS GovCloud to New Relic To start receiving Amazon data with New Relic AWS integrations, connect your Amazon account to New Relic. Obtain your credentials. Go to one.newrelic.com > Infrastructure > GovCloud. Click on Add AWS GovCloud account. Give your AWS account a name, provide the credentials to connect your account, and click Submit. Select the Amazon Web Services to be monitored with New Relic infrastructure integrations, then click Save. How to obtain GovCloud credentials for New Relic From the IAM console, click Add user. For the User name, type NewRelicInfrastructure-Integrations. For Select AWS access type, select as Programmatic access. AWS IAM console > Add user: add NewRelicInfrastructure-Integrations as a user. Attach the Policy: Search for ReadOnlyAccess, select the checkbox for the policy named ReadOnlyAccess, then click Next: Tags (adding tags is optional). Alternatively, you can create your own managed policy and limit the permissions you grant New Relic according to the AWS services you want to monitor. AWS IAM console > Add user > Set permissions: select ReadOnlyAccess. On the Tags page, click Next: Review. Review the user detail summary and click Create user. AWS IAM console > Add user > Set permissions > Tags > Review: verify that the new user information is accurate. Your user should be successfully created. Download the user security credentials by clicking on the Download .csv button and then click Close.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 125.06822,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": ". For instructions, see Enable Lambda monitoring. AWS <em>integrations</em> supported in GovCloud: ALB&#x2F;NLB API Gateway Autoscaling CloudTrail DirectConnect DynamoDB EBS EC2 Elasticsearch ELB (Classic) EMR IAM Lambda RDS Redshift Route53 S3 SNS SQS Step Functions Connect AWS GovCloud to New Relic To <em>start</em>"
      },
      "id": "603e85bc196a675469a83dcd"
    }
  ],
  "/docs/integrations/amazon-integrations/get-started/polling-intervals-aws-integrations": [
    {
      "sections": [
        "Integrations and managed policies",
        "Recommended policy",
        "Important",
        "Optional policy",
        "Option 1: Use our CloudFormation template",
        "CloudFormation template",
        "Option 2: Manually add permissions",
        "Required by all integrations",
        "ALB permissions",
        "API Gateway permissions",
        "Auto Scaling permissions",
        "Billing permissions",
        "Cloudfront permissions",
        "CloudTrail permissions",
        "DynamoDB permissions",
        "EBS permissions",
        "EC2 permissions",
        "ECS/ECR permissions",
        "EFS permissions",
        "ElastiCache permissions",
        "ElasticSearch permissions",
        "Elastic Beanstalk permissions",
        "ELB permissions",
        "EMR permissions",
        "Health permissions",
        "IAM permissions",
        "IoT permissions",
        "Kinesis Firehose permissions",
        "Kinesis Streams permissions",
        "Lambda permissions",
        "RDS, RDS Enhanced Monitoring permissions",
        "Redshift permissions",
        "Route 53 permissions",
        "S3 permissions",
        "Simple Email Service (SES) permissions",
        "SNS permissions",
        "SQS permissions",
        "Trusted Advisor permissions",
        "VPC permissions",
        "X-Ray monitoring permissions"
      ],
      "title": "Integrations and managed policies",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Get started"
      ],
      "external_id": "80e215e7b2ba382de1b7ea758ee1b1f0a1e3c7df",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/get-started/integrations-managed-policies/",
      "published_at": "2021-05-04T18:30:29Z",
      "updated_at": "2021-05-04T18:30:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In order to use infrastructure integrations, you need to grant New Relic permission to read the relevant data from your account. Amazon Web Services (AWS) uses managed policies to grant these permissions. Recommended policy Important Recommendation: Grant an account-wide ReadOnlyAccess managed policy from AWS. AWS automatically updates this policy when new services are added or existing services are modified. New Relic infrastructure integrations have been designed to function with ReadOnlyAccess policies. For instructions, see Connect AWS integrations to infrastructure. Exception: The Trusted Advisor integration is not covered by the ReadOnlyAccess policy. It requires the additional AWSSupportAccess managed policy. This is also the only integration that requires full access permissions (support:*) in order to correctly operate. We notified Amazon about this limitation. Once it's resolved we'll update documentation with more specific permissions required for this integration. Optional policy If you cannot use the ReadOnlyAccess managed policy from AWS, you can create your own customized policy based on the list of permissions. This allows you to specify the optimal permissions required to fetch data from AWS for each integration. While this option is available, it is not recommended because it must be manually updated when you add or modify your integrations. Important New Relic has no way of identifying problems related to custom permissions. If you choose to create a custom policy, it is your responsibility to maintain it and ensure proper data is being collected. There are two ways to set up your customized policy: You can either use our CloudFormation template, or create own yourself by adding the permissions you need. Option 1: Use our CloudFormation template Our CloudFormation template contains all the permissions for all our AWS integrations. A user different than root can be used in the managed policy. CloudFormation template AWSTemplateFormatVersion: 2010-09-09 Outputs: NewRelicRoleArn: Description: NewRelicRole to monitor AWS Lambda Value: !GetAtt - NewRelicIntegrationsTemplate - Arn Parameters: NewRelicAccountNumber: Type: String Description: The Newrelic account number to send data AllowedPattern: '[0-9]+' Resources: NewRelicIntegrationsTemplate: Type: 'AWS::IAM::Role' Properties: RoleName: !Sub NewRelicTemplateTest AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: AWS: !Sub 'arn:aws:iam::754728514883:root' Action: 'sts:AssumeRole' Condition: StringEquals: 'sts:ExternalId': !Ref NewRelicAccountNumber Policies: - PolicyName: NewRelicIntegrations PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'elasticloadbalancing:DescribeLoadBalancers' - 'elasticloadbalancing:DescribeTargetGroups' - 'elasticloadbalancing:DescribeTags' - 'elasticloadbalancing:DescribeLoadBalancerAttributes' - 'elasticloadbalancing:DescribeListeners' - 'elasticloadbalancing:DescribeRules' - 'elasticloadbalancing:DescribeTargetGroupAttributes' - 'elasticloadbalancing:DescribeInstanceHealth' - 'elasticloadbalancing:DescribeLoadBalancerPolicies' - 'elasticloadbalancing:DescribeLoadBalancerPolicyTypes' - 'apigateway:GET' - 'apigateway:HEAD' - 'apigateway:OPTIONS' - 'autoscaling:DescribeLaunchConfigurations' - 'autoscaling:DescribeAutoScalingGroups' - 'autoscaling:DescribePolicies' - 'autoscaling:DescribeTags' - 'autoscaling:DescribeAccountLimits' - 'budgets:ViewBilling' - 'budgets:ViewBudget' - 'cloudfront:ListDistributions' - 'cloudfront:ListStreamingDistributions' - 'cloudfront:ListTagsForResource' - 'cloudtrail:LookupEvents' - 'config:BatchGetResourceConfig' - 'config:ListDiscoveredResources' - 'dynamodb:DescribeLimits' - 'dynamodb:ListTables' - 'dynamodb:DescribeTable' - 'dynamodb:ListGlobalTables' - 'dynamodb:DescribeGlobalTable' - 'dynamodb:ListTagsOfResource' - 'ec2:DescribeVolumeStatus' - 'ec2:DescribeVolumes' - 'ec2:DescribeVolumeAttribute' - 'ec2:DescribeInstanceStatus' - 'ec2:DescribeInstances' - 'ec2:DescribeVpnConnections' - 'ecs:ListServices' - 'ecs:DescribeServices' - 'ecs:DescribeClusters' - 'ecs:ListClusters' - 'ecs:ListTagsForResource' - 'ecs:ListContainerInstances' - 'ecs:DescribeContainerInstances' - 'elasticfilesystem:DescribeMountTargets' - 'elasticfilesystem:DescribeFileSystems' - 'elasticache:DescribeCacheClusters' - 'elasticache:ListTagsForResource' - 'es:ListDomainNames' - 'es:DescribeElasticsearchDomain' - 'es:DescribeElasticsearchDomains' - 'es:ListTags' - 'elasticbeanstalk:DescribeEnvironments' - 'elasticbeanstalk:DescribeInstancesHealth' - 'elasticbeanstalk:DescribeConfigurationSettings' - 'elasticloadbalancing:DescribeLoadBalancers' - 'elasticmapreduce:ListInstances' - 'elasticmapreduce:ListClusters' - 'elasticmapreduce:DescribeCluster' - 'elasticmapreduce:ListInstanceGroups' - 'health:DescribeAffectedEntities' - 'health:DescribeEventDetails' - 'health:DescribeEvents' - 'iam:ListSAMLProviders' - 'iam:ListOpenIDConnectProviders' - 'iam:ListServerCertificates' - 'iam:GetAccountAuthorizationDetails' - 'iam:ListVirtualMFADevices' - 'iam:GetAccountSummary' - 'iot:ListTopicRules' - 'iot:GetTopicRule' - 'iot:ListThings' - 'firehose:DescribeDeliveryStream' - 'firehose:ListDeliveryStreams' - 'kinesis:ListStreams' - 'kinesis:DescribeStream' - 'kinesis:ListTagsForStream' - 'rds:ListTagsForResource' - 'rds:DescribeDBInstances' - 'rds:DescribeDBClusters' - 'redshift:DescribeClusters' - 'redshift:DescribeClusterParameters' - 'route53:ListHealthChecks' - 'route53:GetHostedZone' - 'route53:ListHostedZones' - 'route53:ListResourceRecordSets' - 'route53:ListTagsForResources' - 's3:GetLifecycleConfiguration' - 's3:GetBucketTagging' - 's3:ListAllMyBuckets' - 's3:GetBucketWebsite' - 's3:GetBucketLogging' - 's3:GetBucketCORS' - 's3:GetBucketVersioning' - 's3:GetBucketAcl' - 's3:GetBucketNotification' - 's3:GetBucketPolicy' - 's3:GetReplicationConfiguration' - 's3:GetMetricsConfiguration' - 's3:GetAccelerateConfiguration' - 's3:GetAnalyticsConfiguration' - 's3:GetBucketLocation' - 's3:GetBucketRequestPayment' - 's3:GetEncryptionConfiguration' - 's3:GetInventoryConfiguration' - 's3:GetIpConfiguration' - 'ses:ListConfigurationSets' - 'ses:GetSendQuota' - 'ses:DescribeConfigurationSet' - 'ses:ListReceiptFilters' - 'ses:ListReceiptRuleSets' - 'ses:DescribeReceiptRule' - 'ses:DescribeReceiptRuleSet' - 'sns:GetTopicAttributes' - 'sns:ListTopics' - 'sqs:ListQueues' - 'sqs:ListQueueTags' - 'sqs:GetQueueAttributes' - 'tag:GetResources' - 'ec2:DescribeInternetGateways' - 'ec2:DescribeVpcs' - 'ec2:DescribeNatGateways' - 'ec2:DescribeVpcEndpoints' - 'ec2:DescribeSubnets' - 'ec2:DescribeNetworkAcls' - 'ec2:DescribeVpcAttribute' - 'ec2:DescribeRouteTables' - 'ec2:DescribeSecurityGroups' - 'ec2:DescribeVpcPeeringConnections' - 'ec2:DescribeNetworkInterfaces' - 'lambda:GetAccountSettings' - 'lambda:ListFunctions' - 'lambda:ListAliases' - 'lambda:ListTags' - 'lambda:ListEventSourceMappings' - 'cloudwatch:GetMetricStatistics' - 'cloudwatch:ListMetrics' - 'cloudwatch:GetMetricData' - 'support:*' Resource: '*' Copy Option 2: Manually add permissions To create your own policy using available permissions: Add the permissions for all integrations. Add permissions that are specific to the integrations you need The following permissions are used by New Relic to retrieve data for specific AWS integrations: Required by all integrations Important If an integration is not listed on this page, these permissions are all you need. All integrations Permissions CloudWatch cloudwatch:GetMetricStatistics cloudwatch:ListMetrics cloudwatch:GetMetricData Config API config:BatchGetResourceConfig config:ListDiscoveredResources Resource Tagging API tag:GetResources ALB permissions Additional ALB permissions: elasticloadbalancing:DescribeLoadBalancers elasticloadbalancing:DescribeTargetGroups elasticloadbalancing:DescribeTags elasticloadbalancing:DescribeLoadBalancerAttributes elasticloadbalancing:DescribeListeners elasticloadbalancing:DescribeRules elasticloadbalancing:DescribeTargetGroupAttributes elasticloadbalancing:DescribeInstanceHealth elasticloadbalancing:DescribeLoadBalancerPolicies elasticloadbalancing:DescribeLoadBalancerPolicyTypes API Gateway permissions Additional API Gateway permissions: apigateway:GET apigateway:HEAD apigateway:OPTIONS Auto Scaling permissions Additional Auto Scaling permissions: autoscaling:DescribeLaunchConfigurations autoscaling:DescribeAutoScalingGroups autoscaling:DescribePolicies autoscaling:DescribeTags autoscaling:DescribeAccountLimits Billing permissions Additional Billing permissions: budgets:ViewBilling budgets:ViewBudget Cloudfront permissions Additional Cloudfront permissions: cloudfront:ListDistributions cloudfront:ListStreamingDistributions cloudfront:ListTagsForResource CloudTrail permissions Additional CloudTrail permissions: cloudtrail:LookupEvents DynamoDB permissions Additional DynamoDB permissions: dynamodb:DescribeLimits dynamodb:ListTables dynamodb:DescribeTable dynamodb:ListGlobalTables dynamodb:DescribeGlobalTable dynamodb:ListTagsOfResource EBS permissions Additional EBS permissions: ec2:DescribeVolumeStatus ec2:DescribeVolumes ec2:DescribeVolumeAttribute EC2 permissions Additional EC2 permissions: ec2:DescribeInstanceStatus ec2:DescribeInstances ECS/ECR permissions Additional ECS/ECR permissions: ecs:ListServices ecs:DescribeServices ecs:DescribeClusters ecs:ListClusters ecs:ListTagsForResource ecs:ListContainerInstances ecs:DescribeContainerInstances EFS permissions Additional EFS permissions: elasticfilesystem:DescribeMountTargets elasticfilesystem:DescribeFileSystems ElastiCache permissions Additional ElastiCache permissions: elasticache:DescribeCacheClusters elasticache:ListTagsForResource ElasticSearch permissions Additional ElasticSearch permissions: es:ListDomainNames es:DescribeElasticsearchDomain es:DescribeElasticsearchDomains es:ListTags Elastic Beanstalk permissions Additional Elastic Beanstalk permissions: elasticbeanstalk:DescribeEnvironments elasticbeanstalk:DescribeInstancesHealth elasticbeanstalk:DescribeConfigurationSettings ELB permissions Additional ELB permissions: elasticloadbalancing:DescribeLoadBalancers EMR permissions Additional EMR permissions: elasticmapreduce:ListInstances elasticmapreduce:ListClusters elasticmapreduce:DescribeCluster elasticmapreduce:ListInstanceGroups elasticmapreduce:ListInstanceFleets Health permissions Additional Health permissions: health:DescribeAffectedEntities health:DescribeEventDetails health:DescribeEvents IAM permissions Additional IAM permissions: iam:ListSAMLProviders iam:ListOpenIDConnectProviders iam:ListServerCertificates iam:GetAccountAuthorizationDetails iam:ListVirtualMFADevices iam:GetAccountSummary IoT permissions Additional IoT permissions: iot:ListTopicRules iot:GetTopicRule iot:ListThings Kinesis Firehose permissions Additional Kinesis Firehose permissions: firehose:DescribeDeliveryStream firehose:ListDeliveryStreams Kinesis Streams permissions Additional Kinesis Streams permissions: kinesis:ListStreams kinesis:DescribeStream kinesis:ListTagsForStream Lambda permissions Additional Lambda permissions: lambda:GetAccountSettings lambda:ListFunctions lambda:ListAliases lambda:ListTags lambda:ListEventSourceMappings RDS, RDS Enhanced Monitoring permissions Additional RDS and RDS Enhanced Monitoring permissions: rds:ListTagsForResource rds:DescribeDBInstances rds:DescribeDBClusters Redshift permissions Additional Redshift permissions: redshift:DescribeClusters redshift:DescribeClusterParameters Route 53 permissions Additional Route 53 permissions: route53:ListHealthChecks route53:GetHostedZone route53:ListHostedZones route53:ListResourceRecordSets route53:ListTagsForResources S3 permissions Additional S3 permissions: s3:GetLifecycleConfiguration s3:GetBucketTagging s3:ListAllMyBuckets s3:GetBucketWebsite s3:GetBucketLogging s3:GetBucketCORS s3:GetBucketVersioning s3:GetBucketAcl s3:GetBucketNotification s3:GetBucketPolicy s3:GetReplicationConfiguration s3:GetMetricsConfiguration s3:GetAccelerateConfiguration s3:GetAnalyticsConfiguration s3:GetBucketLocation s3:GetBucketRequestPayment s3:GetEncryptionConfiguration s3:GetInventoryConfiguration s3:GetIpConfiguration Simple Email Service (SES) permissions Additional SES permissions: ses:ListConfigurationSets ses:GetSendQuota ses:DescribeConfigurationSet ses:ListReceiptFilters ses:ListReceiptRuleSets ses:DescribeReceiptRule ses:DescribeReceiptRuleSet SNS permissions Additional SNS permissions: sns:GetTopicAttributes sns:ListTopics SQS permissions Additional SQS permissions: sqs:ListQueues sqs:GetQueueAttributes sqs:ListQueueTags Trusted Advisor permissions Additional Trusted Advisor permissions: support:* See also the note about the Trusted Advisor integration and recommended policies. VPC permissions Additional VPC permissions: ec2:DescribeInternetGateways ec2:DescribeVpcs ec2:DescribeNatGateways ec2:DescribeVpcEndpoints ec2:DescribeSubnets ec2:DescribeNetworkAcls ec2:DescribeVpcAttribute ec2:DescribeRouteTables ec2:DescribeSecurityGroups ec2:DescribeVpcPeeringConnections ec2:DescribeNetworkInterfaces ec2:DescribeVpnConnections X-Ray monitoring permissions Additional X-ray monitoring permissions: xray:BatchGet* xray:Get*",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 212.06653,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Integrations</em> and managed policies",
        "sections": "<em>Integrations</em> and managed policies",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "In order to use infrastructure <em>integrations</em>, you need to grant New Relic permission to read the relevant data from your account. <em>Amazon</em> Web Services (AWS) uses managed policies to grant these permissions. Recommended policy Important Recommendation: Grant an account-wide ReadOnlyAccess managed"
      },
      "id": "6045079fe7b9d27db95799d9"
    },
    {
      "sections": [
        "Introduction to AWS integrations",
        "Tip",
        "Region availability",
        "Connect AWS and New Relic",
        "Integrations and AWS costs",
        "View your AWS data"
      ],
      "title": "Introduction to AWS integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Get started"
      ],
      "external_id": "26a36d0da0ba98b48ccaff2e574ec4e535e68844",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/get-started/introduction-aws-integrations/",
      "published_at": "2021-05-05T01:03:47Z",
      "updated_at": "2021-03-16T05:37:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Amazon integrations let you monitor your AWS data in several New Relic features. To see the features of specific integrations and the data you can collect, see the AWS integrations list. Tip To use Amazon integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Region availability Most AWS services offer regional endpoints to reduce data latency between cloud resources and applications. New Relic can obtain monitoring data from services and endpoints that are located in all AWS regions except from China regions. Connect AWS and New Relic In order to obtain AWS data, follow the procedure to connect AWS to New Relic, or learn more about the types of integration data that New Relic receives. The New Relic AWS integration also supports seamless deployments of your workloads using AWS Outposts. Integrations and AWS costs New Relic integrations use the Amazon CloudWatch API to obtain metrics from the AWS services you monitor. The number of calls to the CloudWatch API increases as you enable more integrations, add AWS resources to those integrations, or scale those integrations across more regions. This can cause requests to the CloudWatch API to exceed the 1 million free limits granted by AWS and increase your CloudWatch bill. View your AWS data Once you follow the configuration process, data from your Amazon Web Services will report directly to New Relic. AWS data will also be visible in the Infrastructure UI. However, unlike standard New Relic dashboards, pre-configured integrations dashboards can't be edited. To view your AWS data: Go to one.newrelic.com > Infrastructure > AWS. For any of the AWS integrations listed: Select an integration name to view data. OR Select the Explore data icon to view AWS data. You can view and reuse NRQL queries both in the pre-configured dashboards and in the Events explorer dashboards. This allows you to tailor queries to your specific needs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 127.94287,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to AWS <em>integrations</em>",
        "sections": "Introduction to AWS <em>integrations</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "<em>Amazon</em> <em>integrations</em> let you monitor your AWS data in several New Relic features. To see the features of specific <em>integrations</em> and the data you can collect, see the AWS <em>integrations</em> list. Tip To use <em>Amazon</em> <em>integrations</em> and the rest of our observability platform, join the New Relic family! Sign up"
      },
      "id": "603e84ec28ccbc9dffeba789"
    },
    {
      "sections": [
        "Connect AWS to New Relic infrastructure monitoring",
        "Tip",
        "Connect AWS to New Relic",
        "Connect multiple AWS integrations",
        "Connect multiple AWS accounts",
        "Add or edit custom tags",
        "Disconnect your AWS integrations",
        "Regional support"
      ],
      "title": "Connect AWS to New Relic infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Get started"
      ],
      "external_id": "a00c91900961871b2c48d88bca610d5457473f11",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/get-started/connect-aws-new-relic-infrastructure-monitoring/",
      "published_at": "2021-05-05T15:16:17Z",
      "updated_at": "2021-03-13T03:47:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To start receiving Amazon data with New Relic AWS integrations, connect your Amazon account to New Relic. Tip To use Amazon integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Connect AWS to New Relic To connect your Amazon account to infrastructure monitoring in New Relic: Go to one.newrelic.com > Infrastructure > AWS. Click on one of the available service tiles. From the IAM console, click Create role, then click Another AWS account. For Account ID, use 754728514883. Check the Require external ID box. For External ID, enter your New Relic account ID. Do not enable the setting to Require MFA (multi-factor authentication). Attach the Policy: Search for ReadOnlyAccess, select the checkbox for the policy named ReadOnlyAccess, then click Next: Review. Alternatively, you can create your own managed policy and limit the permissions you grant New Relic according to the AWS services you want to monitor. For the Role name, enter NewRelicInfrastructure-Integrations, then click Create role. Select the newly created role from the listed roles. On the Role summary page, select and copy the entire Role ARN (required later in this procedure). Configure a Budgets policy: While viewing the Role summary for your new role, select Add inline policy. Create a Custom policy: Enter a policy name (for example, NewRelicBudget), add the following permission statement, and then select Apply policy. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"budgets:ViewBudget\" ], \"Resource\": \"*\" } ] } Copy Return to the New Relic UI to enter your AWS account name and the ARN for the new role. Select the Amazon Web Services to be monitored with New Relic infrastructure integrations, then Save. Connect multiple AWS integrations To connect multiple AWS integrations to a single New Relic account: If you previously set up an ARN with the more restrictive AmazonEC2ReadOnlyAccess policy, first unlink your existing integration, then create a new one with a broader policy. Follow the instructions to connect your Amazon account to New Relic . Provide the ARN that contains the ReadOnlyAccess policy. Once setup is complete, select the integrations you want to monitor: Go to one.newrelic.com > Infrastructure > AWS. Select the edit icon. Select the checkbox for each integration you want to monitor. Connect multiple AWS accounts By default, the Amazon EC2 AmazonEC2ReadOnlyAccess permission grants New Relic access to all EC2 instances in the individual Amazon account you specify during the setup steps. If you have multiple AWS accounts, follow the steps to connect an AWS account for each AWS account you want to associate with New Relic. Add or edit custom tags New Relic automatically imports any custom tags you have added or edited for your EC2 instances. Custom EC2 tags are labeled ec2Tag_TAG_NAME in the Infrastructure UI. If you do not see EC2 tags in the Add filter menu of the Filter sets sidebar within a few minutes, delete the integration and try again: Go to one.newrelic.com > Infrastructure > AWS. Select the edit icon. Remove individual integrations or the entire account linkage as needed. Disconnect your AWS integrations You can disable one or more integrations anytime and still keep your AWS account connected to New Relic. However, New Relic recommends that you do not disable EC2 or EBS monitoring. These two integrations add important metadata to your EC2 instances and EBS volumes in New Relic. To uninstall your services completely from New Relic infrastructure Integrations, unlink your AWS account. Regional support China regions are not supported.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 125.707115,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Connect multiple AWS <em>integrations</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "To <em>start</em> receiving <em>Amazon</em> data with New Relic AWS <em>integrations</em>, connect your <em>Amazon</em> account to New Relic. Tip To use <em>Amazon</em> <em>integrations</em> and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data"
      },
      "id": "6045079f196a679cc1960f2d"
    }
  ],
  "/docs/integrations/amazon-integrations/troubleshooting/authentication-issues": [
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Automated setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-05-04T18:01:54Z",
      "updated_at": "2021-05-04T18:01:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Automated setup using CloudFormation We provide a CloudFormation template that automates this process. This needs to be applied to each AWS account and region you want to monitor in New Relic. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL (US Datacenter): https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL (EU Datacenter): https://cloud-collector.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = cloudwatch-metric-streams. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Similarly as with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 We plan to add metadata from most used AWS services. Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. New Relic and AWS teams are working together to offer more granular controls so that filters can be applied based on tags and other attributes. Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with eu then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and its not the case, please verify the following: Make sure theres no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 137.91489,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": " data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to <em>troubleshoot</em> your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS <em>Troubleshooting</em> docs"
      },
      "id": "606a036de7b9d2bfef9445f2"
    },
    {
      "sections": [
        "Integrations and managed policies",
        "Recommended policy",
        "Important",
        "Optional policy",
        "Option 1: Use our CloudFormation template",
        "CloudFormation template",
        "Option 2: Manually add permissions",
        "Required by all integrations",
        "ALB permissions",
        "API Gateway permissions",
        "Auto Scaling permissions",
        "Billing permissions",
        "Cloudfront permissions",
        "CloudTrail permissions",
        "DynamoDB permissions",
        "EBS permissions",
        "EC2 permissions",
        "ECS/ECR permissions",
        "EFS permissions",
        "ElastiCache permissions",
        "ElasticSearch permissions",
        "Elastic Beanstalk permissions",
        "ELB permissions",
        "EMR permissions",
        "Health permissions",
        "IAM permissions",
        "IoT permissions",
        "Kinesis Firehose permissions",
        "Kinesis Streams permissions",
        "Lambda permissions",
        "RDS, RDS Enhanced Monitoring permissions",
        "Redshift permissions",
        "Route 53 permissions",
        "S3 permissions",
        "Simple Email Service (SES) permissions",
        "SNS permissions",
        "SQS permissions",
        "Trusted Advisor permissions",
        "VPC permissions",
        "X-Ray monitoring permissions"
      ],
      "title": "Integrations and managed policies",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Get started"
      ],
      "external_id": "80e215e7b2ba382de1b7ea758ee1b1f0a1e3c7df",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/get-started/integrations-managed-policies/",
      "published_at": "2021-05-04T18:30:29Z",
      "updated_at": "2021-05-04T18:30:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In order to use infrastructure integrations, you need to grant New Relic permission to read the relevant data from your account. Amazon Web Services (AWS) uses managed policies to grant these permissions. Recommended policy Important Recommendation: Grant an account-wide ReadOnlyAccess managed policy from AWS. AWS automatically updates this policy when new services are added or existing services are modified. New Relic infrastructure integrations have been designed to function with ReadOnlyAccess policies. For instructions, see Connect AWS integrations to infrastructure. Exception: The Trusted Advisor integration is not covered by the ReadOnlyAccess policy. It requires the additional AWSSupportAccess managed policy. This is also the only integration that requires full access permissions (support:*) in order to correctly operate. We notified Amazon about this limitation. Once it's resolved we'll update documentation with more specific permissions required for this integration. Optional policy If you cannot use the ReadOnlyAccess managed policy from AWS, you can create your own customized policy based on the list of permissions. This allows you to specify the optimal permissions required to fetch data from AWS for each integration. While this option is available, it is not recommended because it must be manually updated when you add or modify your integrations. Important New Relic has no way of identifying problems related to custom permissions. If you choose to create a custom policy, it is your responsibility to maintain it and ensure proper data is being collected. There are two ways to set up your customized policy: You can either use our CloudFormation template, or create own yourself by adding the permissions you need. Option 1: Use our CloudFormation template Our CloudFormation template contains all the permissions for all our AWS integrations. A user different than root can be used in the managed policy. CloudFormation template AWSTemplateFormatVersion: 2010-09-09 Outputs: NewRelicRoleArn: Description: NewRelicRole to monitor AWS Lambda Value: !GetAtt - NewRelicIntegrationsTemplate - Arn Parameters: NewRelicAccountNumber: Type: String Description: The Newrelic account number to send data AllowedPattern: '[0-9]+' Resources: NewRelicIntegrationsTemplate: Type: 'AWS::IAM::Role' Properties: RoleName: !Sub NewRelicTemplateTest AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: AWS: !Sub 'arn:aws:iam::754728514883:root' Action: 'sts:AssumeRole' Condition: StringEquals: 'sts:ExternalId': !Ref NewRelicAccountNumber Policies: - PolicyName: NewRelicIntegrations PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'elasticloadbalancing:DescribeLoadBalancers' - 'elasticloadbalancing:DescribeTargetGroups' - 'elasticloadbalancing:DescribeTags' - 'elasticloadbalancing:DescribeLoadBalancerAttributes' - 'elasticloadbalancing:DescribeListeners' - 'elasticloadbalancing:DescribeRules' - 'elasticloadbalancing:DescribeTargetGroupAttributes' - 'elasticloadbalancing:DescribeInstanceHealth' - 'elasticloadbalancing:DescribeLoadBalancerPolicies' - 'elasticloadbalancing:DescribeLoadBalancerPolicyTypes' - 'apigateway:GET' - 'apigateway:HEAD' - 'apigateway:OPTIONS' - 'autoscaling:DescribeLaunchConfigurations' - 'autoscaling:DescribeAutoScalingGroups' - 'autoscaling:DescribePolicies' - 'autoscaling:DescribeTags' - 'autoscaling:DescribeAccountLimits' - 'budgets:ViewBilling' - 'budgets:ViewBudget' - 'cloudfront:ListDistributions' - 'cloudfront:ListStreamingDistributions' - 'cloudfront:ListTagsForResource' - 'cloudtrail:LookupEvents' - 'config:BatchGetResourceConfig' - 'config:ListDiscoveredResources' - 'dynamodb:DescribeLimits' - 'dynamodb:ListTables' - 'dynamodb:DescribeTable' - 'dynamodb:ListGlobalTables' - 'dynamodb:DescribeGlobalTable' - 'dynamodb:ListTagsOfResource' - 'ec2:DescribeVolumeStatus' - 'ec2:DescribeVolumes' - 'ec2:DescribeVolumeAttribute' - 'ec2:DescribeInstanceStatus' - 'ec2:DescribeInstances' - 'ec2:DescribeVpnConnections' - 'ecs:ListServices' - 'ecs:DescribeServices' - 'ecs:DescribeClusters' - 'ecs:ListClusters' - 'ecs:ListTagsForResource' - 'ecs:ListContainerInstances' - 'ecs:DescribeContainerInstances' - 'elasticfilesystem:DescribeMountTargets' - 'elasticfilesystem:DescribeFileSystems' - 'elasticache:DescribeCacheClusters' - 'elasticache:ListTagsForResource' - 'es:ListDomainNames' - 'es:DescribeElasticsearchDomain' - 'es:DescribeElasticsearchDomains' - 'es:ListTags' - 'elasticbeanstalk:DescribeEnvironments' - 'elasticbeanstalk:DescribeInstancesHealth' - 'elasticbeanstalk:DescribeConfigurationSettings' - 'elasticloadbalancing:DescribeLoadBalancers' - 'elasticmapreduce:ListInstances' - 'elasticmapreduce:ListClusters' - 'elasticmapreduce:DescribeCluster' - 'elasticmapreduce:ListInstanceGroups' - 'health:DescribeAffectedEntities' - 'health:DescribeEventDetails' - 'health:DescribeEvents' - 'iam:ListSAMLProviders' - 'iam:ListOpenIDConnectProviders' - 'iam:ListServerCertificates' - 'iam:GetAccountAuthorizationDetails' - 'iam:ListVirtualMFADevices' - 'iam:GetAccountSummary' - 'iot:ListTopicRules' - 'iot:GetTopicRule' - 'iot:ListThings' - 'firehose:DescribeDeliveryStream' - 'firehose:ListDeliveryStreams' - 'kinesis:ListStreams' - 'kinesis:DescribeStream' - 'kinesis:ListTagsForStream' - 'rds:ListTagsForResource' - 'rds:DescribeDBInstances' - 'rds:DescribeDBClusters' - 'redshift:DescribeClusters' - 'redshift:DescribeClusterParameters' - 'route53:ListHealthChecks' - 'route53:GetHostedZone' - 'route53:ListHostedZones' - 'route53:ListResourceRecordSets' - 'route53:ListTagsForResources' - 's3:GetLifecycleConfiguration' - 's3:GetBucketTagging' - 's3:ListAllMyBuckets' - 's3:GetBucketWebsite' - 's3:GetBucketLogging' - 's3:GetBucketCORS' - 's3:GetBucketVersioning' - 's3:GetBucketAcl' - 's3:GetBucketNotification' - 's3:GetBucketPolicy' - 's3:GetReplicationConfiguration' - 's3:GetMetricsConfiguration' - 's3:GetAccelerateConfiguration' - 's3:GetAnalyticsConfiguration' - 's3:GetBucketLocation' - 's3:GetBucketRequestPayment' - 's3:GetEncryptionConfiguration' - 's3:GetInventoryConfiguration' - 's3:GetIpConfiguration' - 'ses:ListConfigurationSets' - 'ses:GetSendQuota' - 'ses:DescribeConfigurationSet' - 'ses:ListReceiptFilters' - 'ses:ListReceiptRuleSets' - 'ses:DescribeReceiptRule' - 'ses:DescribeReceiptRuleSet' - 'sns:GetTopicAttributes' - 'sns:ListTopics' - 'sqs:ListQueues' - 'sqs:ListQueueTags' - 'sqs:GetQueueAttributes' - 'tag:GetResources' - 'ec2:DescribeInternetGateways' - 'ec2:DescribeVpcs' - 'ec2:DescribeNatGateways' - 'ec2:DescribeVpcEndpoints' - 'ec2:DescribeSubnets' - 'ec2:DescribeNetworkAcls' - 'ec2:DescribeVpcAttribute' - 'ec2:DescribeRouteTables' - 'ec2:DescribeSecurityGroups' - 'ec2:DescribeVpcPeeringConnections' - 'ec2:DescribeNetworkInterfaces' - 'lambda:GetAccountSettings' - 'lambda:ListFunctions' - 'lambda:ListAliases' - 'lambda:ListTags' - 'lambda:ListEventSourceMappings' - 'cloudwatch:GetMetricStatistics' - 'cloudwatch:ListMetrics' - 'cloudwatch:GetMetricData' - 'support:*' Resource: '*' Copy Option 2: Manually add permissions To create your own policy using available permissions: Add the permissions for all integrations. Add permissions that are specific to the integrations you need The following permissions are used by New Relic to retrieve data for specific AWS integrations: Required by all integrations Important If an integration is not listed on this page, these permissions are all you need. All integrations Permissions CloudWatch cloudwatch:GetMetricStatistics cloudwatch:ListMetrics cloudwatch:GetMetricData Config API config:BatchGetResourceConfig config:ListDiscoveredResources Resource Tagging API tag:GetResources ALB permissions Additional ALB permissions: elasticloadbalancing:DescribeLoadBalancers elasticloadbalancing:DescribeTargetGroups elasticloadbalancing:DescribeTags elasticloadbalancing:DescribeLoadBalancerAttributes elasticloadbalancing:DescribeListeners elasticloadbalancing:DescribeRules elasticloadbalancing:DescribeTargetGroupAttributes elasticloadbalancing:DescribeInstanceHealth elasticloadbalancing:DescribeLoadBalancerPolicies elasticloadbalancing:DescribeLoadBalancerPolicyTypes API Gateway permissions Additional API Gateway permissions: apigateway:GET apigateway:HEAD apigateway:OPTIONS Auto Scaling permissions Additional Auto Scaling permissions: autoscaling:DescribeLaunchConfigurations autoscaling:DescribeAutoScalingGroups autoscaling:DescribePolicies autoscaling:DescribeTags autoscaling:DescribeAccountLimits Billing permissions Additional Billing permissions: budgets:ViewBilling budgets:ViewBudget Cloudfront permissions Additional Cloudfront permissions: cloudfront:ListDistributions cloudfront:ListStreamingDistributions cloudfront:ListTagsForResource CloudTrail permissions Additional CloudTrail permissions: cloudtrail:LookupEvents DynamoDB permissions Additional DynamoDB permissions: dynamodb:DescribeLimits dynamodb:ListTables dynamodb:DescribeTable dynamodb:ListGlobalTables dynamodb:DescribeGlobalTable dynamodb:ListTagsOfResource EBS permissions Additional EBS permissions: ec2:DescribeVolumeStatus ec2:DescribeVolumes ec2:DescribeVolumeAttribute EC2 permissions Additional EC2 permissions: ec2:DescribeInstanceStatus ec2:DescribeInstances ECS/ECR permissions Additional ECS/ECR permissions: ecs:ListServices ecs:DescribeServices ecs:DescribeClusters ecs:ListClusters ecs:ListTagsForResource ecs:ListContainerInstances ecs:DescribeContainerInstances EFS permissions Additional EFS permissions: elasticfilesystem:DescribeMountTargets elasticfilesystem:DescribeFileSystems ElastiCache permissions Additional ElastiCache permissions: elasticache:DescribeCacheClusters elasticache:ListTagsForResource ElasticSearch permissions Additional ElasticSearch permissions: es:ListDomainNames es:DescribeElasticsearchDomain es:DescribeElasticsearchDomains es:ListTags Elastic Beanstalk permissions Additional Elastic Beanstalk permissions: elasticbeanstalk:DescribeEnvironments elasticbeanstalk:DescribeInstancesHealth elasticbeanstalk:DescribeConfigurationSettings ELB permissions Additional ELB permissions: elasticloadbalancing:DescribeLoadBalancers EMR permissions Additional EMR permissions: elasticmapreduce:ListInstances elasticmapreduce:ListClusters elasticmapreduce:DescribeCluster elasticmapreduce:ListInstanceGroups elasticmapreduce:ListInstanceFleets Health permissions Additional Health permissions: health:DescribeAffectedEntities health:DescribeEventDetails health:DescribeEvents IAM permissions Additional IAM permissions: iam:ListSAMLProviders iam:ListOpenIDConnectProviders iam:ListServerCertificates iam:GetAccountAuthorizationDetails iam:ListVirtualMFADevices iam:GetAccountSummary IoT permissions Additional IoT permissions: iot:ListTopicRules iot:GetTopicRule iot:ListThings Kinesis Firehose permissions Additional Kinesis Firehose permissions: firehose:DescribeDeliveryStream firehose:ListDeliveryStreams Kinesis Streams permissions Additional Kinesis Streams permissions: kinesis:ListStreams kinesis:DescribeStream kinesis:ListTagsForStream Lambda permissions Additional Lambda permissions: lambda:GetAccountSettings lambda:ListFunctions lambda:ListAliases lambda:ListTags lambda:ListEventSourceMappings RDS, RDS Enhanced Monitoring permissions Additional RDS and RDS Enhanced Monitoring permissions: rds:ListTagsForResource rds:DescribeDBInstances rds:DescribeDBClusters Redshift permissions Additional Redshift permissions: redshift:DescribeClusters redshift:DescribeClusterParameters Route 53 permissions Additional Route 53 permissions: route53:ListHealthChecks route53:GetHostedZone route53:ListHostedZones route53:ListResourceRecordSets route53:ListTagsForResources S3 permissions Additional S3 permissions: s3:GetLifecycleConfiguration s3:GetBucketTagging s3:ListAllMyBuckets s3:GetBucketWebsite s3:GetBucketLogging s3:GetBucketCORS s3:GetBucketVersioning s3:GetBucketAcl s3:GetBucketNotification s3:GetBucketPolicy s3:GetReplicationConfiguration s3:GetMetricsConfiguration s3:GetAccelerateConfiguration s3:GetAnalyticsConfiguration s3:GetBucketLocation s3:GetBucketRequestPayment s3:GetEncryptionConfiguration s3:GetInventoryConfiguration s3:GetIpConfiguration Simple Email Service (SES) permissions Additional SES permissions: ses:ListConfigurationSets ses:GetSendQuota ses:DescribeConfigurationSet ses:ListReceiptFilters ses:ListReceiptRuleSets ses:DescribeReceiptRule ses:DescribeReceiptRuleSet SNS permissions Additional SNS permissions: sns:GetTopicAttributes sns:ListTopics SQS permissions Additional SQS permissions: sqs:ListQueues sqs:GetQueueAttributes sqs:ListQueueTags Trusted Advisor permissions Additional Trusted Advisor permissions: support:* See also the note about the Trusted Advisor integration and recommended policies. VPC permissions Additional VPC permissions: ec2:DescribeInternetGateways ec2:DescribeVpcs ec2:DescribeNatGateways ec2:DescribeVpcEndpoints ec2:DescribeSubnets ec2:DescribeNetworkAcls ec2:DescribeVpcAttribute ec2:DescribeRouteTables ec2:DescribeSecurityGroups ec2:DescribeVpcPeeringConnections ec2:DescribeNetworkInterfaces ec2:DescribeVpnConnections X-Ray monitoring permissions Additional X-ray monitoring permissions: xray:BatchGet* xray:Get*",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 135.16376,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Integrations</em> and managed policies",
        "sections": "<em>Integrations</em> and managed policies",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "In order to use infrastructure <em>integrations</em>, you need to grant New Relic permission to read the relevant data from your account. <em>Amazon</em> Web Services (AWS) uses managed policies to grant these permissions. Recommended policy Important Recommendation: Grant an account-wide ReadOnlyAccess managed"
      },
      "id": "6045079fe7b9d27db95799d9"
    },
    {
      "sections": [
        "AWS service specific API rate limiting",
        "Problem",
        "Solution",
        "Verify your Infrastructure account's ARN",
        "Change the polling frequency",
        "Filter your data",
        "Review API usage",
        "Cause"
      ],
      "title": "AWS service specific API rate limiting",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Troubleshooting"
      ],
      "external_id": "785db1a9f5d5d9b89c2d304d1260ce5a8f30a680",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/troubleshooting/aws-service-specific-api-rate-limiting/",
      "published_at": "2021-05-05T15:10:41Z",
      "updated_at": "2021-03-13T03:22:51Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem After enabling Amazon integrations with New Relic Infrastructure, you encounter a rate limit for service-specific APIs. You might see this message in your AWS monitoring software, often with a 503 error: AWS::EC2::Errors::RequestLimitExceeded Request limit exceeded. Solution Verify your Infrastructure account's ARN Ensure that you are not collecting inventory information for the wrong ARN account. Verify that the ARN associated with your Infrastructure account is correct. Change the polling frequency The polling frequency determines how often New Relic gathers data from your cloud provider. By default, the polling frequency is set to the maximum frequency that is available for each service. If you reach your API rate limit, you may want to decrease the polling frequency. Filter your data You can set filters for each integration in order to specify which information you want captured. If you reach your API rate limit, you may want to filter your data. Review API usage To review the API usage for New Relic Infrastructure integrations with Amazon AWS: Go to one.newrelic.com > Infrastructure > AWS > Account status dashboard. Review the New Relic Insights dashboard, which appears automatically. The Insights dashboard includes a chart with your account's Amazon AWS API call count for the last month as well as the CloudWatch API calls (per AWS resource) for the last day. This information is the API usage for New Relic only. It does not include other AWS API or CloudWatch usage that may occur. For assistance determining which services may cause an increase in billing, get support at support.newrelic.com, or contact your New Relic account representative. Cause Infrastructure Amazon integrations leverage the AWS monitoring APIs to gather inventory data. AWS imposes hard rate limits on many of the AWS service-specific APIs consumed by New Relic Infrastructure integrations. Adding New Relic Amazon integrations will increase usage of the service-specific APIs and could impact how quickly you reach your rate limit. This may be caused by either of the following: Enabling Amazon integrations on several plugins for the same service Adding the incorrect Role ARN to your AWS integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 110.62893,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "Problem After enabling <em>Amazon</em> <em>integrations</em> with New Relic Infrastructure, you encounter a rate limit for service-specific APIs. You might see this message in your AWS monitoring software, often with a 503 error: AWS::EC2::Errors::RequestLimitExceeded Request limit exceeded. Solution Verify your"
      },
      "id": "604507c428ccbc013a2c60c4"
    }
  ],
  "/docs/integrations/amazon-integrations/troubleshooting/aws-service-specific-api-rate-limiting": [
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Automated setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-05-04T18:01:54Z",
      "updated_at": "2021-05-04T18:01:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Automated setup using CloudFormation We provide a CloudFormation template that automates this process. This needs to be applied to each AWS account and region you want to monitor in New Relic. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL (US Datacenter): https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL (EU Datacenter): https://cloud-collector.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = cloudwatch-metric-streams. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Similarly as with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 We plan to add metadata from most used AWS services. Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. New Relic and AWS teams are working together to offer more granular controls so that filters can be applied based on tags and other attributes. Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with eu then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and its not the case, please verify the following: Make sure theres no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 137.91483,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": " data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to <em>troubleshoot</em> your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS <em>Troubleshooting</em> docs"
      },
      "id": "606a036de7b9d2bfef9445f2"
    },
    {
      "sections": [
        "Integrations and managed policies",
        "Recommended policy",
        "Important",
        "Optional policy",
        "Option 1: Use our CloudFormation template",
        "CloudFormation template",
        "Option 2: Manually add permissions",
        "Required by all integrations",
        "ALB permissions",
        "API Gateway permissions",
        "Auto Scaling permissions",
        "Billing permissions",
        "Cloudfront permissions",
        "CloudTrail permissions",
        "DynamoDB permissions",
        "EBS permissions",
        "EC2 permissions",
        "ECS/ECR permissions",
        "EFS permissions",
        "ElastiCache permissions",
        "ElasticSearch permissions",
        "Elastic Beanstalk permissions",
        "ELB permissions",
        "EMR permissions",
        "Health permissions",
        "IAM permissions",
        "IoT permissions",
        "Kinesis Firehose permissions",
        "Kinesis Streams permissions",
        "Lambda permissions",
        "RDS, RDS Enhanced Monitoring permissions",
        "Redshift permissions",
        "Route 53 permissions",
        "S3 permissions",
        "Simple Email Service (SES) permissions",
        "SNS permissions",
        "SQS permissions",
        "Trusted Advisor permissions",
        "VPC permissions",
        "X-Ray monitoring permissions"
      ],
      "title": "Integrations and managed policies",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Get started"
      ],
      "external_id": "80e215e7b2ba382de1b7ea758ee1b1f0a1e3c7df",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/get-started/integrations-managed-policies/",
      "published_at": "2021-05-04T18:30:29Z",
      "updated_at": "2021-05-04T18:30:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In order to use infrastructure integrations, you need to grant New Relic permission to read the relevant data from your account. Amazon Web Services (AWS) uses managed policies to grant these permissions. Recommended policy Important Recommendation: Grant an account-wide ReadOnlyAccess managed policy from AWS. AWS automatically updates this policy when new services are added or existing services are modified. New Relic infrastructure integrations have been designed to function with ReadOnlyAccess policies. For instructions, see Connect AWS integrations to infrastructure. Exception: The Trusted Advisor integration is not covered by the ReadOnlyAccess policy. It requires the additional AWSSupportAccess managed policy. This is also the only integration that requires full access permissions (support:*) in order to correctly operate. We notified Amazon about this limitation. Once it's resolved we'll update documentation with more specific permissions required for this integration. Optional policy If you cannot use the ReadOnlyAccess managed policy from AWS, you can create your own customized policy based on the list of permissions. This allows you to specify the optimal permissions required to fetch data from AWS for each integration. While this option is available, it is not recommended because it must be manually updated when you add or modify your integrations. Important New Relic has no way of identifying problems related to custom permissions. If you choose to create a custom policy, it is your responsibility to maintain it and ensure proper data is being collected. There are two ways to set up your customized policy: You can either use our CloudFormation template, or create own yourself by adding the permissions you need. Option 1: Use our CloudFormation template Our CloudFormation template contains all the permissions for all our AWS integrations. A user different than root can be used in the managed policy. CloudFormation template AWSTemplateFormatVersion: 2010-09-09 Outputs: NewRelicRoleArn: Description: NewRelicRole to monitor AWS Lambda Value: !GetAtt - NewRelicIntegrationsTemplate - Arn Parameters: NewRelicAccountNumber: Type: String Description: The Newrelic account number to send data AllowedPattern: '[0-9]+' Resources: NewRelicIntegrationsTemplate: Type: 'AWS::IAM::Role' Properties: RoleName: !Sub NewRelicTemplateTest AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: AWS: !Sub 'arn:aws:iam::754728514883:root' Action: 'sts:AssumeRole' Condition: StringEquals: 'sts:ExternalId': !Ref NewRelicAccountNumber Policies: - PolicyName: NewRelicIntegrations PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'elasticloadbalancing:DescribeLoadBalancers' - 'elasticloadbalancing:DescribeTargetGroups' - 'elasticloadbalancing:DescribeTags' - 'elasticloadbalancing:DescribeLoadBalancerAttributes' - 'elasticloadbalancing:DescribeListeners' - 'elasticloadbalancing:DescribeRules' - 'elasticloadbalancing:DescribeTargetGroupAttributes' - 'elasticloadbalancing:DescribeInstanceHealth' - 'elasticloadbalancing:DescribeLoadBalancerPolicies' - 'elasticloadbalancing:DescribeLoadBalancerPolicyTypes' - 'apigateway:GET' - 'apigateway:HEAD' - 'apigateway:OPTIONS' - 'autoscaling:DescribeLaunchConfigurations' - 'autoscaling:DescribeAutoScalingGroups' - 'autoscaling:DescribePolicies' - 'autoscaling:DescribeTags' - 'autoscaling:DescribeAccountLimits' - 'budgets:ViewBilling' - 'budgets:ViewBudget' - 'cloudfront:ListDistributions' - 'cloudfront:ListStreamingDistributions' - 'cloudfront:ListTagsForResource' - 'cloudtrail:LookupEvents' - 'config:BatchGetResourceConfig' - 'config:ListDiscoveredResources' - 'dynamodb:DescribeLimits' - 'dynamodb:ListTables' - 'dynamodb:DescribeTable' - 'dynamodb:ListGlobalTables' - 'dynamodb:DescribeGlobalTable' - 'dynamodb:ListTagsOfResource' - 'ec2:DescribeVolumeStatus' - 'ec2:DescribeVolumes' - 'ec2:DescribeVolumeAttribute' - 'ec2:DescribeInstanceStatus' - 'ec2:DescribeInstances' - 'ec2:DescribeVpnConnections' - 'ecs:ListServices' - 'ecs:DescribeServices' - 'ecs:DescribeClusters' - 'ecs:ListClusters' - 'ecs:ListTagsForResource' - 'ecs:ListContainerInstances' - 'ecs:DescribeContainerInstances' - 'elasticfilesystem:DescribeMountTargets' - 'elasticfilesystem:DescribeFileSystems' - 'elasticache:DescribeCacheClusters' - 'elasticache:ListTagsForResource' - 'es:ListDomainNames' - 'es:DescribeElasticsearchDomain' - 'es:DescribeElasticsearchDomains' - 'es:ListTags' - 'elasticbeanstalk:DescribeEnvironments' - 'elasticbeanstalk:DescribeInstancesHealth' - 'elasticbeanstalk:DescribeConfigurationSettings' - 'elasticloadbalancing:DescribeLoadBalancers' - 'elasticmapreduce:ListInstances' - 'elasticmapreduce:ListClusters' - 'elasticmapreduce:DescribeCluster' - 'elasticmapreduce:ListInstanceGroups' - 'health:DescribeAffectedEntities' - 'health:DescribeEventDetails' - 'health:DescribeEvents' - 'iam:ListSAMLProviders' - 'iam:ListOpenIDConnectProviders' - 'iam:ListServerCertificates' - 'iam:GetAccountAuthorizationDetails' - 'iam:ListVirtualMFADevices' - 'iam:GetAccountSummary' - 'iot:ListTopicRules' - 'iot:GetTopicRule' - 'iot:ListThings' - 'firehose:DescribeDeliveryStream' - 'firehose:ListDeliveryStreams' - 'kinesis:ListStreams' - 'kinesis:DescribeStream' - 'kinesis:ListTagsForStream' - 'rds:ListTagsForResource' - 'rds:DescribeDBInstances' - 'rds:DescribeDBClusters' - 'redshift:DescribeClusters' - 'redshift:DescribeClusterParameters' - 'route53:ListHealthChecks' - 'route53:GetHostedZone' - 'route53:ListHostedZones' - 'route53:ListResourceRecordSets' - 'route53:ListTagsForResources' - 's3:GetLifecycleConfiguration' - 's3:GetBucketTagging' - 's3:ListAllMyBuckets' - 's3:GetBucketWebsite' - 's3:GetBucketLogging' - 's3:GetBucketCORS' - 's3:GetBucketVersioning' - 's3:GetBucketAcl' - 's3:GetBucketNotification' - 's3:GetBucketPolicy' - 's3:GetReplicationConfiguration' - 's3:GetMetricsConfiguration' - 's3:GetAccelerateConfiguration' - 's3:GetAnalyticsConfiguration' - 's3:GetBucketLocation' - 's3:GetBucketRequestPayment' - 's3:GetEncryptionConfiguration' - 's3:GetInventoryConfiguration' - 's3:GetIpConfiguration' - 'ses:ListConfigurationSets' - 'ses:GetSendQuota' - 'ses:DescribeConfigurationSet' - 'ses:ListReceiptFilters' - 'ses:ListReceiptRuleSets' - 'ses:DescribeReceiptRule' - 'ses:DescribeReceiptRuleSet' - 'sns:GetTopicAttributes' - 'sns:ListTopics' - 'sqs:ListQueues' - 'sqs:ListQueueTags' - 'sqs:GetQueueAttributes' - 'tag:GetResources' - 'ec2:DescribeInternetGateways' - 'ec2:DescribeVpcs' - 'ec2:DescribeNatGateways' - 'ec2:DescribeVpcEndpoints' - 'ec2:DescribeSubnets' - 'ec2:DescribeNetworkAcls' - 'ec2:DescribeVpcAttribute' - 'ec2:DescribeRouteTables' - 'ec2:DescribeSecurityGroups' - 'ec2:DescribeVpcPeeringConnections' - 'ec2:DescribeNetworkInterfaces' - 'lambda:GetAccountSettings' - 'lambda:ListFunctions' - 'lambda:ListAliases' - 'lambda:ListTags' - 'lambda:ListEventSourceMappings' - 'cloudwatch:GetMetricStatistics' - 'cloudwatch:ListMetrics' - 'cloudwatch:GetMetricData' - 'support:*' Resource: '*' Copy Option 2: Manually add permissions To create your own policy using available permissions: Add the permissions for all integrations. Add permissions that are specific to the integrations you need The following permissions are used by New Relic to retrieve data for specific AWS integrations: Required by all integrations Important If an integration is not listed on this page, these permissions are all you need. All integrations Permissions CloudWatch cloudwatch:GetMetricStatistics cloudwatch:ListMetrics cloudwatch:GetMetricData Config API config:BatchGetResourceConfig config:ListDiscoveredResources Resource Tagging API tag:GetResources ALB permissions Additional ALB permissions: elasticloadbalancing:DescribeLoadBalancers elasticloadbalancing:DescribeTargetGroups elasticloadbalancing:DescribeTags elasticloadbalancing:DescribeLoadBalancerAttributes elasticloadbalancing:DescribeListeners elasticloadbalancing:DescribeRules elasticloadbalancing:DescribeTargetGroupAttributes elasticloadbalancing:DescribeInstanceHealth elasticloadbalancing:DescribeLoadBalancerPolicies elasticloadbalancing:DescribeLoadBalancerPolicyTypes API Gateway permissions Additional API Gateway permissions: apigateway:GET apigateway:HEAD apigateway:OPTIONS Auto Scaling permissions Additional Auto Scaling permissions: autoscaling:DescribeLaunchConfigurations autoscaling:DescribeAutoScalingGroups autoscaling:DescribePolicies autoscaling:DescribeTags autoscaling:DescribeAccountLimits Billing permissions Additional Billing permissions: budgets:ViewBilling budgets:ViewBudget Cloudfront permissions Additional Cloudfront permissions: cloudfront:ListDistributions cloudfront:ListStreamingDistributions cloudfront:ListTagsForResource CloudTrail permissions Additional CloudTrail permissions: cloudtrail:LookupEvents DynamoDB permissions Additional DynamoDB permissions: dynamodb:DescribeLimits dynamodb:ListTables dynamodb:DescribeTable dynamodb:ListGlobalTables dynamodb:DescribeGlobalTable dynamodb:ListTagsOfResource EBS permissions Additional EBS permissions: ec2:DescribeVolumeStatus ec2:DescribeVolumes ec2:DescribeVolumeAttribute EC2 permissions Additional EC2 permissions: ec2:DescribeInstanceStatus ec2:DescribeInstances ECS/ECR permissions Additional ECS/ECR permissions: ecs:ListServices ecs:DescribeServices ecs:DescribeClusters ecs:ListClusters ecs:ListTagsForResource ecs:ListContainerInstances ecs:DescribeContainerInstances EFS permissions Additional EFS permissions: elasticfilesystem:DescribeMountTargets elasticfilesystem:DescribeFileSystems ElastiCache permissions Additional ElastiCache permissions: elasticache:DescribeCacheClusters elasticache:ListTagsForResource ElasticSearch permissions Additional ElasticSearch permissions: es:ListDomainNames es:DescribeElasticsearchDomain es:DescribeElasticsearchDomains es:ListTags Elastic Beanstalk permissions Additional Elastic Beanstalk permissions: elasticbeanstalk:DescribeEnvironments elasticbeanstalk:DescribeInstancesHealth elasticbeanstalk:DescribeConfigurationSettings ELB permissions Additional ELB permissions: elasticloadbalancing:DescribeLoadBalancers EMR permissions Additional EMR permissions: elasticmapreduce:ListInstances elasticmapreduce:ListClusters elasticmapreduce:DescribeCluster elasticmapreduce:ListInstanceGroups elasticmapreduce:ListInstanceFleets Health permissions Additional Health permissions: health:DescribeAffectedEntities health:DescribeEventDetails health:DescribeEvents IAM permissions Additional IAM permissions: iam:ListSAMLProviders iam:ListOpenIDConnectProviders iam:ListServerCertificates iam:GetAccountAuthorizationDetails iam:ListVirtualMFADevices iam:GetAccountSummary IoT permissions Additional IoT permissions: iot:ListTopicRules iot:GetTopicRule iot:ListThings Kinesis Firehose permissions Additional Kinesis Firehose permissions: firehose:DescribeDeliveryStream firehose:ListDeliveryStreams Kinesis Streams permissions Additional Kinesis Streams permissions: kinesis:ListStreams kinesis:DescribeStream kinesis:ListTagsForStream Lambda permissions Additional Lambda permissions: lambda:GetAccountSettings lambda:ListFunctions lambda:ListAliases lambda:ListTags lambda:ListEventSourceMappings RDS, RDS Enhanced Monitoring permissions Additional RDS and RDS Enhanced Monitoring permissions: rds:ListTagsForResource rds:DescribeDBInstances rds:DescribeDBClusters Redshift permissions Additional Redshift permissions: redshift:DescribeClusters redshift:DescribeClusterParameters Route 53 permissions Additional Route 53 permissions: route53:ListHealthChecks route53:GetHostedZone route53:ListHostedZones route53:ListResourceRecordSets route53:ListTagsForResources S3 permissions Additional S3 permissions: s3:GetLifecycleConfiguration s3:GetBucketTagging s3:ListAllMyBuckets s3:GetBucketWebsite s3:GetBucketLogging s3:GetBucketCORS s3:GetBucketVersioning s3:GetBucketAcl s3:GetBucketNotification s3:GetBucketPolicy s3:GetReplicationConfiguration s3:GetMetricsConfiguration s3:GetAccelerateConfiguration s3:GetAnalyticsConfiguration s3:GetBucketLocation s3:GetBucketRequestPayment s3:GetEncryptionConfiguration s3:GetInventoryConfiguration s3:GetIpConfiguration Simple Email Service (SES) permissions Additional SES permissions: ses:ListConfigurationSets ses:GetSendQuota ses:DescribeConfigurationSet ses:ListReceiptFilters ses:ListReceiptRuleSets ses:DescribeReceiptRule ses:DescribeReceiptRuleSet SNS permissions Additional SNS permissions: sns:GetTopicAttributes sns:ListTopics SQS permissions Additional SQS permissions: sqs:ListQueues sqs:GetQueueAttributes sqs:ListQueueTags Trusted Advisor permissions Additional Trusted Advisor permissions: support:* See also the note about the Trusted Advisor integration and recommended policies. VPC permissions Additional VPC permissions: ec2:DescribeInternetGateways ec2:DescribeVpcs ec2:DescribeNatGateways ec2:DescribeVpcEndpoints ec2:DescribeSubnets ec2:DescribeNetworkAcls ec2:DescribeVpcAttribute ec2:DescribeRouteTables ec2:DescribeSecurityGroups ec2:DescribeVpcPeeringConnections ec2:DescribeNetworkInterfaces ec2:DescribeVpnConnections X-Ray monitoring permissions Additional X-ray monitoring permissions: xray:BatchGet* xray:Get*",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 135.1637,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Integrations</em> and managed policies",
        "sections": "<em>Integrations</em> and managed policies",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "In order to use infrastructure <em>integrations</em>, you need to grant New Relic permission to read the relevant data from your account. <em>Amazon</em> Web Services (AWS) uses managed policies to grant these permissions. Recommended policy Important Recommendation: Grant an account-wide ReadOnlyAccess managed"
      },
      "id": "6045079fe7b9d27db95799d9"
    },
    {
      "sections": [
        "CloudWatch billing increase",
        "Problem",
        "Solution",
        "Verify your Infrastructure account's ARN",
        "Change the polling frequency",
        "Filter your data",
        "Review API usage",
        "Cause"
      ],
      "title": "CloudWatch billing increase",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Troubleshooting"
      ],
      "external_id": "db3cd732ea370f1d579f2f79a03a342efe77eaa0",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/troubleshooting/cloudwatch-billing-increase/",
      "published_at": "2021-05-05T15:17:48Z",
      "updated_at": "2021-03-13T01:09:54Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem After setting up New Relic Infrastructure Amazon integrations, your usage of the CloudWatch API has increased and subsequently impacted your CloudWatch usage bill. Solution Verify your Infrastructure account's ARN Ensure that you are not collecting inventory information for the wrong ARN account. Verify that the ARN associated with your Infrastructure account is correct. Change the polling frequency The polling frequency determines how often New Relic gathers data from your cloud provider. By default, the polling frequency is set to the maximum frequency that is available for each service. If you need to manage your Amazon CloudWatch bill, you may want to decrease the polling frequency. Filter your data You can set filters for each integration in order to specify which information you want captured. If you need to manage your Amazon CloudWatch bill, you may want to filter your data. Review API usage To review the API usage for New Relic Infrastructure integrations with Amazon AWS: Go to one.newrelic.com > Infrastructure > AWS > Account status dashboard. Review the New Relic Insights dashboard, which appears automatically. The Insights dashboard includes a chart with your account's Amazon AWS API call count for the last month as well as the CloudWatch API calls (per AWS resource) for the last day. This information is the API usage for New Relic only. It does not include other AWS API or CloudWatch usage that may occur. For assistance determining which services may cause an increase in billing, contact your New Relic account representative or get support at support.newrelic.com. Cause New Relic Infrastructure Amazon integrations leverage CloudWatch to gather metrics. AWS charges joint customers for requests that exceed the first one million per month. CloudWatch billing issues may be caused by either of the following: Enabling Amazon integrations on several plugins for the same service Adding the incorrect Role ARN to your AWS integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 109.93478,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "Problem After setting up New Relic Infrastructure <em>Amazon</em> <em>integrations</em>, your usage of the CloudWatch API has increased and subsequently impacted your CloudWatch usage bill. Solution Verify your Infrastructure account&#x27;s ARN Ensure that you are not collecting inventory information for the wrong ARN"
      },
      "id": "604507f9196a67eba2960f36"
    }
  ],
  "/docs/integrations/amazon-integrations/troubleshooting/cannot-create-alert-condition-infrastructure-integration": [
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Automated setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-05-04T18:01:54Z",
      "updated_at": "2021-05-04T18:01:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Automated setup using CloudFormation We provide a CloudFormation template that automates this process. This needs to be applied to each AWS account and region you want to monitor in New Relic. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL (US Datacenter): https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL (EU Datacenter): https://cloud-collector.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = cloudwatch-metric-streams. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Similarly as with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 We plan to add metadata from most used AWS services. Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. New Relic and AWS teams are working together to offer more granular controls so that filters can be applied based on tags and other attributes. Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with eu then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and its not the case, please verify the following: Make sure theres no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 137.91473,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": " data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to <em>troubleshoot</em> your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS <em>Troubleshooting</em> docs"
      },
      "id": "606a036de7b9d2bfef9445f2"
    },
    {
      "sections": [
        "Integrations and managed policies",
        "Recommended policy",
        "Important",
        "Optional policy",
        "Option 1: Use our CloudFormation template",
        "CloudFormation template",
        "Option 2: Manually add permissions",
        "Required by all integrations",
        "ALB permissions",
        "API Gateway permissions",
        "Auto Scaling permissions",
        "Billing permissions",
        "Cloudfront permissions",
        "CloudTrail permissions",
        "DynamoDB permissions",
        "EBS permissions",
        "EC2 permissions",
        "ECS/ECR permissions",
        "EFS permissions",
        "ElastiCache permissions",
        "ElasticSearch permissions",
        "Elastic Beanstalk permissions",
        "ELB permissions",
        "EMR permissions",
        "Health permissions",
        "IAM permissions",
        "IoT permissions",
        "Kinesis Firehose permissions",
        "Kinesis Streams permissions",
        "Lambda permissions",
        "RDS, RDS Enhanced Monitoring permissions",
        "Redshift permissions",
        "Route 53 permissions",
        "S3 permissions",
        "Simple Email Service (SES) permissions",
        "SNS permissions",
        "SQS permissions",
        "Trusted Advisor permissions",
        "VPC permissions",
        "X-Ray monitoring permissions"
      ],
      "title": "Integrations and managed policies",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Get started"
      ],
      "external_id": "80e215e7b2ba382de1b7ea758ee1b1f0a1e3c7df",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/get-started/integrations-managed-policies/",
      "published_at": "2021-05-04T18:30:29Z",
      "updated_at": "2021-05-04T18:30:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In order to use infrastructure integrations, you need to grant New Relic permission to read the relevant data from your account. Amazon Web Services (AWS) uses managed policies to grant these permissions. Recommended policy Important Recommendation: Grant an account-wide ReadOnlyAccess managed policy from AWS. AWS automatically updates this policy when new services are added or existing services are modified. New Relic infrastructure integrations have been designed to function with ReadOnlyAccess policies. For instructions, see Connect AWS integrations to infrastructure. Exception: The Trusted Advisor integration is not covered by the ReadOnlyAccess policy. It requires the additional AWSSupportAccess managed policy. This is also the only integration that requires full access permissions (support:*) in order to correctly operate. We notified Amazon about this limitation. Once it's resolved we'll update documentation with more specific permissions required for this integration. Optional policy If you cannot use the ReadOnlyAccess managed policy from AWS, you can create your own customized policy based on the list of permissions. This allows you to specify the optimal permissions required to fetch data from AWS for each integration. While this option is available, it is not recommended because it must be manually updated when you add or modify your integrations. Important New Relic has no way of identifying problems related to custom permissions. If you choose to create a custom policy, it is your responsibility to maintain it and ensure proper data is being collected. There are two ways to set up your customized policy: You can either use our CloudFormation template, or create own yourself by adding the permissions you need. Option 1: Use our CloudFormation template Our CloudFormation template contains all the permissions for all our AWS integrations. A user different than root can be used in the managed policy. CloudFormation template AWSTemplateFormatVersion: 2010-09-09 Outputs: NewRelicRoleArn: Description: NewRelicRole to monitor AWS Lambda Value: !GetAtt - NewRelicIntegrationsTemplate - Arn Parameters: NewRelicAccountNumber: Type: String Description: The Newrelic account number to send data AllowedPattern: '[0-9]+' Resources: NewRelicIntegrationsTemplate: Type: 'AWS::IAM::Role' Properties: RoleName: !Sub NewRelicTemplateTest AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: AWS: !Sub 'arn:aws:iam::754728514883:root' Action: 'sts:AssumeRole' Condition: StringEquals: 'sts:ExternalId': !Ref NewRelicAccountNumber Policies: - PolicyName: NewRelicIntegrations PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'elasticloadbalancing:DescribeLoadBalancers' - 'elasticloadbalancing:DescribeTargetGroups' - 'elasticloadbalancing:DescribeTags' - 'elasticloadbalancing:DescribeLoadBalancerAttributes' - 'elasticloadbalancing:DescribeListeners' - 'elasticloadbalancing:DescribeRules' - 'elasticloadbalancing:DescribeTargetGroupAttributes' - 'elasticloadbalancing:DescribeInstanceHealth' - 'elasticloadbalancing:DescribeLoadBalancerPolicies' - 'elasticloadbalancing:DescribeLoadBalancerPolicyTypes' - 'apigateway:GET' - 'apigateway:HEAD' - 'apigateway:OPTIONS' - 'autoscaling:DescribeLaunchConfigurations' - 'autoscaling:DescribeAutoScalingGroups' - 'autoscaling:DescribePolicies' - 'autoscaling:DescribeTags' - 'autoscaling:DescribeAccountLimits' - 'budgets:ViewBilling' - 'budgets:ViewBudget' - 'cloudfront:ListDistributions' - 'cloudfront:ListStreamingDistributions' - 'cloudfront:ListTagsForResource' - 'cloudtrail:LookupEvents' - 'config:BatchGetResourceConfig' - 'config:ListDiscoveredResources' - 'dynamodb:DescribeLimits' - 'dynamodb:ListTables' - 'dynamodb:DescribeTable' - 'dynamodb:ListGlobalTables' - 'dynamodb:DescribeGlobalTable' - 'dynamodb:ListTagsOfResource' - 'ec2:DescribeVolumeStatus' - 'ec2:DescribeVolumes' - 'ec2:DescribeVolumeAttribute' - 'ec2:DescribeInstanceStatus' - 'ec2:DescribeInstances' - 'ec2:DescribeVpnConnections' - 'ecs:ListServices' - 'ecs:DescribeServices' - 'ecs:DescribeClusters' - 'ecs:ListClusters' - 'ecs:ListTagsForResource' - 'ecs:ListContainerInstances' - 'ecs:DescribeContainerInstances' - 'elasticfilesystem:DescribeMountTargets' - 'elasticfilesystem:DescribeFileSystems' - 'elasticache:DescribeCacheClusters' - 'elasticache:ListTagsForResource' - 'es:ListDomainNames' - 'es:DescribeElasticsearchDomain' - 'es:DescribeElasticsearchDomains' - 'es:ListTags' - 'elasticbeanstalk:DescribeEnvironments' - 'elasticbeanstalk:DescribeInstancesHealth' - 'elasticbeanstalk:DescribeConfigurationSettings' - 'elasticloadbalancing:DescribeLoadBalancers' - 'elasticmapreduce:ListInstances' - 'elasticmapreduce:ListClusters' - 'elasticmapreduce:DescribeCluster' - 'elasticmapreduce:ListInstanceGroups' - 'health:DescribeAffectedEntities' - 'health:DescribeEventDetails' - 'health:DescribeEvents' - 'iam:ListSAMLProviders' - 'iam:ListOpenIDConnectProviders' - 'iam:ListServerCertificates' - 'iam:GetAccountAuthorizationDetails' - 'iam:ListVirtualMFADevices' - 'iam:GetAccountSummary' - 'iot:ListTopicRules' - 'iot:GetTopicRule' - 'iot:ListThings' - 'firehose:DescribeDeliveryStream' - 'firehose:ListDeliveryStreams' - 'kinesis:ListStreams' - 'kinesis:DescribeStream' - 'kinesis:ListTagsForStream' - 'rds:ListTagsForResource' - 'rds:DescribeDBInstances' - 'rds:DescribeDBClusters' - 'redshift:DescribeClusters' - 'redshift:DescribeClusterParameters' - 'route53:ListHealthChecks' - 'route53:GetHostedZone' - 'route53:ListHostedZones' - 'route53:ListResourceRecordSets' - 'route53:ListTagsForResources' - 's3:GetLifecycleConfiguration' - 's3:GetBucketTagging' - 's3:ListAllMyBuckets' - 's3:GetBucketWebsite' - 's3:GetBucketLogging' - 's3:GetBucketCORS' - 's3:GetBucketVersioning' - 's3:GetBucketAcl' - 's3:GetBucketNotification' - 's3:GetBucketPolicy' - 's3:GetReplicationConfiguration' - 's3:GetMetricsConfiguration' - 's3:GetAccelerateConfiguration' - 's3:GetAnalyticsConfiguration' - 's3:GetBucketLocation' - 's3:GetBucketRequestPayment' - 's3:GetEncryptionConfiguration' - 's3:GetInventoryConfiguration' - 's3:GetIpConfiguration' - 'ses:ListConfigurationSets' - 'ses:GetSendQuota' - 'ses:DescribeConfigurationSet' - 'ses:ListReceiptFilters' - 'ses:ListReceiptRuleSets' - 'ses:DescribeReceiptRule' - 'ses:DescribeReceiptRuleSet' - 'sns:GetTopicAttributes' - 'sns:ListTopics' - 'sqs:ListQueues' - 'sqs:ListQueueTags' - 'sqs:GetQueueAttributes' - 'tag:GetResources' - 'ec2:DescribeInternetGateways' - 'ec2:DescribeVpcs' - 'ec2:DescribeNatGateways' - 'ec2:DescribeVpcEndpoints' - 'ec2:DescribeSubnets' - 'ec2:DescribeNetworkAcls' - 'ec2:DescribeVpcAttribute' - 'ec2:DescribeRouteTables' - 'ec2:DescribeSecurityGroups' - 'ec2:DescribeVpcPeeringConnections' - 'ec2:DescribeNetworkInterfaces' - 'lambda:GetAccountSettings' - 'lambda:ListFunctions' - 'lambda:ListAliases' - 'lambda:ListTags' - 'lambda:ListEventSourceMappings' - 'cloudwatch:GetMetricStatistics' - 'cloudwatch:ListMetrics' - 'cloudwatch:GetMetricData' - 'support:*' Resource: '*' Copy Option 2: Manually add permissions To create your own policy using available permissions: Add the permissions for all integrations. Add permissions that are specific to the integrations you need The following permissions are used by New Relic to retrieve data for specific AWS integrations: Required by all integrations Important If an integration is not listed on this page, these permissions are all you need. All integrations Permissions CloudWatch cloudwatch:GetMetricStatistics cloudwatch:ListMetrics cloudwatch:GetMetricData Config API config:BatchGetResourceConfig config:ListDiscoveredResources Resource Tagging API tag:GetResources ALB permissions Additional ALB permissions: elasticloadbalancing:DescribeLoadBalancers elasticloadbalancing:DescribeTargetGroups elasticloadbalancing:DescribeTags elasticloadbalancing:DescribeLoadBalancerAttributes elasticloadbalancing:DescribeListeners elasticloadbalancing:DescribeRules elasticloadbalancing:DescribeTargetGroupAttributes elasticloadbalancing:DescribeInstanceHealth elasticloadbalancing:DescribeLoadBalancerPolicies elasticloadbalancing:DescribeLoadBalancerPolicyTypes API Gateway permissions Additional API Gateway permissions: apigateway:GET apigateway:HEAD apigateway:OPTIONS Auto Scaling permissions Additional Auto Scaling permissions: autoscaling:DescribeLaunchConfigurations autoscaling:DescribeAutoScalingGroups autoscaling:DescribePolicies autoscaling:DescribeTags autoscaling:DescribeAccountLimits Billing permissions Additional Billing permissions: budgets:ViewBilling budgets:ViewBudget Cloudfront permissions Additional Cloudfront permissions: cloudfront:ListDistributions cloudfront:ListStreamingDistributions cloudfront:ListTagsForResource CloudTrail permissions Additional CloudTrail permissions: cloudtrail:LookupEvents DynamoDB permissions Additional DynamoDB permissions: dynamodb:DescribeLimits dynamodb:ListTables dynamodb:DescribeTable dynamodb:ListGlobalTables dynamodb:DescribeGlobalTable dynamodb:ListTagsOfResource EBS permissions Additional EBS permissions: ec2:DescribeVolumeStatus ec2:DescribeVolumes ec2:DescribeVolumeAttribute EC2 permissions Additional EC2 permissions: ec2:DescribeInstanceStatus ec2:DescribeInstances ECS/ECR permissions Additional ECS/ECR permissions: ecs:ListServices ecs:DescribeServices ecs:DescribeClusters ecs:ListClusters ecs:ListTagsForResource ecs:ListContainerInstances ecs:DescribeContainerInstances EFS permissions Additional EFS permissions: elasticfilesystem:DescribeMountTargets elasticfilesystem:DescribeFileSystems ElastiCache permissions Additional ElastiCache permissions: elasticache:DescribeCacheClusters elasticache:ListTagsForResource ElasticSearch permissions Additional ElasticSearch permissions: es:ListDomainNames es:DescribeElasticsearchDomain es:DescribeElasticsearchDomains es:ListTags Elastic Beanstalk permissions Additional Elastic Beanstalk permissions: elasticbeanstalk:DescribeEnvironments elasticbeanstalk:DescribeInstancesHealth elasticbeanstalk:DescribeConfigurationSettings ELB permissions Additional ELB permissions: elasticloadbalancing:DescribeLoadBalancers EMR permissions Additional EMR permissions: elasticmapreduce:ListInstances elasticmapreduce:ListClusters elasticmapreduce:DescribeCluster elasticmapreduce:ListInstanceGroups elasticmapreduce:ListInstanceFleets Health permissions Additional Health permissions: health:DescribeAffectedEntities health:DescribeEventDetails health:DescribeEvents IAM permissions Additional IAM permissions: iam:ListSAMLProviders iam:ListOpenIDConnectProviders iam:ListServerCertificates iam:GetAccountAuthorizationDetails iam:ListVirtualMFADevices iam:GetAccountSummary IoT permissions Additional IoT permissions: iot:ListTopicRules iot:GetTopicRule iot:ListThings Kinesis Firehose permissions Additional Kinesis Firehose permissions: firehose:DescribeDeliveryStream firehose:ListDeliveryStreams Kinesis Streams permissions Additional Kinesis Streams permissions: kinesis:ListStreams kinesis:DescribeStream kinesis:ListTagsForStream Lambda permissions Additional Lambda permissions: lambda:GetAccountSettings lambda:ListFunctions lambda:ListAliases lambda:ListTags lambda:ListEventSourceMappings RDS, RDS Enhanced Monitoring permissions Additional RDS and RDS Enhanced Monitoring permissions: rds:ListTagsForResource rds:DescribeDBInstances rds:DescribeDBClusters Redshift permissions Additional Redshift permissions: redshift:DescribeClusters redshift:DescribeClusterParameters Route 53 permissions Additional Route 53 permissions: route53:ListHealthChecks route53:GetHostedZone route53:ListHostedZones route53:ListResourceRecordSets route53:ListTagsForResources S3 permissions Additional S3 permissions: s3:GetLifecycleConfiguration s3:GetBucketTagging s3:ListAllMyBuckets s3:GetBucketWebsite s3:GetBucketLogging s3:GetBucketCORS s3:GetBucketVersioning s3:GetBucketAcl s3:GetBucketNotification s3:GetBucketPolicy s3:GetReplicationConfiguration s3:GetMetricsConfiguration s3:GetAccelerateConfiguration s3:GetAnalyticsConfiguration s3:GetBucketLocation s3:GetBucketRequestPayment s3:GetEncryptionConfiguration s3:GetInventoryConfiguration s3:GetIpConfiguration Simple Email Service (SES) permissions Additional SES permissions: ses:ListConfigurationSets ses:GetSendQuota ses:DescribeConfigurationSet ses:ListReceiptFilters ses:ListReceiptRuleSets ses:DescribeReceiptRule ses:DescribeReceiptRuleSet SNS permissions Additional SNS permissions: sns:GetTopicAttributes sns:ListTopics SQS permissions Additional SQS permissions: sqs:ListQueues sqs:GetQueueAttributes sqs:ListQueueTags Trusted Advisor permissions Additional Trusted Advisor permissions: support:* See also the note about the Trusted Advisor integration and recommended policies. VPC permissions Additional VPC permissions: ec2:DescribeInternetGateways ec2:DescribeVpcs ec2:DescribeNatGateways ec2:DescribeVpcEndpoints ec2:DescribeSubnets ec2:DescribeNetworkAcls ec2:DescribeVpcAttribute ec2:DescribeRouteTables ec2:DescribeSecurityGroups ec2:DescribeVpcPeeringConnections ec2:DescribeNetworkInterfaces ec2:DescribeVpnConnections X-Ray monitoring permissions Additional X-ray monitoring permissions: xray:BatchGet* xray:Get*",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 135.1636,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Integrations</em> and managed policies",
        "sections": "<em>Integrations</em> and managed policies",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "In order to use infrastructure <em>integrations</em>, you need to grant New Relic permission to read the relevant data from your account. <em>Amazon</em> Web Services (AWS) uses managed policies to grant these permissions. Recommended policy Important Recommendation: Grant an account-wide ReadOnlyAccess managed"
      },
      "id": "6045079fe7b9d27db95799d9"
    },
    {
      "sections": [
        "AWS service specific API rate limiting",
        "Problem",
        "Solution",
        "Verify your Infrastructure account's ARN",
        "Change the polling frequency",
        "Filter your data",
        "Review API usage",
        "Cause"
      ],
      "title": "AWS service specific API rate limiting",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Troubleshooting"
      ],
      "external_id": "785db1a9f5d5d9b89c2d304d1260ce5a8f30a680",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/troubleshooting/aws-service-specific-api-rate-limiting/",
      "published_at": "2021-05-05T15:10:41Z",
      "updated_at": "2021-03-13T03:22:51Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem After enabling Amazon integrations with New Relic Infrastructure, you encounter a rate limit for service-specific APIs. You might see this message in your AWS monitoring software, often with a 503 error: AWS::EC2::Errors::RequestLimitExceeded Request limit exceeded. Solution Verify your Infrastructure account's ARN Ensure that you are not collecting inventory information for the wrong ARN account. Verify that the ARN associated with your Infrastructure account is correct. Change the polling frequency The polling frequency determines how often New Relic gathers data from your cloud provider. By default, the polling frequency is set to the maximum frequency that is available for each service. If you reach your API rate limit, you may want to decrease the polling frequency. Filter your data You can set filters for each integration in order to specify which information you want captured. If you reach your API rate limit, you may want to filter your data. Review API usage To review the API usage for New Relic Infrastructure integrations with Amazon AWS: Go to one.newrelic.com > Infrastructure > AWS > Account status dashboard. Review the New Relic Insights dashboard, which appears automatically. The Insights dashboard includes a chart with your account's Amazon AWS API call count for the last month as well as the CloudWatch API calls (per AWS resource) for the last day. This information is the API usage for New Relic only. It does not include other AWS API or CloudWatch usage that may occur. For assistance determining which services may cause an increase in billing, get support at support.newrelic.com, or contact your New Relic account representative. Cause Infrastructure Amazon integrations leverage the AWS monitoring APIs to gather inventory data. AWS imposes hard rate limits on many of the AWS service-specific APIs consumed by New Relic Infrastructure integrations. Adding New Relic Amazon integrations will increase usage of the service-specific APIs and could impact how quickly you reach your rate limit. This may be caused by either of the following: Enabling Amazon integrations on several plugins for the same service Adding the incorrect Role ARN to your AWS integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 110.62892,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "Problem After enabling <em>Amazon</em> <em>integrations</em> with New Relic Infrastructure, you encounter a rate limit for service-specific APIs. You might see this message in your AWS monitoring software, often with a 503 error: AWS::EC2::Errors::RequestLimitExceeded Request limit exceeded. Solution Verify your"
      },
      "id": "604507c428ccbc013a2c60c4"
    }
  ],
  "/docs/integrations/amazon-integrations/troubleshooting/cloudwatch-billing-increase": [
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Automated setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-05-04T18:01:54Z",
      "updated_at": "2021-05-04T18:01:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Automated setup using CloudFormation We provide a CloudFormation template that automates this process. This needs to be applied to each AWS account and region you want to monitor in New Relic. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL (US Datacenter): https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL (EU Datacenter): https://cloud-collector.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = cloudwatch-metric-streams. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Similarly as with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 We plan to add metadata from most used AWS services. Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. New Relic and AWS teams are working together to offer more granular controls so that filters can be applied based on tags and other attributes. Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with eu then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and its not the case, please verify the following: Make sure theres no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 137.91473,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": " data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to <em>troubleshoot</em> your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS <em>Troubleshooting</em> docs"
      },
      "id": "606a036de7b9d2bfef9445f2"
    },
    {
      "sections": [
        "Integrations and managed policies",
        "Recommended policy",
        "Important",
        "Optional policy",
        "Option 1: Use our CloudFormation template",
        "CloudFormation template",
        "Option 2: Manually add permissions",
        "Required by all integrations",
        "ALB permissions",
        "API Gateway permissions",
        "Auto Scaling permissions",
        "Billing permissions",
        "Cloudfront permissions",
        "CloudTrail permissions",
        "DynamoDB permissions",
        "EBS permissions",
        "EC2 permissions",
        "ECS/ECR permissions",
        "EFS permissions",
        "ElastiCache permissions",
        "ElasticSearch permissions",
        "Elastic Beanstalk permissions",
        "ELB permissions",
        "EMR permissions",
        "Health permissions",
        "IAM permissions",
        "IoT permissions",
        "Kinesis Firehose permissions",
        "Kinesis Streams permissions",
        "Lambda permissions",
        "RDS, RDS Enhanced Monitoring permissions",
        "Redshift permissions",
        "Route 53 permissions",
        "S3 permissions",
        "Simple Email Service (SES) permissions",
        "SNS permissions",
        "SQS permissions",
        "Trusted Advisor permissions",
        "VPC permissions",
        "X-Ray monitoring permissions"
      ],
      "title": "Integrations and managed policies",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Get started"
      ],
      "external_id": "80e215e7b2ba382de1b7ea758ee1b1f0a1e3c7df",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/get-started/integrations-managed-policies/",
      "published_at": "2021-05-04T18:30:29Z",
      "updated_at": "2021-05-04T18:30:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In order to use infrastructure integrations, you need to grant New Relic permission to read the relevant data from your account. Amazon Web Services (AWS) uses managed policies to grant these permissions. Recommended policy Important Recommendation: Grant an account-wide ReadOnlyAccess managed policy from AWS. AWS automatically updates this policy when new services are added or existing services are modified. New Relic infrastructure integrations have been designed to function with ReadOnlyAccess policies. For instructions, see Connect AWS integrations to infrastructure. Exception: The Trusted Advisor integration is not covered by the ReadOnlyAccess policy. It requires the additional AWSSupportAccess managed policy. This is also the only integration that requires full access permissions (support:*) in order to correctly operate. We notified Amazon about this limitation. Once it's resolved we'll update documentation with more specific permissions required for this integration. Optional policy If you cannot use the ReadOnlyAccess managed policy from AWS, you can create your own customized policy based on the list of permissions. This allows you to specify the optimal permissions required to fetch data from AWS for each integration. While this option is available, it is not recommended because it must be manually updated when you add or modify your integrations. Important New Relic has no way of identifying problems related to custom permissions. If you choose to create a custom policy, it is your responsibility to maintain it and ensure proper data is being collected. There are two ways to set up your customized policy: You can either use our CloudFormation template, or create own yourself by adding the permissions you need. Option 1: Use our CloudFormation template Our CloudFormation template contains all the permissions for all our AWS integrations. A user different than root can be used in the managed policy. CloudFormation template AWSTemplateFormatVersion: 2010-09-09 Outputs: NewRelicRoleArn: Description: NewRelicRole to monitor AWS Lambda Value: !GetAtt - NewRelicIntegrationsTemplate - Arn Parameters: NewRelicAccountNumber: Type: String Description: The Newrelic account number to send data AllowedPattern: '[0-9]+' Resources: NewRelicIntegrationsTemplate: Type: 'AWS::IAM::Role' Properties: RoleName: !Sub NewRelicTemplateTest AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: AWS: !Sub 'arn:aws:iam::754728514883:root' Action: 'sts:AssumeRole' Condition: StringEquals: 'sts:ExternalId': !Ref NewRelicAccountNumber Policies: - PolicyName: NewRelicIntegrations PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'elasticloadbalancing:DescribeLoadBalancers' - 'elasticloadbalancing:DescribeTargetGroups' - 'elasticloadbalancing:DescribeTags' - 'elasticloadbalancing:DescribeLoadBalancerAttributes' - 'elasticloadbalancing:DescribeListeners' - 'elasticloadbalancing:DescribeRules' - 'elasticloadbalancing:DescribeTargetGroupAttributes' - 'elasticloadbalancing:DescribeInstanceHealth' - 'elasticloadbalancing:DescribeLoadBalancerPolicies' - 'elasticloadbalancing:DescribeLoadBalancerPolicyTypes' - 'apigateway:GET' - 'apigateway:HEAD' - 'apigateway:OPTIONS' - 'autoscaling:DescribeLaunchConfigurations' - 'autoscaling:DescribeAutoScalingGroups' - 'autoscaling:DescribePolicies' - 'autoscaling:DescribeTags' - 'autoscaling:DescribeAccountLimits' - 'budgets:ViewBilling' - 'budgets:ViewBudget' - 'cloudfront:ListDistributions' - 'cloudfront:ListStreamingDistributions' - 'cloudfront:ListTagsForResource' - 'cloudtrail:LookupEvents' - 'config:BatchGetResourceConfig' - 'config:ListDiscoveredResources' - 'dynamodb:DescribeLimits' - 'dynamodb:ListTables' - 'dynamodb:DescribeTable' - 'dynamodb:ListGlobalTables' - 'dynamodb:DescribeGlobalTable' - 'dynamodb:ListTagsOfResource' - 'ec2:DescribeVolumeStatus' - 'ec2:DescribeVolumes' - 'ec2:DescribeVolumeAttribute' - 'ec2:DescribeInstanceStatus' - 'ec2:DescribeInstances' - 'ec2:DescribeVpnConnections' - 'ecs:ListServices' - 'ecs:DescribeServices' - 'ecs:DescribeClusters' - 'ecs:ListClusters' - 'ecs:ListTagsForResource' - 'ecs:ListContainerInstances' - 'ecs:DescribeContainerInstances' - 'elasticfilesystem:DescribeMountTargets' - 'elasticfilesystem:DescribeFileSystems' - 'elasticache:DescribeCacheClusters' - 'elasticache:ListTagsForResource' - 'es:ListDomainNames' - 'es:DescribeElasticsearchDomain' - 'es:DescribeElasticsearchDomains' - 'es:ListTags' - 'elasticbeanstalk:DescribeEnvironments' - 'elasticbeanstalk:DescribeInstancesHealth' - 'elasticbeanstalk:DescribeConfigurationSettings' - 'elasticloadbalancing:DescribeLoadBalancers' - 'elasticmapreduce:ListInstances' - 'elasticmapreduce:ListClusters' - 'elasticmapreduce:DescribeCluster' - 'elasticmapreduce:ListInstanceGroups' - 'health:DescribeAffectedEntities' - 'health:DescribeEventDetails' - 'health:DescribeEvents' - 'iam:ListSAMLProviders' - 'iam:ListOpenIDConnectProviders' - 'iam:ListServerCertificates' - 'iam:GetAccountAuthorizationDetails' - 'iam:ListVirtualMFADevices' - 'iam:GetAccountSummary' - 'iot:ListTopicRules' - 'iot:GetTopicRule' - 'iot:ListThings' - 'firehose:DescribeDeliveryStream' - 'firehose:ListDeliveryStreams' - 'kinesis:ListStreams' - 'kinesis:DescribeStream' - 'kinesis:ListTagsForStream' - 'rds:ListTagsForResource' - 'rds:DescribeDBInstances' - 'rds:DescribeDBClusters' - 'redshift:DescribeClusters' - 'redshift:DescribeClusterParameters' - 'route53:ListHealthChecks' - 'route53:GetHostedZone' - 'route53:ListHostedZones' - 'route53:ListResourceRecordSets' - 'route53:ListTagsForResources' - 's3:GetLifecycleConfiguration' - 's3:GetBucketTagging' - 's3:ListAllMyBuckets' - 's3:GetBucketWebsite' - 's3:GetBucketLogging' - 's3:GetBucketCORS' - 's3:GetBucketVersioning' - 's3:GetBucketAcl' - 's3:GetBucketNotification' - 's3:GetBucketPolicy' - 's3:GetReplicationConfiguration' - 's3:GetMetricsConfiguration' - 's3:GetAccelerateConfiguration' - 's3:GetAnalyticsConfiguration' - 's3:GetBucketLocation' - 's3:GetBucketRequestPayment' - 's3:GetEncryptionConfiguration' - 's3:GetInventoryConfiguration' - 's3:GetIpConfiguration' - 'ses:ListConfigurationSets' - 'ses:GetSendQuota' - 'ses:DescribeConfigurationSet' - 'ses:ListReceiptFilters' - 'ses:ListReceiptRuleSets' - 'ses:DescribeReceiptRule' - 'ses:DescribeReceiptRuleSet' - 'sns:GetTopicAttributes' - 'sns:ListTopics' - 'sqs:ListQueues' - 'sqs:ListQueueTags' - 'sqs:GetQueueAttributes' - 'tag:GetResources' - 'ec2:DescribeInternetGateways' - 'ec2:DescribeVpcs' - 'ec2:DescribeNatGateways' - 'ec2:DescribeVpcEndpoints' - 'ec2:DescribeSubnets' - 'ec2:DescribeNetworkAcls' - 'ec2:DescribeVpcAttribute' - 'ec2:DescribeRouteTables' - 'ec2:DescribeSecurityGroups' - 'ec2:DescribeVpcPeeringConnections' - 'ec2:DescribeNetworkInterfaces' - 'lambda:GetAccountSettings' - 'lambda:ListFunctions' - 'lambda:ListAliases' - 'lambda:ListTags' - 'lambda:ListEventSourceMappings' - 'cloudwatch:GetMetricStatistics' - 'cloudwatch:ListMetrics' - 'cloudwatch:GetMetricData' - 'support:*' Resource: '*' Copy Option 2: Manually add permissions To create your own policy using available permissions: Add the permissions for all integrations. Add permissions that are specific to the integrations you need The following permissions are used by New Relic to retrieve data for specific AWS integrations: Required by all integrations Important If an integration is not listed on this page, these permissions are all you need. All integrations Permissions CloudWatch cloudwatch:GetMetricStatistics cloudwatch:ListMetrics cloudwatch:GetMetricData Config API config:BatchGetResourceConfig config:ListDiscoveredResources Resource Tagging API tag:GetResources ALB permissions Additional ALB permissions: elasticloadbalancing:DescribeLoadBalancers elasticloadbalancing:DescribeTargetGroups elasticloadbalancing:DescribeTags elasticloadbalancing:DescribeLoadBalancerAttributes elasticloadbalancing:DescribeListeners elasticloadbalancing:DescribeRules elasticloadbalancing:DescribeTargetGroupAttributes elasticloadbalancing:DescribeInstanceHealth elasticloadbalancing:DescribeLoadBalancerPolicies elasticloadbalancing:DescribeLoadBalancerPolicyTypes API Gateway permissions Additional API Gateway permissions: apigateway:GET apigateway:HEAD apigateway:OPTIONS Auto Scaling permissions Additional Auto Scaling permissions: autoscaling:DescribeLaunchConfigurations autoscaling:DescribeAutoScalingGroups autoscaling:DescribePolicies autoscaling:DescribeTags autoscaling:DescribeAccountLimits Billing permissions Additional Billing permissions: budgets:ViewBilling budgets:ViewBudget Cloudfront permissions Additional Cloudfront permissions: cloudfront:ListDistributions cloudfront:ListStreamingDistributions cloudfront:ListTagsForResource CloudTrail permissions Additional CloudTrail permissions: cloudtrail:LookupEvents DynamoDB permissions Additional DynamoDB permissions: dynamodb:DescribeLimits dynamodb:ListTables dynamodb:DescribeTable dynamodb:ListGlobalTables dynamodb:DescribeGlobalTable dynamodb:ListTagsOfResource EBS permissions Additional EBS permissions: ec2:DescribeVolumeStatus ec2:DescribeVolumes ec2:DescribeVolumeAttribute EC2 permissions Additional EC2 permissions: ec2:DescribeInstanceStatus ec2:DescribeInstances ECS/ECR permissions Additional ECS/ECR permissions: ecs:ListServices ecs:DescribeServices ecs:DescribeClusters ecs:ListClusters ecs:ListTagsForResource ecs:ListContainerInstances ecs:DescribeContainerInstances EFS permissions Additional EFS permissions: elasticfilesystem:DescribeMountTargets elasticfilesystem:DescribeFileSystems ElastiCache permissions Additional ElastiCache permissions: elasticache:DescribeCacheClusters elasticache:ListTagsForResource ElasticSearch permissions Additional ElasticSearch permissions: es:ListDomainNames es:DescribeElasticsearchDomain es:DescribeElasticsearchDomains es:ListTags Elastic Beanstalk permissions Additional Elastic Beanstalk permissions: elasticbeanstalk:DescribeEnvironments elasticbeanstalk:DescribeInstancesHealth elasticbeanstalk:DescribeConfigurationSettings ELB permissions Additional ELB permissions: elasticloadbalancing:DescribeLoadBalancers EMR permissions Additional EMR permissions: elasticmapreduce:ListInstances elasticmapreduce:ListClusters elasticmapreduce:DescribeCluster elasticmapreduce:ListInstanceGroups elasticmapreduce:ListInstanceFleets Health permissions Additional Health permissions: health:DescribeAffectedEntities health:DescribeEventDetails health:DescribeEvents IAM permissions Additional IAM permissions: iam:ListSAMLProviders iam:ListOpenIDConnectProviders iam:ListServerCertificates iam:GetAccountAuthorizationDetails iam:ListVirtualMFADevices iam:GetAccountSummary IoT permissions Additional IoT permissions: iot:ListTopicRules iot:GetTopicRule iot:ListThings Kinesis Firehose permissions Additional Kinesis Firehose permissions: firehose:DescribeDeliveryStream firehose:ListDeliveryStreams Kinesis Streams permissions Additional Kinesis Streams permissions: kinesis:ListStreams kinesis:DescribeStream kinesis:ListTagsForStream Lambda permissions Additional Lambda permissions: lambda:GetAccountSettings lambda:ListFunctions lambda:ListAliases lambda:ListTags lambda:ListEventSourceMappings RDS, RDS Enhanced Monitoring permissions Additional RDS and RDS Enhanced Monitoring permissions: rds:ListTagsForResource rds:DescribeDBInstances rds:DescribeDBClusters Redshift permissions Additional Redshift permissions: redshift:DescribeClusters redshift:DescribeClusterParameters Route 53 permissions Additional Route 53 permissions: route53:ListHealthChecks route53:GetHostedZone route53:ListHostedZones route53:ListResourceRecordSets route53:ListTagsForResources S3 permissions Additional S3 permissions: s3:GetLifecycleConfiguration s3:GetBucketTagging s3:ListAllMyBuckets s3:GetBucketWebsite s3:GetBucketLogging s3:GetBucketCORS s3:GetBucketVersioning s3:GetBucketAcl s3:GetBucketNotification s3:GetBucketPolicy s3:GetReplicationConfiguration s3:GetMetricsConfiguration s3:GetAccelerateConfiguration s3:GetAnalyticsConfiguration s3:GetBucketLocation s3:GetBucketRequestPayment s3:GetEncryptionConfiguration s3:GetInventoryConfiguration s3:GetIpConfiguration Simple Email Service (SES) permissions Additional SES permissions: ses:ListConfigurationSets ses:GetSendQuota ses:DescribeConfigurationSet ses:ListReceiptFilters ses:ListReceiptRuleSets ses:DescribeReceiptRule ses:DescribeReceiptRuleSet SNS permissions Additional SNS permissions: sns:GetTopicAttributes sns:ListTopics SQS permissions Additional SQS permissions: sqs:ListQueues sqs:GetQueueAttributes sqs:ListQueueTags Trusted Advisor permissions Additional Trusted Advisor permissions: support:* See also the note about the Trusted Advisor integration and recommended policies. VPC permissions Additional VPC permissions: ec2:DescribeInternetGateways ec2:DescribeVpcs ec2:DescribeNatGateways ec2:DescribeVpcEndpoints ec2:DescribeSubnets ec2:DescribeNetworkAcls ec2:DescribeVpcAttribute ec2:DescribeRouteTables ec2:DescribeSecurityGroups ec2:DescribeVpcPeeringConnections ec2:DescribeNetworkInterfaces ec2:DescribeVpnConnections X-Ray monitoring permissions Additional X-ray monitoring permissions: xray:BatchGet* xray:Get*",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 135.1636,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Integrations</em> and managed policies",
        "sections": "<em>Integrations</em> and managed policies",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "In order to use infrastructure <em>integrations</em>, you need to grant New Relic permission to read the relevant data from your account. <em>Amazon</em> Web Services (AWS) uses managed policies to grant these permissions. Recommended policy Important Recommendation: Grant an account-wide ReadOnlyAccess managed"
      },
      "id": "6045079fe7b9d27db95799d9"
    },
    {
      "sections": [
        "AWS service specific API rate limiting",
        "Problem",
        "Solution",
        "Verify your Infrastructure account's ARN",
        "Change the polling frequency",
        "Filter your data",
        "Review API usage",
        "Cause"
      ],
      "title": "AWS service specific API rate limiting",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Troubleshooting"
      ],
      "external_id": "785db1a9f5d5d9b89c2d304d1260ce5a8f30a680",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/troubleshooting/aws-service-specific-api-rate-limiting/",
      "published_at": "2021-05-05T15:10:41Z",
      "updated_at": "2021-03-13T03:22:51Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem After enabling Amazon integrations with New Relic Infrastructure, you encounter a rate limit for service-specific APIs. You might see this message in your AWS monitoring software, often with a 503 error: AWS::EC2::Errors::RequestLimitExceeded Request limit exceeded. Solution Verify your Infrastructure account's ARN Ensure that you are not collecting inventory information for the wrong ARN account. Verify that the ARN associated with your Infrastructure account is correct. Change the polling frequency The polling frequency determines how often New Relic gathers data from your cloud provider. By default, the polling frequency is set to the maximum frequency that is available for each service. If you reach your API rate limit, you may want to decrease the polling frequency. Filter your data You can set filters for each integration in order to specify which information you want captured. If you reach your API rate limit, you may want to filter your data. Review API usage To review the API usage for New Relic Infrastructure integrations with Amazon AWS: Go to one.newrelic.com > Infrastructure > AWS > Account status dashboard. Review the New Relic Insights dashboard, which appears automatically. The Insights dashboard includes a chart with your account's Amazon AWS API call count for the last month as well as the CloudWatch API calls (per AWS resource) for the last day. This information is the API usage for New Relic only. It does not include other AWS API or CloudWatch usage that may occur. For assistance determining which services may cause an increase in billing, get support at support.newrelic.com, or contact your New Relic account representative. Cause Infrastructure Amazon integrations leverage the AWS monitoring APIs to gather inventory data. AWS imposes hard rate limits on many of the AWS service-specific APIs consumed by New Relic Infrastructure integrations. Adding New Relic Amazon integrations will increase usage of the service-specific APIs and could impact how quickly you reach your rate limit. This may be caused by either of the following: Enabling Amazon integrations on several plugins for the same service Adding the incorrect Role ARN to your AWS integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 110.62892,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "Problem After enabling <em>Amazon</em> <em>integrations</em> with New Relic Infrastructure, you encounter a rate limit for service-specific APIs. You might see this message in your AWS monitoring software, often with a 503 error: AWS::EC2::Errors::RequestLimitExceeded Request limit exceeded. Solution Verify your"
      },
      "id": "604507c428ccbc013a2c60c4"
    }
  ],
  "/docs/integrations/amazon-integrations/troubleshooting/invalid-principal-error-unsupported-aws-regions": [
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Automated setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-05-04T18:01:54Z",
      "updated_at": "2021-05-04T18:01:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Automated setup using CloudFormation We provide a CloudFormation template that automates this process. This needs to be applied to each AWS account and region you want to monitor in New Relic. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL (US Datacenter): https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL (EU Datacenter): https://cloud-collector.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = cloudwatch-metric-streams. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Similarly as with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 We plan to add metadata from most used AWS services. Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. New Relic and AWS teams are working together to offer more granular controls so that filters can be applied based on tags and other attributes. Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with eu then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and its not the case, please verify the following: Make sure theres no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 137.91467,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": " data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to <em>troubleshoot</em> your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS <em>Troubleshooting</em> docs"
      },
      "id": "606a036de7b9d2bfef9445f2"
    },
    {
      "sections": [
        "Integrations and managed policies",
        "Recommended policy",
        "Important",
        "Optional policy",
        "Option 1: Use our CloudFormation template",
        "CloudFormation template",
        "Option 2: Manually add permissions",
        "Required by all integrations",
        "ALB permissions",
        "API Gateway permissions",
        "Auto Scaling permissions",
        "Billing permissions",
        "Cloudfront permissions",
        "CloudTrail permissions",
        "DynamoDB permissions",
        "EBS permissions",
        "EC2 permissions",
        "ECS/ECR permissions",
        "EFS permissions",
        "ElastiCache permissions",
        "ElasticSearch permissions",
        "Elastic Beanstalk permissions",
        "ELB permissions",
        "EMR permissions",
        "Health permissions",
        "IAM permissions",
        "IoT permissions",
        "Kinesis Firehose permissions",
        "Kinesis Streams permissions",
        "Lambda permissions",
        "RDS, RDS Enhanced Monitoring permissions",
        "Redshift permissions",
        "Route 53 permissions",
        "S3 permissions",
        "Simple Email Service (SES) permissions",
        "SNS permissions",
        "SQS permissions",
        "Trusted Advisor permissions",
        "VPC permissions",
        "X-Ray monitoring permissions"
      ],
      "title": "Integrations and managed policies",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Get started"
      ],
      "external_id": "80e215e7b2ba382de1b7ea758ee1b1f0a1e3c7df",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/get-started/integrations-managed-policies/",
      "published_at": "2021-05-04T18:30:29Z",
      "updated_at": "2021-05-04T18:30:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In order to use infrastructure integrations, you need to grant New Relic permission to read the relevant data from your account. Amazon Web Services (AWS) uses managed policies to grant these permissions. Recommended policy Important Recommendation: Grant an account-wide ReadOnlyAccess managed policy from AWS. AWS automatically updates this policy when new services are added or existing services are modified. New Relic infrastructure integrations have been designed to function with ReadOnlyAccess policies. For instructions, see Connect AWS integrations to infrastructure. Exception: The Trusted Advisor integration is not covered by the ReadOnlyAccess policy. It requires the additional AWSSupportAccess managed policy. This is also the only integration that requires full access permissions (support:*) in order to correctly operate. We notified Amazon about this limitation. Once it's resolved we'll update documentation with more specific permissions required for this integration. Optional policy If you cannot use the ReadOnlyAccess managed policy from AWS, you can create your own customized policy based on the list of permissions. This allows you to specify the optimal permissions required to fetch data from AWS for each integration. While this option is available, it is not recommended because it must be manually updated when you add or modify your integrations. Important New Relic has no way of identifying problems related to custom permissions. If you choose to create a custom policy, it is your responsibility to maintain it and ensure proper data is being collected. There are two ways to set up your customized policy: You can either use our CloudFormation template, or create own yourself by adding the permissions you need. Option 1: Use our CloudFormation template Our CloudFormation template contains all the permissions for all our AWS integrations. A user different than root can be used in the managed policy. CloudFormation template AWSTemplateFormatVersion: 2010-09-09 Outputs: NewRelicRoleArn: Description: NewRelicRole to monitor AWS Lambda Value: !GetAtt - NewRelicIntegrationsTemplate - Arn Parameters: NewRelicAccountNumber: Type: String Description: The Newrelic account number to send data AllowedPattern: '[0-9]+' Resources: NewRelicIntegrationsTemplate: Type: 'AWS::IAM::Role' Properties: RoleName: !Sub NewRelicTemplateTest AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: AWS: !Sub 'arn:aws:iam::754728514883:root' Action: 'sts:AssumeRole' Condition: StringEquals: 'sts:ExternalId': !Ref NewRelicAccountNumber Policies: - PolicyName: NewRelicIntegrations PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'elasticloadbalancing:DescribeLoadBalancers' - 'elasticloadbalancing:DescribeTargetGroups' - 'elasticloadbalancing:DescribeTags' - 'elasticloadbalancing:DescribeLoadBalancerAttributes' - 'elasticloadbalancing:DescribeListeners' - 'elasticloadbalancing:DescribeRules' - 'elasticloadbalancing:DescribeTargetGroupAttributes' - 'elasticloadbalancing:DescribeInstanceHealth' - 'elasticloadbalancing:DescribeLoadBalancerPolicies' - 'elasticloadbalancing:DescribeLoadBalancerPolicyTypes' - 'apigateway:GET' - 'apigateway:HEAD' - 'apigateway:OPTIONS' - 'autoscaling:DescribeLaunchConfigurations' - 'autoscaling:DescribeAutoScalingGroups' - 'autoscaling:DescribePolicies' - 'autoscaling:DescribeTags' - 'autoscaling:DescribeAccountLimits' - 'budgets:ViewBilling' - 'budgets:ViewBudget' - 'cloudfront:ListDistributions' - 'cloudfront:ListStreamingDistributions' - 'cloudfront:ListTagsForResource' - 'cloudtrail:LookupEvents' - 'config:BatchGetResourceConfig' - 'config:ListDiscoveredResources' - 'dynamodb:DescribeLimits' - 'dynamodb:ListTables' - 'dynamodb:DescribeTable' - 'dynamodb:ListGlobalTables' - 'dynamodb:DescribeGlobalTable' - 'dynamodb:ListTagsOfResource' - 'ec2:DescribeVolumeStatus' - 'ec2:DescribeVolumes' - 'ec2:DescribeVolumeAttribute' - 'ec2:DescribeInstanceStatus' - 'ec2:DescribeInstances' - 'ec2:DescribeVpnConnections' - 'ecs:ListServices' - 'ecs:DescribeServices' - 'ecs:DescribeClusters' - 'ecs:ListClusters' - 'ecs:ListTagsForResource' - 'ecs:ListContainerInstances' - 'ecs:DescribeContainerInstances' - 'elasticfilesystem:DescribeMountTargets' - 'elasticfilesystem:DescribeFileSystems' - 'elasticache:DescribeCacheClusters' - 'elasticache:ListTagsForResource' - 'es:ListDomainNames' - 'es:DescribeElasticsearchDomain' - 'es:DescribeElasticsearchDomains' - 'es:ListTags' - 'elasticbeanstalk:DescribeEnvironments' - 'elasticbeanstalk:DescribeInstancesHealth' - 'elasticbeanstalk:DescribeConfigurationSettings' - 'elasticloadbalancing:DescribeLoadBalancers' - 'elasticmapreduce:ListInstances' - 'elasticmapreduce:ListClusters' - 'elasticmapreduce:DescribeCluster' - 'elasticmapreduce:ListInstanceGroups' - 'health:DescribeAffectedEntities' - 'health:DescribeEventDetails' - 'health:DescribeEvents' - 'iam:ListSAMLProviders' - 'iam:ListOpenIDConnectProviders' - 'iam:ListServerCertificates' - 'iam:GetAccountAuthorizationDetails' - 'iam:ListVirtualMFADevices' - 'iam:GetAccountSummary' - 'iot:ListTopicRules' - 'iot:GetTopicRule' - 'iot:ListThings' - 'firehose:DescribeDeliveryStream' - 'firehose:ListDeliveryStreams' - 'kinesis:ListStreams' - 'kinesis:DescribeStream' - 'kinesis:ListTagsForStream' - 'rds:ListTagsForResource' - 'rds:DescribeDBInstances' - 'rds:DescribeDBClusters' - 'redshift:DescribeClusters' - 'redshift:DescribeClusterParameters' - 'route53:ListHealthChecks' - 'route53:GetHostedZone' - 'route53:ListHostedZones' - 'route53:ListResourceRecordSets' - 'route53:ListTagsForResources' - 's3:GetLifecycleConfiguration' - 's3:GetBucketTagging' - 's3:ListAllMyBuckets' - 's3:GetBucketWebsite' - 's3:GetBucketLogging' - 's3:GetBucketCORS' - 's3:GetBucketVersioning' - 's3:GetBucketAcl' - 's3:GetBucketNotification' - 's3:GetBucketPolicy' - 's3:GetReplicationConfiguration' - 's3:GetMetricsConfiguration' - 's3:GetAccelerateConfiguration' - 's3:GetAnalyticsConfiguration' - 's3:GetBucketLocation' - 's3:GetBucketRequestPayment' - 's3:GetEncryptionConfiguration' - 's3:GetInventoryConfiguration' - 's3:GetIpConfiguration' - 'ses:ListConfigurationSets' - 'ses:GetSendQuota' - 'ses:DescribeConfigurationSet' - 'ses:ListReceiptFilters' - 'ses:ListReceiptRuleSets' - 'ses:DescribeReceiptRule' - 'ses:DescribeReceiptRuleSet' - 'sns:GetTopicAttributes' - 'sns:ListTopics' - 'sqs:ListQueues' - 'sqs:ListQueueTags' - 'sqs:GetQueueAttributes' - 'tag:GetResources' - 'ec2:DescribeInternetGateways' - 'ec2:DescribeVpcs' - 'ec2:DescribeNatGateways' - 'ec2:DescribeVpcEndpoints' - 'ec2:DescribeSubnets' - 'ec2:DescribeNetworkAcls' - 'ec2:DescribeVpcAttribute' - 'ec2:DescribeRouteTables' - 'ec2:DescribeSecurityGroups' - 'ec2:DescribeVpcPeeringConnections' - 'ec2:DescribeNetworkInterfaces' - 'lambda:GetAccountSettings' - 'lambda:ListFunctions' - 'lambda:ListAliases' - 'lambda:ListTags' - 'lambda:ListEventSourceMappings' - 'cloudwatch:GetMetricStatistics' - 'cloudwatch:ListMetrics' - 'cloudwatch:GetMetricData' - 'support:*' Resource: '*' Copy Option 2: Manually add permissions To create your own policy using available permissions: Add the permissions for all integrations. Add permissions that are specific to the integrations you need The following permissions are used by New Relic to retrieve data for specific AWS integrations: Required by all integrations Important If an integration is not listed on this page, these permissions are all you need. All integrations Permissions CloudWatch cloudwatch:GetMetricStatistics cloudwatch:ListMetrics cloudwatch:GetMetricData Config API config:BatchGetResourceConfig config:ListDiscoveredResources Resource Tagging API tag:GetResources ALB permissions Additional ALB permissions: elasticloadbalancing:DescribeLoadBalancers elasticloadbalancing:DescribeTargetGroups elasticloadbalancing:DescribeTags elasticloadbalancing:DescribeLoadBalancerAttributes elasticloadbalancing:DescribeListeners elasticloadbalancing:DescribeRules elasticloadbalancing:DescribeTargetGroupAttributes elasticloadbalancing:DescribeInstanceHealth elasticloadbalancing:DescribeLoadBalancerPolicies elasticloadbalancing:DescribeLoadBalancerPolicyTypes API Gateway permissions Additional API Gateway permissions: apigateway:GET apigateway:HEAD apigateway:OPTIONS Auto Scaling permissions Additional Auto Scaling permissions: autoscaling:DescribeLaunchConfigurations autoscaling:DescribeAutoScalingGroups autoscaling:DescribePolicies autoscaling:DescribeTags autoscaling:DescribeAccountLimits Billing permissions Additional Billing permissions: budgets:ViewBilling budgets:ViewBudget Cloudfront permissions Additional Cloudfront permissions: cloudfront:ListDistributions cloudfront:ListStreamingDistributions cloudfront:ListTagsForResource CloudTrail permissions Additional CloudTrail permissions: cloudtrail:LookupEvents DynamoDB permissions Additional DynamoDB permissions: dynamodb:DescribeLimits dynamodb:ListTables dynamodb:DescribeTable dynamodb:ListGlobalTables dynamodb:DescribeGlobalTable dynamodb:ListTagsOfResource EBS permissions Additional EBS permissions: ec2:DescribeVolumeStatus ec2:DescribeVolumes ec2:DescribeVolumeAttribute EC2 permissions Additional EC2 permissions: ec2:DescribeInstanceStatus ec2:DescribeInstances ECS/ECR permissions Additional ECS/ECR permissions: ecs:ListServices ecs:DescribeServices ecs:DescribeClusters ecs:ListClusters ecs:ListTagsForResource ecs:ListContainerInstances ecs:DescribeContainerInstances EFS permissions Additional EFS permissions: elasticfilesystem:DescribeMountTargets elasticfilesystem:DescribeFileSystems ElastiCache permissions Additional ElastiCache permissions: elasticache:DescribeCacheClusters elasticache:ListTagsForResource ElasticSearch permissions Additional ElasticSearch permissions: es:ListDomainNames es:DescribeElasticsearchDomain es:DescribeElasticsearchDomains es:ListTags Elastic Beanstalk permissions Additional Elastic Beanstalk permissions: elasticbeanstalk:DescribeEnvironments elasticbeanstalk:DescribeInstancesHealth elasticbeanstalk:DescribeConfigurationSettings ELB permissions Additional ELB permissions: elasticloadbalancing:DescribeLoadBalancers EMR permissions Additional EMR permissions: elasticmapreduce:ListInstances elasticmapreduce:ListClusters elasticmapreduce:DescribeCluster elasticmapreduce:ListInstanceGroups elasticmapreduce:ListInstanceFleets Health permissions Additional Health permissions: health:DescribeAffectedEntities health:DescribeEventDetails health:DescribeEvents IAM permissions Additional IAM permissions: iam:ListSAMLProviders iam:ListOpenIDConnectProviders iam:ListServerCertificates iam:GetAccountAuthorizationDetails iam:ListVirtualMFADevices iam:GetAccountSummary IoT permissions Additional IoT permissions: iot:ListTopicRules iot:GetTopicRule iot:ListThings Kinesis Firehose permissions Additional Kinesis Firehose permissions: firehose:DescribeDeliveryStream firehose:ListDeliveryStreams Kinesis Streams permissions Additional Kinesis Streams permissions: kinesis:ListStreams kinesis:DescribeStream kinesis:ListTagsForStream Lambda permissions Additional Lambda permissions: lambda:GetAccountSettings lambda:ListFunctions lambda:ListAliases lambda:ListTags lambda:ListEventSourceMappings RDS, RDS Enhanced Monitoring permissions Additional RDS and RDS Enhanced Monitoring permissions: rds:ListTagsForResource rds:DescribeDBInstances rds:DescribeDBClusters Redshift permissions Additional Redshift permissions: redshift:DescribeClusters redshift:DescribeClusterParameters Route 53 permissions Additional Route 53 permissions: route53:ListHealthChecks route53:GetHostedZone route53:ListHostedZones route53:ListResourceRecordSets route53:ListTagsForResources S3 permissions Additional S3 permissions: s3:GetLifecycleConfiguration s3:GetBucketTagging s3:ListAllMyBuckets s3:GetBucketWebsite s3:GetBucketLogging s3:GetBucketCORS s3:GetBucketVersioning s3:GetBucketAcl s3:GetBucketNotification s3:GetBucketPolicy s3:GetReplicationConfiguration s3:GetMetricsConfiguration s3:GetAccelerateConfiguration s3:GetAnalyticsConfiguration s3:GetBucketLocation s3:GetBucketRequestPayment s3:GetEncryptionConfiguration s3:GetInventoryConfiguration s3:GetIpConfiguration Simple Email Service (SES) permissions Additional SES permissions: ses:ListConfigurationSets ses:GetSendQuota ses:DescribeConfigurationSet ses:ListReceiptFilters ses:ListReceiptRuleSets ses:DescribeReceiptRule ses:DescribeReceiptRuleSet SNS permissions Additional SNS permissions: sns:GetTopicAttributes sns:ListTopics SQS permissions Additional SQS permissions: sqs:ListQueues sqs:GetQueueAttributes sqs:ListQueueTags Trusted Advisor permissions Additional Trusted Advisor permissions: support:* See also the note about the Trusted Advisor integration and recommended policies. VPC permissions Additional VPC permissions: ec2:DescribeInternetGateways ec2:DescribeVpcs ec2:DescribeNatGateways ec2:DescribeVpcEndpoints ec2:DescribeSubnets ec2:DescribeNetworkAcls ec2:DescribeVpcAttribute ec2:DescribeRouteTables ec2:DescribeSecurityGroups ec2:DescribeVpcPeeringConnections ec2:DescribeNetworkInterfaces ec2:DescribeVpnConnections X-Ray monitoring permissions Additional X-ray monitoring permissions: xray:BatchGet* xray:Get*",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 135.16354,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Integrations</em> and managed policies",
        "sections": "<em>Integrations</em> and managed policies",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "In order to use infrastructure <em>integrations</em>, you need to grant New Relic permission to read the relevant data from your account. <em>Amazon</em> Web Services (AWS) uses managed policies to grant these permissions. Recommended policy Important Recommendation: Grant an account-wide ReadOnlyAccess managed"
      },
      "id": "6045079fe7b9d27db95799d9"
    },
    {
      "sections": [
        "AWS service specific API rate limiting",
        "Problem",
        "Solution",
        "Verify your Infrastructure account's ARN",
        "Change the polling frequency",
        "Filter your data",
        "Review API usage",
        "Cause"
      ],
      "title": "AWS service specific API rate limiting",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Troubleshooting"
      ],
      "external_id": "785db1a9f5d5d9b89c2d304d1260ce5a8f30a680",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/troubleshooting/aws-service-specific-api-rate-limiting/",
      "published_at": "2021-05-05T15:10:41Z",
      "updated_at": "2021-03-13T03:22:51Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem After enabling Amazon integrations with New Relic Infrastructure, you encounter a rate limit for service-specific APIs. You might see this message in your AWS monitoring software, often with a 503 error: AWS::EC2::Errors::RequestLimitExceeded Request limit exceeded. Solution Verify your Infrastructure account's ARN Ensure that you are not collecting inventory information for the wrong ARN account. Verify that the ARN associated with your Infrastructure account is correct. Change the polling frequency The polling frequency determines how often New Relic gathers data from your cloud provider. By default, the polling frequency is set to the maximum frequency that is available for each service. If you reach your API rate limit, you may want to decrease the polling frequency. Filter your data You can set filters for each integration in order to specify which information you want captured. If you reach your API rate limit, you may want to filter your data. Review API usage To review the API usage for New Relic Infrastructure integrations with Amazon AWS: Go to one.newrelic.com > Infrastructure > AWS > Account status dashboard. Review the New Relic Insights dashboard, which appears automatically. The Insights dashboard includes a chart with your account's Amazon AWS API call count for the last month as well as the CloudWatch API calls (per AWS resource) for the last day. This information is the API usage for New Relic only. It does not include other AWS API or CloudWatch usage that may occur. For assistance determining which services may cause an increase in billing, get support at support.newrelic.com, or contact your New Relic account representative. Cause Infrastructure Amazon integrations leverage the AWS monitoring APIs to gather inventory data. AWS imposes hard rate limits on many of the AWS service-specific APIs consumed by New Relic Infrastructure integrations. Adding New Relic Amazon integrations will increase usage of the service-specific APIs and could impact how quickly you reach your rate limit. This may be caused by either of the following: Enabling Amazon integrations on several plugins for the same service Adding the incorrect Role ARN to your AWS integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 110.62892,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "Problem After enabling <em>Amazon</em> <em>integrations</em> with New Relic Infrastructure, you encounter a rate limit for service-specific APIs. You might see this message in your AWS monitoring software, often with a 503 error: AWS::EC2::Errors::RequestLimitExceeded Request limit exceeded. Solution Verify your"
      },
      "id": "604507c428ccbc013a2c60c4"
    }
  ],
  "/docs/integrations/amazon-integrations/troubleshooting/metric-data-delays-amazon-aws-integrations": [
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Automated setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-05-04T18:01:54Z",
      "updated_at": "2021-05-04T18:01:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Automated setup using CloudFormation We provide a CloudFormation template that automates this process. This needs to be applied to each AWS account and region you want to monitor in New Relic. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL (US Datacenter): https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL (EU Datacenter): https://cloud-collector.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = cloudwatch-metric-streams. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Similarly as with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 We plan to add metadata from most used AWS services. Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. New Relic and AWS teams are working together to offer more granular controls so that filters can be applied based on tags and other attributes. Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with eu then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and its not the case, please verify the following: Make sure theres no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 137.91467,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": " data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to <em>troubleshoot</em> your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS <em>Troubleshooting</em> docs"
      },
      "id": "606a036de7b9d2bfef9445f2"
    },
    {
      "sections": [
        "Integrations and managed policies",
        "Recommended policy",
        "Important",
        "Optional policy",
        "Option 1: Use our CloudFormation template",
        "CloudFormation template",
        "Option 2: Manually add permissions",
        "Required by all integrations",
        "ALB permissions",
        "API Gateway permissions",
        "Auto Scaling permissions",
        "Billing permissions",
        "Cloudfront permissions",
        "CloudTrail permissions",
        "DynamoDB permissions",
        "EBS permissions",
        "EC2 permissions",
        "ECS/ECR permissions",
        "EFS permissions",
        "ElastiCache permissions",
        "ElasticSearch permissions",
        "Elastic Beanstalk permissions",
        "ELB permissions",
        "EMR permissions",
        "Health permissions",
        "IAM permissions",
        "IoT permissions",
        "Kinesis Firehose permissions",
        "Kinesis Streams permissions",
        "Lambda permissions",
        "RDS, RDS Enhanced Monitoring permissions",
        "Redshift permissions",
        "Route 53 permissions",
        "S3 permissions",
        "Simple Email Service (SES) permissions",
        "SNS permissions",
        "SQS permissions",
        "Trusted Advisor permissions",
        "VPC permissions",
        "X-Ray monitoring permissions"
      ],
      "title": "Integrations and managed policies",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Get started"
      ],
      "external_id": "80e215e7b2ba382de1b7ea758ee1b1f0a1e3c7df",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/get-started/integrations-managed-policies/",
      "published_at": "2021-05-04T18:30:29Z",
      "updated_at": "2021-05-04T18:30:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In order to use infrastructure integrations, you need to grant New Relic permission to read the relevant data from your account. Amazon Web Services (AWS) uses managed policies to grant these permissions. Recommended policy Important Recommendation: Grant an account-wide ReadOnlyAccess managed policy from AWS. AWS automatically updates this policy when new services are added or existing services are modified. New Relic infrastructure integrations have been designed to function with ReadOnlyAccess policies. For instructions, see Connect AWS integrations to infrastructure. Exception: The Trusted Advisor integration is not covered by the ReadOnlyAccess policy. It requires the additional AWSSupportAccess managed policy. This is also the only integration that requires full access permissions (support:*) in order to correctly operate. We notified Amazon about this limitation. Once it's resolved we'll update documentation with more specific permissions required for this integration. Optional policy If you cannot use the ReadOnlyAccess managed policy from AWS, you can create your own customized policy based on the list of permissions. This allows you to specify the optimal permissions required to fetch data from AWS for each integration. While this option is available, it is not recommended because it must be manually updated when you add or modify your integrations. Important New Relic has no way of identifying problems related to custom permissions. If you choose to create a custom policy, it is your responsibility to maintain it and ensure proper data is being collected. There are two ways to set up your customized policy: You can either use our CloudFormation template, or create own yourself by adding the permissions you need. Option 1: Use our CloudFormation template Our CloudFormation template contains all the permissions for all our AWS integrations. A user different than root can be used in the managed policy. CloudFormation template AWSTemplateFormatVersion: 2010-09-09 Outputs: NewRelicRoleArn: Description: NewRelicRole to monitor AWS Lambda Value: !GetAtt - NewRelicIntegrationsTemplate - Arn Parameters: NewRelicAccountNumber: Type: String Description: The Newrelic account number to send data AllowedPattern: '[0-9]+' Resources: NewRelicIntegrationsTemplate: Type: 'AWS::IAM::Role' Properties: RoleName: !Sub NewRelicTemplateTest AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: AWS: !Sub 'arn:aws:iam::754728514883:root' Action: 'sts:AssumeRole' Condition: StringEquals: 'sts:ExternalId': !Ref NewRelicAccountNumber Policies: - PolicyName: NewRelicIntegrations PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'elasticloadbalancing:DescribeLoadBalancers' - 'elasticloadbalancing:DescribeTargetGroups' - 'elasticloadbalancing:DescribeTags' - 'elasticloadbalancing:DescribeLoadBalancerAttributes' - 'elasticloadbalancing:DescribeListeners' - 'elasticloadbalancing:DescribeRules' - 'elasticloadbalancing:DescribeTargetGroupAttributes' - 'elasticloadbalancing:DescribeInstanceHealth' - 'elasticloadbalancing:DescribeLoadBalancerPolicies' - 'elasticloadbalancing:DescribeLoadBalancerPolicyTypes' - 'apigateway:GET' - 'apigateway:HEAD' - 'apigateway:OPTIONS' - 'autoscaling:DescribeLaunchConfigurations' - 'autoscaling:DescribeAutoScalingGroups' - 'autoscaling:DescribePolicies' - 'autoscaling:DescribeTags' - 'autoscaling:DescribeAccountLimits' - 'budgets:ViewBilling' - 'budgets:ViewBudget' - 'cloudfront:ListDistributions' - 'cloudfront:ListStreamingDistributions' - 'cloudfront:ListTagsForResource' - 'cloudtrail:LookupEvents' - 'config:BatchGetResourceConfig' - 'config:ListDiscoveredResources' - 'dynamodb:DescribeLimits' - 'dynamodb:ListTables' - 'dynamodb:DescribeTable' - 'dynamodb:ListGlobalTables' - 'dynamodb:DescribeGlobalTable' - 'dynamodb:ListTagsOfResource' - 'ec2:DescribeVolumeStatus' - 'ec2:DescribeVolumes' - 'ec2:DescribeVolumeAttribute' - 'ec2:DescribeInstanceStatus' - 'ec2:DescribeInstances' - 'ec2:DescribeVpnConnections' - 'ecs:ListServices' - 'ecs:DescribeServices' - 'ecs:DescribeClusters' - 'ecs:ListClusters' - 'ecs:ListTagsForResource' - 'ecs:ListContainerInstances' - 'ecs:DescribeContainerInstances' - 'elasticfilesystem:DescribeMountTargets' - 'elasticfilesystem:DescribeFileSystems' - 'elasticache:DescribeCacheClusters' - 'elasticache:ListTagsForResource' - 'es:ListDomainNames' - 'es:DescribeElasticsearchDomain' - 'es:DescribeElasticsearchDomains' - 'es:ListTags' - 'elasticbeanstalk:DescribeEnvironments' - 'elasticbeanstalk:DescribeInstancesHealth' - 'elasticbeanstalk:DescribeConfigurationSettings' - 'elasticloadbalancing:DescribeLoadBalancers' - 'elasticmapreduce:ListInstances' - 'elasticmapreduce:ListClusters' - 'elasticmapreduce:DescribeCluster' - 'elasticmapreduce:ListInstanceGroups' - 'health:DescribeAffectedEntities' - 'health:DescribeEventDetails' - 'health:DescribeEvents' - 'iam:ListSAMLProviders' - 'iam:ListOpenIDConnectProviders' - 'iam:ListServerCertificates' - 'iam:GetAccountAuthorizationDetails' - 'iam:ListVirtualMFADevices' - 'iam:GetAccountSummary' - 'iot:ListTopicRules' - 'iot:GetTopicRule' - 'iot:ListThings' - 'firehose:DescribeDeliveryStream' - 'firehose:ListDeliveryStreams' - 'kinesis:ListStreams' - 'kinesis:DescribeStream' - 'kinesis:ListTagsForStream' - 'rds:ListTagsForResource' - 'rds:DescribeDBInstances' - 'rds:DescribeDBClusters' - 'redshift:DescribeClusters' - 'redshift:DescribeClusterParameters' - 'route53:ListHealthChecks' - 'route53:GetHostedZone' - 'route53:ListHostedZones' - 'route53:ListResourceRecordSets' - 'route53:ListTagsForResources' - 's3:GetLifecycleConfiguration' - 's3:GetBucketTagging' - 's3:ListAllMyBuckets' - 's3:GetBucketWebsite' - 's3:GetBucketLogging' - 's3:GetBucketCORS' - 's3:GetBucketVersioning' - 's3:GetBucketAcl' - 's3:GetBucketNotification' - 's3:GetBucketPolicy' - 's3:GetReplicationConfiguration' - 's3:GetMetricsConfiguration' - 's3:GetAccelerateConfiguration' - 's3:GetAnalyticsConfiguration' - 's3:GetBucketLocation' - 's3:GetBucketRequestPayment' - 's3:GetEncryptionConfiguration' - 's3:GetInventoryConfiguration' - 's3:GetIpConfiguration' - 'ses:ListConfigurationSets' - 'ses:GetSendQuota' - 'ses:DescribeConfigurationSet' - 'ses:ListReceiptFilters' - 'ses:ListReceiptRuleSets' - 'ses:DescribeReceiptRule' - 'ses:DescribeReceiptRuleSet' - 'sns:GetTopicAttributes' - 'sns:ListTopics' - 'sqs:ListQueues' - 'sqs:ListQueueTags' - 'sqs:GetQueueAttributes' - 'tag:GetResources' - 'ec2:DescribeInternetGateways' - 'ec2:DescribeVpcs' - 'ec2:DescribeNatGateways' - 'ec2:DescribeVpcEndpoints' - 'ec2:DescribeSubnets' - 'ec2:DescribeNetworkAcls' - 'ec2:DescribeVpcAttribute' - 'ec2:DescribeRouteTables' - 'ec2:DescribeSecurityGroups' - 'ec2:DescribeVpcPeeringConnections' - 'ec2:DescribeNetworkInterfaces' - 'lambda:GetAccountSettings' - 'lambda:ListFunctions' - 'lambda:ListAliases' - 'lambda:ListTags' - 'lambda:ListEventSourceMappings' - 'cloudwatch:GetMetricStatistics' - 'cloudwatch:ListMetrics' - 'cloudwatch:GetMetricData' - 'support:*' Resource: '*' Copy Option 2: Manually add permissions To create your own policy using available permissions: Add the permissions for all integrations. Add permissions that are specific to the integrations you need The following permissions are used by New Relic to retrieve data for specific AWS integrations: Required by all integrations Important If an integration is not listed on this page, these permissions are all you need. All integrations Permissions CloudWatch cloudwatch:GetMetricStatistics cloudwatch:ListMetrics cloudwatch:GetMetricData Config API config:BatchGetResourceConfig config:ListDiscoveredResources Resource Tagging API tag:GetResources ALB permissions Additional ALB permissions: elasticloadbalancing:DescribeLoadBalancers elasticloadbalancing:DescribeTargetGroups elasticloadbalancing:DescribeTags elasticloadbalancing:DescribeLoadBalancerAttributes elasticloadbalancing:DescribeListeners elasticloadbalancing:DescribeRules elasticloadbalancing:DescribeTargetGroupAttributes elasticloadbalancing:DescribeInstanceHealth elasticloadbalancing:DescribeLoadBalancerPolicies elasticloadbalancing:DescribeLoadBalancerPolicyTypes API Gateway permissions Additional API Gateway permissions: apigateway:GET apigateway:HEAD apigateway:OPTIONS Auto Scaling permissions Additional Auto Scaling permissions: autoscaling:DescribeLaunchConfigurations autoscaling:DescribeAutoScalingGroups autoscaling:DescribePolicies autoscaling:DescribeTags autoscaling:DescribeAccountLimits Billing permissions Additional Billing permissions: budgets:ViewBilling budgets:ViewBudget Cloudfront permissions Additional Cloudfront permissions: cloudfront:ListDistributions cloudfront:ListStreamingDistributions cloudfront:ListTagsForResource CloudTrail permissions Additional CloudTrail permissions: cloudtrail:LookupEvents DynamoDB permissions Additional DynamoDB permissions: dynamodb:DescribeLimits dynamodb:ListTables dynamodb:DescribeTable dynamodb:ListGlobalTables dynamodb:DescribeGlobalTable dynamodb:ListTagsOfResource EBS permissions Additional EBS permissions: ec2:DescribeVolumeStatus ec2:DescribeVolumes ec2:DescribeVolumeAttribute EC2 permissions Additional EC2 permissions: ec2:DescribeInstanceStatus ec2:DescribeInstances ECS/ECR permissions Additional ECS/ECR permissions: ecs:ListServices ecs:DescribeServices ecs:DescribeClusters ecs:ListClusters ecs:ListTagsForResource ecs:ListContainerInstances ecs:DescribeContainerInstances EFS permissions Additional EFS permissions: elasticfilesystem:DescribeMountTargets elasticfilesystem:DescribeFileSystems ElastiCache permissions Additional ElastiCache permissions: elasticache:DescribeCacheClusters elasticache:ListTagsForResource ElasticSearch permissions Additional ElasticSearch permissions: es:ListDomainNames es:DescribeElasticsearchDomain es:DescribeElasticsearchDomains es:ListTags Elastic Beanstalk permissions Additional Elastic Beanstalk permissions: elasticbeanstalk:DescribeEnvironments elasticbeanstalk:DescribeInstancesHealth elasticbeanstalk:DescribeConfigurationSettings ELB permissions Additional ELB permissions: elasticloadbalancing:DescribeLoadBalancers EMR permissions Additional EMR permissions: elasticmapreduce:ListInstances elasticmapreduce:ListClusters elasticmapreduce:DescribeCluster elasticmapreduce:ListInstanceGroups elasticmapreduce:ListInstanceFleets Health permissions Additional Health permissions: health:DescribeAffectedEntities health:DescribeEventDetails health:DescribeEvents IAM permissions Additional IAM permissions: iam:ListSAMLProviders iam:ListOpenIDConnectProviders iam:ListServerCertificates iam:GetAccountAuthorizationDetails iam:ListVirtualMFADevices iam:GetAccountSummary IoT permissions Additional IoT permissions: iot:ListTopicRules iot:GetTopicRule iot:ListThings Kinesis Firehose permissions Additional Kinesis Firehose permissions: firehose:DescribeDeliveryStream firehose:ListDeliveryStreams Kinesis Streams permissions Additional Kinesis Streams permissions: kinesis:ListStreams kinesis:DescribeStream kinesis:ListTagsForStream Lambda permissions Additional Lambda permissions: lambda:GetAccountSettings lambda:ListFunctions lambda:ListAliases lambda:ListTags lambda:ListEventSourceMappings RDS, RDS Enhanced Monitoring permissions Additional RDS and RDS Enhanced Monitoring permissions: rds:ListTagsForResource rds:DescribeDBInstances rds:DescribeDBClusters Redshift permissions Additional Redshift permissions: redshift:DescribeClusters redshift:DescribeClusterParameters Route 53 permissions Additional Route 53 permissions: route53:ListHealthChecks route53:GetHostedZone route53:ListHostedZones route53:ListResourceRecordSets route53:ListTagsForResources S3 permissions Additional S3 permissions: s3:GetLifecycleConfiguration s3:GetBucketTagging s3:ListAllMyBuckets s3:GetBucketWebsite s3:GetBucketLogging s3:GetBucketCORS s3:GetBucketVersioning s3:GetBucketAcl s3:GetBucketNotification s3:GetBucketPolicy s3:GetReplicationConfiguration s3:GetMetricsConfiguration s3:GetAccelerateConfiguration s3:GetAnalyticsConfiguration s3:GetBucketLocation s3:GetBucketRequestPayment s3:GetEncryptionConfiguration s3:GetInventoryConfiguration s3:GetIpConfiguration Simple Email Service (SES) permissions Additional SES permissions: ses:ListConfigurationSets ses:GetSendQuota ses:DescribeConfigurationSet ses:ListReceiptFilters ses:ListReceiptRuleSets ses:DescribeReceiptRule ses:DescribeReceiptRuleSet SNS permissions Additional SNS permissions: sns:GetTopicAttributes sns:ListTopics SQS permissions Additional SQS permissions: sqs:ListQueues sqs:GetQueueAttributes sqs:ListQueueTags Trusted Advisor permissions Additional Trusted Advisor permissions: support:* See also the note about the Trusted Advisor integration and recommended policies. VPC permissions Additional VPC permissions: ec2:DescribeInternetGateways ec2:DescribeVpcs ec2:DescribeNatGateways ec2:DescribeVpcEndpoints ec2:DescribeSubnets ec2:DescribeNetworkAcls ec2:DescribeVpcAttribute ec2:DescribeRouteTables ec2:DescribeSecurityGroups ec2:DescribeVpcPeeringConnections ec2:DescribeNetworkInterfaces ec2:DescribeVpnConnections X-Ray monitoring permissions Additional X-ray monitoring permissions: xray:BatchGet* xray:Get*",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 135.16354,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Integrations</em> and managed policies",
        "sections": "<em>Integrations</em> and managed policies",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "In order to use infrastructure <em>integrations</em>, you need to grant New Relic permission to read the relevant data from your account. <em>Amazon</em> Web Services (AWS) uses managed policies to grant these permissions. Recommended policy Important Recommendation: Grant an account-wide ReadOnlyAccess managed"
      },
      "id": "6045079fe7b9d27db95799d9"
    },
    {
      "sections": [
        "AWS service specific API rate limiting",
        "Problem",
        "Solution",
        "Verify your Infrastructure account's ARN",
        "Change the polling frequency",
        "Filter your data",
        "Review API usage",
        "Cause"
      ],
      "title": "AWS service specific API rate limiting",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Troubleshooting"
      ],
      "external_id": "785db1a9f5d5d9b89c2d304d1260ce5a8f30a680",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/troubleshooting/aws-service-specific-api-rate-limiting/",
      "published_at": "2021-05-05T15:10:41Z",
      "updated_at": "2021-03-13T03:22:51Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem After enabling Amazon integrations with New Relic Infrastructure, you encounter a rate limit for service-specific APIs. You might see this message in your AWS monitoring software, often with a 503 error: AWS::EC2::Errors::RequestLimitExceeded Request limit exceeded. Solution Verify your Infrastructure account's ARN Ensure that you are not collecting inventory information for the wrong ARN account. Verify that the ARN associated with your Infrastructure account is correct. Change the polling frequency The polling frequency determines how often New Relic gathers data from your cloud provider. By default, the polling frequency is set to the maximum frequency that is available for each service. If you reach your API rate limit, you may want to decrease the polling frequency. Filter your data You can set filters for each integration in order to specify which information you want captured. If you reach your API rate limit, you may want to filter your data. Review API usage To review the API usage for New Relic Infrastructure integrations with Amazon AWS: Go to one.newrelic.com > Infrastructure > AWS > Account status dashboard. Review the New Relic Insights dashboard, which appears automatically. The Insights dashboard includes a chart with your account's Amazon AWS API call count for the last month as well as the CloudWatch API calls (per AWS resource) for the last day. This information is the API usage for New Relic only. It does not include other AWS API or CloudWatch usage that may occur. For assistance determining which services may cause an increase in billing, get support at support.newrelic.com, or contact your New Relic account representative. Cause Infrastructure Amazon integrations leverage the AWS monitoring APIs to gather inventory data. AWS imposes hard rate limits on many of the AWS service-specific APIs consumed by New Relic Infrastructure integrations. Adding New Relic Amazon integrations will increase usage of the service-specific APIs and could impact how quickly you reach your rate limit. This may be caused by either of the following: Enabling Amazon integrations on several plugins for the same service Adding the incorrect Role ARN to your AWS integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 110.62892,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "Problem After enabling <em>Amazon</em> <em>integrations</em> with New Relic Infrastructure, you encounter a rate limit for service-specific APIs. You might see this message in your AWS monitoring software, often with a 503 error: AWS::EC2::Errors::RequestLimitExceeded Request limit exceeded. Solution Verify your"
      },
      "id": "604507c428ccbc013a2c60c4"
    }
  ],
  "/docs/integrations/amazon-integrations/troubleshooting/no-data-appears-aws-integrations": [
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Automated setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-05-04T18:01:54Z",
      "updated_at": "2021-05-04T18:01:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Automated setup using CloudFormation We provide a CloudFormation template that automates this process. This needs to be applied to each AWS account and region you want to monitor in New Relic. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL (US Datacenter): https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL (EU Datacenter): https://cloud-collector.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = cloudwatch-metric-streams. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Similarly as with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 We plan to add metadata from most used AWS services. Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. New Relic and AWS teams are working together to offer more granular controls so that filters can be applied based on tags and other attributes. Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with eu then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and its not the case, please verify the following: Make sure theres no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 137.91458,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": " data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to <em>troubleshoot</em> your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS <em>Troubleshooting</em> docs"
      },
      "id": "606a036de7b9d2bfef9445f2"
    },
    {
      "sections": [
        "Integrations and managed policies",
        "Recommended policy",
        "Important",
        "Optional policy",
        "Option 1: Use our CloudFormation template",
        "CloudFormation template",
        "Option 2: Manually add permissions",
        "Required by all integrations",
        "ALB permissions",
        "API Gateway permissions",
        "Auto Scaling permissions",
        "Billing permissions",
        "Cloudfront permissions",
        "CloudTrail permissions",
        "DynamoDB permissions",
        "EBS permissions",
        "EC2 permissions",
        "ECS/ECR permissions",
        "EFS permissions",
        "ElastiCache permissions",
        "ElasticSearch permissions",
        "Elastic Beanstalk permissions",
        "ELB permissions",
        "EMR permissions",
        "Health permissions",
        "IAM permissions",
        "IoT permissions",
        "Kinesis Firehose permissions",
        "Kinesis Streams permissions",
        "Lambda permissions",
        "RDS, RDS Enhanced Monitoring permissions",
        "Redshift permissions",
        "Route 53 permissions",
        "S3 permissions",
        "Simple Email Service (SES) permissions",
        "SNS permissions",
        "SQS permissions",
        "Trusted Advisor permissions",
        "VPC permissions",
        "X-Ray monitoring permissions"
      ],
      "title": "Integrations and managed policies",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Get started"
      ],
      "external_id": "80e215e7b2ba382de1b7ea758ee1b1f0a1e3c7df",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/get-started/integrations-managed-policies/",
      "published_at": "2021-05-04T18:30:29Z",
      "updated_at": "2021-05-04T18:30:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In order to use infrastructure integrations, you need to grant New Relic permission to read the relevant data from your account. Amazon Web Services (AWS) uses managed policies to grant these permissions. Recommended policy Important Recommendation: Grant an account-wide ReadOnlyAccess managed policy from AWS. AWS automatically updates this policy when new services are added or existing services are modified. New Relic infrastructure integrations have been designed to function with ReadOnlyAccess policies. For instructions, see Connect AWS integrations to infrastructure. Exception: The Trusted Advisor integration is not covered by the ReadOnlyAccess policy. It requires the additional AWSSupportAccess managed policy. This is also the only integration that requires full access permissions (support:*) in order to correctly operate. We notified Amazon about this limitation. Once it's resolved we'll update documentation with more specific permissions required for this integration. Optional policy If you cannot use the ReadOnlyAccess managed policy from AWS, you can create your own customized policy based on the list of permissions. This allows you to specify the optimal permissions required to fetch data from AWS for each integration. While this option is available, it is not recommended because it must be manually updated when you add or modify your integrations. Important New Relic has no way of identifying problems related to custom permissions. If you choose to create a custom policy, it is your responsibility to maintain it and ensure proper data is being collected. There are two ways to set up your customized policy: You can either use our CloudFormation template, or create own yourself by adding the permissions you need. Option 1: Use our CloudFormation template Our CloudFormation template contains all the permissions for all our AWS integrations. A user different than root can be used in the managed policy. CloudFormation template AWSTemplateFormatVersion: 2010-09-09 Outputs: NewRelicRoleArn: Description: NewRelicRole to monitor AWS Lambda Value: !GetAtt - NewRelicIntegrationsTemplate - Arn Parameters: NewRelicAccountNumber: Type: String Description: The Newrelic account number to send data AllowedPattern: '[0-9]+' Resources: NewRelicIntegrationsTemplate: Type: 'AWS::IAM::Role' Properties: RoleName: !Sub NewRelicTemplateTest AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: AWS: !Sub 'arn:aws:iam::754728514883:root' Action: 'sts:AssumeRole' Condition: StringEquals: 'sts:ExternalId': !Ref NewRelicAccountNumber Policies: - PolicyName: NewRelicIntegrations PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'elasticloadbalancing:DescribeLoadBalancers' - 'elasticloadbalancing:DescribeTargetGroups' - 'elasticloadbalancing:DescribeTags' - 'elasticloadbalancing:DescribeLoadBalancerAttributes' - 'elasticloadbalancing:DescribeListeners' - 'elasticloadbalancing:DescribeRules' - 'elasticloadbalancing:DescribeTargetGroupAttributes' - 'elasticloadbalancing:DescribeInstanceHealth' - 'elasticloadbalancing:DescribeLoadBalancerPolicies' - 'elasticloadbalancing:DescribeLoadBalancerPolicyTypes' - 'apigateway:GET' - 'apigateway:HEAD' - 'apigateway:OPTIONS' - 'autoscaling:DescribeLaunchConfigurations' - 'autoscaling:DescribeAutoScalingGroups' - 'autoscaling:DescribePolicies' - 'autoscaling:DescribeTags' - 'autoscaling:DescribeAccountLimits' - 'budgets:ViewBilling' - 'budgets:ViewBudget' - 'cloudfront:ListDistributions' - 'cloudfront:ListStreamingDistributions' - 'cloudfront:ListTagsForResource' - 'cloudtrail:LookupEvents' - 'config:BatchGetResourceConfig' - 'config:ListDiscoveredResources' - 'dynamodb:DescribeLimits' - 'dynamodb:ListTables' - 'dynamodb:DescribeTable' - 'dynamodb:ListGlobalTables' - 'dynamodb:DescribeGlobalTable' - 'dynamodb:ListTagsOfResource' - 'ec2:DescribeVolumeStatus' - 'ec2:DescribeVolumes' - 'ec2:DescribeVolumeAttribute' - 'ec2:DescribeInstanceStatus' - 'ec2:DescribeInstances' - 'ec2:DescribeVpnConnections' - 'ecs:ListServices' - 'ecs:DescribeServices' - 'ecs:DescribeClusters' - 'ecs:ListClusters' - 'ecs:ListTagsForResource' - 'ecs:ListContainerInstances' - 'ecs:DescribeContainerInstances' - 'elasticfilesystem:DescribeMountTargets' - 'elasticfilesystem:DescribeFileSystems' - 'elasticache:DescribeCacheClusters' - 'elasticache:ListTagsForResource' - 'es:ListDomainNames' - 'es:DescribeElasticsearchDomain' - 'es:DescribeElasticsearchDomains' - 'es:ListTags' - 'elasticbeanstalk:DescribeEnvironments' - 'elasticbeanstalk:DescribeInstancesHealth' - 'elasticbeanstalk:DescribeConfigurationSettings' - 'elasticloadbalancing:DescribeLoadBalancers' - 'elasticmapreduce:ListInstances' - 'elasticmapreduce:ListClusters' - 'elasticmapreduce:DescribeCluster' - 'elasticmapreduce:ListInstanceGroups' - 'health:DescribeAffectedEntities' - 'health:DescribeEventDetails' - 'health:DescribeEvents' - 'iam:ListSAMLProviders' - 'iam:ListOpenIDConnectProviders' - 'iam:ListServerCertificates' - 'iam:GetAccountAuthorizationDetails' - 'iam:ListVirtualMFADevices' - 'iam:GetAccountSummary' - 'iot:ListTopicRules' - 'iot:GetTopicRule' - 'iot:ListThings' - 'firehose:DescribeDeliveryStream' - 'firehose:ListDeliveryStreams' - 'kinesis:ListStreams' - 'kinesis:DescribeStream' - 'kinesis:ListTagsForStream' - 'rds:ListTagsForResource' - 'rds:DescribeDBInstances' - 'rds:DescribeDBClusters' - 'redshift:DescribeClusters' - 'redshift:DescribeClusterParameters' - 'route53:ListHealthChecks' - 'route53:GetHostedZone' - 'route53:ListHostedZones' - 'route53:ListResourceRecordSets' - 'route53:ListTagsForResources' - 's3:GetLifecycleConfiguration' - 's3:GetBucketTagging' - 's3:ListAllMyBuckets' - 's3:GetBucketWebsite' - 's3:GetBucketLogging' - 's3:GetBucketCORS' - 's3:GetBucketVersioning' - 's3:GetBucketAcl' - 's3:GetBucketNotification' - 's3:GetBucketPolicy' - 's3:GetReplicationConfiguration' - 's3:GetMetricsConfiguration' - 's3:GetAccelerateConfiguration' - 's3:GetAnalyticsConfiguration' - 's3:GetBucketLocation' - 's3:GetBucketRequestPayment' - 's3:GetEncryptionConfiguration' - 's3:GetInventoryConfiguration' - 's3:GetIpConfiguration' - 'ses:ListConfigurationSets' - 'ses:GetSendQuota' - 'ses:DescribeConfigurationSet' - 'ses:ListReceiptFilters' - 'ses:ListReceiptRuleSets' - 'ses:DescribeReceiptRule' - 'ses:DescribeReceiptRuleSet' - 'sns:GetTopicAttributes' - 'sns:ListTopics' - 'sqs:ListQueues' - 'sqs:ListQueueTags' - 'sqs:GetQueueAttributes' - 'tag:GetResources' - 'ec2:DescribeInternetGateways' - 'ec2:DescribeVpcs' - 'ec2:DescribeNatGateways' - 'ec2:DescribeVpcEndpoints' - 'ec2:DescribeSubnets' - 'ec2:DescribeNetworkAcls' - 'ec2:DescribeVpcAttribute' - 'ec2:DescribeRouteTables' - 'ec2:DescribeSecurityGroups' - 'ec2:DescribeVpcPeeringConnections' - 'ec2:DescribeNetworkInterfaces' - 'lambda:GetAccountSettings' - 'lambda:ListFunctions' - 'lambda:ListAliases' - 'lambda:ListTags' - 'lambda:ListEventSourceMappings' - 'cloudwatch:GetMetricStatistics' - 'cloudwatch:ListMetrics' - 'cloudwatch:GetMetricData' - 'support:*' Resource: '*' Copy Option 2: Manually add permissions To create your own policy using available permissions: Add the permissions for all integrations. Add permissions that are specific to the integrations you need The following permissions are used by New Relic to retrieve data for specific AWS integrations: Required by all integrations Important If an integration is not listed on this page, these permissions are all you need. All integrations Permissions CloudWatch cloudwatch:GetMetricStatistics cloudwatch:ListMetrics cloudwatch:GetMetricData Config API config:BatchGetResourceConfig config:ListDiscoveredResources Resource Tagging API tag:GetResources ALB permissions Additional ALB permissions: elasticloadbalancing:DescribeLoadBalancers elasticloadbalancing:DescribeTargetGroups elasticloadbalancing:DescribeTags elasticloadbalancing:DescribeLoadBalancerAttributes elasticloadbalancing:DescribeListeners elasticloadbalancing:DescribeRules elasticloadbalancing:DescribeTargetGroupAttributes elasticloadbalancing:DescribeInstanceHealth elasticloadbalancing:DescribeLoadBalancerPolicies elasticloadbalancing:DescribeLoadBalancerPolicyTypes API Gateway permissions Additional API Gateway permissions: apigateway:GET apigateway:HEAD apigateway:OPTIONS Auto Scaling permissions Additional Auto Scaling permissions: autoscaling:DescribeLaunchConfigurations autoscaling:DescribeAutoScalingGroups autoscaling:DescribePolicies autoscaling:DescribeTags autoscaling:DescribeAccountLimits Billing permissions Additional Billing permissions: budgets:ViewBilling budgets:ViewBudget Cloudfront permissions Additional Cloudfront permissions: cloudfront:ListDistributions cloudfront:ListStreamingDistributions cloudfront:ListTagsForResource CloudTrail permissions Additional CloudTrail permissions: cloudtrail:LookupEvents DynamoDB permissions Additional DynamoDB permissions: dynamodb:DescribeLimits dynamodb:ListTables dynamodb:DescribeTable dynamodb:ListGlobalTables dynamodb:DescribeGlobalTable dynamodb:ListTagsOfResource EBS permissions Additional EBS permissions: ec2:DescribeVolumeStatus ec2:DescribeVolumes ec2:DescribeVolumeAttribute EC2 permissions Additional EC2 permissions: ec2:DescribeInstanceStatus ec2:DescribeInstances ECS/ECR permissions Additional ECS/ECR permissions: ecs:ListServices ecs:DescribeServices ecs:DescribeClusters ecs:ListClusters ecs:ListTagsForResource ecs:ListContainerInstances ecs:DescribeContainerInstances EFS permissions Additional EFS permissions: elasticfilesystem:DescribeMountTargets elasticfilesystem:DescribeFileSystems ElastiCache permissions Additional ElastiCache permissions: elasticache:DescribeCacheClusters elasticache:ListTagsForResource ElasticSearch permissions Additional ElasticSearch permissions: es:ListDomainNames es:DescribeElasticsearchDomain es:DescribeElasticsearchDomains es:ListTags Elastic Beanstalk permissions Additional Elastic Beanstalk permissions: elasticbeanstalk:DescribeEnvironments elasticbeanstalk:DescribeInstancesHealth elasticbeanstalk:DescribeConfigurationSettings ELB permissions Additional ELB permissions: elasticloadbalancing:DescribeLoadBalancers EMR permissions Additional EMR permissions: elasticmapreduce:ListInstances elasticmapreduce:ListClusters elasticmapreduce:DescribeCluster elasticmapreduce:ListInstanceGroups elasticmapreduce:ListInstanceFleets Health permissions Additional Health permissions: health:DescribeAffectedEntities health:DescribeEventDetails health:DescribeEvents IAM permissions Additional IAM permissions: iam:ListSAMLProviders iam:ListOpenIDConnectProviders iam:ListServerCertificates iam:GetAccountAuthorizationDetails iam:ListVirtualMFADevices iam:GetAccountSummary IoT permissions Additional IoT permissions: iot:ListTopicRules iot:GetTopicRule iot:ListThings Kinesis Firehose permissions Additional Kinesis Firehose permissions: firehose:DescribeDeliveryStream firehose:ListDeliveryStreams Kinesis Streams permissions Additional Kinesis Streams permissions: kinesis:ListStreams kinesis:DescribeStream kinesis:ListTagsForStream Lambda permissions Additional Lambda permissions: lambda:GetAccountSettings lambda:ListFunctions lambda:ListAliases lambda:ListTags lambda:ListEventSourceMappings RDS, RDS Enhanced Monitoring permissions Additional RDS and RDS Enhanced Monitoring permissions: rds:ListTagsForResource rds:DescribeDBInstances rds:DescribeDBClusters Redshift permissions Additional Redshift permissions: redshift:DescribeClusters redshift:DescribeClusterParameters Route 53 permissions Additional Route 53 permissions: route53:ListHealthChecks route53:GetHostedZone route53:ListHostedZones route53:ListResourceRecordSets route53:ListTagsForResources S3 permissions Additional S3 permissions: s3:GetLifecycleConfiguration s3:GetBucketTagging s3:ListAllMyBuckets s3:GetBucketWebsite s3:GetBucketLogging s3:GetBucketCORS s3:GetBucketVersioning s3:GetBucketAcl s3:GetBucketNotification s3:GetBucketPolicy s3:GetReplicationConfiguration s3:GetMetricsConfiguration s3:GetAccelerateConfiguration s3:GetAnalyticsConfiguration s3:GetBucketLocation s3:GetBucketRequestPayment s3:GetEncryptionConfiguration s3:GetInventoryConfiguration s3:GetIpConfiguration Simple Email Service (SES) permissions Additional SES permissions: ses:ListConfigurationSets ses:GetSendQuota ses:DescribeConfigurationSet ses:ListReceiptFilters ses:ListReceiptRuleSets ses:DescribeReceiptRule ses:DescribeReceiptRuleSet SNS permissions Additional SNS permissions: sns:GetTopicAttributes sns:ListTopics SQS permissions Additional SQS permissions: sqs:ListQueues sqs:GetQueueAttributes sqs:ListQueueTags Trusted Advisor permissions Additional Trusted Advisor permissions: support:* See also the note about the Trusted Advisor integration and recommended policies. VPC permissions Additional VPC permissions: ec2:DescribeInternetGateways ec2:DescribeVpcs ec2:DescribeNatGateways ec2:DescribeVpcEndpoints ec2:DescribeSubnets ec2:DescribeNetworkAcls ec2:DescribeVpcAttribute ec2:DescribeRouteTables ec2:DescribeSecurityGroups ec2:DescribeVpcPeeringConnections ec2:DescribeNetworkInterfaces ec2:DescribeVpnConnections X-Ray monitoring permissions Additional X-ray monitoring permissions: xray:BatchGet* xray:Get*",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 135.16345,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Integrations</em> and managed policies",
        "sections": "<em>Integrations</em> and managed policies",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "In order to use infrastructure <em>integrations</em>, you need to grant New Relic permission to read the relevant data from your account. <em>Amazon</em> Web Services (AWS) uses managed policies to grant these permissions. Recommended policy Important Recommendation: Grant an account-wide ReadOnlyAccess managed"
      },
      "id": "6045079fe7b9d27db95799d9"
    },
    {
      "sections": [
        "AWS service specific API rate limiting",
        "Problem",
        "Solution",
        "Verify your Infrastructure account's ARN",
        "Change the polling frequency",
        "Filter your data",
        "Review API usage",
        "Cause"
      ],
      "title": "AWS service specific API rate limiting",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Troubleshooting"
      ],
      "external_id": "785db1a9f5d5d9b89c2d304d1260ce5a8f30a680",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/troubleshooting/aws-service-specific-api-rate-limiting/",
      "published_at": "2021-05-05T15:10:41Z",
      "updated_at": "2021-03-13T03:22:51Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem After enabling Amazon integrations with New Relic Infrastructure, you encounter a rate limit for service-specific APIs. You might see this message in your AWS monitoring software, often with a 503 error: AWS::EC2::Errors::RequestLimitExceeded Request limit exceeded. Solution Verify your Infrastructure account's ARN Ensure that you are not collecting inventory information for the wrong ARN account. Verify that the ARN associated with your Infrastructure account is correct. Change the polling frequency The polling frequency determines how often New Relic gathers data from your cloud provider. By default, the polling frequency is set to the maximum frequency that is available for each service. If you reach your API rate limit, you may want to decrease the polling frequency. Filter your data You can set filters for each integration in order to specify which information you want captured. If you reach your API rate limit, you may want to filter your data. Review API usage To review the API usage for New Relic Infrastructure integrations with Amazon AWS: Go to one.newrelic.com > Infrastructure > AWS > Account status dashboard. Review the New Relic Insights dashboard, which appears automatically. The Insights dashboard includes a chart with your account's Amazon AWS API call count for the last month as well as the CloudWatch API calls (per AWS resource) for the last day. This information is the API usage for New Relic only. It does not include other AWS API or CloudWatch usage that may occur. For assistance determining which services may cause an increase in billing, get support at support.newrelic.com, or contact your New Relic account representative. Cause Infrastructure Amazon integrations leverage the AWS monitoring APIs to gather inventory data. AWS imposes hard rate limits on many of the AWS service-specific APIs consumed by New Relic Infrastructure integrations. Adding New Relic Amazon integrations will increase usage of the service-specific APIs and could impact how quickly you reach your rate limit. This may be caused by either of the following: Enabling Amazon integrations on several plugins for the same service Adding the incorrect Role ARN to your AWS integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 110.62892,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "Problem After enabling <em>Amazon</em> <em>integrations</em> with New Relic Infrastructure, you encounter a rate limit for service-specific APIs. You might see this message in your AWS monitoring software, often with a 503 error: AWS::EC2::Errors::RequestLimitExceeded Request limit exceeded. Solution Verify your"
      },
      "id": "604507c428ccbc013a2c60c4"
    }
  ],
  "/docs/integrations/amazon-integrations/troubleshooting/partial-or-missing-logs-rds-vpc-aws-lambda": [
    {
      "sections": [
        "Amazon CloudWatch Metric Streams integration",
        "Why does this matter?",
        "Set up a Metric Stream to send CloudWatch metrics to New Relic",
        "How to map New Relic and AWS accounts and regions",
        "Automated setup using CloudFormation",
        "Manual setup using AWS Console, API, or calls",
        "Validate your data is received correctly",
        "Metrics naming convention",
        "Query Experience, metric storage and mapping",
        "AWS namespaces' entities in the New Relic Explorer",
        "Important",
        "Set alert conditions",
        "Tags collection",
        "Metadata collection",
        "Curated dashboards",
        "Get access to the Quickstarts App",
        "Import dashboards from Quickstarts App",
        "Manage your data",
        "Migrating from poll-based AWS integrations",
        "Query, dashboard, and alert considerations",
        "Troubleshooting",
        "No metrics or errors appear on New Relic",
        "Missing metrics for certain AWS namespaces",
        "Metric values discrepancies between AWS CloudWatch and New Relic",
        "AWS Metric Streams Operation",
        "Errors in the Status Dashboard"
      ],
      "title": "Amazon CloudWatch Metric Streams integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "4ccc7fb5ba31643ae4f58f7fc647d71b8145d61e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-metric-stream/",
      "published_at": "2021-05-04T18:01:54Z",
      "updated_at": "2021-05-04T18:01:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic currently provides independent integrations with AWS to collect performance metrics and metadata for more than 50 AWS services. With the new AWS Metric Streams integration, you only need a single service, AWS CloudWatch, to gather all AWS metrics and custom namespaces and send them to New Relic. Why does this matter? Our current system, which relies on individual integrations, runs on a polling fleet and calls multiple AWS APIs at regular intervals to retrieve the metrics and metadata. Using AWS CloudWatch significantly improves how metrics are gathered, overcoming some of the limitations of using the individual integrations. API mode Stream mode It requires an integration with each AWS service to collect the metrics. All metrics from all AWS services and custom namespaces are available in New Relic at once, without needing a specific integration to be built or updated. There are two exceptions: percentiles and a small number of metrics that are made available to CloudWatch with more than 2 hours delay. It adds an additional delay to metrics being available in New Relic for alerting and dashboarding. The fastest polling interval we offer today is 5 minutes. Latency is significantly improved, since metrics are streamed in less than two minutes since they are made available in AWS CouldWatch. It may lead to AWS API throttling for large AWS environments. AWS API throttling is eliminated. Set up a Metric Stream to send CloudWatch metrics to New Relic To stream CloudWatch metrics to New Relic you need to create Kinesis Data Firehose and point it to New Relic and then create a CloudWatch Metric Stream that sends metrics to that Firehose. How to map New Relic and AWS accounts and regions If you manage multiple AWS accounts, then each account needs to be connected to New Relic. If you manage multiple regions within those accounts, then each region needs to be configured with a different Kinesis Data Firehose pointing to New Relic. You will typically map one or many AWS accounts to a single New Relic account. Automated setup using CloudFormation We provide a CloudFormation template that automates this process. This needs to be applied to each AWS account and region you want to monitor in New Relic. Manual setup using AWS Console, API, or calls Create a Kinesis Data Firehose Delivery Stream and configure the following destination parameters: Source: Direct PUT or other sources Data transformation: Disabled Record format conversion: Disabled Destination: Third-party service provider Ensure the following settings are defined: Third-party service provider: New Relic - Metrics New Relic configuration HTTP endpoint URL (US Datacenter): https://aws-api.newrelic.com/cloudwatch-metrics/v1 HTTP endpoint URL (EU Datacenter): https://cloud-collector.eu01.nr-data.net/cloudwatch-metrics/v1 API key: Enter your license key Content encoding: GZIP Retry duration: 60 S3 backup mode: Failed data only S3 bucket: select a bucket or create a new one to store metrics that failed to be sent. New Relic buffer conditions Buffer size: 1 MB Buffer interval: 60 (seconds) Permissions IAM role: Create or update IAM role Create the metric stream. Go to CloudWatch service in your AWS console and select the Streams option under the Metrics menu. Click on Create metric stream. Determine the right configuration based on your use cases: Use inclusion and exclusion filters to select which services should push metrics to New Relic. Select your Kinesis Data Firehose. Define a meaningful name for the stream (for example, newrelic-metric-stream). Confirm the creation of the metric stream. Alternatively, you can find instructions on the AWS documentation in order to create the CloudWatch metric stream using a CloudFormation template, API, or the CLI. Add the new AWS account in the Metric streams mode in the New Relic UI. Go to one.newrelic.com > Infrastructure > AWS, click on Add an AWS account, then on Use metric streams, and follow the steps. Validate your data is received correctly To confirm you are receiving data from the Metric Streams, follow the steps below: Go to one.newrelic.com > Infrastructure > AWS, and search for the Stream accounts. You can check the following: Account status dashboard. Useful to confirm that metric data is being received (errors, number of namespaces/metrics ingested, etc.) Explore your data. Use the Data Explorer to find a specific set of metrics, access all dimensions available for a given metric and more. Metrics naming convention Metrics received from AWS CloudWatch are stored in New Relic as dimensional metrics following this convention: Metrics are prefixed by the AWS namespace, all lowercase, where / is replaced with . : AWS/EC2 -> aws.ec2 AWS/ApplicationELB -> aws.applicationelb The original AWS metric name with its original case: aws.ec2.CPUUtilization aws.s3.5xxErrors aws.sns.NumberOfMessagesPublished If the resource the metric belongs to has a specific namespace prefix, it is used. If the resource the metric belongs to doesn't have a specific namespace prefix, metrics use the aws. prefix. aws.Region aws.s3.BucketName Current namespaces supported by AWS can be found in the CloudWatch documentation website. Query Experience, metric storage and mapping Metrics coming from AWS CloudWatch are stored as dimensional metrics of type summary and can be queried using NRQL. We have mapped metrics from the current cloud integrations to the new mappings that will come from AWS Metric Streams. You can continue to use the current metric naming, and queries will continue to work and pick data from AWS Metric Streams and the current cloud integrations. Check our documentation on how current cloud integrations metrics map to the new metric naming. All metrics coming from the metric stream will have these attributes: aws.MetricStreamArn collector.name = cloudwatch-metric-streams. AWS namespaces' entities in the New Relic Explorer We generate New Relic entities for most used AWS namespaces and will continue adding support for more namespaces. When we generate New Relic entities for a namespace you can expect to: Browse those entities in the New Relic Explorer. Access an out-of-the-box entity dashboard for those entities. Get metrics and entities from that namespace decorated with AWS tags. Collecting AWS tags requires that you have given New Relic the tag:GetResources permission which is part of the setup process in the UI. AWS tags show in metrics as tag.AWSTagName; for example, if you have set a Team AWS tag on the resource, it will show as tag.Team. Leverage all the built-in features that are part of the Explorer. Important Lookout view in Entity Explorer is not compatible with entities created from the AWS Metric Streams integration at this time. Set alert conditions You can create NRQL alert conditions on metrics from a metric stream. Make sure your filter limits data to metrics from the CloudWatch metric stream only. To do that, construct your queries like this: SELECT sum('aws.s3.5xxErrors') FROM Metric WHERE collector.name = 'cloudwatch-metric-streams' FACET aws.accountId, aws.s3.BucketName Copy Then, to make sure that alerts processes the data correctly, configure the advanced signal settings. These settings are needed because AWS CloudWatch receives metrics from services with a certain delay (for example, Amazon guarantees that 90% of EC2 metrics are available in CloudWatch within 7 minutes of them being generated). Moreover, streaming metrics from AWS to New Relic adds up to 1 minute additional delay, mostly due to buffering data in the Firehose. To configure the signal settings, under Condition Settings, click on Advanced Signal Settings and enter the following values: Aggregation window. We recommend setting it to 1 minute. If you are having issues with flapping alerts or alerts not triggering, consider increasing it to 2 minutes. Offset evaluation by. Depending on the service, CloudWatch may send metrics with a certain delay. The value is set in windows. With a 1-minute aggregation window, setting the offset to 8 ensures the majority of the metrics are evaluated correctly. You may be able to use a lower offset if the delay introduced by AWS and Firehose is less. Fill data gaps with. Leave this void, or use Last known value if gaps in the data coming from AWS lead to false positives or negatives. See our documentation on how to create NRQL alerts for more details. Tags collection New Relic provides enhanced dimensions from metrics coming from AWS CloudWatch metric streams. Resource and custom tags are automatically pulled from all services and are used to decorate metrics with additional dimensions. Use the data explorer to see which tags are available on each AWS metric. The following query shows an example of tags being collected and queried as dimensions in metrics: SELECT average(`aws.rds.CPUUtilization`) FROM Metric FACET `tags.mycustomtag` SINCE 30 MINUTES AGO TIMESERIES Copy Metadata collection Similarly as with custom tags, New Relic also pulls metadata information from relevant AWS services in order to decorate AWS CloudWatch metrics with enriched metadata collected from AWS Services APIs. This is an optional capability that's complementary to the CloudWatch Metric Streams integration. The solution relies on AWS Config, which might incur in additional costs in your AWS account. AWS Config provides granular controls to determine which services and resources are recorded. New Relic will only ingest metadata from the available resources in your AWS account. The following services / namespaces are supported: EC2 Lambda RDS ALB/NLB S3 We plan to add metadata from most used AWS services. Curated dashboards A set of dashboards for different AWS Services is available in the New Relic One Quickstarts app. Get access to the Quickstarts App Follow these steps in order to browse and import dashboards: Navigate to the Apps catalog in New Relic One. Search and select the Quickstarts app. Click on the Add this app link in the top-right corner. To enable the app, select target accounts and confirm clicking the Update account button. If applicable, review and confirm the Terms and Conditions of Use. Note that it might take a few minutes until permissions are applied and the app is ready to be used. Once available, confirm the app is enabled. The Quickstarts app will be listed in the Apps catalog. Import dashboards from Quickstarts App Follow these steps in order to import any of the curated dashboards: Navigate to Apps and open the Quickstarts app. Browse the AWS Dashboards. If you don't find a dashboard for your AWS service please submit your feedback on the top navigation bar or feel free to contribute with your own dashboard. Select the dashboard, confirm the target account and initial dashboard name. A copy of the dashboard should be imported in the target account. Additional customization is possible using any of the New Relic One dashboarding features. Manage your data New Relic provides a set of tools to keep track of the data being ingested in your account. Go to Manage your data in the settings menu to see all details. Metrics ingested from AWS Metric Streams integrations are considered in the Metric bucket. If you need a more granular view of the data you can use the bytecountestimate() function on Metric in order to estimate the data being ingested. For example, the following query represents data ingested from all metrics processed via AWS Metric Streams integration in the last 30 days (in bytes): FROM Metric SELECT bytecountestimate() where collector.name='cloudwatch-metric-streams' since 30 day ago Copy We recommend the following actions to control the data being ingested: Make sure metric streams are enabled only on the AWS accounts and regions you want to monitor with New Relic. Use the inclusion and exclusion filters in CloudWatch Metric Stream is order to select which services / namespaces are being collected. New Relic and AWS teams are working together to offer more granular controls so that filters can be applied based on tags and other attributes. Important Metrics sent via AWS Metric Streams count against your Metric API limits for the New Relic account where data will be ingested. Migrating from poll-based AWS integrations When metrics are sent via Metric Streams to New Relic, if the same metrics are being retrieved using the current poll-based integrations, those metrics will be duplicated. For example, alerts and dashboards that use sum or count will return twice the actual number. This includes alerts and dashboards that use metrics that have a .Sum suffix. We recommend sending the data to a non-production New Relic account where you can safely do tests. If that is not an option, then AWS CloudWatch Metric Stream filters are available to include or exclude certain namespaces that can cause trouble. Alternatively, you can use filtering on queries to distinguish between metrics that come from Metric Streams and those that come through polling. All metrics coming from Metric Streams are tagged with collector.name='cloudwatch-metric-streams'. Query, dashboard, and alert considerations AWS Metric Streams integration uses the Metric API to push metrics in the dimensional metric format. Poll-based integrations push metrics based on events (for example, ComputeSample event), and will be migrated to dimensional metrics in the future. To assist in this transition, New Relic provides a mechanism (known as shimming) that transparently lets you write queries in any format. Then these queries are processed as expected based on the source that's available (metrics or events). This mechanism works both ways, from events to metrics, and viceversa. Please consider the following when migrating from poll-based integrations: Custom dashboards that use poll-based AWS integration events should work as expected. Alert conditions that use poll-based AWS events might need to be adapted with dimensional metric format. Use the NRQL source for the alert condition. Troubleshooting No metrics or errors appear on New Relic If you are not seeing data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to troubleshoot your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS Troubleshooting docs for additional details. Check the Metric Stream metrics under AWS/CloudWatch/MetricStreams namespace. You will see a count of metric updates and errors per Metric Streams. This will indicate that the Metric Stream is successfully emitting data. If you see errors, confirm the IAM role specified in the Metric Streams configuration grants the CloudWatch service principal permissions to write to it. Check the Monitoring tab of the Kinesis Data Firehose in the Kinesis console to see if the Firehose is successfully receiving data. You can enable CloudWatch error logging on your Kinesis Data Firehose to get more detailed information for debugging issues. Refer to Amazon Kinesis Data Firehose official documentation for more details. Confirm that you have configured your Kinesis Data Firehose with the correct destination details: Ensure the New Relic API Key/License Key contains your 40 hexadecimal chars license key. Ensure the right data center US or EU has been selected for your New Relic account (hint: if the license_key starts with eu then you need to select the EU data center). Check that your Kinesis Data Firehose has permissions to write to the configured destination, for example: the S3 bucket policy allows write. Missing metrics for certain AWS namespaces New Relic does not apply any filter on the metrics received from the AWS CloudWatch metric stream. If you are expecting certain metrics to be ingested and its not the case, please verify the following: Make sure theres no Inclusion or Exclusion filter in your CloudWatch Metric Stream. Make sure metrics are available in AWS as part of CloudWatch. Confirm you see the metrics in the AWS CloudWatch interface. Important AWS CloudWatch doesn't include metrics that are not available in less than 2 hours. For example, some S3 metrics are aggregated on a daily basis. We plan to make some of these special metrics available in New Relic. Metric values discrepancies between AWS CloudWatch and New Relic Metrics are processed, mapped, and stored as received from AWS CloudWatch metric stream. Some discrepancies might be observed when comparing AWS CloudWatch and New Relic dashboards. On limited scenarios, AWS CloudWatch applies specific functions and logic before rendering the metrics. These guidelines should help understand the root cause of the discrepancy: Check that the same function is used on the metrics (for example average, min, max). On the New Relic side, make sure you filter the same timestamp or timeframe (considering the timezone) to show the exact same time as in AWS CloudWatch. When using timeseries, the New Relic user interface might perform some rounding based on intervals. You can get a list of the raw metric received by time using a query like this one (note that no function is applied to the selected metric): FROM Metric SELECT aws.outposts.InstanceTypeCapacityUtilization WHERE collector.name = 'cloudwatch-metric-streams' Copy Remember that AWS fixes the maximum resolution for every metric reported in AWS CloudWatch (for example: 1 minute, 5 minutes, etc). AWS Metric Streams Operation You can see the state of the Metric Stream(s) in the Streams tab in the CloudWatch console. In particular, a Metric Stream can be in one of two states: running or stopped. Running: The stream is running correctly. Note that there may not be any metric data been streamed due to the configured filters. Although there is no data corresponding to the configured filters, the status of the Metric Stream can be running still. Stopped: The stream has been explicitly set to the halted state (not because of an error). This state is useful to temporarily stop the streaming of data without deleting the configuration. Errors in the Status Dashboard New Relic relies on the AWS Config service to collect additional metadata from resources in order to enrich metrics received via CloudWatch Metric Stream. Make sure AWS Config is enabled in your AWS Account, and ensure the linked Role has the following permission or inline policy created: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"config:BatchGetResourceConfig\", \"Resource\": \"*\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 137.91458,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "sections": "<em>Amazon</em> CloudWatch Metric Streams <em>integration</em>",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": " data in New Relic once the AWS CloudWatch Metric Stream has been connected to AWS Kinesis Data Firehose, then follow the steps below to <em>troubleshoot</em> your configuration: Check that the Metric Stream is in a state of Running via the AWS console or API. Please refer to AWS <em>Troubleshooting</em> docs"
      },
      "id": "606a036de7b9d2bfef9445f2"
    },
    {
      "sections": [
        "Integrations and managed policies",
        "Recommended policy",
        "Important",
        "Optional policy",
        "Option 1: Use our CloudFormation template",
        "CloudFormation template",
        "Option 2: Manually add permissions",
        "Required by all integrations",
        "ALB permissions",
        "API Gateway permissions",
        "Auto Scaling permissions",
        "Billing permissions",
        "Cloudfront permissions",
        "CloudTrail permissions",
        "DynamoDB permissions",
        "EBS permissions",
        "EC2 permissions",
        "ECS/ECR permissions",
        "EFS permissions",
        "ElastiCache permissions",
        "ElasticSearch permissions",
        "Elastic Beanstalk permissions",
        "ELB permissions",
        "EMR permissions",
        "Health permissions",
        "IAM permissions",
        "IoT permissions",
        "Kinesis Firehose permissions",
        "Kinesis Streams permissions",
        "Lambda permissions",
        "RDS, RDS Enhanced Monitoring permissions",
        "Redshift permissions",
        "Route 53 permissions",
        "S3 permissions",
        "Simple Email Service (SES) permissions",
        "SNS permissions",
        "SQS permissions",
        "Trusted Advisor permissions",
        "VPC permissions",
        "X-Ray monitoring permissions"
      ],
      "title": "Integrations and managed policies",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Get started"
      ],
      "external_id": "80e215e7b2ba382de1b7ea758ee1b1f0a1e3c7df",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/get-started/integrations-managed-policies/",
      "published_at": "2021-05-04T18:30:29Z",
      "updated_at": "2021-05-04T18:30:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In order to use infrastructure integrations, you need to grant New Relic permission to read the relevant data from your account. Amazon Web Services (AWS) uses managed policies to grant these permissions. Recommended policy Important Recommendation: Grant an account-wide ReadOnlyAccess managed policy from AWS. AWS automatically updates this policy when new services are added or existing services are modified. New Relic infrastructure integrations have been designed to function with ReadOnlyAccess policies. For instructions, see Connect AWS integrations to infrastructure. Exception: The Trusted Advisor integration is not covered by the ReadOnlyAccess policy. It requires the additional AWSSupportAccess managed policy. This is also the only integration that requires full access permissions (support:*) in order to correctly operate. We notified Amazon about this limitation. Once it's resolved we'll update documentation with more specific permissions required for this integration. Optional policy If you cannot use the ReadOnlyAccess managed policy from AWS, you can create your own customized policy based on the list of permissions. This allows you to specify the optimal permissions required to fetch data from AWS for each integration. While this option is available, it is not recommended because it must be manually updated when you add or modify your integrations. Important New Relic has no way of identifying problems related to custom permissions. If you choose to create a custom policy, it is your responsibility to maintain it and ensure proper data is being collected. There are two ways to set up your customized policy: You can either use our CloudFormation template, or create own yourself by adding the permissions you need. Option 1: Use our CloudFormation template Our CloudFormation template contains all the permissions for all our AWS integrations. A user different than root can be used in the managed policy. CloudFormation template AWSTemplateFormatVersion: 2010-09-09 Outputs: NewRelicRoleArn: Description: NewRelicRole to monitor AWS Lambda Value: !GetAtt - NewRelicIntegrationsTemplate - Arn Parameters: NewRelicAccountNumber: Type: String Description: The Newrelic account number to send data AllowedPattern: '[0-9]+' Resources: NewRelicIntegrationsTemplate: Type: 'AWS::IAM::Role' Properties: RoleName: !Sub NewRelicTemplateTest AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: AWS: !Sub 'arn:aws:iam::754728514883:root' Action: 'sts:AssumeRole' Condition: StringEquals: 'sts:ExternalId': !Ref NewRelicAccountNumber Policies: - PolicyName: NewRelicIntegrations PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'elasticloadbalancing:DescribeLoadBalancers' - 'elasticloadbalancing:DescribeTargetGroups' - 'elasticloadbalancing:DescribeTags' - 'elasticloadbalancing:DescribeLoadBalancerAttributes' - 'elasticloadbalancing:DescribeListeners' - 'elasticloadbalancing:DescribeRules' - 'elasticloadbalancing:DescribeTargetGroupAttributes' - 'elasticloadbalancing:DescribeInstanceHealth' - 'elasticloadbalancing:DescribeLoadBalancerPolicies' - 'elasticloadbalancing:DescribeLoadBalancerPolicyTypes' - 'apigateway:GET' - 'apigateway:HEAD' - 'apigateway:OPTIONS' - 'autoscaling:DescribeLaunchConfigurations' - 'autoscaling:DescribeAutoScalingGroups' - 'autoscaling:DescribePolicies' - 'autoscaling:DescribeTags' - 'autoscaling:DescribeAccountLimits' - 'budgets:ViewBilling' - 'budgets:ViewBudget' - 'cloudfront:ListDistributions' - 'cloudfront:ListStreamingDistributions' - 'cloudfront:ListTagsForResource' - 'cloudtrail:LookupEvents' - 'config:BatchGetResourceConfig' - 'config:ListDiscoveredResources' - 'dynamodb:DescribeLimits' - 'dynamodb:ListTables' - 'dynamodb:DescribeTable' - 'dynamodb:ListGlobalTables' - 'dynamodb:DescribeGlobalTable' - 'dynamodb:ListTagsOfResource' - 'ec2:DescribeVolumeStatus' - 'ec2:DescribeVolumes' - 'ec2:DescribeVolumeAttribute' - 'ec2:DescribeInstanceStatus' - 'ec2:DescribeInstances' - 'ec2:DescribeVpnConnections' - 'ecs:ListServices' - 'ecs:DescribeServices' - 'ecs:DescribeClusters' - 'ecs:ListClusters' - 'ecs:ListTagsForResource' - 'ecs:ListContainerInstances' - 'ecs:DescribeContainerInstances' - 'elasticfilesystem:DescribeMountTargets' - 'elasticfilesystem:DescribeFileSystems' - 'elasticache:DescribeCacheClusters' - 'elasticache:ListTagsForResource' - 'es:ListDomainNames' - 'es:DescribeElasticsearchDomain' - 'es:DescribeElasticsearchDomains' - 'es:ListTags' - 'elasticbeanstalk:DescribeEnvironments' - 'elasticbeanstalk:DescribeInstancesHealth' - 'elasticbeanstalk:DescribeConfigurationSettings' - 'elasticloadbalancing:DescribeLoadBalancers' - 'elasticmapreduce:ListInstances' - 'elasticmapreduce:ListClusters' - 'elasticmapreduce:DescribeCluster' - 'elasticmapreduce:ListInstanceGroups' - 'health:DescribeAffectedEntities' - 'health:DescribeEventDetails' - 'health:DescribeEvents' - 'iam:ListSAMLProviders' - 'iam:ListOpenIDConnectProviders' - 'iam:ListServerCertificates' - 'iam:GetAccountAuthorizationDetails' - 'iam:ListVirtualMFADevices' - 'iam:GetAccountSummary' - 'iot:ListTopicRules' - 'iot:GetTopicRule' - 'iot:ListThings' - 'firehose:DescribeDeliveryStream' - 'firehose:ListDeliveryStreams' - 'kinesis:ListStreams' - 'kinesis:DescribeStream' - 'kinesis:ListTagsForStream' - 'rds:ListTagsForResource' - 'rds:DescribeDBInstances' - 'rds:DescribeDBClusters' - 'redshift:DescribeClusters' - 'redshift:DescribeClusterParameters' - 'route53:ListHealthChecks' - 'route53:GetHostedZone' - 'route53:ListHostedZones' - 'route53:ListResourceRecordSets' - 'route53:ListTagsForResources' - 's3:GetLifecycleConfiguration' - 's3:GetBucketTagging' - 's3:ListAllMyBuckets' - 's3:GetBucketWebsite' - 's3:GetBucketLogging' - 's3:GetBucketCORS' - 's3:GetBucketVersioning' - 's3:GetBucketAcl' - 's3:GetBucketNotification' - 's3:GetBucketPolicy' - 's3:GetReplicationConfiguration' - 's3:GetMetricsConfiguration' - 's3:GetAccelerateConfiguration' - 's3:GetAnalyticsConfiguration' - 's3:GetBucketLocation' - 's3:GetBucketRequestPayment' - 's3:GetEncryptionConfiguration' - 's3:GetInventoryConfiguration' - 's3:GetIpConfiguration' - 'ses:ListConfigurationSets' - 'ses:GetSendQuota' - 'ses:DescribeConfigurationSet' - 'ses:ListReceiptFilters' - 'ses:ListReceiptRuleSets' - 'ses:DescribeReceiptRule' - 'ses:DescribeReceiptRuleSet' - 'sns:GetTopicAttributes' - 'sns:ListTopics' - 'sqs:ListQueues' - 'sqs:ListQueueTags' - 'sqs:GetQueueAttributes' - 'tag:GetResources' - 'ec2:DescribeInternetGateways' - 'ec2:DescribeVpcs' - 'ec2:DescribeNatGateways' - 'ec2:DescribeVpcEndpoints' - 'ec2:DescribeSubnets' - 'ec2:DescribeNetworkAcls' - 'ec2:DescribeVpcAttribute' - 'ec2:DescribeRouteTables' - 'ec2:DescribeSecurityGroups' - 'ec2:DescribeVpcPeeringConnections' - 'ec2:DescribeNetworkInterfaces' - 'lambda:GetAccountSettings' - 'lambda:ListFunctions' - 'lambda:ListAliases' - 'lambda:ListTags' - 'lambda:ListEventSourceMappings' - 'cloudwatch:GetMetricStatistics' - 'cloudwatch:ListMetrics' - 'cloudwatch:GetMetricData' - 'support:*' Resource: '*' Copy Option 2: Manually add permissions To create your own policy using available permissions: Add the permissions for all integrations. Add permissions that are specific to the integrations you need The following permissions are used by New Relic to retrieve data for specific AWS integrations: Required by all integrations Important If an integration is not listed on this page, these permissions are all you need. All integrations Permissions CloudWatch cloudwatch:GetMetricStatistics cloudwatch:ListMetrics cloudwatch:GetMetricData Config API config:BatchGetResourceConfig config:ListDiscoveredResources Resource Tagging API tag:GetResources ALB permissions Additional ALB permissions: elasticloadbalancing:DescribeLoadBalancers elasticloadbalancing:DescribeTargetGroups elasticloadbalancing:DescribeTags elasticloadbalancing:DescribeLoadBalancerAttributes elasticloadbalancing:DescribeListeners elasticloadbalancing:DescribeRules elasticloadbalancing:DescribeTargetGroupAttributes elasticloadbalancing:DescribeInstanceHealth elasticloadbalancing:DescribeLoadBalancerPolicies elasticloadbalancing:DescribeLoadBalancerPolicyTypes API Gateway permissions Additional API Gateway permissions: apigateway:GET apigateway:HEAD apigateway:OPTIONS Auto Scaling permissions Additional Auto Scaling permissions: autoscaling:DescribeLaunchConfigurations autoscaling:DescribeAutoScalingGroups autoscaling:DescribePolicies autoscaling:DescribeTags autoscaling:DescribeAccountLimits Billing permissions Additional Billing permissions: budgets:ViewBilling budgets:ViewBudget Cloudfront permissions Additional Cloudfront permissions: cloudfront:ListDistributions cloudfront:ListStreamingDistributions cloudfront:ListTagsForResource CloudTrail permissions Additional CloudTrail permissions: cloudtrail:LookupEvents DynamoDB permissions Additional DynamoDB permissions: dynamodb:DescribeLimits dynamodb:ListTables dynamodb:DescribeTable dynamodb:ListGlobalTables dynamodb:DescribeGlobalTable dynamodb:ListTagsOfResource EBS permissions Additional EBS permissions: ec2:DescribeVolumeStatus ec2:DescribeVolumes ec2:DescribeVolumeAttribute EC2 permissions Additional EC2 permissions: ec2:DescribeInstanceStatus ec2:DescribeInstances ECS/ECR permissions Additional ECS/ECR permissions: ecs:ListServices ecs:DescribeServices ecs:DescribeClusters ecs:ListClusters ecs:ListTagsForResource ecs:ListContainerInstances ecs:DescribeContainerInstances EFS permissions Additional EFS permissions: elasticfilesystem:DescribeMountTargets elasticfilesystem:DescribeFileSystems ElastiCache permissions Additional ElastiCache permissions: elasticache:DescribeCacheClusters elasticache:ListTagsForResource ElasticSearch permissions Additional ElasticSearch permissions: es:ListDomainNames es:DescribeElasticsearchDomain es:DescribeElasticsearchDomains es:ListTags Elastic Beanstalk permissions Additional Elastic Beanstalk permissions: elasticbeanstalk:DescribeEnvironments elasticbeanstalk:DescribeInstancesHealth elasticbeanstalk:DescribeConfigurationSettings ELB permissions Additional ELB permissions: elasticloadbalancing:DescribeLoadBalancers EMR permissions Additional EMR permissions: elasticmapreduce:ListInstances elasticmapreduce:ListClusters elasticmapreduce:DescribeCluster elasticmapreduce:ListInstanceGroups elasticmapreduce:ListInstanceFleets Health permissions Additional Health permissions: health:DescribeAffectedEntities health:DescribeEventDetails health:DescribeEvents IAM permissions Additional IAM permissions: iam:ListSAMLProviders iam:ListOpenIDConnectProviders iam:ListServerCertificates iam:GetAccountAuthorizationDetails iam:ListVirtualMFADevices iam:GetAccountSummary IoT permissions Additional IoT permissions: iot:ListTopicRules iot:GetTopicRule iot:ListThings Kinesis Firehose permissions Additional Kinesis Firehose permissions: firehose:DescribeDeliveryStream firehose:ListDeliveryStreams Kinesis Streams permissions Additional Kinesis Streams permissions: kinesis:ListStreams kinesis:DescribeStream kinesis:ListTagsForStream Lambda permissions Additional Lambda permissions: lambda:GetAccountSettings lambda:ListFunctions lambda:ListAliases lambda:ListTags lambda:ListEventSourceMappings RDS, RDS Enhanced Monitoring permissions Additional RDS and RDS Enhanced Monitoring permissions: rds:ListTagsForResource rds:DescribeDBInstances rds:DescribeDBClusters Redshift permissions Additional Redshift permissions: redshift:DescribeClusters redshift:DescribeClusterParameters Route 53 permissions Additional Route 53 permissions: route53:ListHealthChecks route53:GetHostedZone route53:ListHostedZones route53:ListResourceRecordSets route53:ListTagsForResources S3 permissions Additional S3 permissions: s3:GetLifecycleConfiguration s3:GetBucketTagging s3:ListAllMyBuckets s3:GetBucketWebsite s3:GetBucketLogging s3:GetBucketCORS s3:GetBucketVersioning s3:GetBucketAcl s3:GetBucketNotification s3:GetBucketPolicy s3:GetReplicationConfiguration s3:GetMetricsConfiguration s3:GetAccelerateConfiguration s3:GetAnalyticsConfiguration s3:GetBucketLocation s3:GetBucketRequestPayment s3:GetEncryptionConfiguration s3:GetInventoryConfiguration s3:GetIpConfiguration Simple Email Service (SES) permissions Additional SES permissions: ses:ListConfigurationSets ses:GetSendQuota ses:DescribeConfigurationSet ses:ListReceiptFilters ses:ListReceiptRuleSets ses:DescribeReceiptRule ses:DescribeReceiptRuleSet SNS permissions Additional SNS permissions: sns:GetTopicAttributes sns:ListTopics SQS permissions Additional SQS permissions: sqs:ListQueues sqs:GetQueueAttributes sqs:ListQueueTags Trusted Advisor permissions Additional Trusted Advisor permissions: support:* See also the note about the Trusted Advisor integration and recommended policies. VPC permissions Additional VPC permissions: ec2:DescribeInternetGateways ec2:DescribeVpcs ec2:DescribeNatGateways ec2:DescribeVpcEndpoints ec2:DescribeSubnets ec2:DescribeNetworkAcls ec2:DescribeVpcAttribute ec2:DescribeRouteTables ec2:DescribeSecurityGroups ec2:DescribeVpcPeeringConnections ec2:DescribeNetworkInterfaces ec2:DescribeVpnConnections X-Ray monitoring permissions Additional X-ray monitoring permissions: xray:BatchGet* xray:Get*",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 135.16345,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Integrations</em> and managed policies",
        "sections": "<em>Integrations</em> and managed policies",
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "In order to use infrastructure <em>integrations</em>, you need to grant New Relic permission to read the relevant data from your account. <em>Amazon</em> Web Services (AWS) uses managed policies to grant these permissions. Recommended policy Important Recommendation: Grant an account-wide ReadOnlyAccess managed"
      },
      "id": "6045079fe7b9d27db95799d9"
    },
    {
      "sections": [
        "AWS service specific API rate limiting",
        "Problem",
        "Solution",
        "Verify your Infrastructure account's ARN",
        "Change the polling frequency",
        "Filter your data",
        "Review API usage",
        "Cause"
      ],
      "title": "AWS service specific API rate limiting",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Troubleshooting"
      ],
      "external_id": "785db1a9f5d5d9b89c2d304d1260ce5a8f30a680",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/troubleshooting/aws-service-specific-api-rate-limiting/",
      "published_at": "2021-05-05T15:10:41Z",
      "updated_at": "2021-03-13T03:22:51Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem After enabling Amazon integrations with New Relic Infrastructure, you encounter a rate limit for service-specific APIs. You might see this message in your AWS monitoring software, often with a 503 error: AWS::EC2::Errors::RequestLimitExceeded Request limit exceeded. Solution Verify your Infrastructure account's ARN Ensure that you are not collecting inventory information for the wrong ARN account. Verify that the ARN associated with your Infrastructure account is correct. Change the polling frequency The polling frequency determines how often New Relic gathers data from your cloud provider. By default, the polling frequency is set to the maximum frequency that is available for each service. If you reach your API rate limit, you may want to decrease the polling frequency. Filter your data You can set filters for each integration in order to specify which information you want captured. If you reach your API rate limit, you may want to filter your data. Review API usage To review the API usage for New Relic Infrastructure integrations with Amazon AWS: Go to one.newrelic.com > Infrastructure > AWS > Account status dashboard. Review the New Relic Insights dashboard, which appears automatically. The Insights dashboard includes a chart with your account's Amazon AWS API call count for the last month as well as the CloudWatch API calls (per AWS resource) for the last day. This information is the API usage for New Relic only. It does not include other AWS API or CloudWatch usage that may occur. For assistance determining which services may cause an increase in billing, get support at support.newrelic.com, or contact your New Relic account representative. Cause Infrastructure Amazon integrations leverage the AWS monitoring APIs to gather inventory data. AWS imposes hard rate limits on many of the AWS service-specific APIs consumed by New Relic Infrastructure integrations. Adding New Relic Amazon integrations will increase usage of the service-specific APIs and could impact how quickly you reach your rate limit. This may be caused by either of the following: Enabling Amazon integrations on several plugins for the same service Adding the incorrect Role ARN to your AWS integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 110.62892,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Amazon</em> <em>integrations</em>",
        "body": "Problem After enabling <em>Amazon</em> <em>integrations</em> with New Relic Infrastructure, you encounter a rate limit for service-specific APIs. You might see this message in your AWS monitoring software, often with a 503 error: AWS::EC2::Errors::RequestLimitExceeded Request limit exceeded. Solution Verify your"
      },
      "id": "604507c428ccbc013a2c60c4"
    }
  ],
  "/docs/integrations/elastic-container-service-integration/get-started/introduction-amazon-ecs-integration": [
    {
      "sections": [
        "ECS integration troubleshooting: No data appears",
        "Problem",
        "Important",
        "Solution",
        "Troubleshoot via awscli",
        "Troubleshoot in the UI",
        "Reasons for stopped tasks",
        "AWS Secrets Manager",
        "AWS Systems Manager Parameter Store"
      ],
      "title": "ECS integration troubleshooting: No data appears",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Troubleshooting"
      ],
      "external_id": "a86730dfe4c4cfdb6d293675c2c97e7393939331",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-no-data-appears/",
      "published_at": "2021-05-05T18:02:51Z",
      "updated_at": "2021-03-30T12:41:02Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed our on-host ECS integration and waited a few minutes, but your cluster is not showing in the explorer. Important We have two ECS integrations: a cloud-based integration and an on-host integration. This document is about the on-host integration. Solution If your New Relic account had previously installed the infrastructure agent or an infrastructure on-host integration, your data should appear in the UI within a few minutes. If your account had not previously done either of those things before installing the on-host ECS integration, it may take tens of minutes for data to appear in the UI. In that case, we recommend waiting up to an hour before doing the following troubleshooting steps or contacting support. There are several options for troubleshooting no data appearing: Troubleshoot via the awscli tool (recommended when talking to New Relic technical support) Troubleshoot via the UI For information about stopped tasks, see Stopped tasks reasons. Troubleshoot via awscli When interacting with New Relic support, use this method and send the generated files with your support request: Retrieve the information related to the newrelic-infra service or the Fargate service that contains a task with a newrelic-infra sidecar: aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service newrelic-infra > newrelic-infra-service.json Copy aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service YOUR_FARGATE_SERVICE_WITH_NEW_RELIC_SIDECAR > newrelic-infra-sidecar-service.json Copy The failures attribute details any errors for the services. Under services is the status attribute. It says ACTIVE if the service has no issues. The desiredCount should match the runningCount. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. The pendingCount attribute should be zero, because all tasks should be running. Inspect the events attribute of services to check for issues with scheduling or starting the tasks. For example: if the service is unable to start tasks successfully, it will display a message like: { \"id\": \"5295a13c-34e6-41e1-96dd-8364c42cc7a9\", \"createdAt\": \"2020-04-06T15:28:18.298000+02:00\", \"message\": \"(service newrelic-ifnra) is unable to consistently start tasks successfully. For more information, see the Troubleshooting section of the Amazon ECS Developer Guide.\" } Copy In the same section, you can also see which tasks were started by the service from the events: { \"id\": \"1c0a6ce2-de2e-49b2-b0ac-6458a804d0f0\", \"createdAt\": \"2020-04-06T15:27:49.614000+02:00\", \"message\": \"(service fargate-fail) has started 1 tasks: (task YOUR_TASK_ID).\" } Copy Retrieve the information related to the task with this command: aws ecs describe-tasks --tasks YOUR_TASK_ID --cluster YOUR_CLUSTER_NAME > newrelic-infra-task.json Copy The desiredStatus and lastStatus should be RUNNING. If the task couldn't start normally, it will have a STOPPED status. Inspect the stopCode and stoppedReason. One reason example: a task that couldn't be started because the task execution role doesn't have the appropriate permissions to download the license-key-containing secret would have the following output: \"stopCode\": \"TaskFailedToStart\", \"stoppedAt\": \"2020-04-06T15:28:54.725000+02:00\", \"stoppedReason\": \"Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/NewRelicECSIntegration-Ne-NewRelicECSTaskExecution-1C0ODHVT4HDNT/8637b461f0f94d649e9247e2f14c3803 is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail-DmLHfs status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\", \"stoppingAt\": \"2020-04-06T15:28:10.953000+02:00\", Copy If the task is running but youre still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Troubleshoot in the UI To use the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 Container Service section. Click on the cluster where you installed the New Relic ECS integration. On the Services tab, use the filter to search for the integration service. If you used the automatic install script, the name of the service will be newrelic-infra. If you are using Fargate, it will be the name of your monitored service. Once found, click on the name. The service page shows the Status of the service. It says ACTIVE if the service has no issues. On the same page, the Desired count should match the Running count. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. Pending count should be zero, because all tasks should be running. Inspect the Events tab to check for issues with scheduling or starting the tasks. In the Tasks tab of your service, you can inspect the running tasks and the stopped tasks by clicking on the Task status selector. Containers that failed to start are shown when you select the Stopped status. Click on a task to go to the task details page. Under Stopped reason, it displays a message explaining why the task was stopped. If the task is running but youre still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Reasons for stopped tasks In the AWS ECS troubleshooting documentation you can find information on common causes of errors related to running tasks and services. See below for details about some reasons for stopped tasks. Task stopped with reason: Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/YOUR_ROLE_NAME is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\" Copy This means that the IAM role specified using executionRoleArn in the task definition doesn't have access to the secret used for the NRIA_LICENSE_KEY. The execution role should have a policy attached that grants it access to read the secret. Get the execution role of your task: aws ecs describe-task-definition --task-definition newrelic-infra --output text --query taskDefinition.executionRoleArn Copy You can replace the --task-definition newrelic-infra with the name of your fargate task that includes the sidecar container. aws ecs describe-task-definition --task-definition YOUR_FARGATE_TASK_NAME --output text --query taskDefinition.executionRoleArn Copy List the policies attached to role: aws iam list-attached-role-policies --role-name YOUR_EXECUTION_ROLE_NAME Copy This should return 3 policies AmazonECSTaskExecutionRolePolicy, AmazonEC2ContainerServiceforEC2Role and a third one that should grant read access to the license key. In the following example the policy it's named NewRelicLicenseKeySecretReadAccess. { \"AttachedPolicies\": [ { \"PolicyName\": \"AmazonECSTaskExecutionRolePolicy\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\" }, { \"PolicyName\": \"AmazonEC2ContainerServiceforEC2Role\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role\" }, { \"PolicyName\": \"YOUR_POLICY_NAME\", \"PolicyArn\": \"arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME\" } ] } Copy Retrieve the default policy version: aws iam get-policy-version --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --version-id $(aws iam get-policy --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --output text --query Policy.DefaultVersionId) Copy This retrieves the policy permissions. There should be an entry for Actionsecretsmanager:GetSecretValue if you used AWS Secrets Manager to store your license key, or an entry for ssm:GetParametersif you used AWS Systems Manager Parameter Store: AWS Secrets Manager { \"PolicyVersion\": { \"Document\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"secretsmanager:GetSecretValue\", \"Resource\": \"arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME\", \"Effect\": \"Allow\" } ] }, \"VersionId\": \"v1\", \"IsDefaultVersion\": true, \"CreateDate\": \"2020-03-31T13:47:07+00:00\" } } Copy AWS Systems Manager Parameter Store { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"ssm:GetParameters\", \"Resource\": [ \"arn:aws:ssm:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:parameter/YOUR_SECRET_NAME\" ], \"Effect\": \"Allow\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 171.82935,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "ECS <em>integration</em> troubleshooting: No data appears",
        "sections": "ECS <em>integration</em> troubleshooting: No data appears",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": ". Troubleshoot in the UI To use the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 <em>Container</em> <em>Service</em> section. Click on the cluster where you installed the New Relic ECS <em>integration</em>. On the Services tab, use the filter to search for the <em>integration</em> <em>service</em>. If you used"
      },
      "id": "60450883196a671c8c960f27"
    },
    {
      "sections": [
        "Install the ECS integration",
        "Tip",
        "Install overview",
        "Install using CloudFormation",
        "EC2 launch type",
        "Fargate launch type",
        "Install with automatic script",
        "Manual install",
        "AWS resources created"
      ],
      "title": "Install the ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Installation"
      ],
      "external_id": "857b78b6e7de76449f3f9569cee4700705b7d7fe",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/installation/install-ecs-integration/",
      "published_at": "2021-05-05T17:59:37Z",
      "updated_at": "2021-03-16T05:40:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's ECS integration reports and displays performance data from your Amazon ECS environment. This document explains how to install this integration. Tip To use ECS integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Install overview Before you install our ECS integration, we recommend reviewing the requirements. Here's a brief overview of what happens during the install process: For EC2 launch type: The infrastructure agent (newrelic-infra) gets deployed onto an ECS cluster as a service using the daemon scheduling strategy. This deployment installs the infrastructure agent in all the container instances of the cluster. The infrastructure agent then monitors ECS and Docker containers. For Fargate launch type: The infrastructure agent (newrelic-infra) gets deployed as a sidecar in every task to monitor. Install options: Install using AWS CloudFormation Install using automatic script Install manually Install using CloudFormation One install option is using AWS CloudFormation. We provide some CloudFormation templates that install the ECS integration onto your AWS account for both EC2 and Fargate launch types: To register the New Relic's ECS integration task, deploy this stack. Ensure youre deploying the stack to your desired region(s). This stack creates the following resources: A secret that stores the license key. A policy to access the license key. An instance role to be used as an ECS task ExecutionRole, with access to the license key. For EC2 launch type: Registers the New Relic Infrastructure ECS integration task. Follow the additional instructions for your launch type: EC2 launch type Additional steps for EC2 launch type: To create a service that runs the task on every container instance, deploy this stack. Fargate launch type Additional steps for Fargate launch type: Download the task definition example with the sidecar container to be deployed: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-fargate-example-latest.json Copy Add the newrelic-infra container in this task definition as a sidecar to the task definitions you want to monitor. In this example task, your application's containers replace the placeholder busybox container. Next steps: Wait a few minutes and then look for your data in the UI. Recommended: Install our ECS cloud integration, which gets you other ECS data, including information about clusters and services. See recommended alert conditions. Understand the AWS resources created by this process. Install with automatic script One install option is using our install script. To use the automatic install script: Download the ECS integration installer: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-installer.sh Copy Add execute permissions to the installer: chmod +x newrelic-infra-ecs-installer.sh Copy Execute it with -h to see the documentation and requirements: ./newrelic-infra-ecs-installer.sh -h Copy Check that your AWS profile points to the same region where your ECS cluster was created: $ aws configure get region us-east-1 $ aws ecs list-clusters YOUR_CLUSTER_ARNS arn:aws:ecs:us-east-1:YOUR_AWS_ACCOUNT:cluster/YOUR_CLUSTER Copy Execute the installer, specifying your license key and cluster name. EC2 launch type: ./newrelic-infra-ecs-installer.sh -c YOUR_CLUSTER_NAME -l YOUR_LICENSE_KEY Copy Fargate launch type: ./newrelic-infra-ecs-installer.sh -fargate -c YOUR_CLUSTER_NAME -l YOUR_LICENSE_KEY Copy Additional steps for Fargate launch type (not EC2 launch type): Download the task definition example with the sidecar container to be deployed: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-fargate-example-latest.json Copy Add the single container in this task definition as a sidecar to the task definitions you want monitored. Next steps: Wait a few minutes and then look for your data in the UI. Recommended: Install our ECS cloud integration, which gets you other ECS data, including information about clusters and services. See recommended alert conditions. Understand the AWS resources created by this process. Manual install One install option is to manually do the steps that are done by the automatic installer script. We will describe how this is done using the awscli tool: Check that your AWS profile points to the same region where your ECS cluster was created: $ aws configure get region us-east-1 $ aws ecs list-clusters YOUR_CLUSTER_ARNS arn:aws:ecs:us-east-1:YOUR_AWS_ACCOUNT:cluster/YOUR_CLUSTER Copy Save your New Relic license key as a Systems Manager (SSM) parameter: aws ssm put-parameter \\ --name \"/newrelic-infra/ecs/license-key\" \\ --type SecureString \\ --description 'New Relic license key for ECS monitoring' \\ --value \"NEW_RELIC_LICENSE_KEY\" Copy Create an IAM policy to access the license key parameter: aws iam create-policy \\ --policy-name \"NewRelicSSMLicenseKeyReadAccess\" \\ --policy-document \"{\"Version\"\\\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Action\":[\"ssm:GetParameters\"],\"Resource\":[\"ARN_OF_LICENSE_KEY_PARAMETER\"]}]}\" --description \"Provides read access to the New Relic SSM license key parameter\" Copy Create an IAM role to be used as the task execution role: aws iam create-role \\ --role-name \"NewRelicECSTaskExecutionRole\" \\ --assume-role-policy-document '{\"Version\":\"2008-10-17\",\"Statement\":[{\"Sid\":\"\",\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"ecs-tasks.amazonaws.com\"},\"Action\":\"sts:AssumeRole\"}]}' \\ --description \"ECS task execution role for New Relic infrastructure\" Copy Attach the policies NewRelicSSMLicenseKeyReadAccess, AmazonEC2ContainerServiceforEC2Role, and AmazonECSTaskExecutionRolePolicy to the role: aws iam attach-role-policy \\ --role-name \"NewRelicECSTaskExecutionRole\" \\ --policy-arn \"POLICY_ARN\" Copy Choose your launch type for more instructions: EC2 launch type Additional steps for EC2 launch type: Download the New Relic ECS integration task definition template file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-ec2-latest.json Copy Replace the task execution role in the template file with the newly created role: \"executionRoleArn\": \"NewRelicECSTaskExecutionRole\", Copy Replace the valueFrom attribute of the secret with the name of the Systems Manager parameter: secrets\": [ { \"valueFrom\": \"/newrelic-infra/ecs/license-key\", \"name\": \"NRIA_LICENSE_KEY\" } ], Copy Register the task definition file: aws ecs register-task-definition --cli-input-json file://newrelic-infra-ecs-ec2-latest.json Copy Create a service with the daemon scheduling strategy for the registered task: aws ecs create-service --cluster \"YOUR_CLUSTER_NAME\" --service-name \"newrelic-infra\" --task-definition \"newrelic-infra\" --scheduling-strategy DAEMON Copy Fargate launch type Additional steps for the Fargate launch type: Download the task definition example with the sidecar container to be deployed: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-fargate-example-latest.json Copy Add the newrelic-infra container in this task definition as a sidecar to the task definitions you want to monitor. In this example task, your application's containers replace the placeholder busybox container. Next steps: Wait a few minutes and then look for your data in the UI. Recommended: Install our ECS cloud integration, a separate integration which gets you supplementary ECS data, including information about clusters and services. See recommended alert conditions. Understand the AWS resources created by this process. AWS resources created When you install the ECS integration using default/recommended values, it does the following in AWS: Creates Systems Manager (SSM) parameter /newrelic-infra/ecs/license-key. This system parameter contains the New Relic license key. Creates IAM policy NewRelicSSMLicenseKeyReadAccess, which enables access to the SSM parameter with the license key. Creates IAM role NewRelicECSTaskExecutionRole used as the task execution role. Policies attached to the role: NewRelicSSMLicenseKeyReadAccess (created by the installer). AmazonEC2ContainerServiceforEC2Role AmazonECSTaskExecutionRolePolicy For EC2 launch type, this is also done: Registers the newrelic-infra ECS task definition. Creates the service newrelic-infra for the registered task using a daemon scheduling strategy.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.54572,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the ECS <em>integration</em>",
        "sections": "Install the ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "New Relic&#x27;s ECS <em>integration</em> reports and displays performance data from your Amazon ECS environment. This document explains how to install this <em>integration</em>. Tip To use ECS <em>integrations</em> and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign"
      },
      "id": "603e9e76196a676684a83de9"
    },
    {
      "sections": [
        "Uninstall the ECS integration",
        "Uninstall",
        "CloudFormation uninstall",
        "Automatic uninstall",
        "Manual uninstall"
      ],
      "title": "Uninstall the ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Installation"
      ],
      "external_id": "78bfa3ecb2059e2641be8e22cd8ebb025da625a3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/installation/uninstall-ecs-integration/",
      "published_at": "2021-05-05T18:16:27Z",
      "updated_at": "2021-03-16T05:40:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's on-host ECS integration reports and displays performance data from your Amazon ECS environment. Read on to learn how to uninstall this integration. Uninstall There are several uninstall options, depending on how you installed: Uninstall with CloudFormation Use automatic installer script Manual uninstall CloudFormation uninstall To uninstall the ECS integration using the CloudFormation templates: Go to the list of stacks in your AWS console. For each New Relic stack: Select the stack Click the delete button Click the delete stack button on the confirmation pop-up. Automatic uninstall To uninstall the ECS integration using the installer script: For EC2 launch type: run $ ./newrelic-infrastructure-ecs-installer.sh -u -c YOUR_CLUSTER_NAME Copy For Fargate launch type: $ ./newrelic-infrastructure-ecs-installer.sh -f -u -c YOUR_CLUSTER_NAME Copy You only need to execute the command once, regardless of the number of nodes in your cluster. The command will delete the AWS resources created during the install procedure. The installer provides a dry run mode that shows you the awscli commands that are going to be executed. The dry run mode for the uninstall process is activated by passing the -d flag to the command: $ ./newrelic-infrastructure-ecs-installer.sh -d -u -c YOUR_CLUSTER_NAME Copy Manual uninstall To uninstall manually, you must delete all the AWS resources related to the integration. To do this: Check that your AWS profile points to the same region where your ECS cluster was created: $ aws configure get region us-east-1 $ aws ecs list-clusters YOUR_CLUSTER_ARNS arn:aws:ecs:us-east-1:YOUR_AWS_ACCOUNT:cluster/YOUR_CLUSTER Copy Delete the Systems Manager (SSM) parameter that stores the New Relic license key: aws ssm delete-parameter --name \"/newrelic-infra/ecs/license-key\" Copy Before deleting the IAM role, you need to detach all of its policies. To get a list of the attached policies: aws iam list-attached-role-policies --role-name \"NewRelicECSTaskExecutionRole\" --output text --query 'AttachedPolicies[*].PolicyArn' Copy Detach all the policies returned in the previous step from the IAM role: aws iam detach-role-policy --role-name \"NewRelicECSTaskExecutionRole\" --policy-arn \"POLICY_ARN\" Copy Delete the IAM role: aws iam delete-role --role-name \"NewRelicECSTaskExecutionRole\" Copy Delete the IAM policy NewRelicSSMLicenseKeyReadAccess, which grants System Manager license key access: aws iam delete-policy --policy-arn \"POLICY_ARN\" Copy The remaining steps are only for EC2 launch type, and not Fargate: Delete the service: aws ecs delete-service --service \"newrelic-infra\" --cluster \"YOUR_CLUSTER_NAME\" Copy List the task definition for the newrelic-infra family of tasks: aws ecs list-task-definitions \\ --family-prefix newrelic-infra \\ --output text \\ --query taskDefinitionArns Copy Deregister the tasks: aws ecs deregister-task-definition --task-definition \"TASK_DEFINITION_ARN\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 163.91078,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Uninstall the ECS <em>integration</em>",
        "sections": "Uninstall the ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "New Relic&#x27;s on-host ECS <em>integration</em> reports and displays performance data from your Amazon ECS environment. Read on to learn how to uninstall this <em>integration</em>. Uninstall There are several uninstall options, depending on how you installed: Uninstall with CloudFormation Use automatic installer script"
      },
      "id": "603e9e7464441fd9cf4e885b"
    }
  ],
  "/docs/integrations/elastic-container-service-integration/installation/install-ecs-integration": [
    {
      "sections": [
        "Uninstall the ECS integration",
        "Uninstall",
        "CloudFormation uninstall",
        "Automatic uninstall",
        "Manual uninstall"
      ],
      "title": "Uninstall the ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Installation"
      ],
      "external_id": "78bfa3ecb2059e2641be8e22cd8ebb025da625a3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/installation/uninstall-ecs-integration/",
      "published_at": "2021-05-05T18:16:27Z",
      "updated_at": "2021-03-16T05:40:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's on-host ECS integration reports and displays performance data from your Amazon ECS environment. Read on to learn how to uninstall this integration. Uninstall There are several uninstall options, depending on how you installed: Uninstall with CloudFormation Use automatic installer script Manual uninstall CloudFormation uninstall To uninstall the ECS integration using the CloudFormation templates: Go to the list of stacks in your AWS console. For each New Relic stack: Select the stack Click the delete button Click the delete stack button on the confirmation pop-up. Automatic uninstall To uninstall the ECS integration using the installer script: For EC2 launch type: run $ ./newrelic-infrastructure-ecs-installer.sh -u -c YOUR_CLUSTER_NAME Copy For Fargate launch type: $ ./newrelic-infrastructure-ecs-installer.sh -f -u -c YOUR_CLUSTER_NAME Copy You only need to execute the command once, regardless of the number of nodes in your cluster. The command will delete the AWS resources created during the install procedure. The installer provides a dry run mode that shows you the awscli commands that are going to be executed. The dry run mode for the uninstall process is activated by passing the -d flag to the command: $ ./newrelic-infrastructure-ecs-installer.sh -d -u -c YOUR_CLUSTER_NAME Copy Manual uninstall To uninstall manually, you must delete all the AWS resources related to the integration. To do this: Check that your AWS profile points to the same region where your ECS cluster was created: $ aws configure get region us-east-1 $ aws ecs list-clusters YOUR_CLUSTER_ARNS arn:aws:ecs:us-east-1:YOUR_AWS_ACCOUNT:cluster/YOUR_CLUSTER Copy Delete the Systems Manager (SSM) parameter that stores the New Relic license key: aws ssm delete-parameter --name \"/newrelic-infra/ecs/license-key\" Copy Before deleting the IAM role, you need to detach all of its policies. To get a list of the attached policies: aws iam list-attached-role-policies --role-name \"NewRelicECSTaskExecutionRole\" --output text --query 'AttachedPolicies[*].PolicyArn' Copy Detach all the policies returned in the previous step from the IAM role: aws iam detach-role-policy --role-name \"NewRelicECSTaskExecutionRole\" --policy-arn \"POLICY_ARN\" Copy Delete the IAM role: aws iam delete-role --role-name \"NewRelicECSTaskExecutionRole\" Copy Delete the IAM policy NewRelicSSMLicenseKeyReadAccess, which grants System Manager license key access: aws iam delete-policy --policy-arn \"POLICY_ARN\" Copy The remaining steps are only for EC2 launch type, and not Fargate: Delete the service: aws ecs delete-service --service \"newrelic-infra\" --cluster \"YOUR_CLUSTER_NAME\" Copy List the task definition for the newrelic-infra family of tasks: aws ecs list-task-definitions \\ --family-prefix newrelic-infra \\ --output text \\ --query taskDefinitionArns Copy Deregister the tasks: aws ecs deregister-task-definition --task-definition \"TASK_DEFINITION_ARN\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 190.68474,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Uninstall the ECS <em>integration</em>",
        "sections": "Uninstall the ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "New Relic&#x27;s on-host ECS <em>integration</em> reports and displays performance data from your Amazon ECS environment. Read on to learn how to uninstall this <em>integration</em>. Uninstall There are several uninstall options, depending on how you installed: Uninstall with CloudFormation Use automatic installer script"
      },
      "id": "603e9e7464441fd9cf4e885b"
    },
    {
      "sections": [
        "Introduction to the Amazon ECS integration",
        "Features",
        "Important",
        "Compatibility and requirements",
        "Install",
        "Check the source code"
      ],
      "title": "Introduction to the Amazon ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Get started"
      ],
      "external_id": "a2af5484b25f8595032cc1937210c9a41024a138",
      "image": "https://docs.newrelic.com/static/986bdb22950fdd8b222a850e205882a9/c1b63/new-relic-ecs-integration-dashboards_0.png",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/get-started/introduction-amazon-ecs-integration/",
      "published_at": "2021-05-05T18:16:27Z",
      "updated_at": "2021-03-30T21:13:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our ECS integration reports and displays performance data from your Amazon ECS environment. The ECS integration works well with other integrations, so you can also monitor services running on ECS. Features Amazon Elastic Container Service (ECS) is a scalable container management service that makes it easy to run, stop, and manage Docker containers on Amazon EC2 clusters. Our ECS integration instruments the underlying container instance (EC2 launch type) and the container layer by reporting metrics from ECS objects. The integration gives you insight into your ECS instances, tasks, services, and containers. one.newrelic.com > Explorer > ECS dashboard: The ECS integration reports performance data about your Amazon ECS containers. Features include: View your data in pre-built dashboards for immediate insight into your ECS environment. Create your own queries and charts in the query builder from automatically reported data. Create alert conditions on ECS data. Explore entities using the New Relic Explorer. Important New Relic also offers an ECS cloud integration, which reports a different data set than our on-host integration. For complete ECS monitoring, we recommend enabling both integrations. Compatibility and requirements Requirements: Amazon ECS container agent 1.21.0 or higher. Windows not supported. This integration uses our infrastructure agent and our Docker instrumentation: applicable requirements and restrictions of those systems apply. Install To install, see Install integration. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.2907,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the Amazon ECS <em>integration</em>",
        "sections": "Introduction to the Amazon ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "Our ECS <em>integration</em> reports and displays performance data from your Amazon ECS environment. The ECS <em>integration</em> works well with other <em>integrations</em>, so you can also monitor services running on ECS. Features Amazon <em>Elastic</em> <em>Container</em> <em>Service</em> (ECS) is a scalable <em>container</em> management <em>service</em> that makes"
      },
      "id": "603eb04b196a6752b5a83dc8"
    },
    {
      "sections": [
        "ECS integration troubleshooting: No data appears",
        "Problem",
        "Important",
        "Solution",
        "Troubleshoot via awscli",
        "Troubleshoot in the UI",
        "Reasons for stopped tasks",
        "AWS Secrets Manager",
        "AWS Systems Manager Parameter Store"
      ],
      "title": "ECS integration troubleshooting: No data appears",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Troubleshooting"
      ],
      "external_id": "a86730dfe4c4cfdb6d293675c2c97e7393939331",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-no-data-appears/",
      "published_at": "2021-05-05T18:02:51Z",
      "updated_at": "2021-03-30T12:41:02Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed our on-host ECS integration and waited a few minutes, but your cluster is not showing in the explorer. Important We have two ECS integrations: a cloud-based integration and an on-host integration. This document is about the on-host integration. Solution If your New Relic account had previously installed the infrastructure agent or an infrastructure on-host integration, your data should appear in the UI within a few minutes. If your account had not previously done either of those things before installing the on-host ECS integration, it may take tens of minutes for data to appear in the UI. In that case, we recommend waiting up to an hour before doing the following troubleshooting steps or contacting support. There are several options for troubleshooting no data appearing: Troubleshoot via the awscli tool (recommended when talking to New Relic technical support) Troubleshoot via the UI For information about stopped tasks, see Stopped tasks reasons. Troubleshoot via awscli When interacting with New Relic support, use this method and send the generated files with your support request: Retrieve the information related to the newrelic-infra service or the Fargate service that contains a task with a newrelic-infra sidecar: aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service newrelic-infra > newrelic-infra-service.json Copy aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service YOUR_FARGATE_SERVICE_WITH_NEW_RELIC_SIDECAR > newrelic-infra-sidecar-service.json Copy The failures attribute details any errors for the services. Under services is the status attribute. It says ACTIVE if the service has no issues. The desiredCount should match the runningCount. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. The pendingCount attribute should be zero, because all tasks should be running. Inspect the events attribute of services to check for issues with scheduling or starting the tasks. For example: if the service is unable to start tasks successfully, it will display a message like: { \"id\": \"5295a13c-34e6-41e1-96dd-8364c42cc7a9\", \"createdAt\": \"2020-04-06T15:28:18.298000+02:00\", \"message\": \"(service newrelic-ifnra) is unable to consistently start tasks successfully. For more information, see the Troubleshooting section of the Amazon ECS Developer Guide.\" } Copy In the same section, you can also see which tasks were started by the service from the events: { \"id\": \"1c0a6ce2-de2e-49b2-b0ac-6458a804d0f0\", \"createdAt\": \"2020-04-06T15:27:49.614000+02:00\", \"message\": \"(service fargate-fail) has started 1 tasks: (task YOUR_TASK_ID).\" } Copy Retrieve the information related to the task with this command: aws ecs describe-tasks --tasks YOUR_TASK_ID --cluster YOUR_CLUSTER_NAME > newrelic-infra-task.json Copy The desiredStatus and lastStatus should be RUNNING. If the task couldn't start normally, it will have a STOPPED status. Inspect the stopCode and stoppedReason. One reason example: a task that couldn't be started because the task execution role doesn't have the appropriate permissions to download the license-key-containing secret would have the following output: \"stopCode\": \"TaskFailedToStart\", \"stoppedAt\": \"2020-04-06T15:28:54.725000+02:00\", \"stoppedReason\": \"Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/NewRelicECSIntegration-Ne-NewRelicECSTaskExecution-1C0ODHVT4HDNT/8637b461f0f94d649e9247e2f14c3803 is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail-DmLHfs status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\", \"stoppingAt\": \"2020-04-06T15:28:10.953000+02:00\", Copy If the task is running but youre still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Troubleshoot in the UI To use the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 Container Service section. Click on the cluster where you installed the New Relic ECS integration. On the Services tab, use the filter to search for the integration service. If you used the automatic install script, the name of the service will be newrelic-infra. If you are using Fargate, it will be the name of your monitored service. Once found, click on the name. The service page shows the Status of the service. It says ACTIVE if the service has no issues. On the same page, the Desired count should match the Running count. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. Pending count should be zero, because all tasks should be running. Inspect the Events tab to check for issues with scheduling or starting the tasks. In the Tasks tab of your service, you can inspect the running tasks and the stopped tasks by clicking on the Task status selector. Containers that failed to start are shown when you select the Stopped status. Click on a task to go to the task details page. Under Stopped reason, it displays a message explaining why the task was stopped. If the task is running but youre still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Reasons for stopped tasks In the AWS ECS troubleshooting documentation you can find information on common causes of errors related to running tasks and services. See below for details about some reasons for stopped tasks. Task stopped with reason: Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/YOUR_ROLE_NAME is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\" Copy This means that the IAM role specified using executionRoleArn in the task definition doesn't have access to the secret used for the NRIA_LICENSE_KEY. The execution role should have a policy attached that grants it access to read the secret. Get the execution role of your task: aws ecs describe-task-definition --task-definition newrelic-infra --output text --query taskDefinition.executionRoleArn Copy You can replace the --task-definition newrelic-infra with the name of your fargate task that includes the sidecar container. aws ecs describe-task-definition --task-definition YOUR_FARGATE_TASK_NAME --output text --query taskDefinition.executionRoleArn Copy List the policies attached to role: aws iam list-attached-role-policies --role-name YOUR_EXECUTION_ROLE_NAME Copy This should return 3 policies AmazonECSTaskExecutionRolePolicy, AmazonEC2ContainerServiceforEC2Role and a third one that should grant read access to the license key. In the following example the policy it's named NewRelicLicenseKeySecretReadAccess. { \"AttachedPolicies\": [ { \"PolicyName\": \"AmazonECSTaskExecutionRolePolicy\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\" }, { \"PolicyName\": \"AmazonEC2ContainerServiceforEC2Role\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role\" }, { \"PolicyName\": \"YOUR_POLICY_NAME\", \"PolicyArn\": \"arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME\" } ] } Copy Retrieve the default policy version: aws iam get-policy-version --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --version-id $(aws iam get-policy --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --output text --query Policy.DefaultVersionId) Copy This retrieves the policy permissions. There should be an entry for Actionsecretsmanager:GetSecretValue if you used AWS Secrets Manager to store your license key, or an entry for ssm:GetParametersif you used AWS Systems Manager Parameter Store: AWS Secrets Manager { \"PolicyVersion\": { \"Document\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"secretsmanager:GetSecretValue\", \"Resource\": \"arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME\", \"Effect\": \"Allow\" } ] }, \"VersionId\": \"v1\", \"IsDefaultVersion\": true, \"CreateDate\": \"2020-03-31T13:47:07+00:00\" } } Copy AWS Systems Manager Parameter Store { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"ssm:GetParameters\", \"Resource\": [ \"arn:aws:ssm:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:parameter/YOUR_SECRET_NAME\" ], \"Effect\": \"Allow\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 171.82935,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "ECS <em>integration</em> troubleshooting: No data appears",
        "sections": "ECS <em>integration</em> troubleshooting: No data appears",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": ". Troubleshoot in the UI To use the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 <em>Container</em> <em>Service</em> section. Click on the cluster where you installed the New Relic ECS <em>integration</em>. On the Services tab, use the filter to search for the <em>integration</em> <em>service</em>. If you used"
      },
      "id": "60450883196a671c8c960f27"
    }
  ],
  "/docs/integrations/elastic-container-service-integration/installation/uninstall-ecs-integration": [
    {
      "sections": [
        "Install the ECS integration",
        "Tip",
        "Install overview",
        "Install using CloudFormation",
        "EC2 launch type",
        "Fargate launch type",
        "Install with automatic script",
        "Manual install",
        "AWS resources created"
      ],
      "title": "Install the ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Installation"
      ],
      "external_id": "857b78b6e7de76449f3f9569cee4700705b7d7fe",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/installation/install-ecs-integration/",
      "published_at": "2021-05-05T17:59:37Z",
      "updated_at": "2021-03-16T05:40:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's ECS integration reports and displays performance data from your Amazon ECS environment. This document explains how to install this integration. Tip To use ECS integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Install overview Before you install our ECS integration, we recommend reviewing the requirements. Here's a brief overview of what happens during the install process: For EC2 launch type: The infrastructure agent (newrelic-infra) gets deployed onto an ECS cluster as a service using the daemon scheduling strategy. This deployment installs the infrastructure agent in all the container instances of the cluster. The infrastructure agent then monitors ECS and Docker containers. For Fargate launch type: The infrastructure agent (newrelic-infra) gets deployed as a sidecar in every task to monitor. Install options: Install using AWS CloudFormation Install using automatic script Install manually Install using CloudFormation One install option is using AWS CloudFormation. We provide some CloudFormation templates that install the ECS integration onto your AWS account for both EC2 and Fargate launch types: To register the New Relic's ECS integration task, deploy this stack. Ensure youre deploying the stack to your desired region(s). This stack creates the following resources: A secret that stores the license key. A policy to access the license key. An instance role to be used as an ECS task ExecutionRole, with access to the license key. For EC2 launch type: Registers the New Relic Infrastructure ECS integration task. Follow the additional instructions for your launch type: EC2 launch type Additional steps for EC2 launch type: To create a service that runs the task on every container instance, deploy this stack. Fargate launch type Additional steps for Fargate launch type: Download the task definition example with the sidecar container to be deployed: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-fargate-example-latest.json Copy Add the newrelic-infra container in this task definition as a sidecar to the task definitions you want to monitor. In this example task, your application's containers replace the placeholder busybox container. Next steps: Wait a few minutes and then look for your data in the UI. Recommended: Install our ECS cloud integration, which gets you other ECS data, including information about clusters and services. See recommended alert conditions. Understand the AWS resources created by this process. Install with automatic script One install option is using our install script. To use the automatic install script: Download the ECS integration installer: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-installer.sh Copy Add execute permissions to the installer: chmod +x newrelic-infra-ecs-installer.sh Copy Execute it with -h to see the documentation and requirements: ./newrelic-infra-ecs-installer.sh -h Copy Check that your AWS profile points to the same region where your ECS cluster was created: $ aws configure get region us-east-1 $ aws ecs list-clusters YOUR_CLUSTER_ARNS arn:aws:ecs:us-east-1:YOUR_AWS_ACCOUNT:cluster/YOUR_CLUSTER Copy Execute the installer, specifying your license key and cluster name. EC2 launch type: ./newrelic-infra-ecs-installer.sh -c YOUR_CLUSTER_NAME -l YOUR_LICENSE_KEY Copy Fargate launch type: ./newrelic-infra-ecs-installer.sh -fargate -c YOUR_CLUSTER_NAME -l YOUR_LICENSE_KEY Copy Additional steps for Fargate launch type (not EC2 launch type): Download the task definition example with the sidecar container to be deployed: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-fargate-example-latest.json Copy Add the single container in this task definition as a sidecar to the task definitions you want monitored. Next steps: Wait a few minutes and then look for your data in the UI. Recommended: Install our ECS cloud integration, which gets you other ECS data, including information about clusters and services. See recommended alert conditions. Understand the AWS resources created by this process. Manual install One install option is to manually do the steps that are done by the automatic installer script. We will describe how this is done using the awscli tool: Check that your AWS profile points to the same region where your ECS cluster was created: $ aws configure get region us-east-1 $ aws ecs list-clusters YOUR_CLUSTER_ARNS arn:aws:ecs:us-east-1:YOUR_AWS_ACCOUNT:cluster/YOUR_CLUSTER Copy Save your New Relic license key as a Systems Manager (SSM) parameter: aws ssm put-parameter \\ --name \"/newrelic-infra/ecs/license-key\" \\ --type SecureString \\ --description 'New Relic license key for ECS monitoring' \\ --value \"NEW_RELIC_LICENSE_KEY\" Copy Create an IAM policy to access the license key parameter: aws iam create-policy \\ --policy-name \"NewRelicSSMLicenseKeyReadAccess\" \\ --policy-document \"{\"Version\"\\\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Action\":[\"ssm:GetParameters\"],\"Resource\":[\"ARN_OF_LICENSE_KEY_PARAMETER\"]}]}\" --description \"Provides read access to the New Relic SSM license key parameter\" Copy Create an IAM role to be used as the task execution role: aws iam create-role \\ --role-name \"NewRelicECSTaskExecutionRole\" \\ --assume-role-policy-document '{\"Version\":\"2008-10-17\",\"Statement\":[{\"Sid\":\"\",\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"ecs-tasks.amazonaws.com\"},\"Action\":\"sts:AssumeRole\"}]}' \\ --description \"ECS task execution role for New Relic infrastructure\" Copy Attach the policies NewRelicSSMLicenseKeyReadAccess, AmazonEC2ContainerServiceforEC2Role, and AmazonECSTaskExecutionRolePolicy to the role: aws iam attach-role-policy \\ --role-name \"NewRelicECSTaskExecutionRole\" \\ --policy-arn \"POLICY_ARN\" Copy Choose your launch type for more instructions: EC2 launch type Additional steps for EC2 launch type: Download the New Relic ECS integration task definition template file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-ec2-latest.json Copy Replace the task execution role in the template file with the newly created role: \"executionRoleArn\": \"NewRelicECSTaskExecutionRole\", Copy Replace the valueFrom attribute of the secret with the name of the Systems Manager parameter: secrets\": [ { \"valueFrom\": \"/newrelic-infra/ecs/license-key\", \"name\": \"NRIA_LICENSE_KEY\" } ], Copy Register the task definition file: aws ecs register-task-definition --cli-input-json file://newrelic-infra-ecs-ec2-latest.json Copy Create a service with the daemon scheduling strategy for the registered task: aws ecs create-service --cluster \"YOUR_CLUSTER_NAME\" --service-name \"newrelic-infra\" --task-definition \"newrelic-infra\" --scheduling-strategy DAEMON Copy Fargate launch type Additional steps for the Fargate launch type: Download the task definition example with the sidecar container to be deployed: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-fargate-example-latest.json Copy Add the newrelic-infra container in this task definition as a sidecar to the task definitions you want to monitor. In this example task, your application's containers replace the placeholder busybox container. Next steps: Wait a few minutes and then look for your data in the UI. Recommended: Install our ECS cloud integration, a separate integration which gets you supplementary ECS data, including information about clusters and services. See recommended alert conditions. Understand the AWS resources created by this process. AWS resources created When you install the ECS integration using default/recommended values, it does the following in AWS: Creates Systems Manager (SSM) parameter /newrelic-infra/ecs/license-key. This system parameter contains the New Relic license key. Creates IAM policy NewRelicSSMLicenseKeyReadAccess, which enables access to the SSM parameter with the license key. Creates IAM role NewRelicECSTaskExecutionRole used as the task execution role. Policies attached to the role: NewRelicSSMLicenseKeyReadAccess (created by the installer). AmazonEC2ContainerServiceforEC2Role AmazonECSTaskExecutionRolePolicy For EC2 launch type, this is also done: Registers the newrelic-infra ECS task definition. Creates the service newrelic-infra for the registered task using a daemon scheduling strategy.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 193.31966,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the ECS <em>integration</em>",
        "sections": "<em>Install</em> the ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "New Relic&#x27;s ECS <em>integration</em> reports and displays performance data from your Amazon ECS environment. This document explains how to install this <em>integration</em>. Tip To use ECS <em>integrations</em> and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign"
      },
      "id": "603e9e76196a676684a83de9"
    },
    {
      "sections": [
        "Introduction to the Amazon ECS integration",
        "Features",
        "Important",
        "Compatibility and requirements",
        "Install",
        "Check the source code"
      ],
      "title": "Introduction to the Amazon ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Get started"
      ],
      "external_id": "a2af5484b25f8595032cc1937210c9a41024a138",
      "image": "https://docs.newrelic.com/static/986bdb22950fdd8b222a850e205882a9/c1b63/new-relic-ecs-integration-dashboards_0.png",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/get-started/introduction-amazon-ecs-integration/",
      "published_at": "2021-05-05T18:16:27Z",
      "updated_at": "2021-03-30T21:13:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our ECS integration reports and displays performance data from your Amazon ECS environment. The ECS integration works well with other integrations, so you can also monitor services running on ECS. Features Amazon Elastic Container Service (ECS) is a scalable container management service that makes it easy to run, stop, and manage Docker containers on Amazon EC2 clusters. Our ECS integration instruments the underlying container instance (EC2 launch type) and the container layer by reporting metrics from ECS objects. The integration gives you insight into your ECS instances, tasks, services, and containers. one.newrelic.com > Explorer > ECS dashboard: The ECS integration reports performance data about your Amazon ECS containers. Features include: View your data in pre-built dashboards for immediate insight into your ECS environment. Create your own queries and charts in the query builder from automatically reported data. Create alert conditions on ECS data. Explore entities using the New Relic Explorer. Important New Relic also offers an ECS cloud integration, which reports a different data set than our on-host integration. For complete ECS monitoring, we recommend enabling both integrations. Compatibility and requirements Requirements: Amazon ECS container agent 1.21.0 or higher. Windows not supported. This integration uses our infrastructure agent and our Docker instrumentation: applicable requirements and restrictions of those systems apply. Install To install, see Install integration. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.29068,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the Amazon ECS <em>integration</em>",
        "sections": "Introduction to the Amazon ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "Our ECS <em>integration</em> reports and displays performance data from your Amazon ECS environment. The ECS <em>integration</em> works well with other <em>integrations</em>, so you can also monitor services running on ECS. Features Amazon <em>Elastic</em> <em>Container</em> <em>Service</em> (ECS) is a scalable <em>container</em> management <em>service</em> that makes"
      },
      "id": "603eb04b196a6752b5a83dc8"
    },
    {
      "sections": [
        "ECS integration troubleshooting: No data appears",
        "Problem",
        "Important",
        "Solution",
        "Troubleshoot via awscli",
        "Troubleshoot in the UI",
        "Reasons for stopped tasks",
        "AWS Secrets Manager",
        "AWS Systems Manager Parameter Store"
      ],
      "title": "ECS integration troubleshooting: No data appears",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Troubleshooting"
      ],
      "external_id": "a86730dfe4c4cfdb6d293675c2c97e7393939331",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-no-data-appears/",
      "published_at": "2021-05-05T18:02:51Z",
      "updated_at": "2021-03-30T12:41:02Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed our on-host ECS integration and waited a few minutes, but your cluster is not showing in the explorer. Important We have two ECS integrations: a cloud-based integration and an on-host integration. This document is about the on-host integration. Solution If your New Relic account had previously installed the infrastructure agent or an infrastructure on-host integration, your data should appear in the UI within a few minutes. If your account had not previously done either of those things before installing the on-host ECS integration, it may take tens of minutes for data to appear in the UI. In that case, we recommend waiting up to an hour before doing the following troubleshooting steps or contacting support. There are several options for troubleshooting no data appearing: Troubleshoot via the awscli tool (recommended when talking to New Relic technical support) Troubleshoot via the UI For information about stopped tasks, see Stopped tasks reasons. Troubleshoot via awscli When interacting with New Relic support, use this method and send the generated files with your support request: Retrieve the information related to the newrelic-infra service or the Fargate service that contains a task with a newrelic-infra sidecar: aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service newrelic-infra > newrelic-infra-service.json Copy aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service YOUR_FARGATE_SERVICE_WITH_NEW_RELIC_SIDECAR > newrelic-infra-sidecar-service.json Copy The failures attribute details any errors for the services. Under services is the status attribute. It says ACTIVE if the service has no issues. The desiredCount should match the runningCount. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. The pendingCount attribute should be zero, because all tasks should be running. Inspect the events attribute of services to check for issues with scheduling or starting the tasks. For example: if the service is unable to start tasks successfully, it will display a message like: { \"id\": \"5295a13c-34e6-41e1-96dd-8364c42cc7a9\", \"createdAt\": \"2020-04-06T15:28:18.298000+02:00\", \"message\": \"(service newrelic-ifnra) is unable to consistently start tasks successfully. For more information, see the Troubleshooting section of the Amazon ECS Developer Guide.\" } Copy In the same section, you can also see which tasks were started by the service from the events: { \"id\": \"1c0a6ce2-de2e-49b2-b0ac-6458a804d0f0\", \"createdAt\": \"2020-04-06T15:27:49.614000+02:00\", \"message\": \"(service fargate-fail) has started 1 tasks: (task YOUR_TASK_ID).\" } Copy Retrieve the information related to the task with this command: aws ecs describe-tasks --tasks YOUR_TASK_ID --cluster YOUR_CLUSTER_NAME > newrelic-infra-task.json Copy The desiredStatus and lastStatus should be RUNNING. If the task couldn't start normally, it will have a STOPPED status. Inspect the stopCode and stoppedReason. One reason example: a task that couldn't be started because the task execution role doesn't have the appropriate permissions to download the license-key-containing secret would have the following output: \"stopCode\": \"TaskFailedToStart\", \"stoppedAt\": \"2020-04-06T15:28:54.725000+02:00\", \"stoppedReason\": \"Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/NewRelicECSIntegration-Ne-NewRelicECSTaskExecution-1C0ODHVT4HDNT/8637b461f0f94d649e9247e2f14c3803 is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail-DmLHfs status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\", \"stoppingAt\": \"2020-04-06T15:28:10.953000+02:00\", Copy If the task is running but youre still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Troubleshoot in the UI To use the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 Container Service section. Click on the cluster where you installed the New Relic ECS integration. On the Services tab, use the filter to search for the integration service. If you used the automatic install script, the name of the service will be newrelic-infra. If you are using Fargate, it will be the name of your monitored service. Once found, click on the name. The service page shows the Status of the service. It says ACTIVE if the service has no issues. On the same page, the Desired count should match the Running count. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. Pending count should be zero, because all tasks should be running. Inspect the Events tab to check for issues with scheduling or starting the tasks. In the Tasks tab of your service, you can inspect the running tasks and the stopped tasks by clicking on the Task status selector. Containers that failed to start are shown when you select the Stopped status. Click on a task to go to the task details page. Under Stopped reason, it displays a message explaining why the task was stopped. If the task is running but youre still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Reasons for stopped tasks In the AWS ECS troubleshooting documentation you can find information on common causes of errors related to running tasks and services. See below for details about some reasons for stopped tasks. Task stopped with reason: Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/YOUR_ROLE_NAME is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\" Copy This means that the IAM role specified using executionRoleArn in the task definition doesn't have access to the secret used for the NRIA_LICENSE_KEY. The execution role should have a policy attached that grants it access to read the secret. Get the execution role of your task: aws ecs describe-task-definition --task-definition newrelic-infra --output text --query taskDefinition.executionRoleArn Copy You can replace the --task-definition newrelic-infra with the name of your fargate task that includes the sidecar container. aws ecs describe-task-definition --task-definition YOUR_FARGATE_TASK_NAME --output text --query taskDefinition.executionRoleArn Copy List the policies attached to role: aws iam list-attached-role-policies --role-name YOUR_EXECUTION_ROLE_NAME Copy This should return 3 policies AmazonECSTaskExecutionRolePolicy, AmazonEC2ContainerServiceforEC2Role and a third one that should grant read access to the license key. In the following example the policy it's named NewRelicLicenseKeySecretReadAccess. { \"AttachedPolicies\": [ { \"PolicyName\": \"AmazonECSTaskExecutionRolePolicy\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\" }, { \"PolicyName\": \"AmazonEC2ContainerServiceforEC2Role\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role\" }, { \"PolicyName\": \"YOUR_POLICY_NAME\", \"PolicyArn\": \"arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME\" } ] } Copy Retrieve the default policy version: aws iam get-policy-version --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --version-id $(aws iam get-policy --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --output text --query Policy.DefaultVersionId) Copy This retrieves the policy permissions. There should be an entry for Actionsecretsmanager:GetSecretValue if you used AWS Secrets Manager to store your license key, or an entry for ssm:GetParametersif you used AWS Systems Manager Parameter Store: AWS Secrets Manager { \"PolicyVersion\": { \"Document\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"secretsmanager:GetSecretValue\", \"Resource\": \"arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME\", \"Effect\": \"Allow\" } ] }, \"VersionId\": \"v1\", \"IsDefaultVersion\": true, \"CreateDate\": \"2020-03-31T13:47:07+00:00\" } } Copy AWS Systems Manager Parameter Store { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"ssm:GetParameters\", \"Resource\": [ \"arn:aws:ssm:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:parameter/YOUR_SECRET_NAME\" ], \"Effect\": \"Allow\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 171.82933,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "ECS <em>integration</em> troubleshooting: No data appears",
        "sections": "ECS <em>integration</em> troubleshooting: No data appears",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": ". Troubleshoot in the UI To use the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 <em>Container</em> <em>Service</em> section. Click on the cluster where you installed the New Relic ECS <em>integration</em>. On the Services tab, use the filter to search for the <em>integration</em> <em>service</em>. If you used"
      },
      "id": "60450883196a671c8c960f27"
    }
  ],
  "/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-generate-verbose-logs": [
    {
      "sections": [
        "ECS integration troubleshooting: No data appears",
        "Problem",
        "Important",
        "Solution",
        "Troubleshoot via awscli",
        "Troubleshoot in the UI",
        "Reasons for stopped tasks",
        "AWS Secrets Manager",
        "AWS Systems Manager Parameter Store"
      ],
      "title": "ECS integration troubleshooting: No data appears",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Troubleshooting"
      ],
      "external_id": "a86730dfe4c4cfdb6d293675c2c97e7393939331",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-no-data-appears/",
      "published_at": "2021-05-05T18:02:51Z",
      "updated_at": "2021-03-30T12:41:02Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed our on-host ECS integration and waited a few minutes, but your cluster is not showing in the explorer. Important We have two ECS integrations: a cloud-based integration and an on-host integration. This document is about the on-host integration. Solution If your New Relic account had previously installed the infrastructure agent or an infrastructure on-host integration, your data should appear in the UI within a few minutes. If your account had not previously done either of those things before installing the on-host ECS integration, it may take tens of minutes for data to appear in the UI. In that case, we recommend waiting up to an hour before doing the following troubleshooting steps or contacting support. There are several options for troubleshooting no data appearing: Troubleshoot via the awscli tool (recommended when talking to New Relic technical support) Troubleshoot via the UI For information about stopped tasks, see Stopped tasks reasons. Troubleshoot via awscli When interacting with New Relic support, use this method and send the generated files with your support request: Retrieve the information related to the newrelic-infra service or the Fargate service that contains a task with a newrelic-infra sidecar: aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service newrelic-infra > newrelic-infra-service.json Copy aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service YOUR_FARGATE_SERVICE_WITH_NEW_RELIC_SIDECAR > newrelic-infra-sidecar-service.json Copy The failures attribute details any errors for the services. Under services is the status attribute. It says ACTIVE if the service has no issues. The desiredCount should match the runningCount. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. The pendingCount attribute should be zero, because all tasks should be running. Inspect the events attribute of services to check for issues with scheduling or starting the tasks. For example: if the service is unable to start tasks successfully, it will display a message like: { \"id\": \"5295a13c-34e6-41e1-96dd-8364c42cc7a9\", \"createdAt\": \"2020-04-06T15:28:18.298000+02:00\", \"message\": \"(service newrelic-ifnra) is unable to consistently start tasks successfully. For more information, see the Troubleshooting section of the Amazon ECS Developer Guide.\" } Copy In the same section, you can also see which tasks were started by the service from the events: { \"id\": \"1c0a6ce2-de2e-49b2-b0ac-6458a804d0f0\", \"createdAt\": \"2020-04-06T15:27:49.614000+02:00\", \"message\": \"(service fargate-fail) has started 1 tasks: (task YOUR_TASK_ID).\" } Copy Retrieve the information related to the task with this command: aws ecs describe-tasks --tasks YOUR_TASK_ID --cluster YOUR_CLUSTER_NAME > newrelic-infra-task.json Copy The desiredStatus and lastStatus should be RUNNING. If the task couldn't start normally, it will have a STOPPED status. Inspect the stopCode and stoppedReason. One reason example: a task that couldn't be started because the task execution role doesn't have the appropriate permissions to download the license-key-containing secret would have the following output: \"stopCode\": \"TaskFailedToStart\", \"stoppedAt\": \"2020-04-06T15:28:54.725000+02:00\", \"stoppedReason\": \"Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/NewRelicECSIntegration-Ne-NewRelicECSTaskExecution-1C0ODHVT4HDNT/8637b461f0f94d649e9247e2f14c3803 is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail-DmLHfs status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\", \"stoppingAt\": \"2020-04-06T15:28:10.953000+02:00\", Copy If the task is running but youre still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Troubleshoot in the UI To use the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 Container Service section. Click on the cluster where you installed the New Relic ECS integration. On the Services tab, use the filter to search for the integration service. If you used the automatic install script, the name of the service will be newrelic-infra. If you are using Fargate, it will be the name of your monitored service. Once found, click on the name. The service page shows the Status of the service. It says ACTIVE if the service has no issues. On the same page, the Desired count should match the Running count. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. Pending count should be zero, because all tasks should be running. Inspect the Events tab to check for issues with scheduling or starting the tasks. In the Tasks tab of your service, you can inspect the running tasks and the stopped tasks by clicking on the Task status selector. Containers that failed to start are shown when you select the Stopped status. Click on a task to go to the task details page. Under Stopped reason, it displays a message explaining why the task was stopped. If the task is running but youre still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Reasons for stopped tasks In the AWS ECS troubleshooting documentation you can find information on common causes of errors related to running tasks and services. See below for details about some reasons for stopped tasks. Task stopped with reason: Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/YOUR_ROLE_NAME is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\" Copy This means that the IAM role specified using executionRoleArn in the task definition doesn't have access to the secret used for the NRIA_LICENSE_KEY. The execution role should have a policy attached that grants it access to read the secret. Get the execution role of your task: aws ecs describe-task-definition --task-definition newrelic-infra --output text --query taskDefinition.executionRoleArn Copy You can replace the --task-definition newrelic-infra with the name of your fargate task that includes the sidecar container. aws ecs describe-task-definition --task-definition YOUR_FARGATE_TASK_NAME --output text --query taskDefinition.executionRoleArn Copy List the policies attached to role: aws iam list-attached-role-policies --role-name YOUR_EXECUTION_ROLE_NAME Copy This should return 3 policies AmazonECSTaskExecutionRolePolicy, AmazonEC2ContainerServiceforEC2Role and a third one that should grant read access to the license key. In the following example the policy it's named NewRelicLicenseKeySecretReadAccess. { \"AttachedPolicies\": [ { \"PolicyName\": \"AmazonECSTaskExecutionRolePolicy\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\" }, { \"PolicyName\": \"AmazonEC2ContainerServiceforEC2Role\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role\" }, { \"PolicyName\": \"YOUR_POLICY_NAME\", \"PolicyArn\": \"arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME\" } ] } Copy Retrieve the default policy version: aws iam get-policy-version --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --version-id $(aws iam get-policy --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --output text --query Policy.DefaultVersionId) Copy This retrieves the policy permissions. There should be an entry for Actionsecretsmanager:GetSecretValue if you used AWS Secrets Manager to store your license key, or an entry for ssm:GetParametersif you used AWS Systems Manager Parameter Store: AWS Secrets Manager { \"PolicyVersion\": { \"Document\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"secretsmanager:GetSecretValue\", \"Resource\": \"arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME\", \"Effect\": \"Allow\" } ] }, \"VersionId\": \"v1\", \"IsDefaultVersion\": true, \"CreateDate\": \"2020-03-31T13:47:07+00:00\" } } Copy AWS Systems Manager Parameter Store { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"ssm:GetParameters\", \"Resource\": [ \"arn:aws:ssm:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:parameter/YOUR_SECRET_NAME\" ], \"Effect\": \"Allow\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 194.20662,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "ECS <em>integration</em> <em>troubleshooting</em>: No data appears",
        "sections": "ECS <em>integration</em> <em>troubleshooting</em>: No data appears",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": ". <em>Troubleshoot</em> in the UI To use the UI to <em>troubleshoot</em>: Log in to your AWS Console and navigate to the EC2 <em>Container</em> <em>Service</em> section. Click on the cluster where you installed the New Relic ECS <em>integration</em>. On the Services tab, use the filter to search for the <em>integration</em> <em>service</em>. If you used"
      },
      "id": "60450883196a671c8c960f27"
    },
    {
      "sections": [
        "Introduction to the Amazon ECS integration",
        "Features",
        "Important",
        "Compatibility and requirements",
        "Install",
        "Check the source code"
      ],
      "title": "Introduction to the Amazon ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Get started"
      ],
      "external_id": "a2af5484b25f8595032cc1937210c9a41024a138",
      "image": "https://docs.newrelic.com/static/986bdb22950fdd8b222a850e205882a9/c1b63/new-relic-ecs-integration-dashboards_0.png",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/get-started/introduction-amazon-ecs-integration/",
      "published_at": "2021-05-05T18:16:27Z",
      "updated_at": "2021-03-30T21:13:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our ECS integration reports and displays performance data from your Amazon ECS environment. The ECS integration works well with other integrations, so you can also monitor services running on ECS. Features Amazon Elastic Container Service (ECS) is a scalable container management service that makes it easy to run, stop, and manage Docker containers on Amazon EC2 clusters. Our ECS integration instruments the underlying container instance (EC2 launch type) and the container layer by reporting metrics from ECS objects. The integration gives you insight into your ECS instances, tasks, services, and containers. one.newrelic.com > Explorer > ECS dashboard: The ECS integration reports performance data about your Amazon ECS containers. Features include: View your data in pre-built dashboards for immediate insight into your ECS environment. Create your own queries and charts in the query builder from automatically reported data. Create alert conditions on ECS data. Explore entities using the New Relic Explorer. Important New Relic also offers an ECS cloud integration, which reports a different data set than our on-host integration. For complete ECS monitoring, we recommend enabling both integrations. Compatibility and requirements Requirements: Amazon ECS container agent 1.21.0 or higher. Windows not supported. This integration uses our infrastructure agent and our Docker instrumentation: applicable requirements and restrictions of those systems apply. Install To install, see Install integration. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.93628,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the Amazon ECS <em>integration</em>",
        "sections": "Introduction to the Amazon ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "Our ECS <em>integration</em> reports and displays performance data from your Amazon ECS environment. The ECS <em>integration</em> works well with other <em>integrations</em>, so you can also monitor services running on ECS. Features Amazon <em>Elastic</em> <em>Container</em> <em>Service</em> (ECS) is a scalable <em>container</em> management <em>service</em> that makes"
      },
      "id": "603eb04b196a6752b5a83dc8"
    },
    {
      "sections": [
        "Install the ECS integration",
        "Tip",
        "Install overview",
        "Install using CloudFormation",
        "EC2 launch type",
        "Fargate launch type",
        "Install with automatic script",
        "Manual install",
        "AWS resources created"
      ],
      "title": "Install the ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Installation"
      ],
      "external_id": "857b78b6e7de76449f3f9569cee4700705b7d7fe",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/installation/install-ecs-integration/",
      "published_at": "2021-05-05T17:59:37Z",
      "updated_at": "2021-03-16T05:40:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's ECS integration reports and displays performance data from your Amazon ECS environment. This document explains how to install this integration. Tip To use ECS integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Install overview Before you install our ECS integration, we recommend reviewing the requirements. Here's a brief overview of what happens during the install process: For EC2 launch type: The infrastructure agent (newrelic-infra) gets deployed onto an ECS cluster as a service using the daemon scheduling strategy. This deployment installs the infrastructure agent in all the container instances of the cluster. The infrastructure agent then monitors ECS and Docker containers. For Fargate launch type: The infrastructure agent (newrelic-infra) gets deployed as a sidecar in every task to monitor. Install options: Install using AWS CloudFormation Install using automatic script Install manually Install using CloudFormation One install option is using AWS CloudFormation. We provide some CloudFormation templates that install the ECS integration onto your AWS account for both EC2 and Fargate launch types: To register the New Relic's ECS integration task, deploy this stack. Ensure youre deploying the stack to your desired region(s). This stack creates the following resources: A secret that stores the license key. A policy to access the license key. An instance role to be used as an ECS task ExecutionRole, with access to the license key. For EC2 launch type: Registers the New Relic Infrastructure ECS integration task. Follow the additional instructions for your launch type: EC2 launch type Additional steps for EC2 launch type: To create a service that runs the task on every container instance, deploy this stack. Fargate launch type Additional steps for Fargate launch type: Download the task definition example with the sidecar container to be deployed: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-fargate-example-latest.json Copy Add the newrelic-infra container in this task definition as a sidecar to the task definitions you want to monitor. In this example task, your application's containers replace the placeholder busybox container. Next steps: Wait a few minutes and then look for your data in the UI. Recommended: Install our ECS cloud integration, which gets you other ECS data, including information about clusters and services. See recommended alert conditions. Understand the AWS resources created by this process. Install with automatic script One install option is using our install script. To use the automatic install script: Download the ECS integration installer: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-installer.sh Copy Add execute permissions to the installer: chmod +x newrelic-infra-ecs-installer.sh Copy Execute it with -h to see the documentation and requirements: ./newrelic-infra-ecs-installer.sh -h Copy Check that your AWS profile points to the same region where your ECS cluster was created: $ aws configure get region us-east-1 $ aws ecs list-clusters YOUR_CLUSTER_ARNS arn:aws:ecs:us-east-1:YOUR_AWS_ACCOUNT:cluster/YOUR_CLUSTER Copy Execute the installer, specifying your license key and cluster name. EC2 launch type: ./newrelic-infra-ecs-installer.sh -c YOUR_CLUSTER_NAME -l YOUR_LICENSE_KEY Copy Fargate launch type: ./newrelic-infra-ecs-installer.sh -fargate -c YOUR_CLUSTER_NAME -l YOUR_LICENSE_KEY Copy Additional steps for Fargate launch type (not EC2 launch type): Download the task definition example with the sidecar container to be deployed: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-fargate-example-latest.json Copy Add the single container in this task definition as a sidecar to the task definitions you want monitored. Next steps: Wait a few minutes and then look for your data in the UI. Recommended: Install our ECS cloud integration, which gets you other ECS data, including information about clusters and services. See recommended alert conditions. Understand the AWS resources created by this process. Manual install One install option is to manually do the steps that are done by the automatic installer script. We will describe how this is done using the awscli tool: Check that your AWS profile points to the same region where your ECS cluster was created: $ aws configure get region us-east-1 $ aws ecs list-clusters YOUR_CLUSTER_ARNS arn:aws:ecs:us-east-1:YOUR_AWS_ACCOUNT:cluster/YOUR_CLUSTER Copy Save your New Relic license key as a Systems Manager (SSM) parameter: aws ssm put-parameter \\ --name \"/newrelic-infra/ecs/license-key\" \\ --type SecureString \\ --description 'New Relic license key for ECS monitoring' \\ --value \"NEW_RELIC_LICENSE_KEY\" Copy Create an IAM policy to access the license key parameter: aws iam create-policy \\ --policy-name \"NewRelicSSMLicenseKeyReadAccess\" \\ --policy-document \"{\"Version\"\\\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Action\":[\"ssm:GetParameters\"],\"Resource\":[\"ARN_OF_LICENSE_KEY_PARAMETER\"]}]}\" --description \"Provides read access to the New Relic SSM license key parameter\" Copy Create an IAM role to be used as the task execution role: aws iam create-role \\ --role-name \"NewRelicECSTaskExecutionRole\" \\ --assume-role-policy-document '{\"Version\":\"2008-10-17\",\"Statement\":[{\"Sid\":\"\",\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"ecs-tasks.amazonaws.com\"},\"Action\":\"sts:AssumeRole\"}]}' \\ --description \"ECS task execution role for New Relic infrastructure\" Copy Attach the policies NewRelicSSMLicenseKeyReadAccess, AmazonEC2ContainerServiceforEC2Role, and AmazonECSTaskExecutionRolePolicy to the role: aws iam attach-role-policy \\ --role-name \"NewRelicECSTaskExecutionRole\" \\ --policy-arn \"POLICY_ARN\" Copy Choose your launch type for more instructions: EC2 launch type Additional steps for EC2 launch type: Download the New Relic ECS integration task definition template file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-ec2-latest.json Copy Replace the task execution role in the template file with the newly created role: \"executionRoleArn\": \"NewRelicECSTaskExecutionRole\", Copy Replace the valueFrom attribute of the secret with the name of the Systems Manager parameter: secrets\": [ { \"valueFrom\": \"/newrelic-infra/ecs/license-key\", \"name\": \"NRIA_LICENSE_KEY\" } ], Copy Register the task definition file: aws ecs register-task-definition --cli-input-json file://newrelic-infra-ecs-ec2-latest.json Copy Create a service with the daemon scheduling strategy for the registered task: aws ecs create-service --cluster \"YOUR_CLUSTER_NAME\" --service-name \"newrelic-infra\" --task-definition \"newrelic-infra\" --scheduling-strategy DAEMON Copy Fargate launch type Additional steps for the Fargate launch type: Download the task definition example with the sidecar container to be deployed: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-fargate-example-latest.json Copy Add the newrelic-infra container in this task definition as a sidecar to the task definitions you want to monitor. In this example task, your application's containers replace the placeholder busybox container. Next steps: Wait a few minutes and then look for your data in the UI. Recommended: Install our ECS cloud integration, a separate integration which gets you supplementary ECS data, including information about clusters and services. See recommended alert conditions. Understand the AWS resources created by this process. AWS resources created When you install the ECS integration using default/recommended values, it does the following in AWS: Creates Systems Manager (SSM) parameter /newrelic-infra/ecs/license-key. This system parameter contains the New Relic license key. Creates IAM policy NewRelicSSMLicenseKeyReadAccess, which enables access to the SSM parameter with the license key. Creates IAM role NewRelicECSTaskExecutionRole used as the task execution role. Policies attached to the role: NewRelicSSMLicenseKeyReadAccess (created by the installer). AmazonEC2ContainerServiceforEC2Role AmazonECSTaskExecutionRolePolicy For EC2 launch type, this is also done: Registers the newrelic-infra ECS task definition. Creates the service newrelic-infra for the registered task using a daemon scheduling strategy.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 165.1419,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the ECS <em>integration</em>",
        "sections": "Install the ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "New Relic&#x27;s ECS <em>integration</em> reports and displays performance data from your Amazon ECS environment. This document explains how to install this <em>integration</em>. Tip To use ECS <em>integrations</em> and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign"
      },
      "id": "603e9e76196a676684a83de9"
    }
  ],
  "/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-no-data-appears": [
    {
      "sections": [
        "ECS integration troubleshooting: Generate verbose logs",
        "Problem",
        "Solution",
        "Using task definition environment variable",
        "Retrieve logs via SSH (EC2 launch type only)",
        "Forward logs to CloudWatch and download them with awscli",
        "From running container"
      ],
      "title": "ECS integration troubleshooting: Generate verbose logs",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Troubleshooting"
      ],
      "external_id": "06198f1b2e0faa69bd8a7dfb93f18c8955fea83b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-generate-verbose-logs/",
      "published_at": "2021-05-05T18:02:51Z",
      "updated_at": "2021-03-13T03:35:43Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem When troubleshooting the on-host ECS integration, you can generate verbose logs for a few minutes to find and investigate errors. This can be useful for conducting your own troubleshooting or when providing information to New Relic support. Verbose logging generates a lot of data very quickly. When finished generating logs, be sure to set verbose: 0 to reduce disk space consumption. You can automate this process by using the newrelic-infra-ctl command. For more information, see Troubleshooting a running agent. Solution Generating verbose log files requires editing your task definition file. For a sample config file that includes all applicable settings, see Infrastructure configuration settings. You have several options for implementing verbose logs: Change the task definition environment variable and do a task restart For EC2 launch type: Retrieve logs via SSH Forward to CloudWatch and download with awscli Run a command from the running container Using task definition environment variable To enable verbose logs by changing the environment variable and doing a task restart: Edit your task definition. Change the value of NRIA_VERBOSE from 0 to: 1 for always-on verbose logs 2 for smart logging 3 for sending to New Relic Read more about these options. Save your task definition. Update your service to use the newly registered task definition. If you chose NRIA_VERBOSE=3 and you're not sending the logs directly to New Relic, you have two options for viewing and downloading the logs: For EC2 launch type: you can retrieve the logs via SSH, or Forward logs to CloudWatch Return your settings to default: Disable verbose logging by editing your task definition and setting NRIA_VERBOSE to 0. Save your task definition. Update your service to the latest version of your task. Examine the log file for errors. If you need to send your log file to New Relic support: Include the line in your log file that contains the ECS integration version: New Relic ECS integration version X.YY.ZZZ Copy Attach the log file to your support ticket, along with your task definition .yml file. Retrieve logs via SSH (EC2 launch type only) To get logs via SSH: Edit your task definition. Change the value of NRIA_VERBOSE from 0 to: 1 for always-on verbose logs 2 for smart logging 3 for sending to New Relic Read more about these options. SSH into one of your container instances. Find the container ID of the New Relic integration container, by running the command docker ps -a. The name of the container should be nri-ecs. Save the logs from the container with the command docker logs NRI_ECS_CONTAINER_ID > logs.txt. Leave the command running for about three minutes to generate sufficient logging data. Continue with the instructions in the enable verbose logs section. Forward logs to CloudWatch and download them with awscli To get logs via CloudWatch: Edit your task definition. Change the value of NRIA_VERBOSE from 0 to: 1 for always-on verbose logs 2 for smart logging 3 for sending to New Relic Read more about these options. We use a CloudWatch log group called /newrelic-infra/ecs to forward the logs to. To see if it already exists, run: aws logs describe-log-groups --log-group-name-prefix /newrelic-infra/ecs Copy If a log group exists with that prefix, you'll get this output: { \"logGroups\": [ { \"logGroupName\": \"/newrelic-infra/ecs\", \"creationTime\": 1585828615225, \"metricFilterCount\": 0, \"arn\": \"arn:aws:logs:YOUR_REGION:YOUR_AWS_ACCOUNT:log-group:/newrelic-infra/ecs:*\", \"storedBytes\": 122539356 } ] } Copy Because this command matches log groups with prefixes, ensure the log group name returned is exactly /newrelic-infra/ecs. If the log group doesn't exist, the output will be: { \"logGroups\": [] } Copy If the log group doesn't exist, create it by running: aws logs create-log-group --log-group-name /newrelic-infra/ecs Copy Edit your task definition. In the container definition for the newrelic-infra container, add the following logConfiguration: \"logConfiguration\": { \"logDriver\": \"awslogs\", \"options\": { \"awslogs-group\": \"/newrelic-infra/ecs\", \"awslogs-region\": \"AWS_REGION_OF_YOUR_CLUSTER\", \"awslogs-stream-prefix\": \"verbose\" } } Copy Register the new task version and update your service. Next you'll look for the relevant log stream. If you have multiple instances of the task running, they'll all send their logs to the same log group but each will have its own log stream. Log streams names follow the structure AWSLOGS_STREAM_PREFIX/TASK_FAMILY_NAME/TASK_ID. In this case, it will be verbose/newrelic-infra/TASK_ID. To get all the log streams for a given log group, run this command: aws logs describe-log-streams --log-group-name /newrelic-infra/ecs Copy The following is an example output of a log group with two streams: { \"logStreams\": [ { \"logStreamName\": \"verbose/newrelic-infra/9dfb28114e40415ebc399ec1e53a21b7\", \"creationTime\": 1586166741197, \"firstEventTimestamp\": 1586166742030, \"lastEventTimestamp\": 1586173933472, \"lastIngestionTime\": 1586175101220, \"uploadSequenceToken\": \"49599989655680038369205623273330095416487086853777112338\", \"arn\": \"arn:aws:logs:AWS_REGION_OF_YOUR_CLUSTER:YOUR_AWS_ACCOUNT:log-group:/newrelic-infra/ecs:log-stream:verbose/newrelic-infra/9dfb28114e40415ebc399ec1e53a21b7\", \"storedBytes\": 0 }, { \"logStreamName\": \"verbose/newrelic-infra/f6ce0be416804bc4bfa658da5514eb00\", \"creationTime\": 1586166745643, \"firstEventTimestamp\": 1586166746491, \"lastEventTimestamp\": 1586173037927, \"lastIngestionTime\": 1586175100660, \"uploadSequenceToken\": \"49605664273821671319096446647846424799651902350804230514\", \"arn\": \"arn:aws:logs:AWS_REGION_OF_YOUR_CLUSTER:YOUR_AWS_ACCOUNT:log-group:/newrelic-infra/ecs:log-stream:verbose/newrelic-infra/f6ce0be416804bc4bfa658da5514eb00\", \"storedBytes\": 0 } ] } Copy From the previous list of log streams, identify the one with the task ID for which you want to retrieve the logs and use the logStreamName in this command: aws logs get-log-events --log-group-name /newrelic-infra/ecs --log-stream-name \"LOG_STREAM_NAME\" --output text > logs.txt Copy Continue with the enable verbose logs instructions. From running container To enable verbose logs by running a command from the running container: SSH into one of your container instances. Find the container ID of the New Relic integration container by running the command docker ps -a. The name of the container should be nri-ecs. Enable verbose logs for a limited period of time by using newrelic-infra-ctl. Run the command: docker exec INTEGRATION_CONTAINER_ID /usr/bin/newrelic-infra-ctl Copy For more details, see Troubleshoot the agent. Save the logs from the container with the command docker logs INTEGRATION_CONTAINER_ID > logs.txt Copy Leave the command running for about three minutes to generate sufficient logging data. Examine the log file for errors. If you need to send your log file to New Relic support: Include the line in your log file that contains the ECS integration version: New Relic ECS integration version X.YY.ZZZ Copy Attach the log file to your support ticket, along with your task definition .yml file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 183.34897,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "ECS <em>integration</em> <em>troubleshooting</em>: Generate verbose logs",
        "sections": "ECS <em>integration</em> <em>troubleshooting</em>: Generate verbose logs",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": " by running the command docker ps -a. The name of the <em>container</em> should be nri-ecs. Enable verbose logs for a limited period of time by using newrelic-infra-ctl. Run the command: docker exec <em>INTEGRATION_CONTAINER</em>_ID &#x2F;usr&#x2F;bin&#x2F;newrelic-infra-ctl Copy For more details, see <em>Troubleshoot</em> the agent. Save"
      },
      "id": "604507f9196a67c1ae960f5e"
    },
    {
      "sections": [
        "Introduction to the Amazon ECS integration",
        "Features",
        "Important",
        "Compatibility and requirements",
        "Install",
        "Check the source code"
      ],
      "title": "Introduction to the Amazon ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Get started"
      ],
      "external_id": "a2af5484b25f8595032cc1937210c9a41024a138",
      "image": "https://docs.newrelic.com/static/986bdb22950fdd8b222a850e205882a9/c1b63/new-relic-ecs-integration-dashboards_0.png",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/get-started/introduction-amazon-ecs-integration/",
      "published_at": "2021-05-05T18:16:27Z",
      "updated_at": "2021-03-30T21:13:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our ECS integration reports and displays performance data from your Amazon ECS environment. The ECS integration works well with other integrations, so you can also monitor services running on ECS. Features Amazon Elastic Container Service (ECS) is a scalable container management service that makes it easy to run, stop, and manage Docker containers on Amazon EC2 clusters. Our ECS integration instruments the underlying container instance (EC2 launch type) and the container layer by reporting metrics from ECS objects. The integration gives you insight into your ECS instances, tasks, services, and containers. one.newrelic.com > Explorer > ECS dashboard: The ECS integration reports performance data about your Amazon ECS containers. Features include: View your data in pre-built dashboards for immediate insight into your ECS environment. Create your own queries and charts in the query builder from automatically reported data. Create alert conditions on ECS data. Explore entities using the New Relic Explorer. Important New Relic also offers an ECS cloud integration, which reports a different data set than our on-host integration. For complete ECS monitoring, we recommend enabling both integrations. Compatibility and requirements Requirements: Amazon ECS container agent 1.21.0 or higher. Windows not supported. This integration uses our infrastructure agent and our Docker instrumentation: applicable requirements and restrictions of those systems apply. Install To install, see Install integration. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.93628,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the Amazon ECS <em>integration</em>",
        "sections": "Introduction to the Amazon ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "Our ECS <em>integration</em> reports and displays performance data from your Amazon ECS environment. The ECS <em>integration</em> works well with other <em>integrations</em>, so you can also monitor services running on ECS. Features Amazon <em>Elastic</em> <em>Container</em> <em>Service</em> (ECS) is a scalable <em>container</em> management <em>service</em> that makes"
      },
      "id": "603eb04b196a6752b5a83dc8"
    },
    {
      "sections": [
        "Install the ECS integration",
        "Tip",
        "Install overview",
        "Install using CloudFormation",
        "EC2 launch type",
        "Fargate launch type",
        "Install with automatic script",
        "Manual install",
        "AWS resources created"
      ],
      "title": "Install the ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Installation"
      ],
      "external_id": "857b78b6e7de76449f3f9569cee4700705b7d7fe",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/installation/install-ecs-integration/",
      "published_at": "2021-05-05T17:59:37Z",
      "updated_at": "2021-03-16T05:40:02Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's ECS integration reports and displays performance data from your Amazon ECS environment. This document explains how to install this integration. Tip To use ECS integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Install overview Before you install our ECS integration, we recommend reviewing the requirements. Here's a brief overview of what happens during the install process: For EC2 launch type: The infrastructure agent (newrelic-infra) gets deployed onto an ECS cluster as a service using the daemon scheduling strategy. This deployment installs the infrastructure agent in all the container instances of the cluster. The infrastructure agent then monitors ECS and Docker containers. For Fargate launch type: The infrastructure agent (newrelic-infra) gets deployed as a sidecar in every task to monitor. Install options: Install using AWS CloudFormation Install using automatic script Install manually Install using CloudFormation One install option is using AWS CloudFormation. We provide some CloudFormation templates that install the ECS integration onto your AWS account for both EC2 and Fargate launch types: To register the New Relic's ECS integration task, deploy this stack. Ensure youre deploying the stack to your desired region(s). This stack creates the following resources: A secret that stores the license key. A policy to access the license key. An instance role to be used as an ECS task ExecutionRole, with access to the license key. For EC2 launch type: Registers the New Relic Infrastructure ECS integration task. Follow the additional instructions for your launch type: EC2 launch type Additional steps for EC2 launch type: To create a service that runs the task on every container instance, deploy this stack. Fargate launch type Additional steps for Fargate launch type: Download the task definition example with the sidecar container to be deployed: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-fargate-example-latest.json Copy Add the newrelic-infra container in this task definition as a sidecar to the task definitions you want to monitor. In this example task, your application's containers replace the placeholder busybox container. Next steps: Wait a few minutes and then look for your data in the UI. Recommended: Install our ECS cloud integration, which gets you other ECS data, including information about clusters and services. See recommended alert conditions. Understand the AWS resources created by this process. Install with automatic script One install option is using our install script. To use the automatic install script: Download the ECS integration installer: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-installer.sh Copy Add execute permissions to the installer: chmod +x newrelic-infra-ecs-installer.sh Copy Execute it with -h to see the documentation and requirements: ./newrelic-infra-ecs-installer.sh -h Copy Check that your AWS profile points to the same region where your ECS cluster was created: $ aws configure get region us-east-1 $ aws ecs list-clusters YOUR_CLUSTER_ARNS arn:aws:ecs:us-east-1:YOUR_AWS_ACCOUNT:cluster/YOUR_CLUSTER Copy Execute the installer, specifying your license key and cluster name. EC2 launch type: ./newrelic-infra-ecs-installer.sh -c YOUR_CLUSTER_NAME -l YOUR_LICENSE_KEY Copy Fargate launch type: ./newrelic-infra-ecs-installer.sh -fargate -c YOUR_CLUSTER_NAME -l YOUR_LICENSE_KEY Copy Additional steps for Fargate launch type (not EC2 launch type): Download the task definition example with the sidecar container to be deployed: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-fargate-example-latest.json Copy Add the single container in this task definition as a sidecar to the task definitions you want monitored. Next steps: Wait a few minutes and then look for your data in the UI. Recommended: Install our ECS cloud integration, which gets you other ECS data, including information about clusters and services. See recommended alert conditions. Understand the AWS resources created by this process. Manual install One install option is to manually do the steps that are done by the automatic installer script. We will describe how this is done using the awscli tool: Check that your AWS profile points to the same region where your ECS cluster was created: $ aws configure get region us-east-1 $ aws ecs list-clusters YOUR_CLUSTER_ARNS arn:aws:ecs:us-east-1:YOUR_AWS_ACCOUNT:cluster/YOUR_CLUSTER Copy Save your New Relic license key as a Systems Manager (SSM) parameter: aws ssm put-parameter \\ --name \"/newrelic-infra/ecs/license-key\" \\ --type SecureString \\ --description 'New Relic license key for ECS monitoring' \\ --value \"NEW_RELIC_LICENSE_KEY\" Copy Create an IAM policy to access the license key parameter: aws iam create-policy \\ --policy-name \"NewRelicSSMLicenseKeyReadAccess\" \\ --policy-document \"{\"Version\"\\\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Action\":[\"ssm:GetParameters\"],\"Resource\":[\"ARN_OF_LICENSE_KEY_PARAMETER\"]}]}\" --description \"Provides read access to the New Relic SSM license key parameter\" Copy Create an IAM role to be used as the task execution role: aws iam create-role \\ --role-name \"NewRelicECSTaskExecutionRole\" \\ --assume-role-policy-document '{\"Version\":\"2008-10-17\",\"Statement\":[{\"Sid\":\"\",\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"ecs-tasks.amazonaws.com\"},\"Action\":\"sts:AssumeRole\"}]}' \\ --description \"ECS task execution role for New Relic infrastructure\" Copy Attach the policies NewRelicSSMLicenseKeyReadAccess, AmazonEC2ContainerServiceforEC2Role, and AmazonECSTaskExecutionRolePolicy to the role: aws iam attach-role-policy \\ --role-name \"NewRelicECSTaskExecutionRole\" \\ --policy-arn \"POLICY_ARN\" Copy Choose your launch type for more instructions: EC2 launch type Additional steps for EC2 launch type: Download the New Relic ECS integration task definition template file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-ec2-latest.json Copy Replace the task execution role in the template file with the newly created role: \"executionRoleArn\": \"NewRelicECSTaskExecutionRole\", Copy Replace the valueFrom attribute of the secret with the name of the Systems Manager parameter: secrets\": [ { \"valueFrom\": \"/newrelic-infra/ecs/license-key\", \"name\": \"NRIA_LICENSE_KEY\" } ], Copy Register the task definition file: aws ecs register-task-definition --cli-input-json file://newrelic-infra-ecs-ec2-latest.json Copy Create a service with the daemon scheduling strategy for the registered task: aws ecs create-service --cluster \"YOUR_CLUSTER_NAME\" --service-name \"newrelic-infra\" --task-definition \"newrelic-infra\" --scheduling-strategy DAEMON Copy Fargate launch type Additional steps for the Fargate launch type: Download the task definition example with the sidecar container to be deployed: curl -O https://download.newrelic.com/infrastructure_agent/integrations/ecs/newrelic-infra-ecs-fargate-example-latest.json Copy Add the newrelic-infra container in this task definition as a sidecar to the task definitions you want to monitor. In this example task, your application's containers replace the placeholder busybox container. Next steps: Wait a few minutes and then look for your data in the UI. Recommended: Install our ECS cloud integration, a separate integration which gets you supplementary ECS data, including information about clusters and services. See recommended alert conditions. Understand the AWS resources created by this process. AWS resources created When you install the ECS integration using default/recommended values, it does the following in AWS: Creates Systems Manager (SSM) parameter /newrelic-infra/ecs/license-key. This system parameter contains the New Relic license key. Creates IAM policy NewRelicSSMLicenseKeyReadAccess, which enables access to the SSM parameter with the license key. Creates IAM role NewRelicECSTaskExecutionRole used as the task execution role. Policies attached to the role: NewRelicSSMLicenseKeyReadAccess (created by the installer). AmazonEC2ContainerServiceforEC2Role AmazonECSTaskExecutionRolePolicy For EC2 launch type, this is also done: Registers the newrelic-infra ECS task definition. Creates the service newrelic-infra for the registered task using a daemon scheduling strategy.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 165.14189,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the ECS <em>integration</em>",
        "sections": "Install the ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "New Relic&#x27;s ECS <em>integration</em> reports and displays performance data from your Amazon ECS environment. This document explains how to install this <em>integration</em>. Tip To use ECS <em>integrations</em> and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign"
      },
      "id": "603e9e76196a676684a83de9"
    }
  ],
  "/docs/integrations/elastic-container-service-integration/understand-use-data/ecs-integration-recommended-alert-conditions": [
    {
      "sections": [
        "Understand and use ECS data",
        "View data",
        "Query your data"
      ],
      "title": "Understand and use ECS data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Understand use data"
      ],
      "external_id": "16689cc080d4a8482e802b404df9ae45c4283db2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/understand-use-data/understand-use-ecs-data/",
      "published_at": "2021-05-05T18:02:51Z",
      "updated_at": "2021-03-29T20:30:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's on-host ECS integration reports and displays performance data from your Amazon ECS environment. Here we explain how to find, understand, and use the data reported by this integration. View data To view the ECS integration dashboard: Go to one.newrelic.com and select Explorer. On the left, search for ECS clusters, or type the name of your ECS cluster in the search bar. To view a dashboard, select the entity name corresponding to your ECS cluster. In addition to the pre-built dashboards, you can also create your own custom queries and charts using the query builder. To learn how to query this data, see Understand data. Query your data Data reported by this integration is displayed in its dashboards and is also available for querying and the creation of custom charts and dashboards. This integration reports an EcsClusterSample event, with attributes clusterName and arn. Other types of data that may be available for querying: Infrastructure agent-reported events, including Docker All the events reported from an ECS cluster contain the attributes ecsClusterName and ecsClusterArn. Here's an example NRQL query that returns the count of containers associated with each Docker image in an ECS cluster named MyClusterName created in us-east-1: SELECT uniqueCount(containerId) FROM ContainerSample WHERE awsRegion = 'us-east-1' AND ecsClusterName = 'MyClusterName' FACET imageName SINCE 1 HOUR AGO Copy To learn more about creating custom queries and charts: How to query New Relic data Introduction to NRQL",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 242.13684,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Understand</em> and <em>use</em> ECS <em>data</em>",
        "sections": "<em>Understand</em> and <em>use</em> ECS <em>data</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "New Relic&#x27;s on-host ECS <em>integration</em> reports and displays performance <em>data</em> from your Amazon ECS environment. Here we explain how to find, <em>understand</em>, and <em>use</em> the <em>data</em> reported by this <em>integration</em>. View <em>data</em> To view the ECS <em>integration</em> dashboard: Go to one.newrelic.com and select Explorer"
      },
      "id": "603e9eb664441fbaad4e889f"
    },
    {
      "sections": [
        "Introduction to the Amazon ECS integration",
        "Features",
        "Important",
        "Compatibility and requirements",
        "Install",
        "Check the source code"
      ],
      "title": "Introduction to the Amazon ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Get started"
      ],
      "external_id": "a2af5484b25f8595032cc1937210c9a41024a138",
      "image": "https://docs.newrelic.com/static/986bdb22950fdd8b222a850e205882a9/c1b63/new-relic-ecs-integration-dashboards_0.png",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/get-started/introduction-amazon-ecs-integration/",
      "published_at": "2021-05-05T18:16:27Z",
      "updated_at": "2021-03-30T21:13:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our ECS integration reports and displays performance data from your Amazon ECS environment. The ECS integration works well with other integrations, so you can also monitor services running on ECS. Features Amazon Elastic Container Service (ECS) is a scalable container management service that makes it easy to run, stop, and manage Docker containers on Amazon EC2 clusters. Our ECS integration instruments the underlying container instance (EC2 launch type) and the container layer by reporting metrics from ECS objects. The integration gives you insight into your ECS instances, tasks, services, and containers. one.newrelic.com > Explorer > ECS dashboard: The ECS integration reports performance data about your Amazon ECS containers. Features include: View your data in pre-built dashboards for immediate insight into your ECS environment. Create your own queries and charts in the query builder from automatically reported data. Create alert conditions on ECS data. Explore entities using the New Relic Explorer. Important New Relic also offers an ECS cloud integration, which reports a different data set than our on-host integration. For complete ECS monitoring, we recommend enabling both integrations. Compatibility and requirements Requirements: Amazon ECS container agent 1.21.0 or higher. Windows not supported. This integration uses our infrastructure agent and our Docker instrumentation: applicable requirements and restrictions of those systems apply. Install To install, see Install integration. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.29068,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the Amazon ECS <em>integration</em>",
        "sections": "Introduction to the Amazon ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "Our ECS <em>integration</em> reports and displays performance <em>data</em> from your Amazon ECS environment. The ECS <em>integration</em> works well with other <em>integrations</em>, so you can also monitor services running on ECS. Features Amazon <em>Elastic</em> <em>Container</em> <em>Service</em> (ECS) is a scalable <em>container</em> management <em>service</em> that makes"
      },
      "id": "603eb04b196a6752b5a83dc8"
    },
    {
      "sections": [
        "ECS integration troubleshooting: No data appears",
        "Problem",
        "Important",
        "Solution",
        "Troubleshoot via awscli",
        "Troubleshoot in the UI",
        "Reasons for stopped tasks",
        "AWS Secrets Manager",
        "AWS Systems Manager Parameter Store"
      ],
      "title": "ECS integration troubleshooting: No data appears",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Troubleshooting"
      ],
      "external_id": "a86730dfe4c4cfdb6d293675c2c97e7393939331",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-no-data-appears/",
      "published_at": "2021-05-05T18:02:51Z",
      "updated_at": "2021-03-30T12:41:02Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed our on-host ECS integration and waited a few minutes, but your cluster is not showing in the explorer. Important We have two ECS integrations: a cloud-based integration and an on-host integration. This document is about the on-host integration. Solution If your New Relic account had previously installed the infrastructure agent or an infrastructure on-host integration, your data should appear in the UI within a few minutes. If your account had not previously done either of those things before installing the on-host ECS integration, it may take tens of minutes for data to appear in the UI. In that case, we recommend waiting up to an hour before doing the following troubleshooting steps or contacting support. There are several options for troubleshooting no data appearing: Troubleshoot via the awscli tool (recommended when talking to New Relic technical support) Troubleshoot via the UI For information about stopped tasks, see Stopped tasks reasons. Troubleshoot via awscli When interacting with New Relic support, use this method and send the generated files with your support request: Retrieve the information related to the newrelic-infra service or the Fargate service that contains a task with a newrelic-infra sidecar: aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service newrelic-infra > newrelic-infra-service.json Copy aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service YOUR_FARGATE_SERVICE_WITH_NEW_RELIC_SIDECAR > newrelic-infra-sidecar-service.json Copy The failures attribute details any errors for the services. Under services is the status attribute. It says ACTIVE if the service has no issues. The desiredCount should match the runningCount. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. The pendingCount attribute should be zero, because all tasks should be running. Inspect the events attribute of services to check for issues with scheduling or starting the tasks. For example: if the service is unable to start tasks successfully, it will display a message like: { \"id\": \"5295a13c-34e6-41e1-96dd-8364c42cc7a9\", \"createdAt\": \"2020-04-06T15:28:18.298000+02:00\", \"message\": \"(service newrelic-ifnra) is unable to consistently start tasks successfully. For more information, see the Troubleshooting section of the Amazon ECS Developer Guide.\" } Copy In the same section, you can also see which tasks were started by the service from the events: { \"id\": \"1c0a6ce2-de2e-49b2-b0ac-6458a804d0f0\", \"createdAt\": \"2020-04-06T15:27:49.614000+02:00\", \"message\": \"(service fargate-fail) has started 1 tasks: (task YOUR_TASK_ID).\" } Copy Retrieve the information related to the task with this command: aws ecs describe-tasks --tasks YOUR_TASK_ID --cluster YOUR_CLUSTER_NAME > newrelic-infra-task.json Copy The desiredStatus and lastStatus should be RUNNING. If the task couldn't start normally, it will have a STOPPED status. Inspect the stopCode and stoppedReason. One reason example: a task that couldn't be started because the task execution role doesn't have the appropriate permissions to download the license-key-containing secret would have the following output: \"stopCode\": \"TaskFailedToStart\", \"stoppedAt\": \"2020-04-06T15:28:54.725000+02:00\", \"stoppedReason\": \"Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/NewRelicECSIntegration-Ne-NewRelicECSTaskExecution-1C0ODHVT4HDNT/8637b461f0f94d649e9247e2f14c3803 is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail-DmLHfs status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\", \"stoppingAt\": \"2020-04-06T15:28:10.953000+02:00\", Copy If the task is running but youre still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Troubleshoot in the UI To use the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 Container Service section. Click on the cluster where you installed the New Relic ECS integration. On the Services tab, use the filter to search for the integration service. If you used the automatic install script, the name of the service will be newrelic-infra. If you are using Fargate, it will be the name of your monitored service. Once found, click on the name. The service page shows the Status of the service. It says ACTIVE if the service has no issues. On the same page, the Desired count should match the Running count. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. Pending count should be zero, because all tasks should be running. Inspect the Events tab to check for issues with scheduling or starting the tasks. In the Tasks tab of your service, you can inspect the running tasks and the stopped tasks by clicking on the Task status selector. Containers that failed to start are shown when you select the Stopped status. Click on a task to go to the task details page. Under Stopped reason, it displays a message explaining why the task was stopped. If the task is running but youre still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Reasons for stopped tasks In the AWS ECS troubleshooting documentation you can find information on common causes of errors related to running tasks and services. See below for details about some reasons for stopped tasks. Task stopped with reason: Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/YOUR_ROLE_NAME is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\" Copy This means that the IAM role specified using executionRoleArn in the task definition doesn't have access to the secret used for the NRIA_LICENSE_KEY. The execution role should have a policy attached that grants it access to read the secret. Get the execution role of your task: aws ecs describe-task-definition --task-definition newrelic-infra --output text --query taskDefinition.executionRoleArn Copy You can replace the --task-definition newrelic-infra with the name of your fargate task that includes the sidecar container. aws ecs describe-task-definition --task-definition YOUR_FARGATE_TASK_NAME --output text --query taskDefinition.executionRoleArn Copy List the policies attached to role: aws iam list-attached-role-policies --role-name YOUR_EXECUTION_ROLE_NAME Copy This should return 3 policies AmazonECSTaskExecutionRolePolicy, AmazonEC2ContainerServiceforEC2Role and a third one that should grant read access to the license key. In the following example the policy it's named NewRelicLicenseKeySecretReadAccess. { \"AttachedPolicies\": [ { \"PolicyName\": \"AmazonECSTaskExecutionRolePolicy\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\" }, { \"PolicyName\": \"AmazonEC2ContainerServiceforEC2Role\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role\" }, { \"PolicyName\": \"YOUR_POLICY_NAME\", \"PolicyArn\": \"arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME\" } ] } Copy Retrieve the default policy version: aws iam get-policy-version --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --version-id $(aws iam get-policy --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --output text --query Policy.DefaultVersionId) Copy This retrieves the policy permissions. There should be an entry for Actionsecretsmanager:GetSecretValue if you used AWS Secrets Manager to store your license key, or an entry for ssm:GetParametersif you used AWS Systems Manager Parameter Store: AWS Secrets Manager { \"PolicyVersion\": { \"Document\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"secretsmanager:GetSecretValue\", \"Resource\": \"arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME\", \"Effect\": \"Allow\" } ] }, \"VersionId\": \"v1\", \"IsDefaultVersion\": true, \"CreateDate\": \"2020-03-31T13:47:07+00:00\" } } Copy AWS Systems Manager Parameter Store { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"ssm:GetParameters\", \"Resource\": [ \"arn:aws:ssm:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:parameter/YOUR_SECRET_NAME\" ], \"Effect\": \"Allow\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 171.82933,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "ECS <em>integration</em> troubleshooting: No <em>data</em> appears",
        "sections": "ECS <em>integration</em> troubleshooting: No <em>data</em> appears",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": ". Troubleshoot in the UI To <em>use</em> the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 <em>Container</em> <em>Service</em> section. Click on the cluster where you installed the New Relic ECS <em>integration</em>. On the Services tab, <em>use</em> the filter to search for the <em>integration</em> <em>service</em>. If you used"
      },
      "id": "60450883196a671c8c960f27"
    }
  ],
  "/docs/integrations/elastic-container-service-integration/understand-use-data/understand-use-ecs-data": [
    {
      "sections": [
        "Recommended ECS alert conditions",
        "Recommended alert conditions"
      ],
      "title": "Recommended ECS alert conditions",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Understand use data"
      ],
      "external_id": "334d80a75b3ef0a7b6125bf2a15f643ea46d7282",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/understand-use-data/ecs-integration-recommended-alert-conditions/",
      "published_at": "2021-05-05T00:11:39Z",
      "updated_at": "2021-03-16T05:41:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's ECS integration reports and displays performance data from your Amazon ECS environment. This document provides some recommended alert conditions for monitoring ECS performance. Recommended alert conditions Here are some recommended ECS alert conditions. To add these alerts, go to the Alerts UI and add the following NRQL alert conditions to an existing or new alert policy: High CPU usage NRQL: FROM ContainerSample SELECT cpuUsed / cpuLimitCores Critical: > 90% for 5 minutes High memory usage NRQL: FROM ContainerSample SELECT memoryUsageBytes / memorySizeLimitBytes Critical: > 80% for 5 minutes Restart count NRQL: FROM ContainerSample SELECT max(restartCount) - min(restartCount) Critical: > 5 for 5 minutes",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 231.62993,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "New Relic&#x27;s ECS <em>integration</em> reports and displays performance <em>data</em> from your Amazon ECS environment. This document provides some recommended alert conditions for monitoring ECS performance. Recommended alert conditions Here are some recommended ECS alert conditions. To add these alerts, go"
      },
      "id": "603e7eee64441f0f674e889f"
    },
    {
      "sections": [
        "Introduction to the Amazon ECS integration",
        "Features",
        "Important",
        "Compatibility and requirements",
        "Install",
        "Check the source code"
      ],
      "title": "Introduction to the Amazon ECS integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Get started"
      ],
      "external_id": "a2af5484b25f8595032cc1937210c9a41024a138",
      "image": "https://docs.newrelic.com/static/986bdb22950fdd8b222a850e205882a9/c1b63/new-relic-ecs-integration-dashboards_0.png",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/get-started/introduction-amazon-ecs-integration/",
      "published_at": "2021-05-05T18:16:27Z",
      "updated_at": "2021-03-30T21:13:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our ECS integration reports and displays performance data from your Amazon ECS environment. The ECS integration works well with other integrations, so you can also monitor services running on ECS. Features Amazon Elastic Container Service (ECS) is a scalable container management service that makes it easy to run, stop, and manage Docker containers on Amazon EC2 clusters. Our ECS integration instruments the underlying container instance (EC2 launch type) and the container layer by reporting metrics from ECS objects. The integration gives you insight into your ECS instances, tasks, services, and containers. one.newrelic.com > Explorer > ECS dashboard: The ECS integration reports performance data about your Amazon ECS containers. Features include: View your data in pre-built dashboards for immediate insight into your ECS environment. Create your own queries and charts in the query builder from automatically reported data. Create alert conditions on ECS data. Explore entities using the New Relic Explorer. Important New Relic also offers an ECS cloud integration, which reports a different data set than our on-host integration. For complete ECS monitoring, we recommend enabling both integrations. Compatibility and requirements Requirements: Amazon ECS container agent 1.21.0 or higher. Windows not supported. This integration uses our infrastructure agent and our Docker instrumentation: applicable requirements and restrictions of those systems apply. Install To install, see Install integration. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.29066,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the Amazon ECS <em>integration</em>",
        "sections": "Introduction to the Amazon ECS <em>integration</em>",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": "Our ECS <em>integration</em> reports and displays performance <em>data</em> from your Amazon ECS environment. The ECS <em>integration</em> works well with other <em>integrations</em>, so you can also monitor services running on ECS. Features Amazon <em>Elastic</em> <em>Container</em> <em>Service</em> (ECS) is a scalable <em>container</em> management <em>service</em> that makes"
      },
      "id": "603eb04b196a6752b5a83dc8"
    },
    {
      "sections": [
        "ECS integration troubleshooting: No data appears",
        "Problem",
        "Important",
        "Solution",
        "Troubleshoot via awscli",
        "Troubleshoot in the UI",
        "Reasons for stopped tasks",
        "AWS Secrets Manager",
        "AWS Systems Manager Parameter Store"
      ],
      "title": "ECS integration troubleshooting: No data appears",
      "type": "docs",
      "tags": [
        "Integrations",
        "Elastic Container Service integration",
        "Troubleshooting"
      ],
      "external_id": "a86730dfe4c4cfdb6d293675c2c97e7393939331",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/elastic-container-service-integration/troubleshooting/ecs-integration-troubleshooting-no-data-appears/",
      "published_at": "2021-05-05T18:02:51Z",
      "updated_at": "2021-03-30T12:41:02Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed our on-host ECS integration and waited a few minutes, but your cluster is not showing in the explorer. Important We have two ECS integrations: a cloud-based integration and an on-host integration. This document is about the on-host integration. Solution If your New Relic account had previously installed the infrastructure agent or an infrastructure on-host integration, your data should appear in the UI within a few minutes. If your account had not previously done either of those things before installing the on-host ECS integration, it may take tens of minutes for data to appear in the UI. In that case, we recommend waiting up to an hour before doing the following troubleshooting steps or contacting support. There are several options for troubleshooting no data appearing: Troubleshoot via the awscli tool (recommended when talking to New Relic technical support) Troubleshoot via the UI For information about stopped tasks, see Stopped tasks reasons. Troubleshoot via awscli When interacting with New Relic support, use this method and send the generated files with your support request: Retrieve the information related to the newrelic-infra service or the Fargate service that contains a task with a newrelic-infra sidecar: aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service newrelic-infra > newrelic-infra-service.json Copy aws ecs describe-services --cluster YOUR_CLUSTER_NAME --service YOUR_FARGATE_SERVICE_WITH_NEW_RELIC_SIDECAR > newrelic-infra-sidecar-service.json Copy The failures attribute details any errors for the services. Under services is the status attribute. It says ACTIVE if the service has no issues. The desiredCount should match the runningCount. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. The pendingCount attribute should be zero, because all tasks should be running. Inspect the events attribute of services to check for issues with scheduling or starting the tasks. For example: if the service is unable to start tasks successfully, it will display a message like: { \"id\": \"5295a13c-34e6-41e1-96dd-8364c42cc7a9\", \"createdAt\": \"2020-04-06T15:28:18.298000+02:00\", \"message\": \"(service newrelic-ifnra) is unable to consistently start tasks successfully. For more information, see the Troubleshooting section of the Amazon ECS Developer Guide.\" } Copy In the same section, you can also see which tasks were started by the service from the events: { \"id\": \"1c0a6ce2-de2e-49b2-b0ac-6458a804d0f0\", \"createdAt\": \"2020-04-06T15:27:49.614000+02:00\", \"message\": \"(service fargate-fail) has started 1 tasks: (task YOUR_TASK_ID).\" } Copy Retrieve the information related to the task with this command: aws ecs describe-tasks --tasks YOUR_TASK_ID --cluster YOUR_CLUSTER_NAME > newrelic-infra-task.json Copy The desiredStatus and lastStatus should be RUNNING. If the task couldn't start normally, it will have a STOPPED status. Inspect the stopCode and stoppedReason. One reason example: a task that couldn't be started because the task execution role doesn't have the appropriate permissions to download the license-key-containing secret would have the following output: \"stopCode\": \"TaskFailedToStart\", \"stoppedAt\": \"2020-04-06T15:28:54.725000+02:00\", \"stoppedReason\": \"Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/NewRelicECSIntegration-Ne-NewRelicECSTaskExecution-1C0ODHVT4HDNT/8637b461f0f94d649e9247e2f14c3803 is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:NewRelicLicenseKeySecret-Dh2dLkgV8VyJ-80RAHS-fail-DmLHfs status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\", \"stoppingAt\": \"2020-04-06T15:28:10.953000+02:00\", Copy If the task is running but youre still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Troubleshoot in the UI To use the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 Container Service section. Click on the cluster where you installed the New Relic ECS integration. On the Services tab, use the filter to search for the integration service. If you used the automatic install script, the name of the service will be newrelic-infra. If you are using Fargate, it will be the name of your monitored service. Once found, click on the name. The service page shows the Status of the service. It says ACTIVE if the service has no issues. On the same page, the Desired count should match the Running count. This is the number of tasks the service is handling. Because we use the daemon service type, there should be one task per container instance in your cluster. Pending count should be zero, because all tasks should be running. Inspect the Events tab to check for issues with scheduling or starting the tasks. In the Tasks tab of your service, you can inspect the running tasks and the stopped tasks by clicking on the Task status selector. Containers that failed to start are shown when you select the Stopped status. Click on a task to go to the task details page. Under Stopped reason, it displays a message explaining why the task was stopped. If the task is running but youre still not seeing data, generate verbose logs and examine them for errors. For details about reasons for stopped tasks, see Stopped tasks. Reasons for stopped tasks In the AWS ECS troubleshooting documentation you can find information on common causes of errors related to running tasks and services. See below for details about some reasons for stopped tasks. Task stopped with reason: Fetching secret data from AWS Secrets Manager in region YOUR_AWS_REGION: secret arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME: AccessDeniedException: User: arn:aws:sts::YOUR_AWS_ACCOUNT:assumed-role/YOUR_ROLE_NAME is not authorized to perform: secretsmanager:GetSecretValue on resource: arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME status code: 400, request id: 9cf1881e-14d7-4257-b4a8-be9b56e09e3c\" Copy This means that the IAM role specified using executionRoleArn in the task definition doesn't have access to the secret used for the NRIA_LICENSE_KEY. The execution role should have a policy attached that grants it access to read the secret. Get the execution role of your task: aws ecs describe-task-definition --task-definition newrelic-infra --output text --query taskDefinition.executionRoleArn Copy You can replace the --task-definition newrelic-infra with the name of your fargate task that includes the sidecar container. aws ecs describe-task-definition --task-definition YOUR_FARGATE_TASK_NAME --output text --query taskDefinition.executionRoleArn Copy List the policies attached to role: aws iam list-attached-role-policies --role-name YOUR_EXECUTION_ROLE_NAME Copy This should return 3 policies AmazonECSTaskExecutionRolePolicy, AmazonEC2ContainerServiceforEC2Role and a third one that should grant read access to the license key. In the following example the policy it's named NewRelicLicenseKeySecretReadAccess. { \"AttachedPolicies\": [ { \"PolicyName\": \"AmazonECSTaskExecutionRolePolicy\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\" }, { \"PolicyName\": \"AmazonEC2ContainerServiceforEC2Role\", \"PolicyArn\": \"arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role\" }, { \"PolicyName\": \"YOUR_POLICY_NAME\", \"PolicyArn\": \"arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME\" } ] } Copy Retrieve the default policy version: aws iam get-policy-version --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --version-id $(aws iam get-policy --policy-arn arn:aws:iam::YOUR_AWS_ACCOUNT:policy/YOUR_POLICY_NAME --output text --query Policy.DefaultVersionId) Copy This retrieves the policy permissions. There should be an entry for Actionsecretsmanager:GetSecretValue if you used AWS Secrets Manager to store your license key, or an entry for ssm:GetParametersif you used AWS Systems Manager Parameter Store: AWS Secrets Manager { \"PolicyVersion\": { \"Document\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"secretsmanager:GetSecretValue\", \"Resource\": \"arn:aws:secretsmanager:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:secret:YOUR_SECRET_NAME\", \"Effect\": \"Allow\" } ] }, \"VersionId\": \"v1\", \"IsDefaultVersion\": true, \"CreateDate\": \"2020-03-31T13:47:07+00:00\" } } Copy AWS Systems Manager Parameter Store { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"ssm:GetParameters\", \"Resource\": [ \"arn:aws:ssm:YOUR_AWS_REGION:YOUR_AWS_ACCOUNT:parameter/YOUR_SECRET_NAME\" ], \"Effect\": \"Allow\" } ] } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 171.82932,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "ECS <em>integration</em> troubleshooting: No <em>data</em> appears",
        "sections": "ECS <em>integration</em> troubleshooting: No <em>data</em> appears",
        "tags": "<em>Elastic</em> <em>Container</em> <em>Service</em> <em>integration</em>",
        "body": ". Troubleshoot in the UI To <em>use</em> the UI to troubleshoot: Log in to your AWS Console and navigate to the EC2 <em>Container</em> <em>Service</em> section. Click on the cluster where you installed the New Relic ECS <em>integration</em>. On the Services tab, <em>use</em> the filter to search for the <em>integration</em> <em>service</em>. If you used"
      },
      "id": "60450883196a671c8c960f27"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-app-engine-monitoring-integration": [
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-05-04T16:50:33Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.3892,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-05-05T01:23:56Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.7067,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-05-05T01:27:00Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70596,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-bigquery-monitoring-integration": [
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-05-04T16:50:33Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.3892,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-05-05T01:23:56Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.7067,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-05-05T01:27:00Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70596,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-bigtable-monitoring-integration": [
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-05-04T16:50:33Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.3892,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-05-05T01:23:56Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.7067,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-05-05T01:27:00Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70596,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-composer-monitoring-integration": [
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-05-04T16:50:33Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.3892,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-05-05T01:23:56Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.7067,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-05-05T01:27:00Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70596,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-dataflow-monitoring-integration": [
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-05-04T16:50:33Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.3892,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-05-05T01:23:56Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.7067,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-05-05T01:27:00Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70596,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-dataproc-monitoring-integration": [
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-05-04T16:50:33Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.3892,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-05-05T01:23:56Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.7067,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-05-05T01:27:00Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70596,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-firebase-database-monitoring-integration": [
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-05-04T16:50:33Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.38919,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-05-05T01:23:56Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70668,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-05-05T01:27:00Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70595,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-firebase-hosting-monitoring-integration": [
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-05-04T16:50:33Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.38919,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-05-05T01:23:56Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70668,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-05-05T01:27:00Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70595,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-firebase-storage-monitoring-integration": [
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-05-04T16:50:33Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.38919,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-05-05T01:23:56Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70668,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-05-05T01:27:00Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70595,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-firestore-monitoring-integration": [
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-05-04T16:50:33Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.38919,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-05-05T01:23:56Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70668,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-05-05T01:27:00Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70595,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration": [
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-05-05T01:23:56Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70667,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-05-05T01:27:00Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70593,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    },
    {
      "sections": [
        "Google Compute Engine monitoring integration",
        "Activate integration",
        "Important",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "GcpVirtualMachineSample",
        "GcpVirtualMachineDiskSample",
        "Inventory data",
        "gcp/compute/virtual-machine",
        "gcp/compute/virtual-machine/disk",
        "Learn more"
      ],
      "title": "Google Compute Engine monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "749ce2f670e38c332eb8b591fb0fbf0098ba157f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration/",
      "published_at": "2021-05-05T05:13:26Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "All New Relic Infrastructure accounts, regardless of subscription level, can use New Relic's Compute Engine integration to get a comprehensive, real-time view of their host's performance and status. New Relic Infrastructure's integration with Google Compute Engine reports metadata about instances (virtual machines) hosted on Google's infrastructure. You can monitor and alert on your GCP instances data from New Relic Infrastructure, and you can create custom queries and chart dashboards in New Relic Insights. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your Google Cloud projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic Infrastructure integrations query your GCP services according to a polling interval, which varies depending on the integration. The polling interval for the Google Compute Engine integration is 5 minutes. Find and use data After activating the integration and waiting a few minutes (based on the polling frequency), data will appear in the New Relic UI. To find and use your data, including links to your dashboards and alert settings, go to infrastructure.newrelic.com > GCP > (select an integration). Metric data Metric data that New Relic receives from your GCP Compute Engine integration include: GcpVirtualMachineSample Name Description firewall.DroppedBytes Delta count of incoming bytes dropped by the firewall. firewall.DroppedPackets Delta count of incoming packets dropped by the firewall. instance.cpu.ReservedCores Total number of cores reserved on the host of the instance. GcpVirtualMachineDiskSample Name Description instance.disk.ThrottledReadBytes Delta count of bytes in throttled read operations. instance.disk.ThrottledReadOps Delta count of throttled read operations. instance.disk.ThrottledWriteBytes Delta count of bytes in throttled write operations. instance.disk.ThrottledWriteOps Delta count of throttled write operations. Inventory data Inventory data is information about the status or configuration of a service or host. You can examine inventory data in New Relic Infrastructure and in New Relic Insights. The Google Compute Engine integration reports configuration information and labels for virtual machines and disks through the properties listed below. Virtual machine tags are treated as labels that take the value true. gcp/compute/virtual-machine automaticRestart canIpForward cpuPlatform creationTimestamp deletionProtection description instanceId isPreemptible label.* machineType metadataFingerprint name networkInterfaces onHostMaintenance project status networkTags zone gcp/compute/virtual-machine/disk creationTimestamp description diskId encrypted instanceId instanceName label.* lastAttachTimestamp lastDetachTimestamp licenses name project replicaZones sizeGb sourceImage sourceImageId sourceSnapshot sourceSnapshotId status type users zone Learn more",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70593,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "sections": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " your <em>GCP</em> service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your <em>Google</em> <em>Cloud</em> projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic"
      },
      "id": "603e7d1f28ccbc483ceba771"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-load-balancing-monitoring-integration": [
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-05-04T16:50:33Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.38918,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-05-05T01:23:56Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70667,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-05-05T01:27:00Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70593,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-pubsub-monitoring-integration": [
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-05-04T16:50:33Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.38918,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-05-05T01:23:56Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70667,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-05-05T01:27:00Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70593,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-router-monitoring-integration": [
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-05-04T16:50:33Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.38918,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-05-05T01:23:56Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70667,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-05-05T01:27:00Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70593,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-run-monitoring-integration": [
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-05-04T16:50:33Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.38918,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-05-05T01:23:56Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70667,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-05-05T01:27:00Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70593,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-spanner-monitoring-integration": [
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-05-04T16:50:33Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.38916,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-05-05T01:23:56Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70667,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-05-05T01:27:00Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70592,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-sql-monitoring-integration": [
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-05-04T16:50:33Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.38916,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-05-05T01:23:56Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70667,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-05-05T01:27:00Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70592,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration": [
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-05-04T16:50:33Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.38916,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-05-05T01:23:56Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70665,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Compute Engine monitoring integration",
        "Activate integration",
        "Important",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "GcpVirtualMachineSample",
        "GcpVirtualMachineDiskSample",
        "Inventory data",
        "gcp/compute/virtual-machine",
        "gcp/compute/virtual-machine/disk",
        "Learn more"
      ],
      "title": "Google Compute Engine monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "749ce2f670e38c332eb8b591fb0fbf0098ba157f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration/",
      "published_at": "2021-05-05T05:13:26Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "All New Relic Infrastructure accounts, regardless of subscription level, can use New Relic's Compute Engine integration to get a comprehensive, real-time view of their host's performance and status. New Relic Infrastructure's integration with Google Compute Engine reports metadata about instances (virtual machines) hosted on Google's infrastructure. You can monitor and alert on your GCP instances data from New Relic Infrastructure, and you can create custom queries and chart dashboards in New Relic Insights. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your Google Cloud projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic Infrastructure integrations query your GCP services according to a polling interval, which varies depending on the integration. The polling interval for the Google Compute Engine integration is 5 minutes. Find and use data After activating the integration and waiting a few minutes (based on the polling frequency), data will appear in the New Relic UI. To find and use your data, including links to your dashboards and alert settings, go to infrastructure.newrelic.com > GCP > (select an integration). Metric data Metric data that New Relic receives from your GCP Compute Engine integration include: GcpVirtualMachineSample Name Description firewall.DroppedBytes Delta count of incoming bytes dropped by the firewall. firewall.DroppedPackets Delta count of incoming packets dropped by the firewall. instance.cpu.ReservedCores Total number of cores reserved on the host of the instance. GcpVirtualMachineDiskSample Name Description instance.disk.ThrottledReadBytes Delta count of bytes in throttled read operations. instance.disk.ThrottledReadOps Delta count of throttled read operations. instance.disk.ThrottledWriteBytes Delta count of bytes in throttled write operations. instance.disk.ThrottledWriteOps Delta count of throttled write operations. Inventory data Inventory data is information about the status or configuration of a service or host. You can examine inventory data in New Relic Infrastructure and in New Relic Insights. The Google Compute Engine integration reports configuration information and labels for virtual machines and disks through the properties listed below. Virtual machine tags are treated as labels that take the value true. gcp/compute/virtual-machine automaticRestart canIpForward cpuPlatform creationTimestamp deletionProtection description instanceId isPreemptible label.* machineType metadataFingerprint name networkInterfaces onHostMaintenance project status networkTags zone gcp/compute/virtual-machine/disk creationTimestamp description diskId encrypted instanceId instanceName label.* lastAttachTimestamp lastDetachTimestamp licenses name project replicaZones sizeGb sourceImage sourceImageId sourceSnapshot sourceSnapshotId status type users zone Learn more",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70592,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "sections": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " your <em>GCP</em> service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your <em>Google</em> <em>Cloud</em> projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic"
      },
      "id": "603e7d1f28ccbc483ceba771"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration": [
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-05-04T16:50:33Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.38916,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-05-05T01:23:56Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70665,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-05-05T01:27:00Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70592,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-datastore-monitoring-integration": [
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-05-04T16:50:33Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.38916,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-05-05T01:23:56Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70665,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-05-05T01:27:00Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70592,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-direct-interconnect-monitoring-integration": [
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-05-04T16:50:33Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.38916,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-05-05T01:23:56Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70665,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-05-05T01:27:00Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70592,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-kubernetes-engine-monitoring-integration": [
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-05-04T16:50:33Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.38914,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-05-05T01:23:56Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70663,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-05-05T01:27:00Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.7059,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-memcached": [
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-05-04T16:50:33Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.38914,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-05-05T01:23:56Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70663,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-05-05T01:27:00Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.7059,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-memorystore-redis": [
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-05-04T16:50:33Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.38914,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Serverless VPC Access monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "VPC Access Connector data"
      ],
      "title": "Google Serverless VPC Access monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "a1ee8cb1f9d6a05f4a5e5ec6ac4c5e275868e891",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration/",
      "published_at": "2021-05-05T01:23:56Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your GCP VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the GCP VPC Access integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > GCP and select an integration. Data is attached to the following event type: Entity Event Type Provider Connector GcpVpcaccessConnectorSample GcpVpcaccessConnector For more on how to use your data, see Understand and use integration data. Metric data This integration collects GCP VPC Access data for Connector. VPC Access Connector data Metric Unit Description connector.ReceivedBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.ReceivedPackets Count Delta of packets received by a VPC Access Connector. connector.SentBytes Bytes Delta of bytes transferred by a VPC Access Connector. connector.SentPackets Count Delta of packets sent by a VPC Access Connector.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.70663,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "sections": "<em>Google</em> Serverless VPC Access monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>GCP</em> VPC Access data to our products. Here we explain how to activate the integration and what data it collects. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e9e73196a67f7b0a83da7"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-05-05T01:27:00Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.7059,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-serverless-vpc-access-monitoring-integration": [
    {
      "sections": [
        "Google Cloud Functions monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Important"
      ],
      "title": "Google Cloud Functions monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "2805038e3e7040ea7032a96268fceba1faa0647e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-functions-monitoring-integration/",
      "published_at": "2021-05-04T16:50:33Z",
      "updated_at": "2021-03-16T05:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure integrations with the Google Cloud Platform (GCP) includes one that reports Google Cloud Functions data to our products. This document explains how to activate the GCP Cloud Functions integration and describes the data that can be reported. Features Google Cloud Functions service allows running code in a serverless way. Using the Google UI, developers can create short pieces of code that are intended to do a specific function. The function can then respond to cloud events without the need to manage an application server or runtime environment. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency Our integrations query your GCP services according to a polling interval, which varies depending on the integration. Polling frequency for GCP Cloud Functions: five minutes Resolution: one data point every minute View and use data After activating the integration and then waiting a few minutes (based on the polling frequency), data will appear in the UI. To view and use your data, including links to your dashboards and alert settings, go to one.newrelic.com, in top nav click Infrastructure, click GCP, then (select an integration). Metric data Metric data we receive from your GCP Cloud Functions integration includes: Attribute Description function.Executions Count of functions that executed, by status. function.ExecutionTimeNanos Time for each function to execute, in nanoseconds. function.UserMemoryBytes Memory used for each function, in bytes. Inventory data Inventory data we receive from your GCP Cloud Functions integration includes the following inventory. Important Inventory indicated with * are fetched only when the GCP project is linked to New Relic through a service account. Inventory Description description * User-provided description of a function. entryPoint * The name of the function (as defined in source code) that will be executed. eventTriggerFailurePolicy * For functions that can be triggered by events, the policy for failed executions. eventTriggerResource * For functions that can be triggered by events, the resource(s) from which to observe events. eventTriggerService * For functions that can be triggered by events, the hostname of the service that should be observed. eventTriggerType * For functions that can be triggered by events, the type of event to observe. httpsTriggerUr * For functions that can be triggered via HTTPS endpoint, the deployed URL for the function. label * Labels for the function. memory * The amount of memory in MB available for a function. name The name of the function. project The Google Cloud project that the function belongs to. runtime * The runtime in which the function is going to run. status * Status of the function deployment. timeout * The function execution timeout. Execution is considered failed and can be terminated if the function is not completed at the end of the timeout period. versionId * The version identifier of the Cloud Function. zone The zone where the function is running.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.38914,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Functions monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "Our infrastructure <em>integrations</em> with the <em>Google</em> <em>Cloud</em> <em>Platform</em> (<em>GCP</em>) includes one that reports <em>Google</em> <em>Cloud</em> Functions data to our products. This document explains how to activate the <em>GCP</em> <em>Cloud</em> Functions integration and describes the data that can be reported. Features <em>Google</em> <em>Cloud</em> Functions service"
      },
      "id": "603e8f62e7b9d2fe6b2a081d"
    },
    {
      "sections": [
        "Google Cloud Storage monitoring integration",
        "Features",
        "Activate integration",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "Inventory data"
      ],
      "title": "Google Cloud Storage monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "b82474e156f5c250b2a97c371b450b9254183297",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-cloud-storage-monitoring-integration/",
      "published_at": "2021-05-05T01:27:00Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Google Cloud Storage data to New Relic. Learn how to connect this integration to infrastructure monitoring and about the metric data and inventory data that New Relic reports for this integration. Features Google Cloud Storage is a Google Cloud Platform service that you can use to serve website content, to store data for archival and disaster recovery, and to distribute data objects via direct download. With the Google Cloud Storage integration, you can access these features: View charts and information about the data you are storing and retrieving from Google Cloud Storage. Create custom queries and charts in from automatically captured data. Set alerts on your Google Cloud Storage data directly from the Integrations page. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic. Polling frequency New Relic queries your Google Cloud Storage services based on a polling interval of 5 minutes. Find and use data After connecting the integration to New Relic and waiting a few minutes, data will appear in the New Relic UI. To find and use integration data, including your dashboards and your alert settings, go to one.newrelic.com > Infrastructure > GCP > Google Cloud Storage. To create custom dashboards for the integration, create queries for the GcpStorageBucketSample event type with the provider value GcpStorageBucket. Metric data The integration reports metric data for all values of method and response_code: response_code: The response code of the requests. method: The name of the API method called. The metric data that New Relic receives from your Google Cloud Storage integration includes: Metric Description api.Requests Delta count of API calls. network.ReceivedBytes Delta count of bytes received over the network. network.SentBytes Delta count of bytes sent over the network. Inventory data Inventory data for Google Cloud Storage bucket objects includes the following properties: Inventory data Description acl Access control list for the bucket that lets you specify who has access to your data and to what extent. cors The Cross-Origin Resource Sharing (CORS) configuration for the bucket. createTime Time when the bucket was created. defaultAcl Default access control list configuration for the bucket's blobs. etag HTTP 1.1 entity tag for the bucket. indexPage The bucket's website index page. This behaves as the bucket's directory index where missing blobs are treated as potential directories. labels Labels for the bucket, in key/value pairs. This is only available if the GCP project is linked to New Relic through a service account and extended inventory collection is enabled. metageneration The generation of the metadata for the bucket. name The name of the bucket. notFoundPage The custom object that will be returned when a requested resource is not found. owner The owner of the bucket. A bucket is always owned by the project team owners group. project The name that you assigned to the project. A project consists of a set of users, a set of APIs, and settings for those APIs. requesterPays If set to true, the user accessing the bucket or an object it contains assumes the access transit costs. storageClass The default storage class for a bucket, if you don't specify one for a new object. The storage class defines how Google Cloud Storage stores objects in the bucket and determines the SLA and storage cost. For more information, see storage classes. zone The zone where the bucket is deployed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.7059,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "sections": "<em>Google</em> <em>Cloud</em> Storage monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " and retrieving from <em>Google</em> <em>Cloud</em> Storage. Create custom queries and charts in from automatically captured data. Set alerts on your <em>Google</em> <em>Cloud</em> Storage data directly from the <em>Integrations</em> page. Activate integration To enable the integration follow standard procedures to connect your <em>GCP</em> service to New Relic"
      },
      "id": "603e8f63196a67fa56a83dbc"
    },
    {
      "sections": [
        "Google Compute Engine monitoring integration",
        "Activate integration",
        "Important",
        "Polling frequency",
        "Find and use data",
        "Metric data",
        "GcpVirtualMachineSample",
        "GcpVirtualMachineDiskSample",
        "Inventory data",
        "gcp/compute/virtual-machine",
        "gcp/compute/virtual-machine/disk",
        "Learn more"
      ],
      "title": "Google Compute Engine monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "GCP integrations list"
      ],
      "external_id": "749ce2f670e38c332eb8b591fb0fbf0098ba157f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-compute-engine-monitoring-integration/",
      "published_at": "2021-05-05T05:13:26Z",
      "updated_at": "2021-03-16T05:46:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "All New Relic Infrastructure accounts, regardless of subscription level, can use New Relic's Compute Engine integration to get a comprehensive, real-time view of their host's performance and status. New Relic Infrastructure's integration with Google Compute Engine reports metadata about instances (virtual machines) hosted on Google's infrastructure. You can monitor and alert on your GCP instances data from New Relic Infrastructure, and you can create custom queries and chart dashboards in New Relic Insights. Activate integration To enable the integration follow standard procedures to connect your GCP service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your Google Cloud projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic Infrastructure integrations query your GCP services according to a polling interval, which varies depending on the integration. The polling interval for the Google Compute Engine integration is 5 minutes. Find and use data After activating the integration and waiting a few minutes (based on the polling frequency), data will appear in the New Relic UI. To find and use your data, including links to your dashboards and alert settings, go to infrastructure.newrelic.com > GCP > (select an integration). Metric data Metric data that New Relic receives from your GCP Compute Engine integration include: GcpVirtualMachineSample Name Description firewall.DroppedBytes Delta count of incoming bytes dropped by the firewall. firewall.DroppedPackets Delta count of incoming packets dropped by the firewall. instance.cpu.ReservedCores Total number of cores reserved on the host of the instance. GcpVirtualMachineDiskSample Name Description instance.disk.ThrottledReadBytes Delta count of bytes in throttled read operations. instance.disk.ThrottledReadOps Delta count of throttled read operations. instance.disk.ThrottledWriteBytes Delta count of bytes in throttled write operations. instance.disk.ThrottledWriteOps Delta count of throttled write operations. Inventory data Inventory data is information about the status or configuration of a service or host. You can examine inventory data in New Relic Infrastructure and in New Relic Insights. The Google Compute Engine integration reports configuration information and labels for virtual machines and disks through the properties listed below. Virtual machine tags are treated as labels that take the value true. gcp/compute/virtual-machine automaticRestart canIpForward cpuPlatform creationTimestamp deletionProtection description instanceId isPreemptible label.* machineType metadataFingerprint name networkInterfaces onHostMaintenance project status networkTags zone gcp/compute/virtual-machine/disk creationTimestamp description diskId encrypted instanceId instanceName label.* lastAttachTimestamp lastDetachTimestamp licenses name project replicaZones sizeGb sourceImage sourceImageId sourceSnapshot sourceSnapshotId status type users zone Learn more",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 179.7059,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "sections": "<em>Google</em> Compute Engine monitoring <em>integration</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": " your <em>GCP</em> service to New Relic Infrastructure. Important You must install the Infrastructure agent on each GCE host to see metrics from that host. Connecting your <em>Google</em> <em>Cloud</em> projects allows Infrastructure to access GCE metadata, such as region, type, and tags. Polling frequency New Relic"
      },
      "id": "603e7d1f28ccbc483ceba771"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/get-started/connect-google-cloud-platform-services-new-relic": [
    {
      "sections": [
        "Integrations and custom roles",
        "Recommended role",
        "Optional role",
        "Important",
        "List of permissions",
        "Common permissions",
        "Service-specific permissions",
        "Permissions to link projects through the UI"
      ],
      "title": "Integrations and custom roles",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "d4f60e2d8413ddde9a342980d75a0e216af9baa4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/integrations-custom-roles/",
      "published_at": "2021-05-04T18:31:08Z",
      "updated_at": "2021-04-16T16:37:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To read the relevant data from your Google Cloud Platform (GCP) account, New Relic uses the Google Stackdriver API and also other specific services APIs. To access these APIs in your Google Cloud project, the New Relic authorized account needs to be granted a certain set of permissions; GCP uses roles to grant these permissions. Recommended role By default we highly recommend using the GCP primitive role Project Viewer, which grants \"permissions for read-only actions that do not affect your cloud infrastructure state, such as viewing (but not modifying) existing resources or data.\" This role is automatically managed by Google and updated when new Google Cloud services are released or modified. Optional role Alternatively, you can create your own custom role based on the list of permissions, which specifies the minimum set of permissions required to fetch data from each GCP integration. This will allow you to have more control over the permissions set for the New Relic authorized account. Important New Relic has no way of identifying problems related to custom permissions. If you choose to create a custom role, it is your responsibility to maintain it and ensure proper data is being collected. To customize your role you need to: Create a Google Cloud IAM Custom Role in each one of the GCP projects you want to monitor with New Relic. In each custom role, add the permissions that are specifically required for the cloud services you want to monitor according to the following list. Assign the custom role(s) to the New Relic authorized account. List of permissions Common permissions All integrations need the following permission: monitoring.timeSeries.list service.usage.use Service-specific permissions For some GCP integrations, New Relic will also need the following permissions, mainly to collect labels and inventory attributes. Integration Permissions Google AppEngine n/a; Google App Engine does not require additional permissions. Google BigQuery bigquery.datasets.get bigquery.tables.get bigquery.tables.list Google Cloud Functions cloudfunctions.locations.list Google Cloud Load Balancing n/a; Google Cloud Load Balancing does not require additional permissions. Google Cloud Pub/Sub pubsub.subscriptions.get pubsub.subscriptions.list pubsub.topics.get pubsub.topics.list Google Cloud Spanner spanner.instances.list spanner.databases.list spanner.databases.getDdl Google Cloud SQL cloudsql.instances.list Google Cloud Storage storage.buckets.list Google Compute Engine compute.instances.list compute.disks.get compute.disks.list Google Kubernetes Engine container.clusters.list Permissions to link projects through the UI To be able to see the list of projects that you can link to New Relic through the UI, your New Relic authorized service account needs the following permissions: resourcemanager.projects.get monitoring.monitoredResourceDescriptors.list If you do not want to grant New Relic authorized account the permissions that are needed for the linking process through the UI, you have the following options: Assign the Project Viewer or Monitoring Viewer role initially to the authorized account to link Google Cloud projects to New Relic through the UI. After the projects are linked, assign a Google Cloud custom role to the authorized account. Use New Relic NerdGraph to link Google Cloud projects to New Relic. This does not involve listing the viewable projects. However, you must know the id of the project you want to monitor. For more information, see the NerdGraph GraphiQL cloud integrations API tutorial.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.8572,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Integrations</em> and custom roles",
        "sections": "<em>Integrations</em> and custom roles",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "To read the relevant data from your <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) account, New Relic uses the <em>Google</em> Stackdriver API and also other specific services APIs. To access these APIs in your <em>Google</em> <em>Cloud</em> project, the New Relic authorized account needs to be granted a certain set of permissions; GCP uses"
      },
      "id": "603ebb3564441f34b64e8874"
    },
    {
      "sections": [
        "Introduction to Google Cloud Platform integrations",
        "Connect GCP and New Relic",
        "Tip",
        "View your GCP data"
      ],
      "title": "Introduction to Google Cloud Platform integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "508adec5bbbcaef86a079533911bbbec5e1824c4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/introduction-google-cloud-platform-integrations/",
      "published_at": "2021-05-05T15:54:51Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations monitor the performance of popular products and services. New Relic's Google Cloud Platform (GCP) integrations let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures to connect your GCP service to New Relic. Tip To use Google Cloud Platform integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. View your GCP data Once you follow the configuration process, data from your Google Cloud Platform account will report directly to New Relic. To view your GCP data: Go to one.newrelic.com > Infrastructure > GCP. For any of the integrations listed: Select an integration name to view data in a pre-configured dashboard. OR Select the Explore data icon to view GCP data. You can view and reuse the Insights NRQL queries both in the pre-configured dashboards and in the Events explorer dashboards. This allows you to tailor queries to your specific needs. Inventory, events, and dashboards for all services are available in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.10301,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "sections": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> monitor the performance of popular products and services. New Relic&#x27;s <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) <em>integrations</em> let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures"
      },
      "id": "603e86d3e7b9d20feb2a07ed"
    },
    {
      "sections": [
        "GCP integration metrics",
        "Google Cloud Metrics"
      ],
      "title": "GCP integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "65e4b0551be716988b29175976fd62a33d82a807",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/gcp-integration-metrics/",
      "published_at": "2021-05-05T15:54:05Z",
      "updated_at": "2021-03-16T05:48:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Google Cloud Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine gcp.appengine.flex.cpu.utilization flex.cpu.Utilization GCP App Engine gcp.appengine.flex.disk.read_bytes_count flex.disk.ReadBytes GCP App Engine gcp.appengine.flex.disk.write_bytes_count flex.disk.WriteBytes GCP App Engine gcp.appengine.flex.network.received_bytes_count flex.network.ReceivedBytes GCP App Engine gcp.appengine.flex.network.sent_bytes_count flex.network.SentBytes GCP App Engine gcp.appengine.http.server.dos_intercept_count server.DosIntercepts GCP App Engine gcp.appengine.http.server.quota_denial_count server.QuotaDenials GCP App Engine gcp.appengine.http.server.response_count server.Responses GCP App Engine gcp.appengine.http.server.response_latencies server.ResponseLatenciesMilliseconds GCP App Engine gcp.appengine.http.server.response_style_count http.server.ResponseStyle GCP App Engine gcp.appengine.memcache.centi_mcu_count memcache.CentiMcu GCP App Engine gcp.appengine.memcache.operation_count memcache.Operations GCP App Engine gcp.appengine.memcache.received_bytes_count memcache.ReceivedBytes GCP App Engine gcp.appengine.memcache.sent_bytes_count memcache.SentBytes GCP App Engine gcp.appengine.system.cpu.usage system.cpu.Usage GCP App Engine gcp.appengine.system.instance_count system.Instances GCP App Engine gcp.appengine.system.memory.usage system.memory.UsageBytes GCP App Engine gcp.appengine.system.network.received_bytes_count system.network.ReceivedBytes GCP App Engine gcp.appengine.system.network.sent_bytes_count system.network.SentBytes GCP App Engine gcp.cloudtasks.api.request_count api.Requests GCP App Engine gcp.cloudtasks.queue.task_attempt_count queue.taskAttempts GCP App Engine gcp.cloudtasks.queue.task_attempt_delays queue.taskAttemptDelaysMilliseconds GCP BigQuery gcp.bigquery.storage.stored_bytes storage.StoredBytes GCP BigQuery gcp.bigquery.storage.table_count storage.Tables GCP BigQuery gcp.bigquery.query.count query.Count GCP BigQuery gcp.bigquery.query.execution_times query.ExecutionTimes GCP BigQuery gcp.bigquery.slots.allocated slots.Allocated GCP BigQuery gcp.bigquery.slots.allocated_for_project slots.AllocatedForProject GCP BigQuery gcp.bigquery.slots.allocated_for_project_and_job_type slots.AllocatedForProjectAndJobType GCP BigQuery gcp.bigquery.slots.allocated_for_reservation slots.AllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_allocated_for_reservation slots.TotalAllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_available slots.TotalAvailable GCP BigQuery gcp.bigquery.storage.uploaded_bytes storage.UploadedBytes GCP BigQuery gcp.bigquery.storage.uploaded_bytes_billed storage.UploadedBytesBilled GCP BigQuery gcp.bigquery.storage.uploaded_row_count storage.UploadedRows GCP Dataflow gcp.dataflow.job.billable_shuffle_data_processed job.BillableShuffleDataProcessed GCP Dataflow gcp.dataflow.job.current_num_vcpus job.CurrentNumVcpus GCP Dataflow gcp.dataflow.job.current_shuffle_slots job.CurrentShuffleSlots GCP Dataflow gcp.dataflow.job.data_watermark_age job.DataWatermarkAge GCP Dataflow gcp.dataflow.job.elapsed_time job.ElapsedTime GCP Dataflow gcp.dataflow.job.element_count job.Elements GCP Dataflow gcp.dataflow.job.estimated_byte_count job.EstimatedBytes GCP Dataflow gcp.dataflow.job.is_failed job.IsFailed GCP Dataflow gcp.dataflow.job.per_stage_data_watermark_age job.PerStageDataWatermarkAge GCP Dataflow gcp.dataflow.job.per_stage_system_lag job.PerStageSystemLag GCP Dataflow gcp.dataflow.job.system_lag job.SystemLag GCP Dataflow gcp.dataflow.job.total_memory_usage_time job.TotalMemoryUsageTime GCP Dataflow gcp.dataflow.job.total_pd_usage_time job.TotalPdUsageTime GCP Dataflow gcp.dataflow.job.total_shuffle_data_processed job.TotalShuffleDataProcessed GCP Dataflow gcp.dataflow.job.total_streaming_data_processed job.TotalStreamingDataProcessed GCP Dataflow gcp.dataflow.job.total_vcpu_time job.TotalVcpuTime GCP Dataflow gcp.dataflow.job.user_counter job.UserCounter GCP Dataproc gcp.dataproc.cluster.hdfs.datanodes cluster.hdfs.Datanodes GCP Dataproc gcp.dataproc.cluster.hdfs.storage_capacity cluster.hdfs.StorageCapacity GCP Dataproc gcp.dataproc.cluster.hdfs.storage_utilization cluster.hdfs.StorageUtilization GCP Dataproc gcp.dataproc.cluster.hdfs.unhealthy_blocks cluster.hdfs.UnhealthyBlocks GCP Dataproc gcp.dataproc.cluster.job.completion_time cluster.job.CompletionTime GCP Dataproc gcp.dataproc.cluster.job.duration cluster.job.Duration GCP Dataproc gcp.dataproc.cluster.job.failed_count cluster.job.Failures GCP Dataproc gcp.dataproc.cluster.job.running_count cluster.job.Running GCP Dataproc gcp.dataproc.cluster.job.submitted_count cluster.job.Submitted GCP Dataproc gcp.dataproc.cluster.operation.completion_time cluster.operation.CompletionTime GCP Dataproc gcp.dataproc.cluster.operation.duration cluster.operation.Duration GCP Dataproc gcp.dataproc.cluster.operation.failed_count cluster.operation.Failures GCP Dataproc gcp.dataproc.cluster.operation.running_count cluster.operation.Running GCP Dataproc gcp.dataproc.cluster.operation.submitted_count cluster.operation.Submitted GCP Dataproc gcp.dataproc.cluster.yarn.allocated_memory_percentage cluster.yarn.AllocatedMemoryPercentage GCP Dataproc gcp.dataproc.cluster.yarn.apps cluster.yarn.Apps GCP Dataproc gcp.dataproc.cluster.yarn.containers cluster.yarn.Containers GCP Dataproc gcp.dataproc.cluster.yarn.memory_size cluster.yarn.MemorySize GCP Dataproc gcp.dataproc.cluster.yarn.nodemanagers cluster.yarn.Nodemanagers GCP Dataproc gcp.dataproc.cluster.yarn.pending_memory_size cluster.yarn.PendingMemorySize GCP Dataproc gcp.dataproc.cluster.yarn.virtual_cores cluster.yarn.VirtualCores GCP Datastore gcp.datastore.api.request_count api.Requests GCP Datastore gcp.datastore.entity.read_sizes entity.ReadSizes GCP Datastore gcp.datastore.entity.write_sizes entity.WriteSizes GCP Datastore gcp.datastore.index.write_count index.Writes GCP Firebase Database gcp.firebasedatabase.io.database_load io.DatabaseLoad GCP Firebase Database gcp.firebasedatabase.io.persisted_bytes_count io.PersistedBytes GCP Firebase Database gcp.firebasedatabase.io.sent_responses_count io.SentResponses GCP Firebase Database gcp.firebasedatabase.io.utilization io.Utilization GCP Firebase Database gcp.firebasedatabase.network.active_connections network.ActiveConnections GCP Firebase Database gcp.firebasedatabase.network.api_hits_count network.ApiHits GCP Firebase Database gcp.firebasedatabase.network.broadcast_load network.BroadcastLoad GCP Firebase Database gcp.firebasedatabase.network.https_requests_count network.HttpsRequests GCP Firebase Database gcp.firebasedatabase.network.monthly_sent network.MonthlySent GCP Firebase Database gcp.firebasedatabase.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Database gcp.firebasedatabase.network.sent_bytes_count network.SentBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_and_protocol_bytes_count network.SentPayloadAndProtocolBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_bytes_count network.SentPayloadBytes GCP Firebase Database gcp.firebasedatabase.rules.evaluation_count rules.Evaluation GCP Firebase Database gcp.firebasedatabase.storage.limit storage.Limit GCP Firebase Database gcp.firebasedatabase.storage.total_bytes storage.TotalBytes GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent network.MonthlySent GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Hosting gcp.firebasehosting.network.sent_bytes_count network.SentBytes GCP Firebase Hosting gcp.firebasehosting.storage.limit storage.Limit GCP Firebase Hosting gcp.firebasehosting.storage.total_bytes storage.TotalBytes GCP Firebase Storage gcp.firebasestorage.rules.evaluation_count rules.Evaluation GCP Firestore gcp.firestore.api.request_count api.Request GCP Firestore gcp.firestore.document.delete_count document.Delete GCP Firestore gcp.firestore.document.read_count document.Read GCP Firestore gcp.firestore.document.write_count document.Write GCP Firestore gcp.firestore.network.active_connections network.ActiveConnections GCP Firestore gcp.firestore.network.snapshot_listeners network.SnapshotListeners GCP Firestore gcp.firestore.rules.evaluation_count rules.Evaluation GCP Cloud Functions gcp.cloudfunctions.function.execution_count function.Executions GCP Cloud Functions gcp.cloudfunctions.function.execution_times function.ExecutionTimeNanos GCP Cloud Functions gcp.cloudfunctions.function.user_memory_bytes function.UserMemoryBytes GCP Interconnect gcp.interconnect.network.interconnect.capacity network.interconnect.Capacity GCP Interconnect gcp.interconnect.network.interconnect.dropped_packets_count network.interconnect.DroppedPackets GCP Interconnect gcp.interconnect.network.interconnect.link.rx_power network.interconnect.link.RxPower GCP Interconnect gcp.interconnect.network.interconnect.link.tx_power network.interconnect.link.TxPower GCP Interconnect gcp.interconnect.network.interconnect.receive_errors_count network.interconnect.ReceiveErrors GCP Interconnect gcp.interconnect.network.interconnect.received_bytes_count network.interconnect.ReceivedBytes GCP Interconnect gcp.interconnect.network.interconnect.received_unicast_packets_count network.interconnect.ReceivedUnicastPackets GCP Interconnect gcp.interconnect.network.interconnect.send_errors_count network.interconnect.SendErrors GCP Interconnect gcp.interconnect.network.interconnect.sent_bytes_count network.interconnect.SentBytes GCP Interconnect gcp.interconnect.network.interconnect.sent_unicast_packets_count network.interconnect.SentUnicastPackets GCP Interconnect gcp.interconnect.network.attachment.capacity network.attachment.Capacity GCP Interconnect gcp.interconnect.network.attachment.received_bytes_count network.attachment.ReceivedBytes GCP Interconnect gcp.interconnect.network.attachment.received_packets_count network.attachment.ReceivedPackets GCP Interconnect gcp.interconnect.network.attachment.sent_bytes_count network.attachment.SentBytes GCP Interconnect gcp.interconnect.network.attachment.sent_packets_count network.attachment.SentPackets GCP Kubernetes Engine gcp.kubernetes.container.accelerator.duty_cycle container.accelerator.dutyCycle GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_total container.accelerator.memoryTotal GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_used container.accelerator.memoryUsed GCP Kubernetes Engine gcp.kubernetes.container.accelerator.request container.accelerator.request GCP Kubernetes Engine gcp.kubernetes.container.cpu.core_usage_time container.cpu.usageTime GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_cores container.cpu.limitCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_utilization container.cpu.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_cores container.cpu.requestCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_utilization container.cpu.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_bytes container.memory.limitBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_utilization container.memory.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.request_bytes container.memory.requestBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.request_utilization container.memory.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.used_bytes container.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.container.restart_count container.restartCount GCP Kubernetes Engine gcp.kubernetes.container.uptime container.uptime GCP Kubernetes Engine gcp.kubernetes.node_daemon.cpu.core_usage_time nodeDaemon.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node_daemon.memory.used_bytes nodeDaemon.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_cores node.cpu.allocatableCores GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_utilization node.cpu.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.cpu.core_usage_time node.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node.cpu.total_cores node.cpu.totalCores GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_bytes node.memory.allocatableBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_utilization node.memory.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.memory.total_bytes node.memory.totalBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.used_bytes node.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.network.received_bytes_count node.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.node.network.sent_bytes_count node.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.received_bytes_count pod.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.sent_bytes_count pod.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.volume.total_bytes pod.volume.totalBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.used_bytes pod.volume.usedBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.utilization pod.volume.utilization GCP Load Balancer gcp.loadbalancing.https.backend_latencies https.BackendLatencies GCP Load Balancer gcp.loadbalancing.https.backend_request_bytes_count https.BackendRequestBytes GCP Load Balancer gcp.loadbalancing.https.backend_request_count https.BackendRequests GCP Load Balancer gcp.loadbalancing.https.backend_response_bytes_count https.BackendResponseBytes GCP Load Balancer gcp.loadbalancing.https.frontend_tcp_rtt https.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.https.request_bytes_count https.RequestBytes GCP Load Balancer gcp.loadbalancing.https.request_count https.Requests GCP Load Balancer gcp.loadbalancing.https.response_bytes_count https.ResponseBytes GCP Load Balancer gcp.loadbalancing.https.total_latencies https.TotalLatencies GCP Load Balancer gcp.loadbalancing.l3.internal.egress_bytes_count l3.internal.EgressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.egress_packets_count l3.internal.EgressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_bytes_count l3.internal.IngressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_packets_count l3.internal.IngressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.rtt_latencies l3.internal.RttLatencies GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.closed_connections tcpSslProxy.ClosedConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.egress_bytes_count tcpSslProxy.EgressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.frontend_tcp_rtt tcpSslProxy.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.ingress_bytes_count tcpSslProxy.IngressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.new_connections tcpSslProxy.NewConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.open_connections tcpSslProxy.OpenConnections GCP Pub/Sub gcp.pubsub.subscription.backlog_bytes subscription.BacklogBytes GCP Pub/Sub gcp.pubsub.subscription.byte_cost subscription.ByteCost GCP Pub/Sub gcp.pubsub.subscription.config_updates_count subscription.ConfigUpdates GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_message_operation_count subscription.ModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_request_count subscription.ModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.num_outstanding_messages subscription.NumOutstandingMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages subscription.NumRetainedAckedMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages_by_region subscription.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_unacked_messages_by_region subscription.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_undelivered_messages subscription.NumUndeliveredMessages GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age subscription.OldestRetainedAckedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age_by_region subscription.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age subscription.OldestUnackedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age_by_region subscription.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.pull_ack_message_operation_count subscription.PullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_ack_request_count subscription.PullAckRequest GCP Pub/Sub gcp.pubsub.subscription.pull_message_operation_count subscription.PullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_request_count subscription.PullRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_count subscription.PushRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_latencies subscription.PushRequestLatencies GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes subscription.RetainedAckedBytes GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes_by_region subscription.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_message_operation_count subscription.StreamingPullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_request_count subscription.StreamingPullAckRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_message_operation_count subscription.StreamingPullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_message_operation_count subscription.StreamingPullModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_request_count subscription.StreamingPullModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_response_count subscription.StreamingPullResponse GCP Pub/Sub gcp.pubsub.subscription.unacked_bytes_by_region subscription.UnackedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.byte_cost topic.ByteCost GCP Pub/Sub gcp.pubsub.topic.config_updates_count topic.ConfigUpdates GCP Pub/Sub gcp.pubsub.topic.message_sizes topic.MessageSizes GCP Pub/Sub gcp.pubsub.topic.num_retained_acked_messages_by_region topic.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.num_unacked_messages_by_region topic.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_retained_acked_message_age_by_region topic.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_unacked_message_age_by_region topic.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.retained_acked_bytes_by_region topic.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.send_message_operation_count topic.SendMessageOperation GCP Pub/Sub gcp.pubsub.topic.send_request_count topic.SendRequest GCP Pub/Sub gcp.pubsub.topic.unacked_bytes_by_region topic.UnackedBytesByRegion GCP Router gcp.router.best_received_routes_count BestReceivedRoutes GCP Router gcp.router.bfd.control.receive_intervals bfd.control.ReceiveIntervals GCP Router gcp.router.bfd.control.received_packets_count bfd.control.ReceivedPackets GCP Router gcp.router.bfd.control.rejected_packets_count bfd.control.RejectedPackets GCP Router gcp.router.bfd.control.transmit_intervals bfd.control.TransmitIntervals GCP Router gcp.router.bfd.control.transmitted_packets_count bfd.control.TransmittedPackets GCP Router gcp.router.bfd.session_up bfd.SessionUp GCP Router gcp.router.bgp_sessions_down_count BgpSessionsDown GCP Router gcp.router.bgp_sessions_up_count BgpSessionsUp GCP Router gcp.router.bgp.received_routes_count bgp.ReceivedRoutes GCP Router gcp.router.bgp.sent_routes_count bgp.SentRoutes GCP Router gcp.router.bgp.session_up bgp.SessionUp GCP Router gcp.router.router_up RouterUp GCP Router gcp.router.sent_routes_count SentRoutes GCP Router gcp.router.nat.allocated_ports nat.AllocatedPorts GCP Router gcp.router.nat.closed_connections_count nat.ClosedConnections GCP Router gcp.router.nat.dropped_received_packets_count nat.DroppedReceivedPackets GCP Router gcp.router.nat.new_connections_count nat.NewConnections GCP Router gcp.router.nat.port_usage nat.PortUsage GCP Router gcp.router.nat.received_bytes_count nat.ReceivedBytes GCP Router gcp.router.nat.received_packets_count nat.ReceivedPackets GCP Router gcp.router.nat.sent_bytes_count nat.SentBytes GCP Router gcp.router.nat.sent_packets_count nat.SentPackets GCP Run gcp.run.container.billable_instance_time container.BillableInstanceTime GCP Run gcp.run.container.cpu.allocation_time container.cpu.AllocationTime GCP Run gcp.run.container.memory.allocation_time container.memory.AllocationTime GCP Run gcp.run.request_count Request GCP Run gcp.run.request_latencies RequestLatencies GCP Spanner gcp.spanner.api.received_bytes_count api.ReceivedBytes GCP Spanner gcp.spanner.api.request_count api.Requests GCP Spanner gcp.spanner.api.request_latencies api.RequestLatencies GCP Spanner gcp.spanner.instance.cpu.utilization instance.cpu.Utilization GCP Spanner gcp.spanner.instance.node_count instance.nodes GCP Spanner gcp.spanner.instance.session_count instance.sessions GCP Spanner gcp.spanner.instance.storage.used_bytes instance.storage.UsedBytes GCP Cloud SQL gcp.cloudsql.database.auto_failover_request_count database.AutoFailoverRequest GCP Cloud SQL gcp.cloudsql.database.available_for_failover database.AvailableForFailover GCP Cloud SQL gcp.cloudsql.database.cpu.reserved_cores database.cpu.ReservedCores GCP Cloud SQL gcp.cloudsql.database.cpu.usage_time database.cpu.UsageTime GCP Cloud SQL gcp.cloudsql.database.cpu.utilization database.cpu.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.bytes_used database.disk.BytesUsed GCP Cloud SQL gcp.cloudsql.database.disk.quota database.disk.Quota GCP Cloud SQL gcp.cloudsql.database.disk.read_ops_count database.disk.ReadOps GCP Cloud SQL gcp.cloudsql.database.disk.utilization database.disk.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.write_ops_count database.disk.WriteOps GCP Cloud SQL gcp.cloudsql.database.memory.quota database.memory.Quota GCP Cloud SQL gcp.cloudsql.database.memory.usage database.memory.Usage GCP Cloud SQL gcp.cloudsql.database.memory.utilization database.memory.Utilization GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_dirty database.mysql.InnodbBufferPoolPagesDirty GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_free database.mysql.InnodbBufferPoolPagesFree GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_total database.mysql.InnodbBufferPoolPagesTotal GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_data_fsyncs database.mysql.InnodbDataFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_os_log_fsyncs database.mysql.InnodbOsLogFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_read database.mysql.InnodbPagesRead GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_written database.mysql.InnodbPagesWritten GCP Cloud SQL gcp.cloudsql.database.mysql.queries database.mysql.Queries GCP Cloud SQL gcp.cloudsql.database.mysql.questions database.mysql.Questions GCP Cloud SQL gcp.cloudsql.database.mysql.received_bytes_count database.mysql.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.mysql.replication.seconds_behind_master database.mysql.replication.SecondsBehindMaster GCP Cloud SQL gcp.cloudsql.database.mysql.sent_bytes_count database.mysql.SentBytes GCP Cloud SQL gcp.cloudsql.database.network.connections database.network.Connections GCP Cloud SQL gcp.cloudsql.database.network.received_bytes_count database.network.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.network.sent_bytes_count database.network.SentBytes GCP Cloud SQL gcp.cloudsql.database.postgresql.num_backends database.postgresql.NumBackends GCP Cloud SQL gcp.cloudsql.database.postgresql.replication.replica_byte_lag database.postgresql.replication.ReplicaByteLag GCP Cloud SQL gcp.cloudsql.database.postgresql.transaction_count database.postgresql.Transaction GCP Cloud SQL gcp.cloudsql.database.up database.Up GCP Cloud SQL gcp.cloudsql.database.uptime database.Uptime GCP Cloud Storage gcp.storage.api.request_count api.Requests GCP Cloud Storage gcp.storage.network.received_bytes_count network.ReceivedBytes GCP Cloud Storage gcp.storage.network.sent_bytes_count network.SentBytes GCP VMs gcp.compute.firewall.dropped_bytes_count firewall.DroppedBytes GCP VMs gcp.compute.firewall.dropped_packets_count firewall.DroppedPackets GCP VMs gcp.compute.instance.cpu.reserved_cores instance.cpu.ReservedCores GCP VMs gcp.compute.instance.cpu.utilization instance.cpu.Utilization GCP VMs gcp.compute.instance.disk.read_bytes_count instance.disk.ReadBytes GCP VMs gcp.compute.instance.disk.read_ops_count instance.disk.ReadOps GCP VMs gcp.compute.instance.disk.write_bytes_count instance.disk.WriteBytes GCP VMs gcp.compute.instance.disk.write_ops_count instance.disk.WriteOps GCP VMs gcp.compute.instance.network.received_bytes_count instance.network.ReceivedBytes GCP VMs gcp.compute.instance.network.received_packets_count instance.network.ReceivedPackets GCP VMs gcp.compute.instance.network.sent_bytes_count instance.network.SentBytes GCP VMs gcp.compute.instance.network.sent_packets_count instance.network.SentPackets GCP VMs gcp.compute.instance.disk.throttled_read_bytes_count instance.disk.ThrottledReadBytes GCP VMs gcp.compute.instance.disk.throttled_read_ops_count instance.disk.ThrottledReadOps GCP VMs gcp.compute.instance.disk.throttled_write_bytes_count instance.disk.ThrottledWriteBytes GCP VMs gcp.compute.instance.disk.throttled_write_ops_count instance.disk.ThrottledWriteOps GCP VPC Access gcp.vpcaccess.connector.received_bytes_count connector.ReceivedBytes GCP VPC Access gcp.vpcaccess.connector.received_packets_count connector.ReceivedPackets GCP VPC Access gcp.vpcaccess.connector.sent_bytes_count connector.SentBytes GCP VPC Access gcp.vpcaccess.connector.sent_packets_count connector.SentPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.65994,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "GCP <em>integration</em> metrics",
        "sections": "<em>Google</em> <em>Cloud</em> Metrics",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "<em>Google</em> <em>Cloud</em> Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine gcp.appengine.flex.cpu.utilization"
      },
      "id": "603e8a5264441f524a4e8840"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/get-started/gcp-integration-metrics": [
    {
      "sections": [
        "Integrations and custom roles",
        "Recommended role",
        "Optional role",
        "Important",
        "List of permissions",
        "Common permissions",
        "Service-specific permissions",
        "Permissions to link projects through the UI"
      ],
      "title": "Integrations and custom roles",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "d4f60e2d8413ddde9a342980d75a0e216af9baa4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/integrations-custom-roles/",
      "published_at": "2021-05-04T18:31:08Z",
      "updated_at": "2021-04-16T16:37:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To read the relevant data from your Google Cloud Platform (GCP) account, New Relic uses the Google Stackdriver API and also other specific services APIs. To access these APIs in your Google Cloud project, the New Relic authorized account needs to be granted a certain set of permissions; GCP uses roles to grant these permissions. Recommended role By default we highly recommend using the GCP primitive role Project Viewer, which grants \"permissions for read-only actions that do not affect your cloud infrastructure state, such as viewing (but not modifying) existing resources or data.\" This role is automatically managed by Google and updated when new Google Cloud services are released or modified. Optional role Alternatively, you can create your own custom role based on the list of permissions, which specifies the minimum set of permissions required to fetch data from each GCP integration. This will allow you to have more control over the permissions set for the New Relic authorized account. Important New Relic has no way of identifying problems related to custom permissions. If you choose to create a custom role, it is your responsibility to maintain it and ensure proper data is being collected. To customize your role you need to: Create a Google Cloud IAM Custom Role in each one of the GCP projects you want to monitor with New Relic. In each custom role, add the permissions that are specifically required for the cloud services you want to monitor according to the following list. Assign the custom role(s) to the New Relic authorized account. List of permissions Common permissions All integrations need the following permission: monitoring.timeSeries.list service.usage.use Service-specific permissions For some GCP integrations, New Relic will also need the following permissions, mainly to collect labels and inventory attributes. Integration Permissions Google AppEngine n/a; Google App Engine does not require additional permissions. Google BigQuery bigquery.datasets.get bigquery.tables.get bigquery.tables.list Google Cloud Functions cloudfunctions.locations.list Google Cloud Load Balancing n/a; Google Cloud Load Balancing does not require additional permissions. Google Cloud Pub/Sub pubsub.subscriptions.get pubsub.subscriptions.list pubsub.topics.get pubsub.topics.list Google Cloud Spanner spanner.instances.list spanner.databases.list spanner.databases.getDdl Google Cloud SQL cloudsql.instances.list Google Cloud Storage storage.buckets.list Google Compute Engine compute.instances.list compute.disks.get compute.disks.list Google Kubernetes Engine container.clusters.list Permissions to link projects through the UI To be able to see the list of projects that you can link to New Relic through the UI, your New Relic authorized service account needs the following permissions: resourcemanager.projects.get monitoring.monitoredResourceDescriptors.list If you do not want to grant New Relic authorized account the permissions that are needed for the linking process through the UI, you have the following options: Assign the Project Viewer or Monitoring Viewer role initially to the authorized account to link Google Cloud projects to New Relic through the UI. After the projects are linked, assign a Google Cloud custom role to the authorized account. Use New Relic NerdGraph to link Google Cloud projects to New Relic. This does not involve listing the viewable projects. However, you must know the id of the project you want to monitor. For more information, see the NerdGraph GraphiQL cloud integrations API tutorial.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.8572,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Integrations</em> and custom roles",
        "sections": "<em>Integrations</em> and custom roles",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "To read the relevant data from your <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) account, New Relic uses the <em>Google</em> Stackdriver API and also other specific services APIs. To access these APIs in your <em>Google</em> <em>Cloud</em> project, the New Relic authorized account needs to be granted a certain set of permissions; GCP uses"
      },
      "id": "603ebb3564441f34b64e8874"
    },
    {
      "sections": [
        "Introduction to Google Cloud Platform integrations",
        "Connect GCP and New Relic",
        "Tip",
        "View your GCP data"
      ],
      "title": "Introduction to Google Cloud Platform integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "508adec5bbbcaef86a079533911bbbec5e1824c4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/introduction-google-cloud-platform-integrations/",
      "published_at": "2021-05-05T15:54:51Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations monitor the performance of popular products and services. New Relic's Google Cloud Platform (GCP) integrations let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures to connect your GCP service to New Relic. Tip To use Google Cloud Platform integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. View your GCP data Once you follow the configuration process, data from your Google Cloud Platform account will report directly to New Relic. To view your GCP data: Go to one.newrelic.com > Infrastructure > GCP. For any of the integrations listed: Select an integration name to view data in a pre-configured dashboard. OR Select the Explore data icon to view GCP data. You can view and reuse the Insights NRQL queries both in the pre-configured dashboards and in the Events explorer dashboards. This allows you to tailor queries to your specific needs. Inventory, events, and dashboards for all services are available in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.10301,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "sections": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> monitor the performance of popular products and services. New Relic&#x27;s <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) <em>integrations</em> let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures"
      },
      "id": "603e86d3e7b9d20feb2a07ed"
    },
    {
      "sections": [
        "Connect Google Cloud Platform services to New Relic",
        "Tip",
        "Requirements",
        "Authorization options",
        "Service account (recommended)",
        "User account",
        "Connect GCP to New Relic infrastructure monitoring",
        "Explore app data in New Relic",
        "Link multiple Google projects",
        "Unlink your GCP integrations"
      ],
      "title": "Connect Google Cloud Platform services to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "05934d2b03ec1ac5fa43298b21a06dc2e0f8c3b9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/connect-google-cloud-platform-services-new-relic/",
      "published_at": "2021-05-05T15:54:06Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To start receiving Google Cloud Platform (GCP) data with New Relic GCP integrations, connect your Google project to New Relic infrastructure monitoring. Tip To use Google Cloud Platform integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements These are the requirements for the authorization: GCP integration requirements Comments Monitoring In the GCP project API & Services Library settings, you must enable Google Stackdriver Monitoring API. Authorization For service account authorization (recommended): A user with Project IAM Admin role is needed to add the service account ID as a member in your GCP project. In the GCP project IAM & admin, the service account must have the Project Viewer role and the Service Usage Consumer role or, alternatively, a custom role. For user account authorization: The New Relic user that will integrate the GCP project must have a Google account and must be able to view the GCP project that New Relic will monitor. In the GCP project IAM & admin, the user must have the Project Viewer role. Please note that this authorization method will not allow New Relic to collect labels and other inventory attributes that can be useful for narrowing down your NRQL queries, dashboards and alerts. You can migrate the authorization method from user account to service account from the Manage services link in New Relic's user interface. Project name As part of the online setup process, you must identify Project name of the projects you want to monitor with New Relic. The UI workflow automatically lists active projects you can select. Permissions (only for user account authorization) New Relic requires a specific set of read-only permissions exclusively; this means that, for certain integrations, only partial inventory data will be available. Keep in mind that New Relic doesn't inherit your Google account's permissions and therefore is not authorized to perform any changes in the project. For more information about the API permissions that New Relic uses, see the Google documentation about scopes. Authorization options Integrating your GCP project with New Relic requires you to authorize New Relic to fetch monitoring data from your GCP project. You can choose between two authorization methods: Service accounts or User accounts. Service account (recommended) The service account authorization is recommended. If you authorize New Relic to fetch data through a service account, we will call your GCP project APIs using a service account ID and its associated public/private key pair. New Relic manages a specific Google service account for your New Relic account; you do not need to create it or manage the associated private key. Just add the service account ID as a member with viewing permissions in your project. This authorization method is recommended, especially if your GCP project is managed by a team. It also guarantees that New Relic will collect labels and inventory attributes whenever possible. User account If you authorize New Relic to fetch data through a user account, New Relic will access your GCP project monitoring data on behalf of a particular Google user. The authorization process is achieved through an OAuth workflow, which redirects you from the New Relic UI to a Google authorization interface. However, since the authorization is linked to a particular Google user, this method is not recommended for GCP projects that are managed by large teams. Connect GCP to New Relic infrastructure monitoring To connect your Google account to New Relic with user account authorization: Go to one.newrelic.com > Infrastructure > GCP. At the top of Infrastructure's Google Cloud Services integrations page, select Add a GCP account. Choose Authorization Method: Select either Authorize a Service Account or Authorize a User Account, and follow the instructions in the UI to authorize New Relic. Add projects: Select the projects that you want New Relic to receive data from. Select services: From the list of available services for your GCP account, select the individual services you want New Relic to receive data from, or select all of the services. Tip These services will be enabled for all of the projects that you selected in the previous step. Once the setup process is finished, you can fine-tune the services that you want monitored for each project individually. To complete the setup process, select Finish. If you see API authentication errors, follow the troubleshooting procedures. Explore app data in New Relic After you authorize New Relic to integrate one or more of your Google project's services, New Relic starts monitoring your GCP data at regular polling intervals. After a few minutes, data will appear in the New Relic UI. To find and use your data, including links to dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP. Link multiple Google projects For your convenience, the setup process allows you to select more than one project at a time. After the first setup, if you need to monitor additional GCP projects with New Relic, you can repeat the procedure to connect your GCP services as many times as you need. Unlink your GCP integrations You can disable any of your GCP integrations any time and still keep your Google project connected to New Relic. If you want to... Do this Disable a GCP service monitoring To disconnect individual GCP services but keep the integration with New Relic for other GCP services in your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, make changes to the checkbox options for available services and select Save changes. Unlink your project monitoring To uninstall all of your GCP services completely from New Relic Integrations, unlink your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, select Unlink account and select Save changes. Clean your GCP Projects after unlinking New Relic To clean your GCP project after unlinking, follow these steps if you were using a service account: Open the GCP IAM Console. Select the project you want to unlink from New Relic and click Open. Select the service account that is used by New Relic. Click the Remove icon. Or follow these steps if you were using a user account: Open your Google user account settings. Open the Apps with access to your account section. Choose New Relic application. Choose Remove Access.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.14719,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "sections": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "To <em>start</em> receiving <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) data with New Relic GCP <em>integrations</em>, connect your <em>Google</em> project to New Relic infrastructure monitoring. Tip To use <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em> and the rest of our observability <em>platform</em>, join the New Relic family! Sign up to create your free"
      },
      "id": "603e8309196a67fc4fa83da7"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/get-started/integrations-custom-roles": [
    {
      "sections": [
        "Introduction to Google Cloud Platform integrations",
        "Connect GCP and New Relic",
        "Tip",
        "View your GCP data"
      ],
      "title": "Introduction to Google Cloud Platform integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "508adec5bbbcaef86a079533911bbbec5e1824c4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/introduction-google-cloud-platform-integrations/",
      "published_at": "2021-05-05T15:54:51Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations monitor the performance of popular products and services. New Relic's Google Cloud Platform (GCP) integrations let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures to connect your GCP service to New Relic. Tip To use Google Cloud Platform integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. View your GCP data Once you follow the configuration process, data from your Google Cloud Platform account will report directly to New Relic. To view your GCP data: Go to one.newrelic.com > Infrastructure > GCP. For any of the integrations listed: Select an integration name to view data in a pre-configured dashboard. OR Select the Explore data icon to view GCP data. You can view and reuse the Insights NRQL queries both in the pre-configured dashboards and in the Events explorer dashboards. This allows you to tailor queries to your specific needs. Inventory, events, and dashboards for all services are available in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.10301,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "sections": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> monitor the performance of popular products and services. New Relic&#x27;s <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) <em>integrations</em> let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures"
      },
      "id": "603e86d3e7b9d20feb2a07ed"
    },
    {
      "sections": [
        "Connect Google Cloud Platform services to New Relic",
        "Tip",
        "Requirements",
        "Authorization options",
        "Service account (recommended)",
        "User account",
        "Connect GCP to New Relic infrastructure monitoring",
        "Explore app data in New Relic",
        "Link multiple Google projects",
        "Unlink your GCP integrations"
      ],
      "title": "Connect Google Cloud Platform services to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "05934d2b03ec1ac5fa43298b21a06dc2e0f8c3b9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/connect-google-cloud-platform-services-new-relic/",
      "published_at": "2021-05-05T15:54:06Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To start receiving Google Cloud Platform (GCP) data with New Relic GCP integrations, connect your Google project to New Relic infrastructure monitoring. Tip To use Google Cloud Platform integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements These are the requirements for the authorization: GCP integration requirements Comments Monitoring In the GCP project API & Services Library settings, you must enable Google Stackdriver Monitoring API. Authorization For service account authorization (recommended): A user with Project IAM Admin role is needed to add the service account ID as a member in your GCP project. In the GCP project IAM & admin, the service account must have the Project Viewer role and the Service Usage Consumer role or, alternatively, a custom role. For user account authorization: The New Relic user that will integrate the GCP project must have a Google account and must be able to view the GCP project that New Relic will monitor. In the GCP project IAM & admin, the user must have the Project Viewer role. Please note that this authorization method will not allow New Relic to collect labels and other inventory attributes that can be useful for narrowing down your NRQL queries, dashboards and alerts. You can migrate the authorization method from user account to service account from the Manage services link in New Relic's user interface. Project name As part of the online setup process, you must identify Project name of the projects you want to monitor with New Relic. The UI workflow automatically lists active projects you can select. Permissions (only for user account authorization) New Relic requires a specific set of read-only permissions exclusively; this means that, for certain integrations, only partial inventory data will be available. Keep in mind that New Relic doesn't inherit your Google account's permissions and therefore is not authorized to perform any changes in the project. For more information about the API permissions that New Relic uses, see the Google documentation about scopes. Authorization options Integrating your GCP project with New Relic requires you to authorize New Relic to fetch monitoring data from your GCP project. You can choose between two authorization methods: Service accounts or User accounts. Service account (recommended) The service account authorization is recommended. If you authorize New Relic to fetch data through a service account, we will call your GCP project APIs using a service account ID and its associated public/private key pair. New Relic manages a specific Google service account for your New Relic account; you do not need to create it or manage the associated private key. Just add the service account ID as a member with viewing permissions in your project. This authorization method is recommended, especially if your GCP project is managed by a team. It also guarantees that New Relic will collect labels and inventory attributes whenever possible. User account If you authorize New Relic to fetch data through a user account, New Relic will access your GCP project monitoring data on behalf of a particular Google user. The authorization process is achieved through an OAuth workflow, which redirects you from the New Relic UI to a Google authorization interface. However, since the authorization is linked to a particular Google user, this method is not recommended for GCP projects that are managed by large teams. Connect GCP to New Relic infrastructure monitoring To connect your Google account to New Relic with user account authorization: Go to one.newrelic.com > Infrastructure > GCP. At the top of Infrastructure's Google Cloud Services integrations page, select Add a GCP account. Choose Authorization Method: Select either Authorize a Service Account or Authorize a User Account, and follow the instructions in the UI to authorize New Relic. Add projects: Select the projects that you want New Relic to receive data from. Select services: From the list of available services for your GCP account, select the individual services you want New Relic to receive data from, or select all of the services. Tip These services will be enabled for all of the projects that you selected in the previous step. Once the setup process is finished, you can fine-tune the services that you want monitored for each project individually. To complete the setup process, select Finish. If you see API authentication errors, follow the troubleshooting procedures. Explore app data in New Relic After you authorize New Relic to integrate one or more of your Google project's services, New Relic starts monitoring your GCP data at regular polling intervals. After a few minutes, data will appear in the New Relic UI. To find and use your data, including links to dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP. Link multiple Google projects For your convenience, the setup process allows you to select more than one project at a time. After the first setup, if you need to monitor additional GCP projects with New Relic, you can repeat the procedure to connect your GCP services as many times as you need. Unlink your GCP integrations You can disable any of your GCP integrations any time and still keep your Google project connected to New Relic. If you want to... Do this Disable a GCP service monitoring To disconnect individual GCP services but keep the integration with New Relic for other GCP services in your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, make changes to the checkbox options for available services and select Save changes. Unlink your project monitoring To uninstall all of your GCP services completely from New Relic Integrations, unlink your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, select Unlink account and select Save changes. Clean your GCP Projects after unlinking New Relic To clean your GCP project after unlinking, follow these steps if you were using a service account: Open the GCP IAM Console. Select the project you want to unlink from New Relic and click Open. Select the service account that is used by New Relic. Click the Remove icon. Or follow these steps if you were using a user account: Open your Google user account settings. Open the Apps with access to your account section. Choose New Relic application. Choose Remove Access.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.14717,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "sections": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "To <em>start</em> receiving <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) data with New Relic GCP <em>integrations</em>, connect your <em>Google</em> project to New Relic infrastructure monitoring. Tip To use <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em> and the rest of our observability <em>platform</em>, join the New Relic family! Sign up to create your free"
      },
      "id": "603e8309196a67fc4fa83da7"
    },
    {
      "sections": [
        "GCP integration metrics",
        "Google Cloud Metrics"
      ],
      "title": "GCP integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "65e4b0551be716988b29175976fd62a33d82a807",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/gcp-integration-metrics/",
      "published_at": "2021-05-05T15:54:05Z",
      "updated_at": "2021-03-16T05:48:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Google Cloud Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine gcp.appengine.flex.cpu.utilization flex.cpu.Utilization GCP App Engine gcp.appengine.flex.disk.read_bytes_count flex.disk.ReadBytes GCP App Engine gcp.appengine.flex.disk.write_bytes_count flex.disk.WriteBytes GCP App Engine gcp.appengine.flex.network.received_bytes_count flex.network.ReceivedBytes GCP App Engine gcp.appengine.flex.network.sent_bytes_count flex.network.SentBytes GCP App Engine gcp.appengine.http.server.dos_intercept_count server.DosIntercepts GCP App Engine gcp.appengine.http.server.quota_denial_count server.QuotaDenials GCP App Engine gcp.appengine.http.server.response_count server.Responses GCP App Engine gcp.appengine.http.server.response_latencies server.ResponseLatenciesMilliseconds GCP App Engine gcp.appengine.http.server.response_style_count http.server.ResponseStyle GCP App Engine gcp.appengine.memcache.centi_mcu_count memcache.CentiMcu GCP App Engine gcp.appengine.memcache.operation_count memcache.Operations GCP App Engine gcp.appengine.memcache.received_bytes_count memcache.ReceivedBytes GCP App Engine gcp.appengine.memcache.sent_bytes_count memcache.SentBytes GCP App Engine gcp.appengine.system.cpu.usage system.cpu.Usage GCP App Engine gcp.appengine.system.instance_count system.Instances GCP App Engine gcp.appengine.system.memory.usage system.memory.UsageBytes GCP App Engine gcp.appengine.system.network.received_bytes_count system.network.ReceivedBytes GCP App Engine gcp.appengine.system.network.sent_bytes_count system.network.SentBytes GCP App Engine gcp.cloudtasks.api.request_count api.Requests GCP App Engine gcp.cloudtasks.queue.task_attempt_count queue.taskAttempts GCP App Engine gcp.cloudtasks.queue.task_attempt_delays queue.taskAttemptDelaysMilliseconds GCP BigQuery gcp.bigquery.storage.stored_bytes storage.StoredBytes GCP BigQuery gcp.bigquery.storage.table_count storage.Tables GCP BigQuery gcp.bigquery.query.count query.Count GCP BigQuery gcp.bigquery.query.execution_times query.ExecutionTimes GCP BigQuery gcp.bigquery.slots.allocated slots.Allocated GCP BigQuery gcp.bigquery.slots.allocated_for_project slots.AllocatedForProject GCP BigQuery gcp.bigquery.slots.allocated_for_project_and_job_type slots.AllocatedForProjectAndJobType GCP BigQuery gcp.bigquery.slots.allocated_for_reservation slots.AllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_allocated_for_reservation slots.TotalAllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_available slots.TotalAvailable GCP BigQuery gcp.bigquery.storage.uploaded_bytes storage.UploadedBytes GCP BigQuery gcp.bigquery.storage.uploaded_bytes_billed storage.UploadedBytesBilled GCP BigQuery gcp.bigquery.storage.uploaded_row_count storage.UploadedRows GCP Dataflow gcp.dataflow.job.billable_shuffle_data_processed job.BillableShuffleDataProcessed GCP Dataflow gcp.dataflow.job.current_num_vcpus job.CurrentNumVcpus GCP Dataflow gcp.dataflow.job.current_shuffle_slots job.CurrentShuffleSlots GCP Dataflow gcp.dataflow.job.data_watermark_age job.DataWatermarkAge GCP Dataflow gcp.dataflow.job.elapsed_time job.ElapsedTime GCP Dataflow gcp.dataflow.job.element_count job.Elements GCP Dataflow gcp.dataflow.job.estimated_byte_count job.EstimatedBytes GCP Dataflow gcp.dataflow.job.is_failed job.IsFailed GCP Dataflow gcp.dataflow.job.per_stage_data_watermark_age job.PerStageDataWatermarkAge GCP Dataflow gcp.dataflow.job.per_stage_system_lag job.PerStageSystemLag GCP Dataflow gcp.dataflow.job.system_lag job.SystemLag GCP Dataflow gcp.dataflow.job.total_memory_usage_time job.TotalMemoryUsageTime GCP Dataflow gcp.dataflow.job.total_pd_usage_time job.TotalPdUsageTime GCP Dataflow gcp.dataflow.job.total_shuffle_data_processed job.TotalShuffleDataProcessed GCP Dataflow gcp.dataflow.job.total_streaming_data_processed job.TotalStreamingDataProcessed GCP Dataflow gcp.dataflow.job.total_vcpu_time job.TotalVcpuTime GCP Dataflow gcp.dataflow.job.user_counter job.UserCounter GCP Dataproc gcp.dataproc.cluster.hdfs.datanodes cluster.hdfs.Datanodes GCP Dataproc gcp.dataproc.cluster.hdfs.storage_capacity cluster.hdfs.StorageCapacity GCP Dataproc gcp.dataproc.cluster.hdfs.storage_utilization cluster.hdfs.StorageUtilization GCP Dataproc gcp.dataproc.cluster.hdfs.unhealthy_blocks cluster.hdfs.UnhealthyBlocks GCP Dataproc gcp.dataproc.cluster.job.completion_time cluster.job.CompletionTime GCP Dataproc gcp.dataproc.cluster.job.duration cluster.job.Duration GCP Dataproc gcp.dataproc.cluster.job.failed_count cluster.job.Failures GCP Dataproc gcp.dataproc.cluster.job.running_count cluster.job.Running GCP Dataproc gcp.dataproc.cluster.job.submitted_count cluster.job.Submitted GCP Dataproc gcp.dataproc.cluster.operation.completion_time cluster.operation.CompletionTime GCP Dataproc gcp.dataproc.cluster.operation.duration cluster.operation.Duration GCP Dataproc gcp.dataproc.cluster.operation.failed_count cluster.operation.Failures GCP Dataproc gcp.dataproc.cluster.operation.running_count cluster.operation.Running GCP Dataproc gcp.dataproc.cluster.operation.submitted_count cluster.operation.Submitted GCP Dataproc gcp.dataproc.cluster.yarn.allocated_memory_percentage cluster.yarn.AllocatedMemoryPercentage GCP Dataproc gcp.dataproc.cluster.yarn.apps cluster.yarn.Apps GCP Dataproc gcp.dataproc.cluster.yarn.containers cluster.yarn.Containers GCP Dataproc gcp.dataproc.cluster.yarn.memory_size cluster.yarn.MemorySize GCP Dataproc gcp.dataproc.cluster.yarn.nodemanagers cluster.yarn.Nodemanagers GCP Dataproc gcp.dataproc.cluster.yarn.pending_memory_size cluster.yarn.PendingMemorySize GCP Dataproc gcp.dataproc.cluster.yarn.virtual_cores cluster.yarn.VirtualCores GCP Datastore gcp.datastore.api.request_count api.Requests GCP Datastore gcp.datastore.entity.read_sizes entity.ReadSizes GCP Datastore gcp.datastore.entity.write_sizes entity.WriteSizes GCP Datastore gcp.datastore.index.write_count index.Writes GCP Firebase Database gcp.firebasedatabase.io.database_load io.DatabaseLoad GCP Firebase Database gcp.firebasedatabase.io.persisted_bytes_count io.PersistedBytes GCP Firebase Database gcp.firebasedatabase.io.sent_responses_count io.SentResponses GCP Firebase Database gcp.firebasedatabase.io.utilization io.Utilization GCP Firebase Database gcp.firebasedatabase.network.active_connections network.ActiveConnections GCP Firebase Database gcp.firebasedatabase.network.api_hits_count network.ApiHits GCP Firebase Database gcp.firebasedatabase.network.broadcast_load network.BroadcastLoad GCP Firebase Database gcp.firebasedatabase.network.https_requests_count network.HttpsRequests GCP Firebase Database gcp.firebasedatabase.network.monthly_sent network.MonthlySent GCP Firebase Database gcp.firebasedatabase.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Database gcp.firebasedatabase.network.sent_bytes_count network.SentBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_and_protocol_bytes_count network.SentPayloadAndProtocolBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_bytes_count network.SentPayloadBytes GCP Firebase Database gcp.firebasedatabase.rules.evaluation_count rules.Evaluation GCP Firebase Database gcp.firebasedatabase.storage.limit storage.Limit GCP Firebase Database gcp.firebasedatabase.storage.total_bytes storage.TotalBytes GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent network.MonthlySent GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Hosting gcp.firebasehosting.network.sent_bytes_count network.SentBytes GCP Firebase Hosting gcp.firebasehosting.storage.limit storage.Limit GCP Firebase Hosting gcp.firebasehosting.storage.total_bytes storage.TotalBytes GCP Firebase Storage gcp.firebasestorage.rules.evaluation_count rules.Evaluation GCP Firestore gcp.firestore.api.request_count api.Request GCP Firestore gcp.firestore.document.delete_count document.Delete GCP Firestore gcp.firestore.document.read_count document.Read GCP Firestore gcp.firestore.document.write_count document.Write GCP Firestore gcp.firestore.network.active_connections network.ActiveConnections GCP Firestore gcp.firestore.network.snapshot_listeners network.SnapshotListeners GCP Firestore gcp.firestore.rules.evaluation_count rules.Evaluation GCP Cloud Functions gcp.cloudfunctions.function.execution_count function.Executions GCP Cloud Functions gcp.cloudfunctions.function.execution_times function.ExecutionTimeNanos GCP Cloud Functions gcp.cloudfunctions.function.user_memory_bytes function.UserMemoryBytes GCP Interconnect gcp.interconnect.network.interconnect.capacity network.interconnect.Capacity GCP Interconnect gcp.interconnect.network.interconnect.dropped_packets_count network.interconnect.DroppedPackets GCP Interconnect gcp.interconnect.network.interconnect.link.rx_power network.interconnect.link.RxPower GCP Interconnect gcp.interconnect.network.interconnect.link.tx_power network.interconnect.link.TxPower GCP Interconnect gcp.interconnect.network.interconnect.receive_errors_count network.interconnect.ReceiveErrors GCP Interconnect gcp.interconnect.network.interconnect.received_bytes_count network.interconnect.ReceivedBytes GCP Interconnect gcp.interconnect.network.interconnect.received_unicast_packets_count network.interconnect.ReceivedUnicastPackets GCP Interconnect gcp.interconnect.network.interconnect.send_errors_count network.interconnect.SendErrors GCP Interconnect gcp.interconnect.network.interconnect.sent_bytes_count network.interconnect.SentBytes GCP Interconnect gcp.interconnect.network.interconnect.sent_unicast_packets_count network.interconnect.SentUnicastPackets GCP Interconnect gcp.interconnect.network.attachment.capacity network.attachment.Capacity GCP Interconnect gcp.interconnect.network.attachment.received_bytes_count network.attachment.ReceivedBytes GCP Interconnect gcp.interconnect.network.attachment.received_packets_count network.attachment.ReceivedPackets GCP Interconnect gcp.interconnect.network.attachment.sent_bytes_count network.attachment.SentBytes GCP Interconnect gcp.interconnect.network.attachment.sent_packets_count network.attachment.SentPackets GCP Kubernetes Engine gcp.kubernetes.container.accelerator.duty_cycle container.accelerator.dutyCycle GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_total container.accelerator.memoryTotal GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_used container.accelerator.memoryUsed GCP Kubernetes Engine gcp.kubernetes.container.accelerator.request container.accelerator.request GCP Kubernetes Engine gcp.kubernetes.container.cpu.core_usage_time container.cpu.usageTime GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_cores container.cpu.limitCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_utilization container.cpu.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_cores container.cpu.requestCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_utilization container.cpu.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_bytes container.memory.limitBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_utilization container.memory.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.request_bytes container.memory.requestBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.request_utilization container.memory.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.used_bytes container.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.container.restart_count container.restartCount GCP Kubernetes Engine gcp.kubernetes.container.uptime container.uptime GCP Kubernetes Engine gcp.kubernetes.node_daemon.cpu.core_usage_time nodeDaemon.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node_daemon.memory.used_bytes nodeDaemon.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_cores node.cpu.allocatableCores GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_utilization node.cpu.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.cpu.core_usage_time node.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node.cpu.total_cores node.cpu.totalCores GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_bytes node.memory.allocatableBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_utilization node.memory.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.memory.total_bytes node.memory.totalBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.used_bytes node.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.network.received_bytes_count node.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.node.network.sent_bytes_count node.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.received_bytes_count pod.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.sent_bytes_count pod.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.volume.total_bytes pod.volume.totalBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.used_bytes pod.volume.usedBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.utilization pod.volume.utilization GCP Load Balancer gcp.loadbalancing.https.backend_latencies https.BackendLatencies GCP Load Balancer gcp.loadbalancing.https.backend_request_bytes_count https.BackendRequestBytes GCP Load Balancer gcp.loadbalancing.https.backend_request_count https.BackendRequests GCP Load Balancer gcp.loadbalancing.https.backend_response_bytes_count https.BackendResponseBytes GCP Load Balancer gcp.loadbalancing.https.frontend_tcp_rtt https.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.https.request_bytes_count https.RequestBytes GCP Load Balancer gcp.loadbalancing.https.request_count https.Requests GCP Load Balancer gcp.loadbalancing.https.response_bytes_count https.ResponseBytes GCP Load Balancer gcp.loadbalancing.https.total_latencies https.TotalLatencies GCP Load Balancer gcp.loadbalancing.l3.internal.egress_bytes_count l3.internal.EgressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.egress_packets_count l3.internal.EgressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_bytes_count l3.internal.IngressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_packets_count l3.internal.IngressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.rtt_latencies l3.internal.RttLatencies GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.closed_connections tcpSslProxy.ClosedConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.egress_bytes_count tcpSslProxy.EgressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.frontend_tcp_rtt tcpSslProxy.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.ingress_bytes_count tcpSslProxy.IngressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.new_connections tcpSslProxy.NewConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.open_connections tcpSslProxy.OpenConnections GCP Pub/Sub gcp.pubsub.subscription.backlog_bytes subscription.BacklogBytes GCP Pub/Sub gcp.pubsub.subscription.byte_cost subscription.ByteCost GCP Pub/Sub gcp.pubsub.subscription.config_updates_count subscription.ConfigUpdates GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_message_operation_count subscription.ModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_request_count subscription.ModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.num_outstanding_messages subscription.NumOutstandingMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages subscription.NumRetainedAckedMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages_by_region subscription.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_unacked_messages_by_region subscription.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_undelivered_messages subscription.NumUndeliveredMessages GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age subscription.OldestRetainedAckedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age_by_region subscription.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age subscription.OldestUnackedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age_by_region subscription.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.pull_ack_message_operation_count subscription.PullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_ack_request_count subscription.PullAckRequest GCP Pub/Sub gcp.pubsub.subscription.pull_message_operation_count subscription.PullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_request_count subscription.PullRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_count subscription.PushRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_latencies subscription.PushRequestLatencies GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes subscription.RetainedAckedBytes GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes_by_region subscription.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_message_operation_count subscription.StreamingPullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_request_count subscription.StreamingPullAckRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_message_operation_count subscription.StreamingPullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_message_operation_count subscription.StreamingPullModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_request_count subscription.StreamingPullModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_response_count subscription.StreamingPullResponse GCP Pub/Sub gcp.pubsub.subscription.unacked_bytes_by_region subscription.UnackedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.byte_cost topic.ByteCost GCP Pub/Sub gcp.pubsub.topic.config_updates_count topic.ConfigUpdates GCP Pub/Sub gcp.pubsub.topic.message_sizes topic.MessageSizes GCP Pub/Sub gcp.pubsub.topic.num_retained_acked_messages_by_region topic.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.num_unacked_messages_by_region topic.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_retained_acked_message_age_by_region topic.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_unacked_message_age_by_region topic.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.retained_acked_bytes_by_region topic.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.send_message_operation_count topic.SendMessageOperation GCP Pub/Sub gcp.pubsub.topic.send_request_count topic.SendRequest GCP Pub/Sub gcp.pubsub.topic.unacked_bytes_by_region topic.UnackedBytesByRegion GCP Router gcp.router.best_received_routes_count BestReceivedRoutes GCP Router gcp.router.bfd.control.receive_intervals bfd.control.ReceiveIntervals GCP Router gcp.router.bfd.control.received_packets_count bfd.control.ReceivedPackets GCP Router gcp.router.bfd.control.rejected_packets_count bfd.control.RejectedPackets GCP Router gcp.router.bfd.control.transmit_intervals bfd.control.TransmitIntervals GCP Router gcp.router.bfd.control.transmitted_packets_count bfd.control.TransmittedPackets GCP Router gcp.router.bfd.session_up bfd.SessionUp GCP Router gcp.router.bgp_sessions_down_count BgpSessionsDown GCP Router gcp.router.bgp_sessions_up_count BgpSessionsUp GCP Router gcp.router.bgp.received_routes_count bgp.ReceivedRoutes GCP Router gcp.router.bgp.sent_routes_count bgp.SentRoutes GCP Router gcp.router.bgp.session_up bgp.SessionUp GCP Router gcp.router.router_up RouterUp GCP Router gcp.router.sent_routes_count SentRoutes GCP Router gcp.router.nat.allocated_ports nat.AllocatedPorts GCP Router gcp.router.nat.closed_connections_count nat.ClosedConnections GCP Router gcp.router.nat.dropped_received_packets_count nat.DroppedReceivedPackets GCP Router gcp.router.nat.new_connections_count nat.NewConnections GCP Router gcp.router.nat.port_usage nat.PortUsage GCP Router gcp.router.nat.received_bytes_count nat.ReceivedBytes GCP Router gcp.router.nat.received_packets_count nat.ReceivedPackets GCP Router gcp.router.nat.sent_bytes_count nat.SentBytes GCP Router gcp.router.nat.sent_packets_count nat.SentPackets GCP Run gcp.run.container.billable_instance_time container.BillableInstanceTime GCP Run gcp.run.container.cpu.allocation_time container.cpu.AllocationTime GCP Run gcp.run.container.memory.allocation_time container.memory.AllocationTime GCP Run gcp.run.request_count Request GCP Run gcp.run.request_latencies RequestLatencies GCP Spanner gcp.spanner.api.received_bytes_count api.ReceivedBytes GCP Spanner gcp.spanner.api.request_count api.Requests GCP Spanner gcp.spanner.api.request_latencies api.RequestLatencies GCP Spanner gcp.spanner.instance.cpu.utilization instance.cpu.Utilization GCP Spanner gcp.spanner.instance.node_count instance.nodes GCP Spanner gcp.spanner.instance.session_count instance.sessions GCP Spanner gcp.spanner.instance.storage.used_bytes instance.storage.UsedBytes GCP Cloud SQL gcp.cloudsql.database.auto_failover_request_count database.AutoFailoverRequest GCP Cloud SQL gcp.cloudsql.database.available_for_failover database.AvailableForFailover GCP Cloud SQL gcp.cloudsql.database.cpu.reserved_cores database.cpu.ReservedCores GCP Cloud SQL gcp.cloudsql.database.cpu.usage_time database.cpu.UsageTime GCP Cloud SQL gcp.cloudsql.database.cpu.utilization database.cpu.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.bytes_used database.disk.BytesUsed GCP Cloud SQL gcp.cloudsql.database.disk.quota database.disk.Quota GCP Cloud SQL gcp.cloudsql.database.disk.read_ops_count database.disk.ReadOps GCP Cloud SQL gcp.cloudsql.database.disk.utilization database.disk.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.write_ops_count database.disk.WriteOps GCP Cloud SQL gcp.cloudsql.database.memory.quota database.memory.Quota GCP Cloud SQL gcp.cloudsql.database.memory.usage database.memory.Usage GCP Cloud SQL gcp.cloudsql.database.memory.utilization database.memory.Utilization GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_dirty database.mysql.InnodbBufferPoolPagesDirty GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_free database.mysql.InnodbBufferPoolPagesFree GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_total database.mysql.InnodbBufferPoolPagesTotal GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_data_fsyncs database.mysql.InnodbDataFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_os_log_fsyncs database.mysql.InnodbOsLogFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_read database.mysql.InnodbPagesRead GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_written database.mysql.InnodbPagesWritten GCP Cloud SQL gcp.cloudsql.database.mysql.queries database.mysql.Queries GCP Cloud SQL gcp.cloudsql.database.mysql.questions database.mysql.Questions GCP Cloud SQL gcp.cloudsql.database.mysql.received_bytes_count database.mysql.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.mysql.replication.seconds_behind_master database.mysql.replication.SecondsBehindMaster GCP Cloud SQL gcp.cloudsql.database.mysql.sent_bytes_count database.mysql.SentBytes GCP Cloud SQL gcp.cloudsql.database.network.connections database.network.Connections GCP Cloud SQL gcp.cloudsql.database.network.received_bytes_count database.network.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.network.sent_bytes_count database.network.SentBytes GCP Cloud SQL gcp.cloudsql.database.postgresql.num_backends database.postgresql.NumBackends GCP Cloud SQL gcp.cloudsql.database.postgresql.replication.replica_byte_lag database.postgresql.replication.ReplicaByteLag GCP Cloud SQL gcp.cloudsql.database.postgresql.transaction_count database.postgresql.Transaction GCP Cloud SQL gcp.cloudsql.database.up database.Up GCP Cloud SQL gcp.cloudsql.database.uptime database.Uptime GCP Cloud Storage gcp.storage.api.request_count api.Requests GCP Cloud Storage gcp.storage.network.received_bytes_count network.ReceivedBytes GCP Cloud Storage gcp.storage.network.sent_bytes_count network.SentBytes GCP VMs gcp.compute.firewall.dropped_bytes_count firewall.DroppedBytes GCP VMs gcp.compute.firewall.dropped_packets_count firewall.DroppedPackets GCP VMs gcp.compute.instance.cpu.reserved_cores instance.cpu.ReservedCores GCP VMs gcp.compute.instance.cpu.utilization instance.cpu.Utilization GCP VMs gcp.compute.instance.disk.read_bytes_count instance.disk.ReadBytes GCP VMs gcp.compute.instance.disk.read_ops_count instance.disk.ReadOps GCP VMs gcp.compute.instance.disk.write_bytes_count instance.disk.WriteBytes GCP VMs gcp.compute.instance.disk.write_ops_count instance.disk.WriteOps GCP VMs gcp.compute.instance.network.received_bytes_count instance.network.ReceivedBytes GCP VMs gcp.compute.instance.network.received_packets_count instance.network.ReceivedPackets GCP VMs gcp.compute.instance.network.sent_bytes_count instance.network.SentBytes GCP VMs gcp.compute.instance.network.sent_packets_count instance.network.SentPackets GCP VMs gcp.compute.instance.disk.throttled_read_bytes_count instance.disk.ThrottledReadBytes GCP VMs gcp.compute.instance.disk.throttled_read_ops_count instance.disk.ThrottledReadOps GCP VMs gcp.compute.instance.disk.throttled_write_bytes_count instance.disk.ThrottledWriteBytes GCP VMs gcp.compute.instance.disk.throttled_write_ops_count instance.disk.ThrottledWriteOps GCP VPC Access gcp.vpcaccess.connector.received_bytes_count connector.ReceivedBytes GCP VPC Access gcp.vpcaccess.connector.received_packets_count connector.ReceivedPackets GCP VPC Access gcp.vpcaccess.connector.sent_bytes_count connector.SentBytes GCP VPC Access gcp.vpcaccess.connector.sent_packets_count connector.SentPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.65994,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "GCP <em>integration</em> metrics",
        "sections": "<em>Google</em> <em>Cloud</em> Metrics",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "<em>Google</em> <em>Cloud</em> Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine gcp.appengine.flex.cpu.utilization"
      },
      "id": "603e8a5264441f524a4e8840"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/get-started/introduction-google-cloud-platform-integrations": [
    {
      "sections": [
        "Integrations and custom roles",
        "Recommended role",
        "Optional role",
        "Important",
        "List of permissions",
        "Common permissions",
        "Service-specific permissions",
        "Permissions to link projects through the UI"
      ],
      "title": "Integrations and custom roles",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "d4f60e2d8413ddde9a342980d75a0e216af9baa4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/integrations-custom-roles/",
      "published_at": "2021-05-04T18:31:08Z",
      "updated_at": "2021-04-16T16:37:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To read the relevant data from your Google Cloud Platform (GCP) account, New Relic uses the Google Stackdriver API and also other specific services APIs. To access these APIs in your Google Cloud project, the New Relic authorized account needs to be granted a certain set of permissions; GCP uses roles to grant these permissions. Recommended role By default we highly recommend using the GCP primitive role Project Viewer, which grants \"permissions for read-only actions that do not affect your cloud infrastructure state, such as viewing (but not modifying) existing resources or data.\" This role is automatically managed by Google and updated when new Google Cloud services are released or modified. Optional role Alternatively, you can create your own custom role based on the list of permissions, which specifies the minimum set of permissions required to fetch data from each GCP integration. This will allow you to have more control over the permissions set for the New Relic authorized account. Important New Relic has no way of identifying problems related to custom permissions. If you choose to create a custom role, it is your responsibility to maintain it and ensure proper data is being collected. To customize your role you need to: Create a Google Cloud IAM Custom Role in each one of the GCP projects you want to monitor with New Relic. In each custom role, add the permissions that are specifically required for the cloud services you want to monitor according to the following list. Assign the custom role(s) to the New Relic authorized account. List of permissions Common permissions All integrations need the following permission: monitoring.timeSeries.list service.usage.use Service-specific permissions For some GCP integrations, New Relic will also need the following permissions, mainly to collect labels and inventory attributes. Integration Permissions Google AppEngine n/a; Google App Engine does not require additional permissions. Google BigQuery bigquery.datasets.get bigquery.tables.get bigquery.tables.list Google Cloud Functions cloudfunctions.locations.list Google Cloud Load Balancing n/a; Google Cloud Load Balancing does not require additional permissions. Google Cloud Pub/Sub pubsub.subscriptions.get pubsub.subscriptions.list pubsub.topics.get pubsub.topics.list Google Cloud Spanner spanner.instances.list spanner.databases.list spanner.databases.getDdl Google Cloud SQL cloudsql.instances.list Google Cloud Storage storage.buckets.list Google Compute Engine compute.instances.list compute.disks.get compute.disks.list Google Kubernetes Engine container.clusters.list Permissions to link projects through the UI To be able to see the list of projects that you can link to New Relic through the UI, your New Relic authorized service account needs the following permissions: resourcemanager.projects.get monitoring.monitoredResourceDescriptors.list If you do not want to grant New Relic authorized account the permissions that are needed for the linking process through the UI, you have the following options: Assign the Project Viewer or Monitoring Viewer role initially to the authorized account to link Google Cloud projects to New Relic through the UI. After the projects are linked, assign a Google Cloud custom role to the authorized account. Use New Relic NerdGraph to link Google Cloud projects to New Relic. This does not involve listing the viewable projects. However, you must know the id of the project you want to monitor. For more information, see the NerdGraph GraphiQL cloud integrations API tutorial.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.85715,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Integrations</em> and custom roles",
        "sections": "<em>Integrations</em> and custom roles",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "To read the relevant data from your <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) account, New Relic uses the <em>Google</em> Stackdriver API and also other specific services APIs. To access these APIs in your <em>Google</em> <em>Cloud</em> project, the New Relic authorized account needs to be granted a certain set of permissions; GCP uses"
      },
      "id": "603ebb3564441f34b64e8874"
    },
    {
      "sections": [
        "Connect Google Cloud Platform services to New Relic",
        "Tip",
        "Requirements",
        "Authorization options",
        "Service account (recommended)",
        "User account",
        "Connect GCP to New Relic infrastructure monitoring",
        "Explore app data in New Relic",
        "Link multiple Google projects",
        "Unlink your GCP integrations"
      ],
      "title": "Connect Google Cloud Platform services to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "05934d2b03ec1ac5fa43298b21a06dc2e0f8c3b9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/connect-google-cloud-platform-services-new-relic/",
      "published_at": "2021-05-05T15:54:06Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To start receiving Google Cloud Platform (GCP) data with New Relic GCP integrations, connect your Google project to New Relic infrastructure monitoring. Tip To use Google Cloud Platform integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements These are the requirements for the authorization: GCP integration requirements Comments Monitoring In the GCP project API & Services Library settings, you must enable Google Stackdriver Monitoring API. Authorization For service account authorization (recommended): A user with Project IAM Admin role is needed to add the service account ID as a member in your GCP project. In the GCP project IAM & admin, the service account must have the Project Viewer role and the Service Usage Consumer role or, alternatively, a custom role. For user account authorization: The New Relic user that will integrate the GCP project must have a Google account and must be able to view the GCP project that New Relic will monitor. In the GCP project IAM & admin, the user must have the Project Viewer role. Please note that this authorization method will not allow New Relic to collect labels and other inventory attributes that can be useful for narrowing down your NRQL queries, dashboards and alerts. You can migrate the authorization method from user account to service account from the Manage services link in New Relic's user interface. Project name As part of the online setup process, you must identify Project name of the projects you want to monitor with New Relic. The UI workflow automatically lists active projects you can select. Permissions (only for user account authorization) New Relic requires a specific set of read-only permissions exclusively; this means that, for certain integrations, only partial inventory data will be available. Keep in mind that New Relic doesn't inherit your Google account's permissions and therefore is not authorized to perform any changes in the project. For more information about the API permissions that New Relic uses, see the Google documentation about scopes. Authorization options Integrating your GCP project with New Relic requires you to authorize New Relic to fetch monitoring data from your GCP project. You can choose between two authorization methods: Service accounts or User accounts. Service account (recommended) The service account authorization is recommended. If you authorize New Relic to fetch data through a service account, we will call your GCP project APIs using a service account ID and its associated public/private key pair. New Relic manages a specific Google service account for your New Relic account; you do not need to create it or manage the associated private key. Just add the service account ID as a member with viewing permissions in your project. This authorization method is recommended, especially if your GCP project is managed by a team. It also guarantees that New Relic will collect labels and inventory attributes whenever possible. User account If you authorize New Relic to fetch data through a user account, New Relic will access your GCP project monitoring data on behalf of a particular Google user. The authorization process is achieved through an OAuth workflow, which redirects you from the New Relic UI to a Google authorization interface. However, since the authorization is linked to a particular Google user, this method is not recommended for GCP projects that are managed by large teams. Connect GCP to New Relic infrastructure monitoring To connect your Google account to New Relic with user account authorization: Go to one.newrelic.com > Infrastructure > GCP. At the top of Infrastructure's Google Cloud Services integrations page, select Add a GCP account. Choose Authorization Method: Select either Authorize a Service Account or Authorize a User Account, and follow the instructions in the UI to authorize New Relic. Add projects: Select the projects that you want New Relic to receive data from. Select services: From the list of available services for your GCP account, select the individual services you want New Relic to receive data from, or select all of the services. Tip These services will be enabled for all of the projects that you selected in the previous step. Once the setup process is finished, you can fine-tune the services that you want monitored for each project individually. To complete the setup process, select Finish. If you see API authentication errors, follow the troubleshooting procedures. Explore app data in New Relic After you authorize New Relic to integrate one or more of your Google project's services, New Relic starts monitoring your GCP data at regular polling intervals. After a few minutes, data will appear in the New Relic UI. To find and use your data, including links to dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP. Link multiple Google projects For your convenience, the setup process allows you to select more than one project at a time. After the first setup, if you need to monitor additional GCP projects with New Relic, you can repeat the procedure to connect your GCP services as many times as you need. Unlink your GCP integrations You can disable any of your GCP integrations any time and still keep your Google project connected to New Relic. If you want to... Do this Disable a GCP service monitoring To disconnect individual GCP services but keep the integration with New Relic for other GCP services in your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, make changes to the checkbox options for available services and select Save changes. Unlink your project monitoring To uninstall all of your GCP services completely from New Relic Integrations, unlink your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, select Unlink account and select Save changes. Clean your GCP Projects after unlinking New Relic To clean your GCP project after unlinking, follow these steps if you were using a service account: Open the GCP IAM Console. Select the project you want to unlink from New Relic and click Open. Select the service account that is used by New Relic. Click the Remove icon. Or follow these steps if you were using a user account: Open your Google user account settings. Open the Apps with access to your account section. Choose New Relic application. Choose Remove Access.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.14717,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "sections": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "To <em>start</em> receiving <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) data with New Relic GCP <em>integrations</em>, connect your <em>Google</em> project to New Relic infrastructure monitoring. Tip To use <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em> and the rest of our observability <em>platform</em>, join the New Relic family! Sign up to create your free"
      },
      "id": "603e8309196a67fc4fa83da7"
    },
    {
      "sections": [
        "GCP integration metrics",
        "Google Cloud Metrics"
      ],
      "title": "GCP integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "65e4b0551be716988b29175976fd62a33d82a807",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/gcp-integration-metrics/",
      "published_at": "2021-05-05T15:54:05Z",
      "updated_at": "2021-03-16T05:48:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Google Cloud Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine gcp.appengine.flex.cpu.utilization flex.cpu.Utilization GCP App Engine gcp.appengine.flex.disk.read_bytes_count flex.disk.ReadBytes GCP App Engine gcp.appengine.flex.disk.write_bytes_count flex.disk.WriteBytes GCP App Engine gcp.appengine.flex.network.received_bytes_count flex.network.ReceivedBytes GCP App Engine gcp.appengine.flex.network.sent_bytes_count flex.network.SentBytes GCP App Engine gcp.appengine.http.server.dos_intercept_count server.DosIntercepts GCP App Engine gcp.appengine.http.server.quota_denial_count server.QuotaDenials GCP App Engine gcp.appengine.http.server.response_count server.Responses GCP App Engine gcp.appengine.http.server.response_latencies server.ResponseLatenciesMilliseconds GCP App Engine gcp.appengine.http.server.response_style_count http.server.ResponseStyle GCP App Engine gcp.appengine.memcache.centi_mcu_count memcache.CentiMcu GCP App Engine gcp.appengine.memcache.operation_count memcache.Operations GCP App Engine gcp.appengine.memcache.received_bytes_count memcache.ReceivedBytes GCP App Engine gcp.appengine.memcache.sent_bytes_count memcache.SentBytes GCP App Engine gcp.appengine.system.cpu.usage system.cpu.Usage GCP App Engine gcp.appengine.system.instance_count system.Instances GCP App Engine gcp.appengine.system.memory.usage system.memory.UsageBytes GCP App Engine gcp.appengine.system.network.received_bytes_count system.network.ReceivedBytes GCP App Engine gcp.appengine.system.network.sent_bytes_count system.network.SentBytes GCP App Engine gcp.cloudtasks.api.request_count api.Requests GCP App Engine gcp.cloudtasks.queue.task_attempt_count queue.taskAttempts GCP App Engine gcp.cloudtasks.queue.task_attempt_delays queue.taskAttemptDelaysMilliseconds GCP BigQuery gcp.bigquery.storage.stored_bytes storage.StoredBytes GCP BigQuery gcp.bigquery.storage.table_count storage.Tables GCP BigQuery gcp.bigquery.query.count query.Count GCP BigQuery gcp.bigquery.query.execution_times query.ExecutionTimes GCP BigQuery gcp.bigquery.slots.allocated slots.Allocated GCP BigQuery gcp.bigquery.slots.allocated_for_project slots.AllocatedForProject GCP BigQuery gcp.bigquery.slots.allocated_for_project_and_job_type slots.AllocatedForProjectAndJobType GCP BigQuery gcp.bigquery.slots.allocated_for_reservation slots.AllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_allocated_for_reservation slots.TotalAllocatedForReservation GCP BigQuery gcp.bigquery.slots.total_available slots.TotalAvailable GCP BigQuery gcp.bigquery.storage.uploaded_bytes storage.UploadedBytes GCP BigQuery gcp.bigquery.storage.uploaded_bytes_billed storage.UploadedBytesBilled GCP BigQuery gcp.bigquery.storage.uploaded_row_count storage.UploadedRows GCP Dataflow gcp.dataflow.job.billable_shuffle_data_processed job.BillableShuffleDataProcessed GCP Dataflow gcp.dataflow.job.current_num_vcpus job.CurrentNumVcpus GCP Dataflow gcp.dataflow.job.current_shuffle_slots job.CurrentShuffleSlots GCP Dataflow gcp.dataflow.job.data_watermark_age job.DataWatermarkAge GCP Dataflow gcp.dataflow.job.elapsed_time job.ElapsedTime GCP Dataflow gcp.dataflow.job.element_count job.Elements GCP Dataflow gcp.dataflow.job.estimated_byte_count job.EstimatedBytes GCP Dataflow gcp.dataflow.job.is_failed job.IsFailed GCP Dataflow gcp.dataflow.job.per_stage_data_watermark_age job.PerStageDataWatermarkAge GCP Dataflow gcp.dataflow.job.per_stage_system_lag job.PerStageSystemLag GCP Dataflow gcp.dataflow.job.system_lag job.SystemLag GCP Dataflow gcp.dataflow.job.total_memory_usage_time job.TotalMemoryUsageTime GCP Dataflow gcp.dataflow.job.total_pd_usage_time job.TotalPdUsageTime GCP Dataflow gcp.dataflow.job.total_shuffle_data_processed job.TotalShuffleDataProcessed GCP Dataflow gcp.dataflow.job.total_streaming_data_processed job.TotalStreamingDataProcessed GCP Dataflow gcp.dataflow.job.total_vcpu_time job.TotalVcpuTime GCP Dataflow gcp.dataflow.job.user_counter job.UserCounter GCP Dataproc gcp.dataproc.cluster.hdfs.datanodes cluster.hdfs.Datanodes GCP Dataproc gcp.dataproc.cluster.hdfs.storage_capacity cluster.hdfs.StorageCapacity GCP Dataproc gcp.dataproc.cluster.hdfs.storage_utilization cluster.hdfs.StorageUtilization GCP Dataproc gcp.dataproc.cluster.hdfs.unhealthy_blocks cluster.hdfs.UnhealthyBlocks GCP Dataproc gcp.dataproc.cluster.job.completion_time cluster.job.CompletionTime GCP Dataproc gcp.dataproc.cluster.job.duration cluster.job.Duration GCP Dataproc gcp.dataproc.cluster.job.failed_count cluster.job.Failures GCP Dataproc gcp.dataproc.cluster.job.running_count cluster.job.Running GCP Dataproc gcp.dataproc.cluster.job.submitted_count cluster.job.Submitted GCP Dataproc gcp.dataproc.cluster.operation.completion_time cluster.operation.CompletionTime GCP Dataproc gcp.dataproc.cluster.operation.duration cluster.operation.Duration GCP Dataproc gcp.dataproc.cluster.operation.failed_count cluster.operation.Failures GCP Dataproc gcp.dataproc.cluster.operation.running_count cluster.operation.Running GCP Dataproc gcp.dataproc.cluster.operation.submitted_count cluster.operation.Submitted GCP Dataproc gcp.dataproc.cluster.yarn.allocated_memory_percentage cluster.yarn.AllocatedMemoryPercentage GCP Dataproc gcp.dataproc.cluster.yarn.apps cluster.yarn.Apps GCP Dataproc gcp.dataproc.cluster.yarn.containers cluster.yarn.Containers GCP Dataproc gcp.dataproc.cluster.yarn.memory_size cluster.yarn.MemorySize GCP Dataproc gcp.dataproc.cluster.yarn.nodemanagers cluster.yarn.Nodemanagers GCP Dataproc gcp.dataproc.cluster.yarn.pending_memory_size cluster.yarn.PendingMemorySize GCP Dataproc gcp.dataproc.cluster.yarn.virtual_cores cluster.yarn.VirtualCores GCP Datastore gcp.datastore.api.request_count api.Requests GCP Datastore gcp.datastore.entity.read_sizes entity.ReadSizes GCP Datastore gcp.datastore.entity.write_sizes entity.WriteSizes GCP Datastore gcp.datastore.index.write_count index.Writes GCP Firebase Database gcp.firebasedatabase.io.database_load io.DatabaseLoad GCP Firebase Database gcp.firebasedatabase.io.persisted_bytes_count io.PersistedBytes GCP Firebase Database gcp.firebasedatabase.io.sent_responses_count io.SentResponses GCP Firebase Database gcp.firebasedatabase.io.utilization io.Utilization GCP Firebase Database gcp.firebasedatabase.network.active_connections network.ActiveConnections GCP Firebase Database gcp.firebasedatabase.network.api_hits_count network.ApiHits GCP Firebase Database gcp.firebasedatabase.network.broadcast_load network.BroadcastLoad GCP Firebase Database gcp.firebasedatabase.network.https_requests_count network.HttpsRequests GCP Firebase Database gcp.firebasedatabase.network.monthly_sent network.MonthlySent GCP Firebase Database gcp.firebasedatabase.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Database gcp.firebasedatabase.network.sent_bytes_count network.SentBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_and_protocol_bytes_count network.SentPayloadAndProtocolBytes GCP Firebase Database gcp.firebasedatabase.network.sent_payload_bytes_count network.SentPayloadBytes GCP Firebase Database gcp.firebasedatabase.rules.evaluation_count rules.Evaluation GCP Firebase Database gcp.firebasedatabase.storage.limit storage.Limit GCP Firebase Database gcp.firebasedatabase.storage.total_bytes storage.TotalBytes GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent network.MonthlySent GCP Firebase Hosting gcp.firebasehosting.network.monthly_sent_limit network.MonthlySentLimit GCP Firebase Hosting gcp.firebasehosting.network.sent_bytes_count network.SentBytes GCP Firebase Hosting gcp.firebasehosting.storage.limit storage.Limit GCP Firebase Hosting gcp.firebasehosting.storage.total_bytes storage.TotalBytes GCP Firebase Storage gcp.firebasestorage.rules.evaluation_count rules.Evaluation GCP Firestore gcp.firestore.api.request_count api.Request GCP Firestore gcp.firestore.document.delete_count document.Delete GCP Firestore gcp.firestore.document.read_count document.Read GCP Firestore gcp.firestore.document.write_count document.Write GCP Firestore gcp.firestore.network.active_connections network.ActiveConnections GCP Firestore gcp.firestore.network.snapshot_listeners network.SnapshotListeners GCP Firestore gcp.firestore.rules.evaluation_count rules.Evaluation GCP Cloud Functions gcp.cloudfunctions.function.execution_count function.Executions GCP Cloud Functions gcp.cloudfunctions.function.execution_times function.ExecutionTimeNanos GCP Cloud Functions gcp.cloudfunctions.function.user_memory_bytes function.UserMemoryBytes GCP Interconnect gcp.interconnect.network.interconnect.capacity network.interconnect.Capacity GCP Interconnect gcp.interconnect.network.interconnect.dropped_packets_count network.interconnect.DroppedPackets GCP Interconnect gcp.interconnect.network.interconnect.link.rx_power network.interconnect.link.RxPower GCP Interconnect gcp.interconnect.network.interconnect.link.tx_power network.interconnect.link.TxPower GCP Interconnect gcp.interconnect.network.interconnect.receive_errors_count network.interconnect.ReceiveErrors GCP Interconnect gcp.interconnect.network.interconnect.received_bytes_count network.interconnect.ReceivedBytes GCP Interconnect gcp.interconnect.network.interconnect.received_unicast_packets_count network.interconnect.ReceivedUnicastPackets GCP Interconnect gcp.interconnect.network.interconnect.send_errors_count network.interconnect.SendErrors GCP Interconnect gcp.interconnect.network.interconnect.sent_bytes_count network.interconnect.SentBytes GCP Interconnect gcp.interconnect.network.interconnect.sent_unicast_packets_count network.interconnect.SentUnicastPackets GCP Interconnect gcp.interconnect.network.attachment.capacity network.attachment.Capacity GCP Interconnect gcp.interconnect.network.attachment.received_bytes_count network.attachment.ReceivedBytes GCP Interconnect gcp.interconnect.network.attachment.received_packets_count network.attachment.ReceivedPackets GCP Interconnect gcp.interconnect.network.attachment.sent_bytes_count network.attachment.SentBytes GCP Interconnect gcp.interconnect.network.attachment.sent_packets_count network.attachment.SentPackets GCP Kubernetes Engine gcp.kubernetes.container.accelerator.duty_cycle container.accelerator.dutyCycle GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_total container.accelerator.memoryTotal GCP Kubernetes Engine gcp.kubernetes.container.accelerator.memory_used container.accelerator.memoryUsed GCP Kubernetes Engine gcp.kubernetes.container.accelerator.request container.accelerator.request GCP Kubernetes Engine gcp.kubernetes.container.cpu.core_usage_time container.cpu.usageTime GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_cores container.cpu.limitCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.limit_utilization container.cpu.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_cores container.cpu.requestCores GCP Kubernetes Engine gcp.kubernetes.container.cpu.request_utilization container.cpu.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_bytes container.memory.limitBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.limit_utilization container.memory.limitUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.request_bytes container.memory.requestBytes GCP Kubernetes Engine gcp.kubernetes.container.memory.request_utilization container.memory.requestUtilization GCP Kubernetes Engine gcp.kubernetes.container.memory.used_bytes container.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.container.restart_count container.restartCount GCP Kubernetes Engine gcp.kubernetes.container.uptime container.uptime GCP Kubernetes Engine gcp.kubernetes.node_daemon.cpu.core_usage_time nodeDaemon.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node_daemon.memory.used_bytes nodeDaemon.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_cores node.cpu.allocatableCores GCP Kubernetes Engine gcp.kubernetes.node.cpu.allocatable_utilization node.cpu.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.cpu.core_usage_time node.cpu.coreUsageTime GCP Kubernetes Engine gcp.kubernetes.node.cpu.total_cores node.cpu.totalCores GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_bytes node.memory.allocatableBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.allocatable_utilization node.memory.allocatableUtilization GCP Kubernetes Engine gcp.kubernetes.node.memory.total_bytes node.memory.totalBytes GCP Kubernetes Engine gcp.kubernetes.node.memory.used_bytes node.memory.usedBytes GCP Kubernetes Engine gcp.kubernetes.node.network.received_bytes_count node.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.node.network.sent_bytes_count node.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.received_bytes_count pod.network.receivedBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.network.sent_bytes_count pod.network.sentBytesCount GCP Kubernetes Engine gcp.kubernetes.pod.volume.total_bytes pod.volume.totalBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.used_bytes pod.volume.usedBytes GCP Kubernetes Engine gcp.kubernetes.pod.volume.utilization pod.volume.utilization GCP Load Balancer gcp.loadbalancing.https.backend_latencies https.BackendLatencies GCP Load Balancer gcp.loadbalancing.https.backend_request_bytes_count https.BackendRequestBytes GCP Load Balancer gcp.loadbalancing.https.backend_request_count https.BackendRequests GCP Load Balancer gcp.loadbalancing.https.backend_response_bytes_count https.BackendResponseBytes GCP Load Balancer gcp.loadbalancing.https.frontend_tcp_rtt https.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.https.request_bytes_count https.RequestBytes GCP Load Balancer gcp.loadbalancing.https.request_count https.Requests GCP Load Balancer gcp.loadbalancing.https.response_bytes_count https.ResponseBytes GCP Load Balancer gcp.loadbalancing.https.total_latencies https.TotalLatencies GCP Load Balancer gcp.loadbalancing.l3.internal.egress_bytes_count l3.internal.EgressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.egress_packets_count l3.internal.EgressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_bytes_count l3.internal.IngressBytes GCP Load Balancer gcp.loadbalancing.l3.internal.ingress_packets_count l3.internal.IngressPackets GCP Load Balancer gcp.loadbalancing.l3.internal.rtt_latencies l3.internal.RttLatencies GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.closed_connections tcpSslProxy.ClosedConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.egress_bytes_count tcpSslProxy.EgressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.frontend_tcp_rtt tcpSslProxy.FrontendTcpRtt GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.ingress_bytes_count tcpSslProxy.IngressBytes GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.new_connections tcpSslProxy.NewConnections GCP Load Balancer gcp.loadbalancing.tcp_ssl_proxy.open_connections tcpSslProxy.OpenConnections GCP Pub/Sub gcp.pubsub.subscription.backlog_bytes subscription.BacklogBytes GCP Pub/Sub gcp.pubsub.subscription.byte_cost subscription.ByteCost GCP Pub/Sub gcp.pubsub.subscription.config_updates_count subscription.ConfigUpdates GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_message_operation_count subscription.ModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.mod_ack_deadline_request_count subscription.ModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.num_outstanding_messages subscription.NumOutstandingMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages subscription.NumRetainedAckedMessages GCP Pub/Sub gcp.pubsub.subscription.num_retained_acked_messages_by_region subscription.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_unacked_messages_by_region subscription.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.subscription.num_undelivered_messages subscription.NumUndeliveredMessages GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age subscription.OldestRetainedAckedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_retained_acked_message_age_by_region subscription.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age subscription.OldestUnackedMessageAge GCP Pub/Sub gcp.pubsub.subscription.oldest_unacked_message_age_by_region subscription.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.subscription.pull_ack_message_operation_count subscription.PullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_ack_request_count subscription.PullAckRequest GCP Pub/Sub gcp.pubsub.subscription.pull_message_operation_count subscription.PullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.pull_request_count subscription.PullRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_count subscription.PushRequest GCP Pub/Sub gcp.pubsub.subscription.push_request_latencies subscription.PushRequestLatencies GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes subscription.RetainedAckedBytes GCP Pub/Sub gcp.pubsub.subscription.retained_acked_bytes_by_region subscription.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_message_operation_count subscription.StreamingPullAckMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_ack_request_count subscription.StreamingPullAckRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_message_operation_count subscription.StreamingPullMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_message_operation_count subscription.StreamingPullModAckDeadlineMessageOperation GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_mod_ack_deadline_request_count subscription.StreamingPullModAckDeadlineRequest GCP Pub/Sub gcp.pubsub.subscription.streaming_pull_response_count subscription.StreamingPullResponse GCP Pub/Sub gcp.pubsub.subscription.unacked_bytes_by_region subscription.UnackedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.byte_cost topic.ByteCost GCP Pub/Sub gcp.pubsub.topic.config_updates_count topic.ConfigUpdates GCP Pub/Sub gcp.pubsub.topic.message_sizes topic.MessageSizes GCP Pub/Sub gcp.pubsub.topic.num_retained_acked_messages_by_region topic.NumRetainedAckedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.num_unacked_messages_by_region topic.NumUnackedMessagesByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_retained_acked_message_age_by_region topic.OldestRetainedAckedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.oldest_unacked_message_age_by_region topic.OldestUnackedMessageAgeByRegion GCP Pub/Sub gcp.pubsub.topic.retained_acked_bytes_by_region topic.RetainedAckedBytesByRegion GCP Pub/Sub gcp.pubsub.topic.send_message_operation_count topic.SendMessageOperation GCP Pub/Sub gcp.pubsub.topic.send_request_count topic.SendRequest GCP Pub/Sub gcp.pubsub.topic.unacked_bytes_by_region topic.UnackedBytesByRegion GCP Router gcp.router.best_received_routes_count BestReceivedRoutes GCP Router gcp.router.bfd.control.receive_intervals bfd.control.ReceiveIntervals GCP Router gcp.router.bfd.control.received_packets_count bfd.control.ReceivedPackets GCP Router gcp.router.bfd.control.rejected_packets_count bfd.control.RejectedPackets GCP Router gcp.router.bfd.control.transmit_intervals bfd.control.TransmitIntervals GCP Router gcp.router.bfd.control.transmitted_packets_count bfd.control.TransmittedPackets GCP Router gcp.router.bfd.session_up bfd.SessionUp GCP Router gcp.router.bgp_sessions_down_count BgpSessionsDown GCP Router gcp.router.bgp_sessions_up_count BgpSessionsUp GCP Router gcp.router.bgp.received_routes_count bgp.ReceivedRoutes GCP Router gcp.router.bgp.sent_routes_count bgp.SentRoutes GCP Router gcp.router.bgp.session_up bgp.SessionUp GCP Router gcp.router.router_up RouterUp GCP Router gcp.router.sent_routes_count SentRoutes GCP Router gcp.router.nat.allocated_ports nat.AllocatedPorts GCP Router gcp.router.nat.closed_connections_count nat.ClosedConnections GCP Router gcp.router.nat.dropped_received_packets_count nat.DroppedReceivedPackets GCP Router gcp.router.nat.new_connections_count nat.NewConnections GCP Router gcp.router.nat.port_usage nat.PortUsage GCP Router gcp.router.nat.received_bytes_count nat.ReceivedBytes GCP Router gcp.router.nat.received_packets_count nat.ReceivedPackets GCP Router gcp.router.nat.sent_bytes_count nat.SentBytes GCP Router gcp.router.nat.sent_packets_count nat.SentPackets GCP Run gcp.run.container.billable_instance_time container.BillableInstanceTime GCP Run gcp.run.container.cpu.allocation_time container.cpu.AllocationTime GCP Run gcp.run.container.memory.allocation_time container.memory.AllocationTime GCP Run gcp.run.request_count Request GCP Run gcp.run.request_latencies RequestLatencies GCP Spanner gcp.spanner.api.received_bytes_count api.ReceivedBytes GCP Spanner gcp.spanner.api.request_count api.Requests GCP Spanner gcp.spanner.api.request_latencies api.RequestLatencies GCP Spanner gcp.spanner.instance.cpu.utilization instance.cpu.Utilization GCP Spanner gcp.spanner.instance.node_count instance.nodes GCP Spanner gcp.spanner.instance.session_count instance.sessions GCP Spanner gcp.spanner.instance.storage.used_bytes instance.storage.UsedBytes GCP Cloud SQL gcp.cloudsql.database.auto_failover_request_count database.AutoFailoverRequest GCP Cloud SQL gcp.cloudsql.database.available_for_failover database.AvailableForFailover GCP Cloud SQL gcp.cloudsql.database.cpu.reserved_cores database.cpu.ReservedCores GCP Cloud SQL gcp.cloudsql.database.cpu.usage_time database.cpu.UsageTime GCP Cloud SQL gcp.cloudsql.database.cpu.utilization database.cpu.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.bytes_used database.disk.BytesUsed GCP Cloud SQL gcp.cloudsql.database.disk.quota database.disk.Quota GCP Cloud SQL gcp.cloudsql.database.disk.read_ops_count database.disk.ReadOps GCP Cloud SQL gcp.cloudsql.database.disk.utilization database.disk.Utilization GCP Cloud SQL gcp.cloudsql.database.disk.write_ops_count database.disk.WriteOps GCP Cloud SQL gcp.cloudsql.database.memory.quota database.memory.Quota GCP Cloud SQL gcp.cloudsql.database.memory.usage database.memory.Usage GCP Cloud SQL gcp.cloudsql.database.memory.utilization database.memory.Utilization GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_dirty database.mysql.InnodbBufferPoolPagesDirty GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_free database.mysql.InnodbBufferPoolPagesFree GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_buffer_pool_pages_total database.mysql.InnodbBufferPoolPagesTotal GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_data_fsyncs database.mysql.InnodbDataFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_os_log_fsyncs database.mysql.InnodbOsLogFsyncs GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_read database.mysql.InnodbPagesRead GCP Cloud SQL gcp.cloudsql.database.mysql.innodb_pages_written database.mysql.InnodbPagesWritten GCP Cloud SQL gcp.cloudsql.database.mysql.queries database.mysql.Queries GCP Cloud SQL gcp.cloudsql.database.mysql.questions database.mysql.Questions GCP Cloud SQL gcp.cloudsql.database.mysql.received_bytes_count database.mysql.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.mysql.replication.seconds_behind_master database.mysql.replication.SecondsBehindMaster GCP Cloud SQL gcp.cloudsql.database.mysql.sent_bytes_count database.mysql.SentBytes GCP Cloud SQL gcp.cloudsql.database.network.connections database.network.Connections GCP Cloud SQL gcp.cloudsql.database.network.received_bytes_count database.network.ReceivedBytes GCP Cloud SQL gcp.cloudsql.database.network.sent_bytes_count database.network.SentBytes GCP Cloud SQL gcp.cloudsql.database.postgresql.num_backends database.postgresql.NumBackends GCP Cloud SQL gcp.cloudsql.database.postgresql.replication.replica_byte_lag database.postgresql.replication.ReplicaByteLag GCP Cloud SQL gcp.cloudsql.database.postgresql.transaction_count database.postgresql.Transaction GCP Cloud SQL gcp.cloudsql.database.up database.Up GCP Cloud SQL gcp.cloudsql.database.uptime database.Uptime GCP Cloud Storage gcp.storage.api.request_count api.Requests GCP Cloud Storage gcp.storage.network.received_bytes_count network.ReceivedBytes GCP Cloud Storage gcp.storage.network.sent_bytes_count network.SentBytes GCP VMs gcp.compute.firewall.dropped_bytes_count firewall.DroppedBytes GCP VMs gcp.compute.firewall.dropped_packets_count firewall.DroppedPackets GCP VMs gcp.compute.instance.cpu.reserved_cores instance.cpu.ReservedCores GCP VMs gcp.compute.instance.cpu.utilization instance.cpu.Utilization GCP VMs gcp.compute.instance.disk.read_bytes_count instance.disk.ReadBytes GCP VMs gcp.compute.instance.disk.read_ops_count instance.disk.ReadOps GCP VMs gcp.compute.instance.disk.write_bytes_count instance.disk.WriteBytes GCP VMs gcp.compute.instance.disk.write_ops_count instance.disk.WriteOps GCP VMs gcp.compute.instance.network.received_bytes_count instance.network.ReceivedBytes GCP VMs gcp.compute.instance.network.received_packets_count instance.network.ReceivedPackets GCP VMs gcp.compute.instance.network.sent_bytes_count instance.network.SentBytes GCP VMs gcp.compute.instance.network.sent_packets_count instance.network.SentPackets GCP VMs gcp.compute.instance.disk.throttled_read_bytes_count instance.disk.ThrottledReadBytes GCP VMs gcp.compute.instance.disk.throttled_read_ops_count instance.disk.ThrottledReadOps GCP VMs gcp.compute.instance.disk.throttled_write_bytes_count instance.disk.ThrottledWriteBytes GCP VMs gcp.compute.instance.disk.throttled_write_ops_count instance.disk.ThrottledWriteOps GCP VPC Access gcp.vpcaccess.connector.received_bytes_count connector.ReceivedBytes GCP VPC Access gcp.vpcaccess.connector.received_packets_count connector.ReceivedPackets GCP VPC Access gcp.vpcaccess.connector.sent_bytes_count connector.SentBytes GCP VPC Access gcp.vpcaccess.connector.sent_packets_count connector.SentPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.65994,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "GCP <em>integration</em> metrics",
        "sections": "<em>Google</em> <em>Cloud</em> Metrics",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "<em>Google</em> <em>Cloud</em> Metrics The following table contains the metrics we collect for GCP. Integration Dimensional Metric Name (new) Sample Metric Name (previous) GCP App Engine gcp.appengine.flex.cpu.reserved_cores flex.cpu.ReservedCores GCP App Engine gcp.appengine.flex.cpu.utilization"
      },
      "id": "603e8a5264441f524a4e8840"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/getting-started/polling-intervals-gcp-integrations": [
    {
      "sections": [
        "Integrations and custom roles",
        "Recommended role",
        "Optional role",
        "Important",
        "List of permissions",
        "Common permissions",
        "Service-specific permissions",
        "Permissions to link projects through the UI"
      ],
      "title": "Integrations and custom roles",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "d4f60e2d8413ddde9a342980d75a0e216af9baa4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/integrations-custom-roles/",
      "published_at": "2021-05-04T18:31:08Z",
      "updated_at": "2021-04-16T16:37:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To read the relevant data from your Google Cloud Platform (GCP) account, New Relic uses the Google Stackdriver API and also other specific services APIs. To access these APIs in your Google Cloud project, the New Relic authorized account needs to be granted a certain set of permissions; GCP uses roles to grant these permissions. Recommended role By default we highly recommend using the GCP primitive role Project Viewer, which grants \"permissions for read-only actions that do not affect your cloud infrastructure state, such as viewing (but not modifying) existing resources or data.\" This role is automatically managed by Google and updated when new Google Cloud services are released or modified. Optional role Alternatively, you can create your own custom role based on the list of permissions, which specifies the minimum set of permissions required to fetch data from each GCP integration. This will allow you to have more control over the permissions set for the New Relic authorized account. Important New Relic has no way of identifying problems related to custom permissions. If you choose to create a custom role, it is your responsibility to maintain it and ensure proper data is being collected. To customize your role you need to: Create a Google Cloud IAM Custom Role in each one of the GCP projects you want to monitor with New Relic. In each custom role, add the permissions that are specifically required for the cloud services you want to monitor according to the following list. Assign the custom role(s) to the New Relic authorized account. List of permissions Common permissions All integrations need the following permission: monitoring.timeSeries.list service.usage.use Service-specific permissions For some GCP integrations, New Relic will also need the following permissions, mainly to collect labels and inventory attributes. Integration Permissions Google AppEngine n/a; Google App Engine does not require additional permissions. Google BigQuery bigquery.datasets.get bigquery.tables.get bigquery.tables.list Google Cloud Functions cloudfunctions.locations.list Google Cloud Load Balancing n/a; Google Cloud Load Balancing does not require additional permissions. Google Cloud Pub/Sub pubsub.subscriptions.get pubsub.subscriptions.list pubsub.topics.get pubsub.topics.list Google Cloud Spanner spanner.instances.list spanner.databases.list spanner.databases.getDdl Google Cloud SQL cloudsql.instances.list Google Cloud Storage storage.buckets.list Google Compute Engine compute.instances.list compute.disks.get compute.disks.list Google Kubernetes Engine container.clusters.list Permissions to link projects through the UI To be able to see the list of projects that you can link to New Relic through the UI, your New Relic authorized service account needs the following permissions: resourcemanager.projects.get monitoring.monitoredResourceDescriptors.list If you do not want to grant New Relic authorized account the permissions that are needed for the linking process through the UI, you have the following options: Assign the Project Viewer or Monitoring Viewer role initially to the authorized account to link Google Cloud projects to New Relic through the UI. After the projects are linked, assign a Google Cloud custom role to the authorized account. Use New Relic NerdGraph to link Google Cloud projects to New Relic. This does not involve listing the viewable projects. However, you must know the id of the project you want to monitor. For more information, see the NerdGraph GraphiQL cloud integrations API tutorial.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.85715,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Integrations</em> and custom roles",
        "sections": "<em>Integrations</em> and custom roles",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "To read the relevant data from your <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) account, New Relic uses the <em>Google</em> Stackdriver API and also other specific services APIs. To access these APIs in your <em>Google</em> <em>Cloud</em> project, the New Relic authorized account needs to be granted a certain set of permissions; GCP uses"
      },
      "id": "603ebb3564441f34b64e8874"
    },
    {
      "sections": [
        "Introduction to Google Cloud Platform integrations",
        "Connect GCP and New Relic",
        "Tip",
        "View your GCP data"
      ],
      "title": "Introduction to Google Cloud Platform integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "508adec5bbbcaef86a079533911bbbec5e1824c4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/introduction-google-cloud-platform-integrations/",
      "published_at": "2021-05-05T15:54:51Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations monitor the performance of popular products and services. New Relic's Google Cloud Platform (GCP) integrations let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures to connect your GCP service to New Relic. Tip To use Google Cloud Platform integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. View your GCP data Once you follow the configuration process, data from your Google Cloud Platform account will report directly to New Relic. To view your GCP data: Go to one.newrelic.com > Infrastructure > GCP. For any of the integrations listed: Select an integration name to view data in a pre-configured dashboard. OR Select the Explore data icon to view GCP data. You can view and reuse the Insights NRQL queries both in the pre-configured dashboards and in the Events explorer dashboards. This allows you to tailor queries to your specific needs. Inventory, events, and dashboards for all services are available in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.103,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "sections": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> monitor the performance of popular products and services. New Relic&#x27;s <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) <em>integrations</em> let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures"
      },
      "id": "603e86d3e7b9d20feb2a07ed"
    },
    {
      "sections": [
        "Connect Google Cloud Platform services to New Relic",
        "Tip",
        "Requirements",
        "Authorization options",
        "Service account (recommended)",
        "User account",
        "Connect GCP to New Relic infrastructure monitoring",
        "Explore app data in New Relic",
        "Link multiple Google projects",
        "Unlink your GCP integrations"
      ],
      "title": "Connect Google Cloud Platform services to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "05934d2b03ec1ac5fa43298b21a06dc2e0f8c3b9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/connect-google-cloud-platform-services-new-relic/",
      "published_at": "2021-05-05T15:54:06Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To start receiving Google Cloud Platform (GCP) data with New Relic GCP integrations, connect your Google project to New Relic infrastructure monitoring. Tip To use Google Cloud Platform integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements These are the requirements for the authorization: GCP integration requirements Comments Monitoring In the GCP project API & Services Library settings, you must enable Google Stackdriver Monitoring API. Authorization For service account authorization (recommended): A user with Project IAM Admin role is needed to add the service account ID as a member in your GCP project. In the GCP project IAM & admin, the service account must have the Project Viewer role and the Service Usage Consumer role or, alternatively, a custom role. For user account authorization: The New Relic user that will integrate the GCP project must have a Google account and must be able to view the GCP project that New Relic will monitor. In the GCP project IAM & admin, the user must have the Project Viewer role. Please note that this authorization method will not allow New Relic to collect labels and other inventory attributes that can be useful for narrowing down your NRQL queries, dashboards and alerts. You can migrate the authorization method from user account to service account from the Manage services link in New Relic's user interface. Project name As part of the online setup process, you must identify Project name of the projects you want to monitor with New Relic. The UI workflow automatically lists active projects you can select. Permissions (only for user account authorization) New Relic requires a specific set of read-only permissions exclusively; this means that, for certain integrations, only partial inventory data will be available. Keep in mind that New Relic doesn't inherit your Google account's permissions and therefore is not authorized to perform any changes in the project. For more information about the API permissions that New Relic uses, see the Google documentation about scopes. Authorization options Integrating your GCP project with New Relic requires you to authorize New Relic to fetch monitoring data from your GCP project. You can choose between two authorization methods: Service accounts or User accounts. Service account (recommended) The service account authorization is recommended. If you authorize New Relic to fetch data through a service account, we will call your GCP project APIs using a service account ID and its associated public/private key pair. New Relic manages a specific Google service account for your New Relic account; you do not need to create it or manage the associated private key. Just add the service account ID as a member with viewing permissions in your project. This authorization method is recommended, especially if your GCP project is managed by a team. It also guarantees that New Relic will collect labels and inventory attributes whenever possible. User account If you authorize New Relic to fetch data through a user account, New Relic will access your GCP project monitoring data on behalf of a particular Google user. The authorization process is achieved through an OAuth workflow, which redirects you from the New Relic UI to a Google authorization interface. However, since the authorization is linked to a particular Google user, this method is not recommended for GCP projects that are managed by large teams. Connect GCP to New Relic infrastructure monitoring To connect your Google account to New Relic with user account authorization: Go to one.newrelic.com > Infrastructure > GCP. At the top of Infrastructure's Google Cloud Services integrations page, select Add a GCP account. Choose Authorization Method: Select either Authorize a Service Account or Authorize a User Account, and follow the instructions in the UI to authorize New Relic. Add projects: Select the projects that you want New Relic to receive data from. Select services: From the list of available services for your GCP account, select the individual services you want New Relic to receive data from, or select all of the services. Tip These services will be enabled for all of the projects that you selected in the previous step. Once the setup process is finished, you can fine-tune the services that you want monitored for each project individually. To complete the setup process, select Finish. If you see API authentication errors, follow the troubleshooting procedures. Explore app data in New Relic After you authorize New Relic to integrate one or more of your Google project's services, New Relic starts monitoring your GCP data at regular polling intervals. After a few minutes, data will appear in the New Relic UI. To find and use your data, including links to dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP. Link multiple Google projects For your convenience, the setup process allows you to select more than one project at a time. After the first setup, if you need to monitor additional GCP projects with New Relic, you can repeat the procedure to connect your GCP services as many times as you need. Unlink your GCP integrations You can disable any of your GCP integrations any time and still keep your Google project connected to New Relic. If you want to... Do this Disable a GCP service monitoring To disconnect individual GCP services but keep the integration with New Relic for other GCP services in your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, make changes to the checkbox options for available services and select Save changes. Unlink your project monitoring To uninstall all of your GCP services completely from New Relic Integrations, unlink your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, select Unlink account and select Save changes. Clean your GCP Projects after unlinking New Relic To clean your GCP project after unlinking, follow these steps if you were using a service account: Open the GCP IAM Console. Select the project you want to unlink from New Relic and click Open. Select the service account that is used by New Relic. Click the Remove icon. Or follow these steps if you were using a user account: Open your Google user account settings. Open the Apps with access to your account section. Choose New Relic application. Choose Remove Access.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.14717,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "sections": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "To <em>start</em> receiving <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) data with New Relic GCP <em>integrations</em>, connect your <em>Google</em> project to New Relic infrastructure monitoring. Tip To use <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em> and the rest of our observability <em>platform</em>, join the New Relic family! Sign up to create your free"
      },
      "id": "603e8309196a67fc4fa83da7"
    }
  ],
  "/docs/integrations/google-cloud-platform-integrations/troubleshooting/gcp-integration-api-authentication-errors": [
    {
      "sections": [
        "Integrations and custom roles",
        "Recommended role",
        "Optional role",
        "Important",
        "List of permissions",
        "Common permissions",
        "Service-specific permissions",
        "Permissions to link projects through the UI"
      ],
      "title": "Integrations and custom roles",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "d4f60e2d8413ddde9a342980d75a0e216af9baa4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/integrations-custom-roles/",
      "published_at": "2021-05-04T18:31:08Z",
      "updated_at": "2021-04-16T16:37:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To read the relevant data from your Google Cloud Platform (GCP) account, New Relic uses the Google Stackdriver API and also other specific services APIs. To access these APIs in your Google Cloud project, the New Relic authorized account needs to be granted a certain set of permissions; GCP uses roles to grant these permissions. Recommended role By default we highly recommend using the GCP primitive role Project Viewer, which grants \"permissions for read-only actions that do not affect your cloud infrastructure state, such as viewing (but not modifying) existing resources or data.\" This role is automatically managed by Google and updated when new Google Cloud services are released or modified. Optional role Alternatively, you can create your own custom role based on the list of permissions, which specifies the minimum set of permissions required to fetch data from each GCP integration. This will allow you to have more control over the permissions set for the New Relic authorized account. Important New Relic has no way of identifying problems related to custom permissions. If you choose to create a custom role, it is your responsibility to maintain it and ensure proper data is being collected. To customize your role you need to: Create a Google Cloud IAM Custom Role in each one of the GCP projects you want to monitor with New Relic. In each custom role, add the permissions that are specifically required for the cloud services you want to monitor according to the following list. Assign the custom role(s) to the New Relic authorized account. List of permissions Common permissions All integrations need the following permission: monitoring.timeSeries.list service.usage.use Service-specific permissions For some GCP integrations, New Relic will also need the following permissions, mainly to collect labels and inventory attributes. Integration Permissions Google AppEngine n/a; Google App Engine does not require additional permissions. Google BigQuery bigquery.datasets.get bigquery.tables.get bigquery.tables.list Google Cloud Functions cloudfunctions.locations.list Google Cloud Load Balancing n/a; Google Cloud Load Balancing does not require additional permissions. Google Cloud Pub/Sub pubsub.subscriptions.get pubsub.subscriptions.list pubsub.topics.get pubsub.topics.list Google Cloud Spanner spanner.instances.list spanner.databases.list spanner.databases.getDdl Google Cloud SQL cloudsql.instances.list Google Cloud Storage storage.buckets.list Google Compute Engine compute.instances.list compute.disks.get compute.disks.list Google Kubernetes Engine container.clusters.list Permissions to link projects through the UI To be able to see the list of projects that you can link to New Relic through the UI, your New Relic authorized service account needs the following permissions: resourcemanager.projects.get monitoring.monitoredResourceDescriptors.list If you do not want to grant New Relic authorized account the permissions that are needed for the linking process through the UI, you have the following options: Assign the Project Viewer or Monitoring Viewer role initially to the authorized account to link Google Cloud projects to New Relic through the UI. After the projects are linked, assign a Google Cloud custom role to the authorized account. Use New Relic NerdGraph to link Google Cloud projects to New Relic. This does not involve listing the viewable projects. However, you must know the id of the project you want to monitor. For more information, see the NerdGraph GraphiQL cloud integrations API tutorial.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 159.74132,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Integrations</em> and custom roles",
        "sections": "<em>Integrations</em> and custom roles",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "To read the relevant data from your <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) account, New Relic uses the <em>Google</em> Stackdriver API and also other specific services APIs. To access these APIs in your <em>Google</em> <em>Cloud</em> project, the New Relic authorized account needs to be granted a certain set of permissions; GCP uses"
      },
      "id": "603ebb3564441f34b64e8874"
    },
    {
      "sections": [
        "Connect Google Cloud Platform services to New Relic",
        "Tip",
        "Requirements",
        "Authorization options",
        "Service account (recommended)",
        "User account",
        "Connect GCP to New Relic infrastructure monitoring",
        "Explore app data in New Relic",
        "Link multiple Google projects",
        "Unlink your GCP integrations"
      ],
      "title": "Connect Google Cloud Platform services to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "05934d2b03ec1ac5fa43298b21a06dc2e0f8c3b9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/connect-google-cloud-platform-services-new-relic/",
      "published_at": "2021-05-05T15:54:06Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To start receiving Google Cloud Platform (GCP) data with New Relic GCP integrations, connect your Google project to New Relic infrastructure monitoring. Tip To use Google Cloud Platform integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements These are the requirements for the authorization: GCP integration requirements Comments Monitoring In the GCP project API & Services Library settings, you must enable Google Stackdriver Monitoring API. Authorization For service account authorization (recommended): A user with Project IAM Admin role is needed to add the service account ID as a member in your GCP project. In the GCP project IAM & admin, the service account must have the Project Viewer role and the Service Usage Consumer role or, alternatively, a custom role. For user account authorization: The New Relic user that will integrate the GCP project must have a Google account and must be able to view the GCP project that New Relic will monitor. In the GCP project IAM & admin, the user must have the Project Viewer role. Please note that this authorization method will not allow New Relic to collect labels and other inventory attributes that can be useful for narrowing down your NRQL queries, dashboards and alerts. You can migrate the authorization method from user account to service account from the Manage services link in New Relic's user interface. Project name As part of the online setup process, you must identify Project name of the projects you want to monitor with New Relic. The UI workflow automatically lists active projects you can select. Permissions (only for user account authorization) New Relic requires a specific set of read-only permissions exclusively; this means that, for certain integrations, only partial inventory data will be available. Keep in mind that New Relic doesn't inherit your Google account's permissions and therefore is not authorized to perform any changes in the project. For more information about the API permissions that New Relic uses, see the Google documentation about scopes. Authorization options Integrating your GCP project with New Relic requires you to authorize New Relic to fetch monitoring data from your GCP project. You can choose between two authorization methods: Service accounts or User accounts. Service account (recommended) The service account authorization is recommended. If you authorize New Relic to fetch data through a service account, we will call your GCP project APIs using a service account ID and its associated public/private key pair. New Relic manages a specific Google service account for your New Relic account; you do not need to create it or manage the associated private key. Just add the service account ID as a member with viewing permissions in your project. This authorization method is recommended, especially if your GCP project is managed by a team. It also guarantees that New Relic will collect labels and inventory attributes whenever possible. User account If you authorize New Relic to fetch data through a user account, New Relic will access your GCP project monitoring data on behalf of a particular Google user. The authorization process is achieved through an OAuth workflow, which redirects you from the New Relic UI to a Google authorization interface. However, since the authorization is linked to a particular Google user, this method is not recommended for GCP projects that are managed by large teams. Connect GCP to New Relic infrastructure monitoring To connect your Google account to New Relic with user account authorization: Go to one.newrelic.com > Infrastructure > GCP. At the top of Infrastructure's Google Cloud Services integrations page, select Add a GCP account. Choose Authorization Method: Select either Authorize a Service Account or Authorize a User Account, and follow the instructions in the UI to authorize New Relic. Add projects: Select the projects that you want New Relic to receive data from. Select services: From the list of available services for your GCP account, select the individual services you want New Relic to receive data from, or select all of the services. Tip These services will be enabled for all of the projects that you selected in the previous step. Once the setup process is finished, you can fine-tune the services that you want monitored for each project individually. To complete the setup process, select Finish. If you see API authentication errors, follow the troubleshooting procedures. Explore app data in New Relic After you authorize New Relic to integrate one or more of your Google project's services, New Relic starts monitoring your GCP data at regular polling intervals. After a few minutes, data will appear in the New Relic UI. To find and use your data, including links to dashboards and alert settings, go to one.newrelic.com > Infrastructure > GCP. Link multiple Google projects For your convenience, the setup process allows you to select more than one project at a time. After the first setup, if you need to monitor additional GCP projects with New Relic, you can repeat the procedure to connect your GCP services as many times as you need. Unlink your GCP integrations You can disable any of your GCP integrations any time and still keep your Google project connected to New Relic. If you want to... Do this Disable a GCP service monitoring To disconnect individual GCP services but keep the integration with New Relic for other GCP services in your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, make changes to the checkbox options for available services and select Save changes. Unlink your project monitoring To uninstall all of your GCP services completely from New Relic Integrations, unlink your Google account: Go to one.newrelic.com > Infrastructure > GCP and select Manage services. From your GCP account page, select Unlink account and select Save changes. Clean your GCP Projects after unlinking New Relic To clean your GCP project after unlinking, follow these steps if you were using a service account: Open the GCP IAM Console. Select the project you want to unlink from New Relic and click Open. Select the service account that is used by New Relic. Click the Remove icon. Or follow these steps if you were using a user account: Open your Google user account settings. Open the Apps with access to your account section. Choose New Relic application. Choose Remove Access.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.19049,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "sections": "Connect <em>Google</em> <em>Cloud</em> <em>Platform</em> services to New Relic",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "To start receiving <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) data with New Relic GCP <em>integrations</em>, connect your <em>Google</em> project to New Relic infrastructure monitoring. Tip To use <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em> and the rest of our observability <em>platform</em>, join the New Relic family! Sign up to create your free"
      },
      "id": "603e8309196a67fc4fa83da7"
    },
    {
      "sections": [
        "Introduction to Google Cloud Platform integrations",
        "Connect GCP and New Relic",
        "Tip",
        "View your GCP data"
      ],
      "title": "Introduction to Google Cloud Platform integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Google Cloud Platform integrations",
        "Get started"
      ],
      "external_id": "508adec5bbbcaef86a079533911bbbec5e1824c4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/google-cloud-platform-integrations/get-started/introduction-google-cloud-platform-integrations/",
      "published_at": "2021-05-05T15:54:51Z",
      "updated_at": "2021-03-16T05:48:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations monitor the performance of popular products and services. New Relic's Google Cloud Platform (GCP) integrations let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures to connect your GCP service to New Relic. Tip To use Google Cloud Platform integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. View your GCP data Once you follow the configuration process, data from your Google Cloud Platform account will report directly to New Relic. To view your GCP data: Go to one.newrelic.com > Infrastructure > GCP. For any of the integrations listed: Select an integration name to view data in a pre-configured dashboard. OR Select the Explore data icon to view GCP data. You can view and reuse the Insights NRQL queries both in the pre-configured dashboards and in the Events explorer dashboards. This allows you to tailor queries to your specific needs. Inventory, events, and dashboards for all services are available in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 142.041,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "sections": "Introduction to <em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "tags": "<em>Google</em> <em>Cloud</em> <em>Platform</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> monitor the performance of popular products and services. New Relic&#x27;s <em>Google</em> <em>Cloud</em> <em>Platform</em> (GCP) <em>integrations</em> let you monitor your GCP data in several New Relic features. Connect GCP and New Relic In order to obtain GCP data, follow standard procedures"
      },
      "id": "603e86d3e7b9d20feb2a07ed"
    }
  ],
  "/docs/integrations/grafana-integrations/get-started/grafana-support-prometheus-promql": [
    {
      "sections": [
        "Integrations and managed policies",
        "Recommended policy",
        "Important",
        "Optional policy",
        "Option 1: Use our CloudFormation template",
        "CloudFormation template",
        "Option 2: Manually add permissions",
        "Required by all integrations",
        "ALB permissions",
        "API Gateway permissions",
        "Auto Scaling permissions",
        "Billing permissions",
        "Cloudfront permissions",
        "CloudTrail permissions",
        "DynamoDB permissions",
        "EBS permissions",
        "EC2 permissions",
        "ECS/ECR permissions",
        "EFS permissions",
        "ElastiCache permissions",
        "ElasticSearch permissions",
        "Elastic Beanstalk permissions",
        "ELB permissions",
        "EMR permissions",
        "Health permissions",
        "IAM permissions",
        "IoT permissions",
        "Kinesis Firehose permissions",
        "Kinesis Streams permissions",
        "Lambda permissions",
        "RDS, RDS Enhanced Monitoring permissions",
        "Redshift permissions",
        "Route 53 permissions",
        "S3 permissions",
        "Simple Email Service (SES) permissions",
        "SNS permissions",
        "SQS permissions",
        "Trusted Advisor permissions",
        "VPC permissions",
        "X-Ray monitoring permissions"
      ],
      "title": "Integrations and managed policies",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Get started"
      ],
      "external_id": "80e215e7b2ba382de1b7ea758ee1b1f0a1e3c7df",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/get-started/integrations-managed-policies/",
      "published_at": "2021-05-04T18:30:29Z",
      "updated_at": "2021-05-04T18:30:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In order to use infrastructure integrations, you need to grant New Relic permission to read the relevant data from your account. Amazon Web Services (AWS) uses managed policies to grant these permissions. Recommended policy Important Recommendation: Grant an account-wide ReadOnlyAccess managed policy from AWS. AWS automatically updates this policy when new services are added or existing services are modified. New Relic infrastructure integrations have been designed to function with ReadOnlyAccess policies. For instructions, see Connect AWS integrations to infrastructure. Exception: The Trusted Advisor integration is not covered by the ReadOnlyAccess policy. It requires the additional AWSSupportAccess managed policy. This is also the only integration that requires full access permissions (support:*) in order to correctly operate. We notified Amazon about this limitation. Once it's resolved we'll update documentation with more specific permissions required for this integration. Optional policy If you cannot use the ReadOnlyAccess managed policy from AWS, you can create your own customized policy based on the list of permissions. This allows you to specify the optimal permissions required to fetch data from AWS for each integration. While this option is available, it is not recommended because it must be manually updated when you add or modify your integrations. Important New Relic has no way of identifying problems related to custom permissions. If you choose to create a custom policy, it is your responsibility to maintain it and ensure proper data is being collected. There are two ways to set up your customized policy: You can either use our CloudFormation template, or create own yourself by adding the permissions you need. Option 1: Use our CloudFormation template Our CloudFormation template contains all the permissions for all our AWS integrations. A user different than root can be used in the managed policy. CloudFormation template AWSTemplateFormatVersion: 2010-09-09 Outputs: NewRelicRoleArn: Description: NewRelicRole to monitor AWS Lambda Value: !GetAtt - NewRelicIntegrationsTemplate - Arn Parameters: NewRelicAccountNumber: Type: String Description: The Newrelic account number to send data AllowedPattern: '[0-9]+' Resources: NewRelicIntegrationsTemplate: Type: 'AWS::IAM::Role' Properties: RoleName: !Sub NewRelicTemplateTest AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: AWS: !Sub 'arn:aws:iam::754728514883:root' Action: 'sts:AssumeRole' Condition: StringEquals: 'sts:ExternalId': !Ref NewRelicAccountNumber Policies: - PolicyName: NewRelicIntegrations PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'elasticloadbalancing:DescribeLoadBalancers' - 'elasticloadbalancing:DescribeTargetGroups' - 'elasticloadbalancing:DescribeTags' - 'elasticloadbalancing:DescribeLoadBalancerAttributes' - 'elasticloadbalancing:DescribeListeners' - 'elasticloadbalancing:DescribeRules' - 'elasticloadbalancing:DescribeTargetGroupAttributes' - 'elasticloadbalancing:DescribeInstanceHealth' - 'elasticloadbalancing:DescribeLoadBalancerPolicies' - 'elasticloadbalancing:DescribeLoadBalancerPolicyTypes' - 'apigateway:GET' - 'apigateway:HEAD' - 'apigateway:OPTIONS' - 'autoscaling:DescribeLaunchConfigurations' - 'autoscaling:DescribeAutoScalingGroups' - 'autoscaling:DescribePolicies' - 'autoscaling:DescribeTags' - 'autoscaling:DescribeAccountLimits' - 'budgets:ViewBilling' - 'budgets:ViewBudget' - 'cloudfront:ListDistributions' - 'cloudfront:ListStreamingDistributions' - 'cloudfront:ListTagsForResource' - 'cloudtrail:LookupEvents' - 'config:BatchGetResourceConfig' - 'config:ListDiscoveredResources' - 'dynamodb:DescribeLimits' - 'dynamodb:ListTables' - 'dynamodb:DescribeTable' - 'dynamodb:ListGlobalTables' - 'dynamodb:DescribeGlobalTable' - 'dynamodb:ListTagsOfResource' - 'ec2:DescribeVolumeStatus' - 'ec2:DescribeVolumes' - 'ec2:DescribeVolumeAttribute' - 'ec2:DescribeInstanceStatus' - 'ec2:DescribeInstances' - 'ec2:DescribeVpnConnections' - 'ecs:ListServices' - 'ecs:DescribeServices' - 'ecs:DescribeClusters' - 'ecs:ListClusters' - 'ecs:ListTagsForResource' - 'ecs:ListContainerInstances' - 'ecs:DescribeContainerInstances' - 'elasticfilesystem:DescribeMountTargets' - 'elasticfilesystem:DescribeFileSystems' - 'elasticache:DescribeCacheClusters' - 'elasticache:ListTagsForResource' - 'es:ListDomainNames' - 'es:DescribeElasticsearchDomain' - 'es:DescribeElasticsearchDomains' - 'es:ListTags' - 'elasticbeanstalk:DescribeEnvironments' - 'elasticbeanstalk:DescribeInstancesHealth' - 'elasticbeanstalk:DescribeConfigurationSettings' - 'elasticloadbalancing:DescribeLoadBalancers' - 'elasticmapreduce:ListInstances' - 'elasticmapreduce:ListClusters' - 'elasticmapreduce:DescribeCluster' - 'elasticmapreduce:ListInstanceGroups' - 'health:DescribeAffectedEntities' - 'health:DescribeEventDetails' - 'health:DescribeEvents' - 'iam:ListSAMLProviders' - 'iam:ListOpenIDConnectProviders' - 'iam:ListServerCertificates' - 'iam:GetAccountAuthorizationDetails' - 'iam:ListVirtualMFADevices' - 'iam:GetAccountSummary' - 'iot:ListTopicRules' - 'iot:GetTopicRule' - 'iot:ListThings' - 'firehose:DescribeDeliveryStream' - 'firehose:ListDeliveryStreams' - 'kinesis:ListStreams' - 'kinesis:DescribeStream' - 'kinesis:ListTagsForStream' - 'rds:ListTagsForResource' - 'rds:DescribeDBInstances' - 'rds:DescribeDBClusters' - 'redshift:DescribeClusters' - 'redshift:DescribeClusterParameters' - 'route53:ListHealthChecks' - 'route53:GetHostedZone' - 'route53:ListHostedZones' - 'route53:ListResourceRecordSets' - 'route53:ListTagsForResources' - 's3:GetLifecycleConfiguration' - 's3:GetBucketTagging' - 's3:ListAllMyBuckets' - 's3:GetBucketWebsite' - 's3:GetBucketLogging' - 's3:GetBucketCORS' - 's3:GetBucketVersioning' - 's3:GetBucketAcl' - 's3:GetBucketNotification' - 's3:GetBucketPolicy' - 's3:GetReplicationConfiguration' - 's3:GetMetricsConfiguration' - 's3:GetAccelerateConfiguration' - 's3:GetAnalyticsConfiguration' - 's3:GetBucketLocation' - 's3:GetBucketRequestPayment' - 's3:GetEncryptionConfiguration' - 's3:GetInventoryConfiguration' - 's3:GetIpConfiguration' - 'ses:ListConfigurationSets' - 'ses:GetSendQuota' - 'ses:DescribeConfigurationSet' - 'ses:ListReceiptFilters' - 'ses:ListReceiptRuleSets' - 'ses:DescribeReceiptRule' - 'ses:DescribeReceiptRuleSet' - 'sns:GetTopicAttributes' - 'sns:ListTopics' - 'sqs:ListQueues' - 'sqs:ListQueueTags' - 'sqs:GetQueueAttributes' - 'tag:GetResources' - 'ec2:DescribeInternetGateways' - 'ec2:DescribeVpcs' - 'ec2:DescribeNatGateways' - 'ec2:DescribeVpcEndpoints' - 'ec2:DescribeSubnets' - 'ec2:DescribeNetworkAcls' - 'ec2:DescribeVpcAttribute' - 'ec2:DescribeRouteTables' - 'ec2:DescribeSecurityGroups' - 'ec2:DescribeVpcPeeringConnections' - 'ec2:DescribeNetworkInterfaces' - 'lambda:GetAccountSettings' - 'lambda:ListFunctions' - 'lambda:ListAliases' - 'lambda:ListTags' - 'lambda:ListEventSourceMappings' - 'cloudwatch:GetMetricStatistics' - 'cloudwatch:ListMetrics' - 'cloudwatch:GetMetricData' - 'support:*' Resource: '*' Copy Option 2: Manually add permissions To create your own policy using available permissions: Add the permissions for all integrations. Add permissions that are specific to the integrations you need The following permissions are used by New Relic to retrieve data for specific AWS integrations: Required by all integrations Important If an integration is not listed on this page, these permissions are all you need. All integrations Permissions CloudWatch cloudwatch:GetMetricStatistics cloudwatch:ListMetrics cloudwatch:GetMetricData Config API config:BatchGetResourceConfig config:ListDiscoveredResources Resource Tagging API tag:GetResources ALB permissions Additional ALB permissions: elasticloadbalancing:DescribeLoadBalancers elasticloadbalancing:DescribeTargetGroups elasticloadbalancing:DescribeTags elasticloadbalancing:DescribeLoadBalancerAttributes elasticloadbalancing:DescribeListeners elasticloadbalancing:DescribeRules elasticloadbalancing:DescribeTargetGroupAttributes elasticloadbalancing:DescribeInstanceHealth elasticloadbalancing:DescribeLoadBalancerPolicies elasticloadbalancing:DescribeLoadBalancerPolicyTypes API Gateway permissions Additional API Gateway permissions: apigateway:GET apigateway:HEAD apigateway:OPTIONS Auto Scaling permissions Additional Auto Scaling permissions: autoscaling:DescribeLaunchConfigurations autoscaling:DescribeAutoScalingGroups autoscaling:DescribePolicies autoscaling:DescribeTags autoscaling:DescribeAccountLimits Billing permissions Additional Billing permissions: budgets:ViewBilling budgets:ViewBudget Cloudfront permissions Additional Cloudfront permissions: cloudfront:ListDistributions cloudfront:ListStreamingDistributions cloudfront:ListTagsForResource CloudTrail permissions Additional CloudTrail permissions: cloudtrail:LookupEvents DynamoDB permissions Additional DynamoDB permissions: dynamodb:DescribeLimits dynamodb:ListTables dynamodb:DescribeTable dynamodb:ListGlobalTables dynamodb:DescribeGlobalTable dynamodb:ListTagsOfResource EBS permissions Additional EBS permissions: ec2:DescribeVolumeStatus ec2:DescribeVolumes ec2:DescribeVolumeAttribute EC2 permissions Additional EC2 permissions: ec2:DescribeInstanceStatus ec2:DescribeInstances ECS/ECR permissions Additional ECS/ECR permissions: ecs:ListServices ecs:DescribeServices ecs:DescribeClusters ecs:ListClusters ecs:ListTagsForResource ecs:ListContainerInstances ecs:DescribeContainerInstances EFS permissions Additional EFS permissions: elasticfilesystem:DescribeMountTargets elasticfilesystem:DescribeFileSystems ElastiCache permissions Additional ElastiCache permissions: elasticache:DescribeCacheClusters elasticache:ListTagsForResource ElasticSearch permissions Additional ElasticSearch permissions: es:ListDomainNames es:DescribeElasticsearchDomain es:DescribeElasticsearchDomains es:ListTags Elastic Beanstalk permissions Additional Elastic Beanstalk permissions: elasticbeanstalk:DescribeEnvironments elasticbeanstalk:DescribeInstancesHealth elasticbeanstalk:DescribeConfigurationSettings ELB permissions Additional ELB permissions: elasticloadbalancing:DescribeLoadBalancers EMR permissions Additional EMR permissions: elasticmapreduce:ListInstances elasticmapreduce:ListClusters elasticmapreduce:DescribeCluster elasticmapreduce:ListInstanceGroups elasticmapreduce:ListInstanceFleets Health permissions Additional Health permissions: health:DescribeAffectedEntities health:DescribeEventDetails health:DescribeEvents IAM permissions Additional IAM permissions: iam:ListSAMLProviders iam:ListOpenIDConnectProviders iam:ListServerCertificates iam:GetAccountAuthorizationDetails iam:ListVirtualMFADevices iam:GetAccountSummary IoT permissions Additional IoT permissions: iot:ListTopicRules iot:GetTopicRule iot:ListThings Kinesis Firehose permissions Additional Kinesis Firehose permissions: firehose:DescribeDeliveryStream firehose:ListDeliveryStreams Kinesis Streams permissions Additional Kinesis Streams permissions: kinesis:ListStreams kinesis:DescribeStream kinesis:ListTagsForStream Lambda permissions Additional Lambda permissions: lambda:GetAccountSettings lambda:ListFunctions lambda:ListAliases lambda:ListTags lambda:ListEventSourceMappings RDS, RDS Enhanced Monitoring permissions Additional RDS and RDS Enhanced Monitoring permissions: rds:ListTagsForResource rds:DescribeDBInstances rds:DescribeDBClusters Redshift permissions Additional Redshift permissions: redshift:DescribeClusters redshift:DescribeClusterParameters Route 53 permissions Additional Route 53 permissions: route53:ListHealthChecks route53:GetHostedZone route53:ListHostedZones route53:ListResourceRecordSets route53:ListTagsForResources S3 permissions Additional S3 permissions: s3:GetLifecycleConfiguration s3:GetBucketTagging s3:ListAllMyBuckets s3:GetBucketWebsite s3:GetBucketLogging s3:GetBucketCORS s3:GetBucketVersioning s3:GetBucketAcl s3:GetBucketNotification s3:GetBucketPolicy s3:GetReplicationConfiguration s3:GetMetricsConfiguration s3:GetAccelerateConfiguration s3:GetAnalyticsConfiguration s3:GetBucketLocation s3:GetBucketRequestPayment s3:GetEncryptionConfiguration s3:GetInventoryConfiguration s3:GetIpConfiguration Simple Email Service (SES) permissions Additional SES permissions: ses:ListConfigurationSets ses:GetSendQuota ses:DescribeConfigurationSet ses:ListReceiptFilters ses:ListReceiptRuleSets ses:DescribeReceiptRule ses:DescribeReceiptRuleSet SNS permissions Additional SNS permissions: sns:GetTopicAttributes sns:ListTopics SQS permissions Additional SQS permissions: sqs:ListQueues sqs:GetQueueAttributes sqs:ListQueueTags Trusted Advisor permissions Additional Trusted Advisor permissions: support:* See also the note about the Trusted Advisor integration and recommended policies. VPC permissions Additional VPC permissions: ec2:DescribeInternetGateways ec2:DescribeVpcs ec2:DescribeNatGateways ec2:DescribeVpcEndpoints ec2:DescribeSubnets ec2:DescribeNetworkAcls ec2:DescribeVpcAttribute ec2:DescribeRouteTables ec2:DescribeSecurityGroups ec2:DescribeVpcPeeringConnections ec2:DescribeNetworkInterfaces ec2:DescribeVpnConnections X-Ray monitoring permissions Additional X-ray monitoring permissions: xray:BatchGet* xray:Get*",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 126.408775,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Integrations</em> and managed policies",
        "sections": "<em>Integrations</em> and managed policies",
        "tags": "<em>Get</em> <em>started</em>",
        "body": " CloudFormation template contains all the permissions for all our AWS <em>integrations</em>. A user different than root can be used in the managed policy. CloudFormation template AWSTemplateFormatVersion: 2010-09-09 Outputs: NewRelicRoleArn: Description: NewRelicRole to monitor AWS Lambda Value: !<em>Get</em>"
      },
      "id": "6045079fe7b9d27db95799d9"
    },
    {
      "sections": [
        "Introduction to New Relic NerdGraph, our GraphQL API",
        "Tip",
        "What is NerdGraph?",
        "Important",
        "Use the GraphiQL explorer",
        "Requirements and endpoints",
        "What can you do with NerdGraph?",
        "NerdGraph terminology",
        "Tips on using the GraphiQL explorer",
        "Query accounts a New Relic user can access",
        "Query user, account, and NRQL in one request"
      ],
      "title": "Introduction to New Relic NerdGraph, our GraphQL API",
      "type": "docs",
      "tags": [
        "APIs",
        "NerdGraph",
        "Get started"
      ],
      "external_id": "e8e96c16cd75f494ebfacb3bc53b4ee9ccf1c727",
      "image": "",
      "url": "https://docs.newrelic.com/docs/apis/nerdgraph/get-started/introduction-new-relic-nerdgraph/",
      "published_at": "2021-05-04T16:42:33Z",
      "updated_at": "2021-05-04T16:42:33Z",
      "document_type": "page",
      "popularity": 1,
      "body": "NerdGraph is our GraphQL-format API that lets you query New Relic data and configure some New Relic features. Tip To use our APIs, or the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is NerdGraph? New Relic has several APIs. NerdGraph is our preferred API for querying New Relic data, and for performing some specific configurations (learn more about features). NerdGraph provides a single API interface for returning data from New Relics various APIs and microservices. Over time, other configuration capabilities will be added to NerdGraph. Important NerdGraph isnt used for data ingest. For that, you'd use our data ingest APIs. NerdGraph is built using GraphQL, which is an open source API format that allows you to request exactly the data needed, with no over-fetching or under-fetching. For a lesson in how to use NerdGraph, watch this 7-minute video: Want to watch more video tutorials? Go to the New Relic Universitys Intro to NerdGraph. Or see the online course on New Relic APIs. Use the GraphiQL explorer To get started using GraphQL, we recommend playing around with our GraphiQL explorer (GraphiQL is an open source graphical interface for using GraphQL). You can use it to explore our data schema, to read built-in object definitions, and to build and execute queries. To use GraphQL, youll need a user-specific New Relic API key called a user key. You can generate one or find an existing one from the GraphiQL explorers API key dropdown. To find the GraphiQL explorer: If your New Relic account uses an EU data center, go to api.eu.newrelic.com/graphiql. Otherwise use api.newrelic.com/graphiql. For tips on how to build queries, see Build queries. Requirements and endpoints To use NerdGraph, you need a New Relic user key, which can be generated and accessed from the GraphiQL explorer. The endpoints are: Main endpoint: https://api.newrelic.com/graphql Endpoint for accounts using EU data center: https://api.eu.newrelic.com/graphql To access the endpoint, use the following cURL command: curl -X POST https://api.newrelic.com/graphql \\ -H 'Content-Type: application/json' \\ -H 'API-Key: YOUR_NEW_RELIC_USER_KEY' \\ -d '{ \"query\": \"{ requestContext { userId apiKey } }\" } ' Copy What can you do with NerdGraph? NerdGraph functionality can be broken down into two main categories: Querying New Relic data. You can fetch data for a variety of purposes, including using it in a programmatic workflow, or building a New Relic One app for custom data visualizations. Configuring New Relic features. There are a variety of configurations available and more will be added over time. You can do things like add tags, configure workloads, or customize \"golden metrics.\" You can use NerdGraph to return a wide range of New Relic data but weve created some tutorials for common use cases: Topic Tutorials Your monitored entities Get data about entities Understand entity relationships and dependencies (used to build service maps) Query and configure \"golden metrics\" (important entity metrics) Querying data Query using NRQL (our query language) Tags Add and manage tags Dashboards Create dashboards Export dashboards to other accounts Export dashboards as files Alerts See all alert-related tutorials Applied Intelligence View and configure topology Workloads View and configure workloads Manage keys Create and manage keys (license keys used for data ingest, and user keys) Manage data Convert event data to metric data Drop data Distributed tracing Query distributed tracing data Configure Infinite Tracing New Relic One apps Build a New Relic One app Cloud integrations (AWS, Azure, GCP) Configure cloud integrations Partners and resellers Manage subscriptions (only for partners using original pricing plan) NerdGraph terminology The following are terms that originate with GraphQL (the API format NerdGraph uses). Term Definition Queries and mutations There are two classes of GraphQL operations: Queries are basic requests used only to fetch data. These queries are not static, meaning that you can ask for more data or less data, depending on your needs. For each query, you can specify exactly what data you want to retrieve, as long as it is supported by the schema. Mutations are requests that perform an action, such as creating a resource or changing configuration. Mutations require the keyword mutation, as well as the name of the mutation. Type Data in GraphQL is organized into types. Types can be scalars (like strings, numbers, or booleans) or object types. An object type is a custom type made up of a collection of fields. For example, an object type called User may represent a user in a system. Field A field represents a piece of information on an object type that can be queried. Fields can be scalars, lists, or objects. For example, a User object type could have a string field called name. Interface An interface is an abstract type that represents a collection of common fields that other object types can implement. Tips on using the GraphiQL explorer You can make queries with the NerdGraph GraphiQL explorer. The explorer provides built-in schema definitions and features, including auto-complete and query validation. Query accounts a New Relic user can access You can query for the name of an account that an actor (a New Relic authorized user) has access to: query { actor { account(id: YOUR_ACCOUNT_ID) { name } } } Copy The response will mirror the query structure you defined in the request, making it easy to ask for the specific data that you want. { \"data\": { \"actor\": { \"account\": { \"name\": \"Data Nerd\" } } } } Copy Query user, account, and NRQL in one request The graph structure shows its capabilities when queries become more complex. For example, you can query for user information, account information, and make a NRQL query with one request. With REST API, this would take three different requests to three different endpoints. query { actor { account(id: YOUR_ACCOUNT_ID) { name nrql(query: \"SELECT * FROM Transaction\") { results } } user { name id } } } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 113.52831,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Get</em> <em>started</em>",
        "body": ", with no over-fetching or under-fetching. For a lesson in how to use NerdGraph, watch this 7-minute video: Want to watch more video tutorials? Go to the New Relic Universitys Intro to NerdGraph. Or see the online course on New Relic APIs. Use the GraphiQL explorer To <em>get</em> <em>started</em> using GraphQL, we"
      },
      "id": "6043ff97196a67d0a0960f55"
    },
    {
      "sections": [
        "Configure New Relic as a Prometheus data source for Grafana",
        "Add a Prometheus data source",
        "Important",
        "Tip",
        "Sample configuration image",
        "Versioning considerations",
        "Customize Prometheus API behavior",
        "X-Query-Key (Query key)",
        "X-Prometheus-Only",
        "X-Prometheus-Server",
        "Delete a Prometheus data source"
      ],
      "title": "Configure New Relic as a Prometheus data source for Grafana",
      "type": "docs",
      "tags": [
        "Integrations",
        "Grafana integrations",
        "Set up and configure"
      ],
      "external_id": "956450eb909b41acd578807135b73dcbd1c09d40",
      "image": "https://docs.newrelic.com/static/c6e2c58b42bcca57c0dffaef5318927e/99f37/Grafana-Data-Source-Config_0.png",
      "url": "https://docs.newrelic.com/docs/integrations/grafana-integrations/set-configure/configure-new-relic-prometheus-data-source-grafana/",
      "published_at": "2021-05-05T18:14:07Z",
      "updated_at": "2021-04-22T13:30:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can configure a Prometheus data source in Grafana to query data stored in the New Relic Database (NRDB) using our PromQL-style query language. Add a Prometheus data source Follow these steps to add New Relic as a Prometheus data source for Grafana. These instructions detail how to complete the process when working with Grafana versions 6.7 and higher. Important You must complete the Prometheus remote-write integration process prior to beginning the configuration process. On New Relic, Create a new Insights query key. Important Note: In Grafana, you'll need put this in a custom X-Query-Key HTTP header (see step 7 below), but it is the same entity as the New Relic Query key. From the Grafana Home screen, go to Configuration > Data Sources and click Add data source. From the Add data source screen under Time series databases options, select Prometheus. Enter the Name you want to use for your new Prometheus data source. Set the Default toggle to the on or off position, depending on whether you want this to be your default data source for Prometheus queries. Off: this is not your default data source On: this is your default data source Enter the correct URL: US: https://prometheus-api.newrelic.com EU: https://prometheus-api.eu.newrelic.com Under Custom Headers, select Add Header. Set the Header name to X-Query-Key. For the Value, enter to the Query key you created in step 1. Click Save & Test. Tip If your graphs appear as groupings of dots and not as connected lines, you can change the graph style to display lines instead. To do this, go to Grafana's Graph panel and select Stacking and null value > connected. Sample configuration image Grafana Data Source Config.png, by dbarnesbrown.newrelic.com Versioning considerations New Relic strongly recommends using versions 6.7.x and higher to configure New Relic as a Prometheus data source. If you do chose to complete the configuration while running an earlier version, you will need to do one of the following to successfully configure your data source: Configure the new data source to use basic authentication and then enter the Query-key as the password in the basic authentication workflow. Configure the new data source URL to include the Query-key: https://prometheus-api.newrelic.com/auth/`<query-key>` Customize Prometheus API behavior Headers are particularly important if you have connected multiple Prometheus servers to New Relic using the remote write integration. Here are some details about customization. X-Query-Key (Query key) The Query key parameter is required to authenticate with New Relic and identify the account containing your metrics. Grafana calls this an X-Query key, while it appears as a Query key in the New Relic UI. Details: Required An API query key used for authentication If you are using the Prometheus remote write integration, the X-Query-Key should correspond to the same account as the X-License-Key used to integrate for remote write X-Prometheus-Only Important Grafana's auto-complete support doesn't handle metrics that fail to strictly conform to Prometheus naming conventions. New Relic recommends you exclude any metrics that do not conform from this parameter. Details: Optional Limits metrics exposed by the API to those originating from Prometheus Default = true if not specified X-Prometheus-Server This parameter is useful if you are collecting metrics from multiple Prometheus servers. For example, if you are using Grafana, you might want to create a data source for each Prometheus server connected to New Relic and then another data source that can be used to query across all Prometheus servers. Details: Optional Limits metrics exposed by the API to those collected from the specified Prometheus server This value should match the prometheus_server URL parameter in the remote write URL used to connect to your prometheus server to New Relic Defaults to return metrics collected from all servers Delete a Prometheus data source To delete a data source in Grafana: Go to Configuration > Data Sources. Click on the data source you want to delete. Click the Delete button at the bottom of the page.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 107.623505,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure New Relic as a Prometheus data source for <em>Grafana</em>",
        "sections": "Configure New Relic as a Prometheus data source for <em>Grafana</em>",
        "tags": "<em>Grafana</em> <em>integrations</em>",
        "body": "You can configure a Prometheus data source in <em>Grafana</em> to query data stored in the New Relic Database (NRDB) using our PromQL-style query language. Add a Prometheus data source Follow these steps to add New Relic as a Prometheus data source for <em>Grafana</em>. These instructions detail how to complete"
      },
      "id": "603e956f64441f19494e8894"
    }
  ],
  "/docs/integrations/grafana-integrations/set-configure/configure-new-relic-prometheus-data-source-grafana": [
    {
      "sections": [
        "Grafana support with Prometheus and PromQL",
        "Use existing Grafana dashboards with New Relic",
        "Compatibility and requirements",
        "Support for PromQL",
        "Get data flowing in Grafana",
        "Whats next?"
      ],
      "title": "Grafana support with Prometheus and PromQL",
      "type": "docs",
      "tags": [
        "Integrations",
        "Grafana integrations",
        "Get started"
      ],
      "external_id": "52addf26732e0146545ae8dee6540d3bd7cab2ff",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/grafana-integrations/get-started/grafana-support-prometheus-promql/",
      "published_at": "2021-05-05T11:00:11Z",
      "updated_at": "2021-04-22T13:30:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In Grafana, you can configure New Relic as a Prometheus data source. Not only that, within Grafana you can query metrics stored in New Relic using the PromQL query language. Use existing Grafana dashboards with New Relic When you integrate Prometheus metrics with New Relic via Remote Write or the OpenMetrics Integration (2.0+) and configure New Relic as a Prometheus data source in Grafana, you can use existing Grafana dashboards and seamlessly tap into the additional monitoring, reliability, and scale we provide. Compatibility and requirements Before you begin, make sure youve finished integrating Prometheus metrics and are running a recent enough version of Grafana. You should have either the Remote Write or the OpenMetrics Integration ( v2.0+) set up before you can configure New Relic Prometheus data sources in Grafana. You can only configure New Relic Prometheus data sources using this method in Grafana versions 6.7.0 or newer. You will need to configure custom headers in the UI, and this isnt possible with earlier versions. For details, see Configure New Relic as a Prometheus data source for Grafana. Support for PromQL Our Prometheus API emulates Prometheus' query APIs. We support the Prometheus query language (PromQL) through our PromQL-style query mode. We do our best to automatically translate PromQL syntax queries into the closest NRQL approximation. For more information on how this works and differences you may observe between Prometheus and New Relic, see Supported PromQL features. Get data flowing in Grafana To make your New Relic data available in Grafana, you can configure a new or existing Prometheus data source in just a couple of simple steps: In the Grafana UI, add and configure a new data source. Save the new data source and start viewing your data. Whats next? Ready to configure a Grafana data source? Read the how-to documentation for setting up the Prometheus remote write integration or the Prometheus OpenMetrics Integration. Read the how-to documentation for configuring Prometheus data sources in Grafana.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 147.69777,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Grafana</em> support with Prometheus <em>and</em> PromQL",
        "sections": "<em>Grafana</em> support with Prometheus <em>and</em> PromQL",
        "tags": "<em>Grafana</em> <em>integrations</em>",
        "body": " integrating Prometheus metrics and are running a recent enough version of <em>Grafana</em>. You should have either the Remote Write or the OpenMetrics Integration ( v2.0+) <em>set</em> <em>up</em> before you can <em>configure</em> New Relic Prometheus data sources in <em>Grafana</em>. You can only <em>configure</em> New Relic Prometheus data sources using"
      },
      "id": "603e94de64441f9a804e8843"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/release-notes/infrastructure-release-notes/infrastructure-agent-release-notes/new-relic-infrastructure-agent-1171/",
      "sections": [
        "Infrastructure agent v1.17.1",
        "Notes",
        "Fixed"
      ],
      "published_at": "2021-05-05T21:55:57Z",
      "title": "Infrastructure agent v1.17.1",
      "updated_at": "2021-05-05T21:55:56Z",
      "type": "docs",
      "external_id": "874f447615357eebc2c48b433df32c4128b9a442",
      "document_type": "release_notes",
      "popularity": 1,
      "body": "Notes A new version of the agent has been released. Follow standard procedures to update the Infrastructure agent. New Relic recommends that you upgrade the agent regularly and at a minimum every 3 months. Fixed Log forwarder (FluentBit) missing libssl library. Error logged: time=\"2021-05-03T13:35:29-07:00\" level=debug msg=\"Launching process.\" component=integrations.Supervisor process=log-forwarder time=\"2021-05-03T13:35:29-07:00\" level=debug msg=\"Running command.\" command=/var/db/newrelic-infra/newrelic-integrations/logging/fluent-bit component=integrations.Executor env=\"[LANG=en_US.UTF-8 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin DBUS_SYSTEM_BUS_ADDRESS=unix:path=/var/run/dbus/system_bus_socket]\" path=/var/db/newrelic-infra/newrelic-integrations/logging/fluent-bit time=\"2021-05-03T13:35:29-07:00\" level=debug msg=\"/var/db/newrelic-infra/newrelic-integrations/logging/fluent-bit: error while loading shared libraries: libssl.so.1.1: cannot open shared object file: No such file or directory\" component=integrations.Supervisor output=stderr process=log-forwarder sr/local/bin:/usr/sbin:/usr/bin DBUS_SYSTEM_BUS_ADDRESS=unix:path=/var/run/dbus/system_bus_socket]\" path=/var/db/newrelic-infra/newrelic-integrations/logging/fluent-bit rectory\" component=integrations.Supervisor output=stderr process=log-forwarderlic-integrations/logging/fluent-bit: error while loading shared libraries: libssl.so.1.1: cannot open shared object file: No such file or di time=\"2021-05-03T13:35:29-07:00\" level=error msg=\"Error occurred while handling the process\" component=integrations.Supervisor error=\"exit status 127\" process=log-forwarder Copy Log forwarder was downgraded to previous agent release versions: linux: output plugin 1.1.0, FB 1.3.0 windows: output plugin 1.4.6, FB 1.4.1 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 62.8098,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": ":29-07:00&quot; level=debug msg=&quot;Launching process.&quot; component=<em>integrations</em>.Supervisor process=log-forwarder time=&quot;2021-05-03T13:35:29-07:00&quot; level=debug msg=&quot;Running command.&quot; command=&#x2F;var&#x2F;db&#x2F;newrelic-infra&#x2F;newrelic-<em>integrations</em>&#x2F;logging&#x2F;fluent-bit component=<em>integrations</em>.Executor env=&quot;[LANG=en_US.UTF-8"
      },
      "id": "6093146d64441f80522f36c5"
    },
    {
      "sections": [
        "Integrations and managed policies",
        "Recommended policy",
        "Important",
        "Optional policy",
        "Option 1: Use our CloudFormation template",
        "CloudFormation template",
        "Option 2: Manually add permissions",
        "Required by all integrations",
        "ALB permissions",
        "API Gateway permissions",
        "Auto Scaling permissions",
        "Billing permissions",
        "Cloudfront permissions",
        "CloudTrail permissions",
        "DynamoDB permissions",
        "EBS permissions",
        "EC2 permissions",
        "ECS/ECR permissions",
        "EFS permissions",
        "ElastiCache permissions",
        "ElasticSearch permissions",
        "Elastic Beanstalk permissions",
        "ELB permissions",
        "EMR permissions",
        "Health permissions",
        "IAM permissions",
        "IoT permissions",
        "Kinesis Firehose permissions",
        "Kinesis Streams permissions",
        "Lambda permissions",
        "RDS, RDS Enhanced Monitoring permissions",
        "Redshift permissions",
        "Route 53 permissions",
        "S3 permissions",
        "Simple Email Service (SES) permissions",
        "SNS permissions",
        "SQS permissions",
        "Trusted Advisor permissions",
        "VPC permissions",
        "X-Ray monitoring permissions"
      ],
      "title": "Integrations and managed policies",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Get started"
      ],
      "external_id": "80e215e7b2ba382de1b7ea758ee1b1f0a1e3c7df",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/get-started/integrations-managed-policies/",
      "published_at": "2021-05-04T18:30:29Z",
      "updated_at": "2021-05-04T18:30:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In order to use infrastructure integrations, you need to grant New Relic permission to read the relevant data from your account. Amazon Web Services (AWS) uses managed policies to grant these permissions. Recommended policy Important Recommendation: Grant an account-wide ReadOnlyAccess managed policy from AWS. AWS automatically updates this policy when new services are added or existing services are modified. New Relic infrastructure integrations have been designed to function with ReadOnlyAccess policies. For instructions, see Connect AWS integrations to infrastructure. Exception: The Trusted Advisor integration is not covered by the ReadOnlyAccess policy. It requires the additional AWSSupportAccess managed policy. This is also the only integration that requires full access permissions (support:*) in order to correctly operate. We notified Amazon about this limitation. Once it's resolved we'll update documentation with more specific permissions required for this integration. Optional policy If you cannot use the ReadOnlyAccess managed policy from AWS, you can create your own customized policy based on the list of permissions. This allows you to specify the optimal permissions required to fetch data from AWS for each integration. While this option is available, it is not recommended because it must be manually updated when you add or modify your integrations. Important New Relic has no way of identifying problems related to custom permissions. If you choose to create a custom policy, it is your responsibility to maintain it and ensure proper data is being collected. There are two ways to set up your customized policy: You can either use our CloudFormation template, or create own yourself by adding the permissions you need. Option 1: Use our CloudFormation template Our CloudFormation template contains all the permissions for all our AWS integrations. A user different than root can be used in the managed policy. CloudFormation template AWSTemplateFormatVersion: 2010-09-09 Outputs: NewRelicRoleArn: Description: NewRelicRole to monitor AWS Lambda Value: !GetAtt - NewRelicIntegrationsTemplate - Arn Parameters: NewRelicAccountNumber: Type: String Description: The Newrelic account number to send data AllowedPattern: '[0-9]+' Resources: NewRelicIntegrationsTemplate: Type: 'AWS::IAM::Role' Properties: RoleName: !Sub NewRelicTemplateTest AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: AWS: !Sub 'arn:aws:iam::754728514883:root' Action: 'sts:AssumeRole' Condition: StringEquals: 'sts:ExternalId': !Ref NewRelicAccountNumber Policies: - PolicyName: NewRelicIntegrations PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'elasticloadbalancing:DescribeLoadBalancers' - 'elasticloadbalancing:DescribeTargetGroups' - 'elasticloadbalancing:DescribeTags' - 'elasticloadbalancing:DescribeLoadBalancerAttributes' - 'elasticloadbalancing:DescribeListeners' - 'elasticloadbalancing:DescribeRules' - 'elasticloadbalancing:DescribeTargetGroupAttributes' - 'elasticloadbalancing:DescribeInstanceHealth' - 'elasticloadbalancing:DescribeLoadBalancerPolicies' - 'elasticloadbalancing:DescribeLoadBalancerPolicyTypes' - 'apigateway:GET' - 'apigateway:HEAD' - 'apigateway:OPTIONS' - 'autoscaling:DescribeLaunchConfigurations' - 'autoscaling:DescribeAutoScalingGroups' - 'autoscaling:DescribePolicies' - 'autoscaling:DescribeTags' - 'autoscaling:DescribeAccountLimits' - 'budgets:ViewBilling' - 'budgets:ViewBudget' - 'cloudfront:ListDistributions' - 'cloudfront:ListStreamingDistributions' - 'cloudfront:ListTagsForResource' - 'cloudtrail:LookupEvents' - 'config:BatchGetResourceConfig' - 'config:ListDiscoveredResources' - 'dynamodb:DescribeLimits' - 'dynamodb:ListTables' - 'dynamodb:DescribeTable' - 'dynamodb:ListGlobalTables' - 'dynamodb:DescribeGlobalTable' - 'dynamodb:ListTagsOfResource' - 'ec2:DescribeVolumeStatus' - 'ec2:DescribeVolumes' - 'ec2:DescribeVolumeAttribute' - 'ec2:DescribeInstanceStatus' - 'ec2:DescribeInstances' - 'ec2:DescribeVpnConnections' - 'ecs:ListServices' - 'ecs:DescribeServices' - 'ecs:DescribeClusters' - 'ecs:ListClusters' - 'ecs:ListTagsForResource' - 'ecs:ListContainerInstances' - 'ecs:DescribeContainerInstances' - 'elasticfilesystem:DescribeMountTargets' - 'elasticfilesystem:DescribeFileSystems' - 'elasticache:DescribeCacheClusters' - 'elasticache:ListTagsForResource' - 'es:ListDomainNames' - 'es:DescribeElasticsearchDomain' - 'es:DescribeElasticsearchDomains' - 'es:ListTags' - 'elasticbeanstalk:DescribeEnvironments' - 'elasticbeanstalk:DescribeInstancesHealth' - 'elasticbeanstalk:DescribeConfigurationSettings' - 'elasticloadbalancing:DescribeLoadBalancers' - 'elasticmapreduce:ListInstances' - 'elasticmapreduce:ListClusters' - 'elasticmapreduce:DescribeCluster' - 'elasticmapreduce:ListInstanceGroups' - 'health:DescribeAffectedEntities' - 'health:DescribeEventDetails' - 'health:DescribeEvents' - 'iam:ListSAMLProviders' - 'iam:ListOpenIDConnectProviders' - 'iam:ListServerCertificates' - 'iam:GetAccountAuthorizationDetails' - 'iam:ListVirtualMFADevices' - 'iam:GetAccountSummary' - 'iot:ListTopicRules' - 'iot:GetTopicRule' - 'iot:ListThings' - 'firehose:DescribeDeliveryStream' - 'firehose:ListDeliveryStreams' - 'kinesis:ListStreams' - 'kinesis:DescribeStream' - 'kinesis:ListTagsForStream' - 'rds:ListTagsForResource' - 'rds:DescribeDBInstances' - 'rds:DescribeDBClusters' - 'redshift:DescribeClusters' - 'redshift:DescribeClusterParameters' - 'route53:ListHealthChecks' - 'route53:GetHostedZone' - 'route53:ListHostedZones' - 'route53:ListResourceRecordSets' - 'route53:ListTagsForResources' - 's3:GetLifecycleConfiguration' - 's3:GetBucketTagging' - 's3:ListAllMyBuckets' - 's3:GetBucketWebsite' - 's3:GetBucketLogging' - 's3:GetBucketCORS' - 's3:GetBucketVersioning' - 's3:GetBucketAcl' - 's3:GetBucketNotification' - 's3:GetBucketPolicy' - 's3:GetReplicationConfiguration' - 's3:GetMetricsConfiguration' - 's3:GetAccelerateConfiguration' - 's3:GetAnalyticsConfiguration' - 's3:GetBucketLocation' - 's3:GetBucketRequestPayment' - 's3:GetEncryptionConfiguration' - 's3:GetInventoryConfiguration' - 's3:GetIpConfiguration' - 'ses:ListConfigurationSets' - 'ses:GetSendQuota' - 'ses:DescribeConfigurationSet' - 'ses:ListReceiptFilters' - 'ses:ListReceiptRuleSets' - 'ses:DescribeReceiptRule' - 'ses:DescribeReceiptRuleSet' - 'sns:GetTopicAttributes' - 'sns:ListTopics' - 'sqs:ListQueues' - 'sqs:ListQueueTags' - 'sqs:GetQueueAttributes' - 'tag:GetResources' - 'ec2:DescribeInternetGateways' - 'ec2:DescribeVpcs' - 'ec2:DescribeNatGateways' - 'ec2:DescribeVpcEndpoints' - 'ec2:DescribeSubnets' - 'ec2:DescribeNetworkAcls' - 'ec2:DescribeVpcAttribute' - 'ec2:DescribeRouteTables' - 'ec2:DescribeSecurityGroups' - 'ec2:DescribeVpcPeeringConnections' - 'ec2:DescribeNetworkInterfaces' - 'lambda:GetAccountSettings' - 'lambda:ListFunctions' - 'lambda:ListAliases' - 'lambda:ListTags' - 'lambda:ListEventSourceMappings' - 'cloudwatch:GetMetricStatistics' - 'cloudwatch:ListMetrics' - 'cloudwatch:GetMetricData' - 'support:*' Resource: '*' Copy Option 2: Manually add permissions To create your own policy using available permissions: Add the permissions for all integrations. Add permissions that are specific to the integrations you need The following permissions are used by New Relic to retrieve data for specific AWS integrations: Required by all integrations Important If an integration is not listed on this page, these permissions are all you need. All integrations Permissions CloudWatch cloudwatch:GetMetricStatistics cloudwatch:ListMetrics cloudwatch:GetMetricData Config API config:BatchGetResourceConfig config:ListDiscoveredResources Resource Tagging API tag:GetResources ALB permissions Additional ALB permissions: elasticloadbalancing:DescribeLoadBalancers elasticloadbalancing:DescribeTargetGroups elasticloadbalancing:DescribeTags elasticloadbalancing:DescribeLoadBalancerAttributes elasticloadbalancing:DescribeListeners elasticloadbalancing:DescribeRules elasticloadbalancing:DescribeTargetGroupAttributes elasticloadbalancing:DescribeInstanceHealth elasticloadbalancing:DescribeLoadBalancerPolicies elasticloadbalancing:DescribeLoadBalancerPolicyTypes API Gateway permissions Additional API Gateway permissions: apigateway:GET apigateway:HEAD apigateway:OPTIONS Auto Scaling permissions Additional Auto Scaling permissions: autoscaling:DescribeLaunchConfigurations autoscaling:DescribeAutoScalingGroups autoscaling:DescribePolicies autoscaling:DescribeTags autoscaling:DescribeAccountLimits Billing permissions Additional Billing permissions: budgets:ViewBilling budgets:ViewBudget Cloudfront permissions Additional Cloudfront permissions: cloudfront:ListDistributions cloudfront:ListStreamingDistributions cloudfront:ListTagsForResource CloudTrail permissions Additional CloudTrail permissions: cloudtrail:LookupEvents DynamoDB permissions Additional DynamoDB permissions: dynamodb:DescribeLimits dynamodb:ListTables dynamodb:DescribeTable dynamodb:ListGlobalTables dynamodb:DescribeGlobalTable dynamodb:ListTagsOfResource EBS permissions Additional EBS permissions: ec2:DescribeVolumeStatus ec2:DescribeVolumes ec2:DescribeVolumeAttribute EC2 permissions Additional EC2 permissions: ec2:DescribeInstanceStatus ec2:DescribeInstances ECS/ECR permissions Additional ECS/ECR permissions: ecs:ListServices ecs:DescribeServices ecs:DescribeClusters ecs:ListClusters ecs:ListTagsForResource ecs:ListContainerInstances ecs:DescribeContainerInstances EFS permissions Additional EFS permissions: elasticfilesystem:DescribeMountTargets elasticfilesystem:DescribeFileSystems ElastiCache permissions Additional ElastiCache permissions: elasticache:DescribeCacheClusters elasticache:ListTagsForResource ElasticSearch permissions Additional ElasticSearch permissions: es:ListDomainNames es:DescribeElasticsearchDomain es:DescribeElasticsearchDomains es:ListTags Elastic Beanstalk permissions Additional Elastic Beanstalk permissions: elasticbeanstalk:DescribeEnvironments elasticbeanstalk:DescribeInstancesHealth elasticbeanstalk:DescribeConfigurationSettings ELB permissions Additional ELB permissions: elasticloadbalancing:DescribeLoadBalancers EMR permissions Additional EMR permissions: elasticmapreduce:ListInstances elasticmapreduce:ListClusters elasticmapreduce:DescribeCluster elasticmapreduce:ListInstanceGroups elasticmapreduce:ListInstanceFleets Health permissions Additional Health permissions: health:DescribeAffectedEntities health:DescribeEventDetails health:DescribeEvents IAM permissions Additional IAM permissions: iam:ListSAMLProviders iam:ListOpenIDConnectProviders iam:ListServerCertificates iam:GetAccountAuthorizationDetails iam:ListVirtualMFADevices iam:GetAccountSummary IoT permissions Additional IoT permissions: iot:ListTopicRules iot:GetTopicRule iot:ListThings Kinesis Firehose permissions Additional Kinesis Firehose permissions: firehose:DescribeDeliveryStream firehose:ListDeliveryStreams Kinesis Streams permissions Additional Kinesis Streams permissions: kinesis:ListStreams kinesis:DescribeStream kinesis:ListTagsForStream Lambda permissions Additional Lambda permissions: lambda:GetAccountSettings lambda:ListFunctions lambda:ListAliases lambda:ListTags lambda:ListEventSourceMappings RDS, RDS Enhanced Monitoring permissions Additional RDS and RDS Enhanced Monitoring permissions: rds:ListTagsForResource rds:DescribeDBInstances rds:DescribeDBClusters Redshift permissions Additional Redshift permissions: redshift:DescribeClusters redshift:DescribeClusterParameters Route 53 permissions Additional Route 53 permissions: route53:ListHealthChecks route53:GetHostedZone route53:ListHostedZones route53:ListResourceRecordSets route53:ListTagsForResources S3 permissions Additional S3 permissions: s3:GetLifecycleConfiguration s3:GetBucketTagging s3:ListAllMyBuckets s3:GetBucketWebsite s3:GetBucketLogging s3:GetBucketCORS s3:GetBucketVersioning s3:GetBucketAcl s3:GetBucketNotification s3:GetBucketPolicy s3:GetReplicationConfiguration s3:GetMetricsConfiguration s3:GetAccelerateConfiguration s3:GetAnalyticsConfiguration s3:GetBucketLocation s3:GetBucketRequestPayment s3:GetEncryptionConfiguration s3:GetInventoryConfiguration s3:GetIpConfiguration Simple Email Service (SES) permissions Additional SES permissions: ses:ListConfigurationSets ses:GetSendQuota ses:DescribeConfigurationSet ses:ListReceiptFilters ses:ListReceiptRuleSets ses:DescribeReceiptRule ses:DescribeReceiptRuleSet SNS permissions Additional SNS permissions: sns:GetTopicAttributes sns:ListTopics SQS permissions Additional SQS permissions: sqs:ListQueues sqs:GetQueueAttributes sqs:ListQueueTags Trusted Advisor permissions Additional Trusted Advisor permissions: support:* See also the note about the Trusted Advisor integration and recommended policies. VPC permissions Additional VPC permissions: ec2:DescribeInternetGateways ec2:DescribeVpcs ec2:DescribeNatGateways ec2:DescribeVpcEndpoints ec2:DescribeSubnets ec2:DescribeNetworkAcls ec2:DescribeVpcAttribute ec2:DescribeRouteTables ec2:DescribeSecurityGroups ec2:DescribeVpcPeeringConnections ec2:DescribeNetworkInterfaces ec2:DescribeVpnConnections X-Ray monitoring permissions Additional X-ray monitoring permissions: xray:BatchGet* xray:Get*",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 50.110878,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Integrations</em> <em>and</em> managed policies",
        "sections": "<em>Integrations</em> <em>and</em> managed policies",
        "tags": "<em>Integrations</em>",
        "body": " a custom policy, it is your responsibility to maintain it and ensure proper data is being collected. There are two ways to <em>set</em> <em>up</em> your customized policy: You can either use our CloudFormation template, or create own yourself by adding the permissions you need. Option 1: Use our CloudFormation template Our"
      },
      "id": "6045079fe7b9d27db95799d9"
    }
  ],
  "/docs/integrations/host-integrations/get-started/introduction-host-integrations": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.8688,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.84534,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and Http<em>Start</em>Stop envelopes to New Relic Logs, including"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.84525,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/apache-monitoring-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.31195,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.27167,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.27148,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/cassandra-monitoring-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.31195,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.27167,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.27148,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/collectd-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.31177,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.27148,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.2713,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/couchbase-monitoring-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.31177,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.27148,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.2713,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration": [
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.2713,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.27112,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    },
    {
      "sections": [
        "MySQL monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Configuration",
        "Activate remote monitoring",
        "Environment variable passthroughs",
        "HOSTNAME",
        "PORT",
        "USERNAME",
        "PASSWORD",
        "DATABASE",
        "EXTENDED_METRICS",
        "EXTENDED_INNODB_METRICS",
        "EXTENDED_MY_ISAM_METRICS",
        "Find and use data",
        "Metric data",
        "Default metrics",
        "Extended metrics",
        "Extended innodb metrics",
        "Extended myisam metrics",
        "Extended slave cluster metrics",
        "Inventory",
        "System metadata",
        "Source code"
      ],
      "title": "MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "50b118a06500c42ca8f26ce475d00f70c6fda148",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/mysql-monitoring-integration/",
      "published_at": "2021-05-04T15:54:52Z",
      "updated_at": "2021-05-02T03:13:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our MySQL integration collects and sends inventory and metrics from your MySQL database to our platform, where you can see the health of your database server and analyze metric data so that you can easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with MySQL version 5.6 or higher. Before installing the integration, make sure that you meet the following requirements: If MySQL is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running MySQL. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Important For MySQL v8.0 and higher we do not support the following metrics: cluster.slaveRunning, db.qCacheFreeMemoryBytes, db.qCacheHitRatio, db.qCacheNotCachedPerSecond. Quick start Instrument your MySQL database quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the MySQL integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-mysql. From the command line, create a user with replication privileges: sudo mysql -e \"CREATE USER 'newrelic'@'localhost' IDENTIFIED BY 'YOUR_SELECTED_PASSWORD';\" Copy sudo mysql -e \"GRANT REPLICATION CLIENT ON *.* TO 'newrelic'@'localhost' WITH MAX_USER_CONNECTIONS 5;\" Copy Change the directory to the integration's folder. cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp mysql-config.yml.sample mysql-config.yml Copy Edit the configuration file mysql-config.yml as explained in the next section. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, mysql-config.yml. The configuration provides a single command, status, that captures the metrics and all the config options. It accepts these arguments: hostname: the MySQL hostname. port: the port where the MySQL server is listening. username: the user connected to the MySQL server. If you used the CREATE USER command in the activation instructions, this should be set to newrelic. password: the password for the user specified above. extended_metrics: captures an extended set of metrics. Disabled by default. Set to 1 to enable. This also enables the capture of slave metrics. extended_innodb_metrics: captures additional innodb metrics. Disabled by default. Set to 1 to enable. extended_myisam_metrics: captures additional MyISAM metrics. Disabled by default. Set to 1 to enable. Optional: labels field. For example, the env label controls the environment inventory data. The default value is production. Optional: metrics field. Set to 1 to disable the collection of inventory. See a sample of a configuration file. Activate remote monitoring The remote_monitoring parameter enables remote monitoring and multi-tenancy for this integration. This parameter is enabled by default and should not be changed unless you require it in your custom environment. Activating remote_monitoring may change some attributes and/or affect your configured alerts. For more information, see remote monitoring in on-host integrations. Important Infrastructure agent version 1.2.25 or higher is required to use remote_monitoring. Environment variable passthroughs Environment variables can be used to control config settings, and are then passed through to the infrastructure agent. For instructions on how to use this feature, see Configure the infrastructure agent. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. HOSTNAME Specifies the hostname or IP where MySQL is running. Type String Default localhost Example: HOSTNAME='MySQL DB' Copy PORT Port on which MySQL server is listening. Type Integer Default 3306 Example: PORT=6379 Copy USERNAME The user connected to the MySQL server. Type String Default (none) Example: USERNAME='DBAdmin' Copy PASSWORD Password for the given user. Type String Default (none) Example: PASSWORD='Hh7$(uvRt' Copy DATABASE Name of the database to be monitored. Type String Default (none) Example: DATABASE='My MySQL DB' Copy EXTENDED_METRICS Captures an extended set of metrics. This also enables the capture of slave metrics. Type Boolean Default false Example: EXTENDED_METRICS=true Copy EXTENDED_INNODB_METRICS Captures additional innodb metrics. Type Boolean Default false Example: EXTENDED_INNODB_METRICS=true Copy EXTENDED_MY_ISAM_METRICS Captures additional MyISAM metrics. Type Boolean Default false Example: EXTENDED_MY_ISAM_METRICS=true Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the MysqlSample event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The MySQL integration collects the following metrics: Default metrics These metrics are captured by default: Name Description cluster.slaveRunning Boolean. 1 if this server is a replication slave that is connected to a replication master, and both the I/O and SQL threads are running; otherwise, it is 0. For metrics reported if enabled, see replication slave metrics. db.handlerRollbackPerSecond Rate of requests for a storage engine to perform a rollback operation, per second. db.innodb.bufferPoolPagesData Number of pages in the InnoDB buffer pool containing data. db.innodb.bufferPoolPagesFree Number of free pages in the InnoDB buffer pool. db.innodb.bufferPoolPagesTotal Total number of pages of the InnoDB buffer pool. db.innodb.dataReadBytesPerSecond Rate at which data is read from InnoDB tables in bytes per second. db.innodb.dataWrittenBytesPerSecond Rate at which data is written to InnoDB tables in bytes per second. db.innodb.logWaitsPerSecond Number of times that the log buffer was too small and a wait was required for it to be flushed before continuing, in waits per second. db.innodb.rowLockCurrentWaits Number of row locks currently being waited for by operations on InnoDB tables. db.innodb.rowLockTimeAvg Average time to acquire a row lock for InnoDB tables, in milliseconds. db.innodb.rowLockWaitsPerSecond Number of times operations on InnoDB tables had to wait for a row lock per second. db.openedTablesPerSecond Number of files that have been opened with my_open() (a mysys library function) per second. Parts of the server that open files without using this function do not increment the count. db.openFiles Number of files that are open. This count includes regular files opened by the server. It does not include other types of files such as sockets or pipes. db.openTables Number of tables that are open. db.qCacheFreeMemoryBytes Amount of free memory in bytes for the query cache. db.qCacheHitRatio Percentage of queries that are retrieved from the cache. db.qCacheNotCachedPerSecond Number of noncached queries (not cacheable, or not cached due to the query_cache_type setting) per second. db.qCacheUtilization Percentage of query cache memory that is being used. db.tablesLocksWaitedPerSecond Number of times per second that a request for a table lock could not be granted immediately and a wait was needed. net.abortedClientsPerSecond Number of connections per second that were aborted because the client died without closing the connection properly. net.abortedConnectsPerSecond Number of failed attempts to connect to the MySQL server, per second. net.bytesReceivedPerSecond Byte throughput received from all clients, per second. net.bytesSentPerSecond Byte throughput sent to all clients, per second. net.connectionErrorsMaxConnectionsPerSecond Rate per second at which connections were refused because the server max_connections limit was reached. net.connectionsPerSecond Number of connection attempts per second. net.maxUsedConnections Maximum number of connections that have been in use simultaneously since the server started. net.threadsConnected Number of currently open connections. net.threadsRunning Number of threads that are not sleeping. query.comCommitPerSecond Number of COMMIT statements executed per second. query.comDeletePerSecond Number of DELETE statements executed per second. query.comDeleteMultiPerSecond Number of DELETE statements that use the multiple-table syntax executed per second. query.comInsertPerSecond Number of INSERT statements executed per second. query.comInsertSelectPerSecond Number of INSERT SELECT statements executed per second. query.comReplaceSelectPerSecond Number of REPLACE SELECT statements executed per second. query.comRollbackPerSecond Number of ROLLBACK statements executed per second. query.comSelectPerSecond Number of SELECT statements executed per second. query.comUpdateMultiPerSecond Number of UPDATE statements that use the multiple-table syntax executed per second. query.comUpdatePerSecond Number of UPDATE statements executed per second. query.preparedStmtCountPerSecond Current number of prepared statements per second. (The maximum number of statements is given by the max_prepared_stmt_count system variable.) query.queriesPerSecond Total number of statements executed by the server per second, including statements executed within stored programs. query.questionsPerSecond Number of statements executed by the server per second, limited to only those sent by clients. query.slowQueriesPerSecond Number of queries per second that have taken more than long_query_time seconds. This counter increments regardless of whether the slow query log is enabled. Extended metrics Additional metrics captured when extended_metrics is enabled (set to 1 in the configuration file): Name Description db.createdTmpDiskTablesPerSecond Number of internal on-disk temporary tables created per second by the server while executing statements. db.createdTmpFilesPerSecond Number of temporary files created per second by mysqld. db.createdTmpTablesPerSecond Number of internal temporary tables created per second by the server while executing statements. db.handlerDeletePerSecond Number of times per second that rows have been deleted from tables. db.handlerReadFirstPerSecond Number of times per second the first entry in an index was read. db.handlerReadKeyPerSecond Number of requests per second to read a row based on a key. db.handlerReadRndNextPerSecond Number of requests per second to read the next row in the data file. db.handlerReadRndPerSecond Number of requests per second to read a row based on a fixed position. db.handlerUpdatePerSecond Number of requests per second to update a row in a table. db.handlerWritePerSecond Number of requests per second to insert a row in a table. db.maxExecutionTimeExceededPerSecond Number of SELECT statements per second for which the execution timeout was exceeded. db.qCacheFreeBlocks Number of free memory blocks in the query cache. db.qCacheHitsPerSecond Number of query cache hits per second. db.qCacheInserts Number of queries added to the query cache. db.qCacheLowmemPrunesPerSecond Number of queries per second that were deleted from the query cache because of low memory. db.qCacheQueriesInCachePerSecond Number of queries per second registered in the query cache. db.qCacheTotalBlocks Total number of blocks in the query cache. db.selectFullJoinPerSecond Number of joins that perform table scans because they do not use indexes, per second. db.selectFullJoinRangePerSecond Number of joins per second that used a range search on a reference table. db.selectRangeCheckPerSecond Number of joins per second without keys that check for key usage after each row. db.selectRangePerSecond Number of joins per second that used ranges on the first table. db.sortMergePassesPerSecond Number of merge passes that the sort algorithm has had to do, per second. db.sortRangePerSecond Number of sorts per second that were done using ranges. db.sortRowsPerSecond Number of sorted rows per second. db.sortScanPerSecond Number of sorts that were done by scanning the table, per second. db.tableOpenCacheHitsPerSecond Number of hits per second for open tables cache lookups. db.tableOpenCacheMissesPerSecond Number of misses per second for open tables cache lookups. db.tableOpenCacheOverflowsPerSecond Number of overflows per second for the open tables cache. db.threadCacheMissRate Percent of threads that need to be created to handle new connections because there are not enough threads available in the cache. db.threadsCached Number of threads in the thread cache. db.threadsCreatedPerSecond Number of threads per second created to handle connections. Extended innodb metrics Additional metrics captured when extended_innodb_metrics is enabled (set to 1 in the configuration file): Name Description db.innodb.bufferPoolPagesDirty Current number of dirty pages in the InnoDB buffer pool. db.innodb.bufferPoolPagesFlushedPerSecond Number of requests per second to flush pages from the InnoDB buffer pool. db.innodb.bufferPoolReadAheadEvictedPerSecond Number of pages per second read into the InnoDB buffer pool by the read-ahead background thread that were subsequently evicted without having been accessed by queries. db.innodb.bufferPoolReadAheadPerSecond Number of pages per second read into the InnoDB buffer pool by the read-ahead background thread. db.innodb.bufferPoolReadAheadRndPerSecond Number of random read-aheads per second initiated by InnoDB. This happens when a query scans a large portion of a table but in random order. db.innodb.bufferPoolReadRequestsPerSecond Number of logical read requests per second. db.innodb.bufferPoolReadsPerSecond Number of logical reads that InnoDB could not satisfy from the buffer pool, and had to read directly from disk, per second. db.innodb.bufferPoolWaitFreePerSecond Number of times per second a read or write to InnoDB had to wait because there were not clean pages available in the buffer pool. db.innodb.bufferPoolWriteRequestsPerSecond Number of writes per second done to the InnoDB buffer pool. db.innodb.dataFsyncsPerSecond Number of fsync() operations per second. db.innodb.dataPendingFsyncs Current number of pending fsync() operations. db.innodb.dataPendingReads Current number of pending reads. db.innodb.dataPendingWrites Current number of pending writes. db.innodb.dataReadsPerSecond Number of data reads (OS file reads) per second. db.innodb.dataWritesPerSecond Number of data writes per second. db.innodb.logWriteRequestsPerSecond Number of write requests for the InnoDB redo log per second. db.innodb.logWritesPerSecond Number of physical writes per second to the InnoDB redo log file. db.innodb.numOpenFiles Number of files InnoDB currently holds open. db.innodb.osLogFsyncsPerSecond Number of fsync() writes per second done to the InnoDB redo log files. db.innodb.osLogPendingFsyncs Number of pending fsync() operations for the InnoDB redo log files. db.innodb.osLogPendingWrites Number of pending writes per second to the InnoDB redo log files. db.innodb.osLogWrittenBytesPerSecond rate Number of bytes written per second to the InnoDB redo log files. db.innodb.pagesCreatedPerSecond The number of pages created per second by operations on InnoDB tables. db.innodb.pagesReadPerSecond Number of pages read per second from the InnoDB buffer pool by operations on InnoDB tables. db.innodb.pagesWrittenPerSecond Number of pages written per second by operations on InnoDB tables. db.innodb.rowsDeletedPerSecond Number of rows deleted per second from InnoDB tables. db.innodb.rowsInsertedPerSecond Number of rows per second inserted into InnoDB tables. db.innodb.rowsReadPerSecond Number of rows per second read from InnoDB tables. db.innodb.rowsUpdatedPerSecond Number of rows per second updated in InnoDB tables. Extended myisam metrics Additional metrics captured when extended_myisam_metrics is enabled in the configuration file: Name Description db.myisam.keyBlocksNotFlushed Number of key blocks in the MyISAM key cache that have changed but have not yet been flushed to disk. db.myisam.keyCacheUtilization Percentage of the key cache that is being used. db.myisam.keyReadRequestsPerSecond Number of requests to read a key block from the MyISAM key cache, per second. db.myisam.keyReadsPerSecond Number of physical reads of a key block from disk into the MyISAM key cache, per second. db.myisam.keyWriteRequestsPerSecond Number of requests per second to write a key block to the MyISAM key cache. db.myisam.keyWritesPerSecond Number of physical writes of a key block from the MyISAM key cache to disk, per second. Extended slave cluster metrics Additional metrics captured when the extended metrics flag is enabled in the configuration file and the cluster.slaveRunning metric is returning a value of 1. Check the MySQL Documentation for more details. Name Description db.relayLogSpace Total combined number of bytes for all existing relay log files. cluster.lastIOErrno Error number of the most recent error that caused the I/O thread to stop. cluster.lastIOError Error message of the most recent error that caused the I/O thread to stop. cluster.lastSQLErrno Error number of the most recent error that caused the SQL thread to stop. cluster.lastSQLError Error message of the most recent error that caused the SQL thread to stop. cluster.slaveIORunning Status of whether the I/O thread is started and has connected successfully to the master. The values can be Yes, No, or Connecting. cluster.slaveSQLRunning Status of whether the SQL thread is started. The values can be Yes or No. cluster.secondsBehindMaster Difference in seconds between the slaves clock time and the timestamp of the query when it was recorded in the masters binary log. When the slave is not correctly connected to the master, this metric wont be reported. cluster.masterLogFile Name of the master binary log file from which the I/O thread is currently reading. cluster.readMasterLogPos Position in the current master binary log file up to which the I/O thread has read. cluster.relayMasterLogFile Name of the master binary log file containing the most recent event executed by the SQL thread. cluster.execMasterLogPos Position in the current master binary log file to which the SQL thread has read and executed, marking the start of the next transaction or event to be processed. Inventory The MySQL integration captures the configuration parameters of the MySQL node returned by SHOW GLOBAL VARIABLES. The data is available on the Inventory page, under the config/mysql source. System metadata The MySQL integration collects the following metadata attributes about your MySQL system: Name Description software.edition software.edition takes the value of the MySQL version_comment variable. software.version The MySQL server version. cluster.nodeType Either master or slave, depending on the role of the MySQL node being monitored. Source code The MySQL integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 276.9209,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "MySQL monitoring <em>integration</em>",
        "sections": "MySQL monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration"
      },
      "id": "6043a211e7b9d294bc5799d1"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/f5-monitoring-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.3116,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.2713,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.27112,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/flex-integration-tool-build-your-own-integration": [
    {
      "sections": [
        "Introduction to New Relic integrations",
        "Tip",
        "Choose what's right for you",
        "Create your own solutions"
      ],
      "title": "Introduction to New Relic integrations",
      "type": "docs",
      "tags": [
        "Full-Stack Observability",
        "Instrument everything",
        "Get started"
      ],
      "external_id": "03217983a29af22737c1163da9ef0811b29c2bcd",
      "image": "",
      "url": "https://docs.newrelic.com/docs/full-stack-observability/instrument-everything/get-started-new-relic-instrumentation/introduction-new-relic-integrations/",
      "published_at": "2021-05-06T04:28:30Z",
      "updated_at": "2021-03-16T07:30:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We provide hundreds of solutions to get your data into New Relic so you can analyze the data in one place. They give you a steady flow of useful data to fix problems quickly, maintain complex systems, improve your code, and accelerate your digital transformation. You can bring in data from hundreds of applications, frameworks, services, operating systems, and other technologies. Our integrations gather the data, and the agents send it to New Relic. The solution you need may require you to install both an integration and an agent. In some cases, you can just install our agents that contain integrations, such as our APM agents. Whatever data you need to bring in, chances are that we have options for your environment. If you prefer to make your own solutions, we also offer tools to get you started. Tip To use integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Choose what's right for you We offer a wide range of solutions so you can easily collect data across your environment. You may only need one of our solutions to get the data you need, or you can choose a variety of options to capture a broader range of data types. Go to New Relic Integrations to find solutions that fit your environment. Here is a sample of what youll find there: Application performance monitoring (APM): C, Go, Java, Node, .NET, PHP, Python, and Ruby Mobile apps: Android and iOS Browser monitoring: Google Chrome, Mozilla Firefox, Microsoft Internet Explorer, and Apple Safari Host monitoring: Linux and Microsoft Windows Cloud platform monitoring: Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP) Core infrastructure services: Kubernetes, NGINX, MySQL, and more Open source telemetry integrations: Prometheus, Micrometer, OpenTelemetry, and more Create your own solutions If you are looking for custom options, we have tools to help you create your own: Use New Relic Flex to create lightweight monitoring solutions using infrastructure monitoring. Use New Relic Telemetry SDKs to build custom solutions for sending metrics, traces, and more. Build your own New Relic One applications that you can share with your colleagues, or edit open source applications in our catalog.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.6995,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to New Relic <em>integrations</em>",
        "sections": "Introduction to New Relic <em>integrations</em>",
        "tags": "<em>Full</em>-<em>Stack</em> <em>Observability</em>",
        "body": " <em>integrations</em>, such as our APM agents. Whatever data you need to bring in, chances are that we have options for <em>your</em> environment. If you prefer to make <em>your</em> <em>own</em> solutions, we also offer tools to get you started. Tip To use <em>integrations</em> and infrastructure monitoring, as well as the rest of our"
      },
      "id": "603e817f28ccbc4857eba798"
    },
    {
      "sections": [
        "Cloud services integrations",
        "AWS integrations",
        "GCP integrations",
        "Azure integrations"
      ],
      "title": "Cloud services integrations",
      "type": "docs",
      "tags": [
        "Full-Stack Observability",
        "Instrument everything",
        "Instrument core services and applications"
      ],
      "external_id": "71020c70edfb43072cbf081b3eccd3b18f9e6289",
      "image": "https://docs.newrelic.com/static/78ac85c1fc41f94776fce7235e327f01/69538/img-integration-aws%25402x.png",
      "url": "https://docs.newrelic.com/docs/full-stack-observability/instrument-everything/instrument-core-services-applications/cloud-services-integrations/",
      "published_at": "2021-05-06T04:29:41Z",
      "updated_at": "2021-03-16T06:35:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic you can easily instrument your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 154.47595,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Cloud services <em>integrations</em>",
        "sections": "Cloud services <em>integrations</em>",
        "tags": "<em>Full</em>-<em>Stack</em> <em>Observability</em>",
        "body": "With New Relic you can easily <em>instrument</em> <em>your</em> services in AWS, Google Cloud Platform, and Azure. AWS <em>integrations</em> Introduction to AWS <em>integrations</em> List of AWS <em>integrations</em> GCP <em>integrations</em> Introduction to GCP <em>integrations</em> List of GCP <em>integrations</em> Azure <em>integrations</em> Introduction to Azure <em>integrations</em> List of Azure <em>integrations</em>"
      },
      "id": "603e829ae7b9d20bb12a080c"
    },
    {
      "sections": [
        "New Relic guided install overview",
        "Supported APM agents",
        "Why it matters",
        "Some technical detail",
        "Important",
        "On-host integration (OHI) recipes"
      ],
      "title": "New Relic guided install overview",
      "type": "docs",
      "tags": [
        "Full-Stack Observability",
        "Observe everything",
        "Get started"
      ],
      "external_id": "2058522f6cb1e82dbbe111a176c22ec4aa515ae5",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/full-stack-observability/observe-everything/get-started/new-relic-guided-install-overview/",
      "published_at": "2021-05-04T22:25:26Z",
      "updated_at": "2021-04-12T05:47:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Instrument your systems and send telemetry data to New Relic with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click the Guided install button. If your account reports data through our EU datacenter, click EU Guided install. Guided install EU Guided install Our infrastructure agent discovers the applications and infrastructure and log sources running in your environment, and recommends which ones should be instrumented. The install automates the configuration and deployment of each system you choose to instrument. Supported APM agents If you have a .NET Windows application on IIS, the guided install configures and enables an APM agent. Guided install for .NET EU Guided install for .NET Why it matters With our guided install, you can instrument your applications and infrastructure and start seeing your data in New Relic in minutes. The guided install uses our command line interface (CLI), the infrastructure agent for your host environment, and a library of installation recipes to instrument your applications and infrastructure for you. That means less toil for you. Because our instrumentation recipes are open source, you can modify existing recipes, or build new ones, to suit your needs. Some technical detail The New Relic guided install uses open source installation recipes to instrument on-host integrations. These recipes include installation and setup commands, information about logs, and metadata related to whats being installed. They're collected in a YAML file for each type of system and have all of the installation details necessary to install the infrastructure agent for a specific integration. Important On Windows, our guided install only supports Microsoft SQL Server, logs, and the infrastructure agent. All other integrations are only supported on Linux. On-host integration (OHI) recipes The guided install automates the discovery, configuration, and installation of OHIs. However, there may be times when you want to instrument them one-by-one using the CLI install command. To install any individual on-host integration, run this command: curl -Ls https://raw.githubusercontent.com/newrelic/newrelic-cli/master/scripts/install.sh | bash && sudo NEW_RELIC_API_KEY=API_KEY NEW_RELIC_ACCOUNT_ID=ACCOUNT_ID /usr/local/bin/newrelic install -n INTEGRATION-FLAG Copy For example: curl -Ls https://raw.githubusercontent.com/newrelic/newrelic-cli/master/scripts/install.sh | bash && sudo NEW_RELIC_API_KEY=<API_KEY> NEW_RELIC_ACCOUNT_ID=<ACCOUNT_ID> /usr/local/bin/newrelic install -n apache-open-source-integration Copy The table lists the integrations supported by the guided install CLI command. The specific on-host integration commands are provided for your reference. Our open source integrations send performance metrics and inventory data from your servers and applications to the New Relic platform. You can view pre-built dashboards of your metric data, create alert policies, and create your own custom queries and charts. Integration Command Apache newrelic install -n apache-open-source-integration Cassandra newrelic install -n cassandra-open-source-integration Couchbase newrelic install -n couchbase-open-source-integration ElasticSearch newrelic install -n elasticsearch-open-source-integration HAProxy newrelic install -n haproxy-open-source-integration HashiCorp Consul newrelic install -n hashicorp-consul-open-source-integration JMX newrelic install -n jmx-open-source-integration Memcached newrelic install -n memcached-open-source-integration Microsoft SQL Server (Windows only) newrelic install -n mssql-server-integration-installer MongoDB newrelic install -n mongodb-open-source-integration MySQL newrelic install -n mysql-open-source-integration Nagios newrelic install -n nagios-open-source-integration Nginx newrelic install -n nginx-open-source-integration PostgreSQL newrelic install -n postgres-open-source-integration RabbitMQ newrelic install -n rabbitmq-open-source-integration Redis newrelic install -n redis-open-source-integration Varnish Cache newrelic install -n varnish-cache-open-source-integration",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 126.30585,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "On-host <em>integration</em> (OHI) recipes",
        "tags": "<em>Full</em>-<em>Stack</em> <em>Observability</em>",
        "body": ". That means less toil for you. Because our instrumentation recipes are open source, you can modify existing recipes, or build new ones, to suit <em>your</em> needs. Some technical detail The New Relic guided install uses open source installation recipes to <em>instrument</em> on-host <em>integrations</em>. These recipes include"
      },
      "id": "604130a7e7b9d299cb2a07c0"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/go-insights-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.31128,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.27097,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.2708,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/haproxy-monitoring-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.31128,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.27097,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.2708,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/hashicorp-consul-monitoring-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.3111,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.2708,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.27063,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/jmx-monitoring-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.3111,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.2708,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.27063,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.3109,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.27063,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.27045,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/memcached-monitoring-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.3109,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.27063,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.27045,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.31076,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.27045,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.27026,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/mongodb-monitoring-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.31076,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.27045,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.27026,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.3106,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.27026,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "MySQL monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Configuration",
        "Activate remote monitoring",
        "Environment variable passthroughs",
        "HOSTNAME",
        "PORT",
        "USERNAME",
        "PASSWORD",
        "DATABASE",
        "EXTENDED_METRICS",
        "EXTENDED_INNODB_METRICS",
        "EXTENDED_MY_ISAM_METRICS",
        "Find and use data",
        "Metric data",
        "Default metrics",
        "Extended metrics",
        "Extended innodb metrics",
        "Extended myisam metrics",
        "Extended slave cluster metrics",
        "Inventory",
        "System metadata",
        "Source code"
      ],
      "title": "MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "50b118a06500c42ca8f26ce475d00f70c6fda148",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/mysql-monitoring-integration/",
      "published_at": "2021-05-04T15:54:52Z",
      "updated_at": "2021-05-02T03:13:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our MySQL integration collects and sends inventory and metrics from your MySQL database to our platform, where you can see the health of your database server and analyze metric data so that you can easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with MySQL version 5.6 or higher. Before installing the integration, make sure that you meet the following requirements: If MySQL is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running MySQL. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Important For MySQL v8.0 and higher we do not support the following metrics: cluster.slaveRunning, db.qCacheFreeMemoryBytes, db.qCacheHitRatio, db.qCacheNotCachedPerSecond. Quick start Instrument your MySQL database quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the MySQL integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-mysql. From the command line, create a user with replication privileges: sudo mysql -e \"CREATE USER 'newrelic'@'localhost' IDENTIFIED BY 'YOUR_SELECTED_PASSWORD';\" Copy sudo mysql -e \"GRANT REPLICATION CLIENT ON *.* TO 'newrelic'@'localhost' WITH MAX_USER_CONNECTIONS 5;\" Copy Change the directory to the integration's folder. cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp mysql-config.yml.sample mysql-config.yml Copy Edit the configuration file mysql-config.yml as explained in the next section. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, mysql-config.yml. The configuration provides a single command, status, that captures the metrics and all the config options. It accepts these arguments: hostname: the MySQL hostname. port: the port where the MySQL server is listening. username: the user connected to the MySQL server. If you used the CREATE USER command in the activation instructions, this should be set to newrelic. password: the password for the user specified above. extended_metrics: captures an extended set of metrics. Disabled by default. Set to 1 to enable. This also enables the capture of slave metrics. extended_innodb_metrics: captures additional innodb metrics. Disabled by default. Set to 1 to enable. extended_myisam_metrics: captures additional MyISAM metrics. Disabled by default. Set to 1 to enable. Optional: labels field. For example, the env label controls the environment inventory data. The default value is production. Optional: metrics field. Set to 1 to disable the collection of inventory. See a sample of a configuration file. Activate remote monitoring The remote_monitoring parameter enables remote monitoring and multi-tenancy for this integration. This parameter is enabled by default and should not be changed unless you require it in your custom environment. Activating remote_monitoring may change some attributes and/or affect your configured alerts. For more information, see remote monitoring in on-host integrations. Important Infrastructure agent version 1.2.25 or higher is required to use remote_monitoring. Environment variable passthroughs Environment variables can be used to control config settings, and are then passed through to the infrastructure agent. For instructions on how to use this feature, see Configure the infrastructure agent. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. HOSTNAME Specifies the hostname or IP where MySQL is running. Type String Default localhost Example: HOSTNAME='MySQL DB' Copy PORT Port on which MySQL server is listening. Type Integer Default 3306 Example: PORT=6379 Copy USERNAME The user connected to the MySQL server. Type String Default (none) Example: USERNAME='DBAdmin' Copy PASSWORD Password for the given user. Type String Default (none) Example: PASSWORD='Hh7$(uvRt' Copy DATABASE Name of the database to be monitored. Type String Default (none) Example: DATABASE='My MySQL DB' Copy EXTENDED_METRICS Captures an extended set of metrics. This also enables the capture of slave metrics. Type Boolean Default false Example: EXTENDED_METRICS=true Copy EXTENDED_INNODB_METRICS Captures additional innodb metrics. Type Boolean Default false Example: EXTENDED_INNODB_METRICS=true Copy EXTENDED_MY_ISAM_METRICS Captures additional MyISAM metrics. Type Boolean Default false Example: EXTENDED_MY_ISAM_METRICS=true Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the MysqlSample event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The MySQL integration collects the following metrics: Default metrics These metrics are captured by default: Name Description cluster.slaveRunning Boolean. 1 if this server is a replication slave that is connected to a replication master, and both the I/O and SQL threads are running; otherwise, it is 0. For metrics reported if enabled, see replication slave metrics. db.handlerRollbackPerSecond Rate of requests for a storage engine to perform a rollback operation, per second. db.innodb.bufferPoolPagesData Number of pages in the InnoDB buffer pool containing data. db.innodb.bufferPoolPagesFree Number of free pages in the InnoDB buffer pool. db.innodb.bufferPoolPagesTotal Total number of pages of the InnoDB buffer pool. db.innodb.dataReadBytesPerSecond Rate at which data is read from InnoDB tables in bytes per second. db.innodb.dataWrittenBytesPerSecond Rate at which data is written to InnoDB tables in bytes per second. db.innodb.logWaitsPerSecond Number of times that the log buffer was too small and a wait was required for it to be flushed before continuing, in waits per second. db.innodb.rowLockCurrentWaits Number of row locks currently being waited for by operations on InnoDB tables. db.innodb.rowLockTimeAvg Average time to acquire a row lock for InnoDB tables, in milliseconds. db.innodb.rowLockWaitsPerSecond Number of times operations on InnoDB tables had to wait for a row lock per second. db.openedTablesPerSecond Number of files that have been opened with my_open() (a mysys library function) per second. Parts of the server that open files without using this function do not increment the count. db.openFiles Number of files that are open. This count includes regular files opened by the server. It does not include other types of files such as sockets or pipes. db.openTables Number of tables that are open. db.qCacheFreeMemoryBytes Amount of free memory in bytes for the query cache. db.qCacheHitRatio Percentage of queries that are retrieved from the cache. db.qCacheNotCachedPerSecond Number of noncached queries (not cacheable, or not cached due to the query_cache_type setting) per second. db.qCacheUtilization Percentage of query cache memory that is being used. db.tablesLocksWaitedPerSecond Number of times per second that a request for a table lock could not be granted immediately and a wait was needed. net.abortedClientsPerSecond Number of connections per second that were aborted because the client died without closing the connection properly. net.abortedConnectsPerSecond Number of failed attempts to connect to the MySQL server, per second. net.bytesReceivedPerSecond Byte throughput received from all clients, per second. net.bytesSentPerSecond Byte throughput sent to all clients, per second. net.connectionErrorsMaxConnectionsPerSecond Rate per second at which connections were refused because the server max_connections limit was reached. net.connectionsPerSecond Number of connection attempts per second. net.maxUsedConnections Maximum number of connections that have been in use simultaneously since the server started. net.threadsConnected Number of currently open connections. net.threadsRunning Number of threads that are not sleeping. query.comCommitPerSecond Number of COMMIT statements executed per second. query.comDeletePerSecond Number of DELETE statements executed per second. query.comDeleteMultiPerSecond Number of DELETE statements that use the multiple-table syntax executed per second. query.comInsertPerSecond Number of INSERT statements executed per second. query.comInsertSelectPerSecond Number of INSERT SELECT statements executed per second. query.comReplaceSelectPerSecond Number of REPLACE SELECT statements executed per second. query.comRollbackPerSecond Number of ROLLBACK statements executed per second. query.comSelectPerSecond Number of SELECT statements executed per second. query.comUpdateMultiPerSecond Number of UPDATE statements that use the multiple-table syntax executed per second. query.comUpdatePerSecond Number of UPDATE statements executed per second. query.preparedStmtCountPerSecond Current number of prepared statements per second. (The maximum number of statements is given by the max_prepared_stmt_count system variable.) query.queriesPerSecond Total number of statements executed by the server per second, including statements executed within stored programs. query.questionsPerSecond Number of statements executed by the server per second, limited to only those sent by clients. query.slowQueriesPerSecond Number of queries per second that have taken more than long_query_time seconds. This counter increments regardless of whether the slow query log is enabled. Extended metrics Additional metrics captured when extended_metrics is enabled (set to 1 in the configuration file): Name Description db.createdTmpDiskTablesPerSecond Number of internal on-disk temporary tables created per second by the server while executing statements. db.createdTmpFilesPerSecond Number of temporary files created per second by mysqld. db.createdTmpTablesPerSecond Number of internal temporary tables created per second by the server while executing statements. db.handlerDeletePerSecond Number of times per second that rows have been deleted from tables. db.handlerReadFirstPerSecond Number of times per second the first entry in an index was read. db.handlerReadKeyPerSecond Number of requests per second to read a row based on a key. db.handlerReadRndNextPerSecond Number of requests per second to read the next row in the data file. db.handlerReadRndPerSecond Number of requests per second to read a row based on a fixed position. db.handlerUpdatePerSecond Number of requests per second to update a row in a table. db.handlerWritePerSecond Number of requests per second to insert a row in a table. db.maxExecutionTimeExceededPerSecond Number of SELECT statements per second for which the execution timeout was exceeded. db.qCacheFreeBlocks Number of free memory blocks in the query cache. db.qCacheHitsPerSecond Number of query cache hits per second. db.qCacheInserts Number of queries added to the query cache. db.qCacheLowmemPrunesPerSecond Number of queries per second that were deleted from the query cache because of low memory. db.qCacheQueriesInCachePerSecond Number of queries per second registered in the query cache. db.qCacheTotalBlocks Total number of blocks in the query cache. db.selectFullJoinPerSecond Number of joins that perform table scans because they do not use indexes, per second. db.selectFullJoinRangePerSecond Number of joins per second that used a range search on a reference table. db.selectRangeCheckPerSecond Number of joins per second without keys that check for key usage after each row. db.selectRangePerSecond Number of joins per second that used ranges on the first table. db.sortMergePassesPerSecond Number of merge passes that the sort algorithm has had to do, per second. db.sortRangePerSecond Number of sorts per second that were done using ranges. db.sortRowsPerSecond Number of sorted rows per second. db.sortScanPerSecond Number of sorts that were done by scanning the table, per second. db.tableOpenCacheHitsPerSecond Number of hits per second for open tables cache lookups. db.tableOpenCacheMissesPerSecond Number of misses per second for open tables cache lookups. db.tableOpenCacheOverflowsPerSecond Number of overflows per second for the open tables cache. db.threadCacheMissRate Percent of threads that need to be created to handle new connections because there are not enough threads available in the cache. db.threadsCached Number of threads in the thread cache. db.threadsCreatedPerSecond Number of threads per second created to handle connections. Extended innodb metrics Additional metrics captured when extended_innodb_metrics is enabled (set to 1 in the configuration file): Name Description db.innodb.bufferPoolPagesDirty Current number of dirty pages in the InnoDB buffer pool. db.innodb.bufferPoolPagesFlushedPerSecond Number of requests per second to flush pages from the InnoDB buffer pool. db.innodb.bufferPoolReadAheadEvictedPerSecond Number of pages per second read into the InnoDB buffer pool by the read-ahead background thread that were subsequently evicted without having been accessed by queries. db.innodb.bufferPoolReadAheadPerSecond Number of pages per second read into the InnoDB buffer pool by the read-ahead background thread. db.innodb.bufferPoolReadAheadRndPerSecond Number of random read-aheads per second initiated by InnoDB. This happens when a query scans a large portion of a table but in random order. db.innodb.bufferPoolReadRequestsPerSecond Number of logical read requests per second. db.innodb.bufferPoolReadsPerSecond Number of logical reads that InnoDB could not satisfy from the buffer pool, and had to read directly from disk, per second. db.innodb.bufferPoolWaitFreePerSecond Number of times per second a read or write to InnoDB had to wait because there were not clean pages available in the buffer pool. db.innodb.bufferPoolWriteRequestsPerSecond Number of writes per second done to the InnoDB buffer pool. db.innodb.dataFsyncsPerSecond Number of fsync() operations per second. db.innodb.dataPendingFsyncs Current number of pending fsync() operations. db.innodb.dataPendingReads Current number of pending reads. db.innodb.dataPendingWrites Current number of pending writes. db.innodb.dataReadsPerSecond Number of data reads (OS file reads) per second. db.innodb.dataWritesPerSecond Number of data writes per second. db.innodb.logWriteRequestsPerSecond Number of write requests for the InnoDB redo log per second. db.innodb.logWritesPerSecond Number of physical writes per second to the InnoDB redo log file. db.innodb.numOpenFiles Number of files InnoDB currently holds open. db.innodb.osLogFsyncsPerSecond Number of fsync() writes per second done to the InnoDB redo log files. db.innodb.osLogPendingFsyncs Number of pending fsync() operations for the InnoDB redo log files. db.innodb.osLogPendingWrites Number of pending writes per second to the InnoDB redo log files. db.innodb.osLogWrittenBytesPerSecond rate Number of bytes written per second to the InnoDB redo log files. db.innodb.pagesCreatedPerSecond The number of pages created per second by operations on InnoDB tables. db.innodb.pagesReadPerSecond Number of pages read per second from the InnoDB buffer pool by operations on InnoDB tables. db.innodb.pagesWrittenPerSecond Number of pages written per second by operations on InnoDB tables. db.innodb.rowsDeletedPerSecond Number of rows deleted per second from InnoDB tables. db.innodb.rowsInsertedPerSecond Number of rows per second inserted into InnoDB tables. db.innodb.rowsReadPerSecond Number of rows per second read from InnoDB tables. db.innodb.rowsUpdatedPerSecond Number of rows per second updated in InnoDB tables. Extended myisam metrics Additional metrics captured when extended_myisam_metrics is enabled in the configuration file: Name Description db.myisam.keyBlocksNotFlushed Number of key blocks in the MyISAM key cache that have changed but have not yet been flushed to disk. db.myisam.keyCacheUtilization Percentage of the key cache that is being used. db.myisam.keyReadRequestsPerSecond Number of requests to read a key block from the MyISAM key cache, per second. db.myisam.keyReadsPerSecond Number of physical reads of a key block from disk into the MyISAM key cache, per second. db.myisam.keyWriteRequestsPerSecond Number of requests per second to write a key block to the MyISAM key cache. db.myisam.keyWritesPerSecond Number of physical writes of a key block from the MyISAM key cache to disk, per second. Extended slave cluster metrics Additional metrics captured when the extended metrics flag is enabled in the configuration file and the cluster.slaveRunning metric is returning a value of 1. Check the MySQL Documentation for more details. Name Description db.relayLogSpace Total combined number of bytes for all existing relay log files. cluster.lastIOErrno Error number of the most recent error that caused the I/O thread to stop. cluster.lastIOError Error message of the most recent error that caused the I/O thread to stop. cluster.lastSQLErrno Error number of the most recent error that caused the SQL thread to stop. cluster.lastSQLError Error message of the most recent error that caused the SQL thread to stop. cluster.slaveIORunning Status of whether the I/O thread is started and has connected successfully to the master. The values can be Yes, No, or Connecting. cluster.slaveSQLRunning Status of whether the SQL thread is started. The values can be Yes or No. cluster.secondsBehindMaster Difference in seconds between the slaves clock time and the timestamp of the query when it was recorded in the masters binary log. When the slave is not correctly connected to the master, this metric wont be reported. cluster.masterLogFile Name of the master binary log file from which the I/O thread is currently reading. cluster.readMasterLogPos Position in the current master binary log file up to which the I/O thread has read. cluster.relayMasterLogFile Name of the master binary log file containing the most recent event executed by the SQL thread. cluster.execMasterLogPos Position in the current master binary log file to which the SQL thread has read and executed, marking the start of the next transaction or event to be processed. Inventory The MySQL integration captures the configuration parameters of the MySQL node returned by SHOW GLOBAL VARIABLES. The data is available on the Inventory page, under the config/mysql source. System metadata The MySQL integration collects the following metadata attributes about your MySQL system: Name Description software.edition software.edition takes the value of the MySQL version_comment variable. software.version The MySQL server version. cluster.nodeType Either master or slave, depending on the role of the MySQL node being monitored. Source code The MySQL integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 276.92026,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "MySQL monitoring <em>integration</em>",
        "sections": "MySQL monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration"
      },
      "id": "6043a211e7b9d294bc5799d1"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/mysql-monitoring-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.3106,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.27026,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.2701,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/nagios-monitoring-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.31042,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.2701,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.26996,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/nfs-monitoring-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.31042,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.2701,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.26996,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/nginx-monitoring-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.31024,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.26996,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.26978,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/oracle-database-monitoring-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.31024,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.26996,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.26978,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/perfmon-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.31006,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.26978,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.2696,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/port-monitoring-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.3099,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.2696,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.2694,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/postgresql-monitoring-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.3099,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.2696,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.2694,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/rabbitmq-monitoring-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.30975,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.2694,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.26926,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/redis-monitoring-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.30975,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.2694,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.26926,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/snmp-monitoring-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.30957,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.26926,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.2691,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.30957,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.26926,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.2691,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.3094,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.2691,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.26892,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/unix-monitoring-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.3092,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.26892,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.26874,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/varnish-cache-monitoring-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.3092,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.26892,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.26874,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.30905,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.26855,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    },
    {
      "sections": [
        "MySQL monitoring integration",
        "Compatibility and requirements",
        "Important",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Configuration",
        "Activate remote monitoring",
        "Environment variable passthroughs",
        "HOSTNAME",
        "PORT",
        "USERNAME",
        "PASSWORD",
        "DATABASE",
        "EXTENDED_METRICS",
        "EXTENDED_INNODB_METRICS",
        "EXTENDED_MY_ISAM_METRICS",
        "Find and use data",
        "Metric data",
        "Default metrics",
        "Extended metrics",
        "Extended innodb metrics",
        "Extended myisam metrics",
        "Extended slave cluster metrics",
        "Inventory",
        "System metadata",
        "Source code"
      ],
      "title": "MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "50b118a06500c42ca8f26ce475d00f70c6fda148",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/mysql-monitoring-integration/",
      "published_at": "2021-05-04T15:54:52Z",
      "updated_at": "2021-05-02T03:13:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our MySQL integration collects and sends inventory and metrics from your MySQL database to our platform, where you can see the health of your database server and analyze metric data so that you can easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with MySQL version 5.6 or higher. Before installing the integration, make sure that you meet the following requirements: If MySQL is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a Linux OS host that's running MySQL. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Important For MySQL v8.0 and higher we do not support the following metrics: cluster.slaveRunning, db.qCacheFreeMemoryBytes, db.qCacheHitRatio, db.qCacheNotCachedPerSecond. Quick start Instrument your MySQL database quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the MySQL integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-mysql. From the command line, create a user with replication privileges: sudo mysql -e \"CREATE USER 'newrelic'@'localhost' IDENTIFIED BY 'YOUR_SELECTED_PASSWORD';\" Copy sudo mysql -e \"GRANT REPLICATION CLIENT ON *.* TO 'newrelic'@'localhost' WITH MAX_USER_CONNECTIONS 5;\" Copy Change the directory to the integration's folder. cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp mysql-config.yml.sample mysql-config.yml Copy Edit the configuration file mysql-config.yml as explained in the next section. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, mysql-config.yml. The configuration provides a single command, status, that captures the metrics and all the config options. It accepts these arguments: hostname: the MySQL hostname. port: the port where the MySQL server is listening. username: the user connected to the MySQL server. If you used the CREATE USER command in the activation instructions, this should be set to newrelic. password: the password for the user specified above. extended_metrics: captures an extended set of metrics. Disabled by default. Set to 1 to enable. This also enables the capture of slave metrics. extended_innodb_metrics: captures additional innodb metrics. Disabled by default. Set to 1 to enable. extended_myisam_metrics: captures additional MyISAM metrics. Disabled by default. Set to 1 to enable. Optional: labels field. For example, the env label controls the environment inventory data. The default value is production. Optional: metrics field. Set to 1 to disable the collection of inventory. See a sample of a configuration file. Activate remote monitoring The remote_monitoring parameter enables remote monitoring and multi-tenancy for this integration. This parameter is enabled by default and should not be changed unless you require it in your custom environment. Activating remote_monitoring may change some attributes and/or affect your configured alerts. For more information, see remote monitoring in on-host integrations. Important Infrastructure agent version 1.2.25 or higher is required to use remote_monitoring. Environment variable passthroughs Environment variables can be used to control config settings, and are then passed through to the infrastructure agent. For instructions on how to use this feature, see Configure the infrastructure agent. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. HOSTNAME Specifies the hostname or IP where MySQL is running. Type String Default localhost Example: HOSTNAME='MySQL DB' Copy PORT Port on which MySQL server is listening. Type Integer Default 3306 Example: PORT=6379 Copy USERNAME The user connected to the MySQL server. Type String Default (none) Example: USERNAME='DBAdmin' Copy PASSWORD Password for the given user. Type String Default (none) Example: PASSWORD='Hh7$(uvRt' Copy DATABASE Name of the database to be monitored. Type String Default (none) Example: DATABASE='My MySQL DB' Copy EXTENDED_METRICS Captures an extended set of metrics. This also enables the capture of slave metrics. Type Boolean Default false Example: EXTENDED_METRICS=true Copy EXTENDED_INNODB_METRICS Captures additional innodb metrics. Type Boolean Default false Example: EXTENDED_INNODB_METRICS=true Copy EXTENDED_MY_ISAM_METRICS Captures additional MyISAM metrics. Type Boolean Default false Example: EXTENDED_MY_ISAM_METRICS=true Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Metrics are attached to the MysqlSample event type. You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The MySQL integration collects the following metrics: Default metrics These metrics are captured by default: Name Description cluster.slaveRunning Boolean. 1 if this server is a replication slave that is connected to a replication master, and both the I/O and SQL threads are running; otherwise, it is 0. For metrics reported if enabled, see replication slave metrics. db.handlerRollbackPerSecond Rate of requests for a storage engine to perform a rollback operation, per second. db.innodb.bufferPoolPagesData Number of pages in the InnoDB buffer pool containing data. db.innodb.bufferPoolPagesFree Number of free pages in the InnoDB buffer pool. db.innodb.bufferPoolPagesTotal Total number of pages of the InnoDB buffer pool. db.innodb.dataReadBytesPerSecond Rate at which data is read from InnoDB tables in bytes per second. db.innodb.dataWrittenBytesPerSecond Rate at which data is written to InnoDB tables in bytes per second. db.innodb.logWaitsPerSecond Number of times that the log buffer was too small and a wait was required for it to be flushed before continuing, in waits per second. db.innodb.rowLockCurrentWaits Number of row locks currently being waited for by operations on InnoDB tables. db.innodb.rowLockTimeAvg Average time to acquire a row lock for InnoDB tables, in milliseconds. db.innodb.rowLockWaitsPerSecond Number of times operations on InnoDB tables had to wait for a row lock per second. db.openedTablesPerSecond Number of files that have been opened with my_open() (a mysys library function) per second. Parts of the server that open files without using this function do not increment the count. db.openFiles Number of files that are open. This count includes regular files opened by the server. It does not include other types of files such as sockets or pipes. db.openTables Number of tables that are open. db.qCacheFreeMemoryBytes Amount of free memory in bytes for the query cache. db.qCacheHitRatio Percentage of queries that are retrieved from the cache. db.qCacheNotCachedPerSecond Number of noncached queries (not cacheable, or not cached due to the query_cache_type setting) per second. db.qCacheUtilization Percentage of query cache memory that is being used. db.tablesLocksWaitedPerSecond Number of times per second that a request for a table lock could not be granted immediately and a wait was needed. net.abortedClientsPerSecond Number of connections per second that were aborted because the client died without closing the connection properly. net.abortedConnectsPerSecond Number of failed attempts to connect to the MySQL server, per second. net.bytesReceivedPerSecond Byte throughput received from all clients, per second. net.bytesSentPerSecond Byte throughput sent to all clients, per second. net.connectionErrorsMaxConnectionsPerSecond Rate per second at which connections were refused because the server max_connections limit was reached. net.connectionsPerSecond Number of connection attempts per second. net.maxUsedConnections Maximum number of connections that have been in use simultaneously since the server started. net.threadsConnected Number of currently open connections. net.threadsRunning Number of threads that are not sleeping. query.comCommitPerSecond Number of COMMIT statements executed per second. query.comDeletePerSecond Number of DELETE statements executed per second. query.comDeleteMultiPerSecond Number of DELETE statements that use the multiple-table syntax executed per second. query.comInsertPerSecond Number of INSERT statements executed per second. query.comInsertSelectPerSecond Number of INSERT SELECT statements executed per second. query.comReplaceSelectPerSecond Number of REPLACE SELECT statements executed per second. query.comRollbackPerSecond Number of ROLLBACK statements executed per second. query.comSelectPerSecond Number of SELECT statements executed per second. query.comUpdateMultiPerSecond Number of UPDATE statements that use the multiple-table syntax executed per second. query.comUpdatePerSecond Number of UPDATE statements executed per second. query.preparedStmtCountPerSecond Current number of prepared statements per second. (The maximum number of statements is given by the max_prepared_stmt_count system variable.) query.queriesPerSecond Total number of statements executed by the server per second, including statements executed within stored programs. query.questionsPerSecond Number of statements executed by the server per second, limited to only those sent by clients. query.slowQueriesPerSecond Number of queries per second that have taken more than long_query_time seconds. This counter increments regardless of whether the slow query log is enabled. Extended metrics Additional metrics captured when extended_metrics is enabled (set to 1 in the configuration file): Name Description db.createdTmpDiskTablesPerSecond Number of internal on-disk temporary tables created per second by the server while executing statements. db.createdTmpFilesPerSecond Number of temporary files created per second by mysqld. db.createdTmpTablesPerSecond Number of internal temporary tables created per second by the server while executing statements. db.handlerDeletePerSecond Number of times per second that rows have been deleted from tables. db.handlerReadFirstPerSecond Number of times per second the first entry in an index was read. db.handlerReadKeyPerSecond Number of requests per second to read a row based on a key. db.handlerReadRndNextPerSecond Number of requests per second to read the next row in the data file. db.handlerReadRndPerSecond Number of requests per second to read a row based on a fixed position. db.handlerUpdatePerSecond Number of requests per second to update a row in a table. db.handlerWritePerSecond Number of requests per second to insert a row in a table. db.maxExecutionTimeExceededPerSecond Number of SELECT statements per second for which the execution timeout was exceeded. db.qCacheFreeBlocks Number of free memory blocks in the query cache. db.qCacheHitsPerSecond Number of query cache hits per second. db.qCacheInserts Number of queries added to the query cache. db.qCacheLowmemPrunesPerSecond Number of queries per second that were deleted from the query cache because of low memory. db.qCacheQueriesInCachePerSecond Number of queries per second registered in the query cache. db.qCacheTotalBlocks Total number of blocks in the query cache. db.selectFullJoinPerSecond Number of joins that perform table scans because they do not use indexes, per second. db.selectFullJoinRangePerSecond Number of joins per second that used a range search on a reference table. db.selectRangeCheckPerSecond Number of joins per second without keys that check for key usage after each row. db.selectRangePerSecond Number of joins per second that used ranges on the first table. db.sortMergePassesPerSecond Number of merge passes that the sort algorithm has had to do, per second. db.sortRangePerSecond Number of sorts per second that were done using ranges. db.sortRowsPerSecond Number of sorted rows per second. db.sortScanPerSecond Number of sorts that were done by scanning the table, per second. db.tableOpenCacheHitsPerSecond Number of hits per second for open tables cache lookups. db.tableOpenCacheMissesPerSecond Number of misses per second for open tables cache lookups. db.tableOpenCacheOverflowsPerSecond Number of overflows per second for the open tables cache. db.threadCacheMissRate Percent of threads that need to be created to handle new connections because there are not enough threads available in the cache. db.threadsCached Number of threads in the thread cache. db.threadsCreatedPerSecond Number of threads per second created to handle connections. Extended innodb metrics Additional metrics captured when extended_innodb_metrics is enabled (set to 1 in the configuration file): Name Description db.innodb.bufferPoolPagesDirty Current number of dirty pages in the InnoDB buffer pool. db.innodb.bufferPoolPagesFlushedPerSecond Number of requests per second to flush pages from the InnoDB buffer pool. db.innodb.bufferPoolReadAheadEvictedPerSecond Number of pages per second read into the InnoDB buffer pool by the read-ahead background thread that were subsequently evicted without having been accessed by queries. db.innodb.bufferPoolReadAheadPerSecond Number of pages per second read into the InnoDB buffer pool by the read-ahead background thread. db.innodb.bufferPoolReadAheadRndPerSecond Number of random read-aheads per second initiated by InnoDB. This happens when a query scans a large portion of a table but in random order. db.innodb.bufferPoolReadRequestsPerSecond Number of logical read requests per second. db.innodb.bufferPoolReadsPerSecond Number of logical reads that InnoDB could not satisfy from the buffer pool, and had to read directly from disk, per second. db.innodb.bufferPoolWaitFreePerSecond Number of times per second a read or write to InnoDB had to wait because there were not clean pages available in the buffer pool. db.innodb.bufferPoolWriteRequestsPerSecond Number of writes per second done to the InnoDB buffer pool. db.innodb.dataFsyncsPerSecond Number of fsync() operations per second. db.innodb.dataPendingFsyncs Current number of pending fsync() operations. db.innodb.dataPendingReads Current number of pending reads. db.innodb.dataPendingWrites Current number of pending writes. db.innodb.dataReadsPerSecond Number of data reads (OS file reads) per second. db.innodb.dataWritesPerSecond Number of data writes per second. db.innodb.logWriteRequestsPerSecond Number of write requests for the InnoDB redo log per second. db.innodb.logWritesPerSecond Number of physical writes per second to the InnoDB redo log file. db.innodb.numOpenFiles Number of files InnoDB currently holds open. db.innodb.osLogFsyncsPerSecond Number of fsync() writes per second done to the InnoDB redo log files. db.innodb.osLogPendingFsyncs Number of pending fsync() operations for the InnoDB redo log files. db.innodb.osLogPendingWrites Number of pending writes per second to the InnoDB redo log files. db.innodb.osLogWrittenBytesPerSecond rate Number of bytes written per second to the InnoDB redo log files. db.innodb.pagesCreatedPerSecond The number of pages created per second by operations on InnoDB tables. db.innodb.pagesReadPerSecond Number of pages read per second from the InnoDB buffer pool by operations on InnoDB tables. db.innodb.pagesWrittenPerSecond Number of pages written per second by operations on InnoDB tables. db.innodb.rowsDeletedPerSecond Number of rows deleted per second from InnoDB tables. db.innodb.rowsInsertedPerSecond Number of rows per second inserted into InnoDB tables. db.innodb.rowsReadPerSecond Number of rows per second read from InnoDB tables. db.innodb.rowsUpdatedPerSecond Number of rows per second updated in InnoDB tables. Extended myisam metrics Additional metrics captured when extended_myisam_metrics is enabled in the configuration file: Name Description db.myisam.keyBlocksNotFlushed Number of key blocks in the MyISAM key cache that have changed but have not yet been flushed to disk. db.myisam.keyCacheUtilization Percentage of the key cache that is being used. db.myisam.keyReadRequestsPerSecond Number of requests to read a key block from the MyISAM key cache, per second. db.myisam.keyReadsPerSecond Number of physical reads of a key block from disk into the MyISAM key cache, per second. db.myisam.keyWriteRequestsPerSecond Number of requests per second to write a key block to the MyISAM key cache. db.myisam.keyWritesPerSecond Number of physical writes of a key block from the MyISAM key cache to disk, per second. Extended slave cluster metrics Additional metrics captured when the extended metrics flag is enabled in the configuration file and the cluster.slaveRunning metric is returning a value of 1. Check the MySQL Documentation for more details. Name Description db.relayLogSpace Total combined number of bytes for all existing relay log files. cluster.lastIOErrno Error number of the most recent error that caused the I/O thread to stop. cluster.lastIOError Error message of the most recent error that caused the I/O thread to stop. cluster.lastSQLErrno Error number of the most recent error that caused the SQL thread to stop. cluster.lastSQLError Error message of the most recent error that caused the SQL thread to stop. cluster.slaveIORunning Status of whether the I/O thread is started and has connected successfully to the master. The values can be Yes, No, or Connecting. cluster.slaveSQLRunning Status of whether the SQL thread is started. The values can be Yes or No. cluster.secondsBehindMaster Difference in seconds between the slaves clock time and the timestamp of the query when it was recorded in the masters binary log. When the slave is not correctly connected to the master, this metric wont be reported. cluster.masterLogFile Name of the master binary log file from which the I/O thread is currently reading. cluster.readMasterLogPos Position in the current master binary log file up to which the I/O thread has read. cluster.relayMasterLogFile Name of the master binary log file containing the most recent event executed by the SQL thread. cluster.execMasterLogPos Position in the current master binary log file to which the SQL thread has read and executed, marking the start of the next transaction or event to be processed. Inventory The MySQL integration captures the configuration parameters of the MySQL node returned by SHOW GLOBAL VARIABLES. The data is available on the Inventory page, under the config/mysql source. System metadata The MySQL integration collects the following metadata attributes about your MySQL system: Name Description software.edition software.edition takes the value of the MySQL version_comment variable. software.version The MySQL server version. cluster.nodeType Either master or slave, depending on the role of the MySQL node being monitored. Source code The MySQL integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 276.91925,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "MySQL monitoring <em>integration</em>",
        "sections": "MySQL monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " the infrastructure agent. Additional notes: Advanced: <em>Integrations</em> are also available in tarball format to allow for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configuration"
      },
      "id": "6043a211e7b9d294bc5799d1"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/vmware-vsphere-monitoring-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.30905,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.26874,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.26855,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/host-integrations-list/zookeeper-monitoring-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.3089,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.26855,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " VMware Tanzu provides a <em>list</em> of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 307.2684,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em> <em>list</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/installation/container-auto-discovery-host-integrations": [
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 196.58368,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the <em>installation</em>. Scale the number of instances"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.86691,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.84335,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Step 1: Enable EC2 to <em>install</em> the infrastructure agent",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/installation/install-infrastructure-host-integrations": [
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 196.58356,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the <em>installation</em>. Scale the number of instances"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.86682,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.84326,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Step 1: Enable EC2 to <em>install</em> the infrastructure agent",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/installation/secrets-management": [
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 196.58356,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the <em>installation</em>. Scale the number of instances"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.86682,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.84326,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Step 1: Enable EC2 to <em>install</em> the infrastructure agent",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/installation/update-infrastructure-host-integration-package": [
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 196.58345,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the <em>installation</em>. Scale the number of instances"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.8667,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.84315,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Step 1: Enable EC2 to <em>install</em> the infrastructure agent",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/open-source-host-integrations-list/f5-open-source-integration": [
    {
      "sections": [
        "F5 monitoring integration",
        "Compatibility and requirements",
        "F5 BIG-IP users and privileges",
        "Tip",
        "Install and activate",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "System sample metrics",
        "Virtual server sample metrics",
        "Pool sample metrics",
        "Pool member sample metrics",
        "Node sample metrics",
        "Inventory data",
        "Pool Inventory",
        "Node inventory",
        "Pool Member Inventory",
        "Virtual Server Inventory",
        "System Inventory",
        "Application Inventory",
        "Check the source code"
      ],
      "title": "F5 monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "86250de7e0529371148dab5e96960893b88288b8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/f5-monitoring-integration/",
      "published_at": "2021-05-04T16:33:16Z",
      "updated_at": "2021-03-11T12:49:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our F5 BIG-IP integration collects and sends inventory and metrics from your F5 BIG-IP instance to our platform, where you can aggregate and visualize key performance metrics. We collect data at the system, application, pool, pool member, virtual server, and node levels. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with F5 BIG-IP 11.6 or higher. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent. Linux distribution or Windows version compatible with the infrastructure agent. F5 BIG-IP user account with Access Auditor-level user privileges and iControl REST API access permissions. F5 BIG-IP users and privileges To create a new user and assign user permissions: Create a user account with, at minimum, Access Auditor-level permissions. For instructions, see the F5 official documentation. Once the user has been created, assign the user iControl REST user permissions. Tip Administrator-level permissions may be required to collect some system sample metrics or system inventory configuration data. For more information on user permission levels, see User role access descriptions. Tip For detailed information on iControl users and permission, download and review the iControl REST User Guide. Install and activate To install the F5 BIG-IP integration, choose your setup: Linux installation Follow the instructions for installing an integration, using the file name nri-f5. Change the directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp f5-config.yml.sample f5-config.yml Copy Edit the f5-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows installation Download the nri-f5 MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-f5/nri-f5-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-f5-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: copy f5-config.yml.sample f5-config.yml Copy Edit the f5-config.yml file as described in the configuration settings. Restart the infrastructure agent. Additional notes: We recommend you install the integration on a separate server and monitor F5 remotely. Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. For an example of the configuration file, see the example config file. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The f5-config.yml file accepts the following commands: all_data: collects both inventory and metrics for the BIG-IP instance. inventory: collects only the inventory (configuration) data for the BIG-IP instance. metrics: collects only the metrics data for the BIG-IP instance. Arguments The f5-config.yml commands accept the following arguments: username: The username for the F5 BIG-IP connection. This field is required. password: The password for the F5 BIG-IP connection. This field is required. hostname: The hostname for the F5 BIG-IP connection. Default: f5-host. port: The port on which F5 BIG-IP instance is running. Default: 443. timeout: The number of seconds to wait before a request times out. Default: 30. ca_bundle_file: Alternative certificate authority bundle file. ca_bundle_dir: Alternative certificate authority bundle directory. partition_filter: An array of the partitions to collect from, in JSON. Default: '[\"Common\"]'. Example configuration Example f5-config.yml file configuration: Example configuration integration_name: com.newrelic.f5 instances: - name: nri-f5 # command can be all_data, metrics, or inventory command: all_data arguments: # Username of the F5 instance username: admin # Password of the F5 instance password: admin # Hostname of the F5 instance hostname: f5-host # Port of the F5 instance port: 443 # CA certificate file ca_bundle_file: /etc/ca_certificate.crt # A JSON array of BIG-IP partitions to collect from. # The partition name should have no leading slash. # Defaults to '[\"Common\"]' partition_filter: '[\"Common\",\"MyOtherPartition\"]' # The number of seconds to wait before a request times out # Defaults to 30 timeout: 10 Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data To find your integration data, go to infrastructure.newrelic.com > Third-party services and select one of the F5 BIG-IP integration links. In New Relic Insights, F5 BIG-IP data is attached to the following Insights event types: F5BigIpSystemSample F5BigIpVirtualServerSample F5BigIpPoolSample F5BigIpPoolMemberSample F5BigIpNodeSample For more on how to find and use your data, see Understand integration data. Metric data The F5 BIG-IP integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as system., virtualserver., or pool.. System sample metrics These attributes can be found by querying the F5BigIpSystemSample event types. Metric Description system.cpuIdleTicksPerSecond Amount of CPU ticks that the CPU was idle per second. Requires Administrator-level user permissions to collect. system.cpuIdleUtilization Average percentage of time the CPU is idle. system.cpuInterruptRequestUtilization Average percentage of time the CPU is handling interrupt requests. system.cpuIOWaitUtilization Average percentage of time the CPU is waiting on IO. system.cpuNiceLevelUtilization Average percentage of time the CPU is handling nice level processes. system.cpuSoftInterruptRequestUtilization Average percentage of time the CPU is handling soft interrupt requests. system.cpuStolenUtilization Average percentage of time the CPU is handling reclaimed cycles by the hypervisor. system.cpuSystemTicksPerSecond Amount of CPU ticks used by the kernel processes per second. Requires Administrator-level user permissions to collect. system.cpuSystemUtilization Average percentage of time the CPU is used by the kernel. system.cpuUserTicksPerSecond Amount of CPU ticks used by user processes per second. Requires Administrator-level user permissions to collect. system.cpuUserUtilization Average percentage of time the CPU is used by user processes. system.memoryFreeInBytes Total amount of memory free, in bytes. system.memoryTotalInBytes Total amount of memory, in bytes. Requires Administrator-level user permissions to collect. system.memoryUsedInBytes Total amount of memory used, in bytes. Requires Administrator-level user permissions to collect. system.otherMemoryFreeInBytes Free memory reserved for control plane processes, in bytes. system.otherMemoryTotalInBytes Total memory reserved for control plane processes, in bytes. system.otherMemoryUsedInBytes Used memory reserved for control plane processes, in bytes. system.swapFreeInBytes Swap space free, in bytes. system.swapTotalInBytes Swap space total, in bytes. system.swapUsedInBytes Swap space used, in bytes. system.tmmMemoryFreeInBytes Free memory reserved for Traffic Management Microkernel (TMM), in bytes. system.tmmMemoryTotalInBytes Total memory reserved for Traffic Management Microkernel (TMM), in bytes. system.tmmMemoryUsedInBytes Used memory reserved for Traffic Management Microkernel (TMM), in bytes. Virtual server sample metrics These attributes can be found by querying the F5BigIpVirtualServerSample event types in Insights. Metric Description virtualserver.avaibilityState The BIG-IP defined availability. Options: 0 = Offline 1 = Unknown 2 = Online virtualserver.clientsideConnectionsPerSecond The rate of connections created through the client side of the object per second. virtualserver.cmpEnabled Indicates whether or not Cluster Multiprocessing (CMP) is enabled. virtualserver.cmpEnableMode Shows the Cluster Multiprocessing (CMP) mode indicators. Options: CMP disabled = none, disable, or single. CMP enabled = enable or all. virtualserver.connections The current number of connections from BIG-IP. virtualserver.csMaxConnDur Maximum connection duration from the client side of the object. virtualserver.csMinConnDur Minimum connection duration from the client side of the object. virtualserver.enabled The current enabled state. Options: 0 = Disabled 1 = Enabled virtualserver.ephemeralBytesInPerSecond Total number of bytes in through the ephemeral port per second. virtualserver.ephemeralBytesOutPerSecond Total number of bytes out through the ephemeral port per second. virtualserver.ephemeralConnectionsPerSecond The rate of connection creation through the ephemeral port per second. virtualserver.ephemeralCurrentConnections The current number of connections through the ephemeral port. virtualserver.ephemeralEvictedConnectionsPerSecond The number of connections that are evicted through the ephemeral port per second. virtualserver.ephemeralMaxConnections Maximum number of connections through the ephemeral port. virtualserver.ephemeralPacketsReceivedPerSecond The number of packets in through the ephemeral port per second. virtualserver.ephemeralPacketsSentPerSecond The number of packets out through the ephemeral port per second. virtualserver.ephemeralSlowKilledPerSecond The number of slow connections that are killed through the ephemeral port per second. virtualserver.evictedConnsPerSecond The rate of connections evicted per second. virtualserver.inDataInBytes The amount of data received from the BIG-IP virtual server, in bytes. virtualserver.outDataInBytes The amount of data sent to the BIG-IP virtual server, in bytes. virtualserver.packetsReceived The number of packets received from the BIG-IP virtual server. virtualserver.packetsSent The number of packets sent to the BIG-IP virtual server. virtualserver.requests The number of requests in the last collection interval to BIG-IP. virtualserver.slowKilledPerSecond The number of slow connections killed through the client side of the object per second. virtualserver.statusReason An explanation of the current status. virtualserver.usageRatio The usage ratio for the virtual server. Pool sample metrics These attributes can be found by querying the F5BigIpPoolSample event types in Insights. Metric Description pool.activeMembers The number of active pool members. pool.availabilityState The current availability state. Options: 0 = Offline 1 = Unknown 2 = Online pool.connections The current number of connections. pool.connqAgeEdm The queue age exponential-decaying max. pool.connqAgeEma The queue age exponential-moving average. pool.connqAgeHead The current queue age head. pool.connqAgeMax The queue age all-time max. pool.connqAllAgeEdm The sum of pool member queue age exponential-decaying max. pool.connqAllAgeEma The sum of pool member queue age exponential-moving average. pool.connqAllAgeHead The sum of pool member queue age head. pool.connqAllAgeMax The sum of pool member queue age all-time max. pool.connqAllDepth The sum of pool member depth. pool.connqDepth The queue depth. pool.currentConnections The current connections. pool.enabled The current enabled state, can be user defined. Options: 0 = Disabled 1 = Enabled pool.inDataInBytes The amount of data received from the BIG-IP pool, in bytes. pool.minActiveMembers Pool minimum active members. pool.outDataInBytes The amount of data sent to the BIG-IP pool, in bytes. pool.packetsReceived The number of packets received from the BIG-IP pool. pool.packetsSent The number of packets sent to the BIG-IP pool. pool.requests The total number of requests to the pool. pool.statusReason Textual property explaining the overall health reason. Pool member sample metrics These attributes can be found by querying the F5BigIpPoolMemberSample event types in Insights. Metric Description member.availabilityState The current availability from the BIG-IP system. Options: 0 = Offline 1 = Unknown 2 = Online member.connections The current connections. member.enabled Enabled state of the pool member with regards to the parent pool. Options: 0 = Disabled 1 = Enabled member.inDataInBytes The amount of data received from the BIG-IP pool member, in bytes. member.monitorStatus The status of the monitor. Options: 0 = Down 1 = Unchecked 2 = Any other status member.outDataInBytes The amount of data sent to the BIG-IP pool member, in bytes. member.packetsReceived The number of packets received from the BIG-IP pool member. member.packetsSent The number of packets sent to the BIG-IP pool member. member.requests The current number of requests over the last collection interval. member.sessions The current session count. member.sessionStatus The current session health status. Options: 0 = Disabled 1 = Enabled member.state The current state. Options: 0 = Down 1 = Up member.statusReason Explanation of the current status. Node sample metrics These attributes can be found by querying the F5BigIpNodeSample event types in Insights. Metric Description node.availabilityState The current BIG-IP availability state to the node. Options: 0 = Offline 1 = Unknown 2 = Online node.connections The current number of network connections from BIG-IP. node.connectionsPerSecond The number of connections made per second. node.enabled The current BIG-IP enabled state. Options: 0 = Disabled 1 = Enabled , node.inDataInBytes The amount of data received from the BIG-IP node, in bytes. node.monitorStatus The current health monitor rule status. Options: 0 = Down 1 = Unchecked 2 = Any other status node.outDataInBytes The amount of data sent to the BIG-IP node, in bytes. node.packetsReceived The number of packets received from the BIG-IP node. node.packetsSent The number of packets sent to the BIG-IP node. node.requests The current number of requests over the last collection from BIG-IP. node.sessions The current number of sessions. node.sessionStatus The current status of the session. Options: 0 = Disabled 1 = Enabled node.statusReason BIG-IP reason for the current status. Inventory data The F5 BIG-IP integration also collects configuration data at system, application, pool, pool member, virtual server, and node levels. The data is available on the Infrastructure Inventory page, under the config/f5 source. For more about inventory data, see Understand integration data. The integration captures data for the following F5 BIG-IP configuration parameters: Pool Inventory Metric Description currentLoadMode Current load balancing mode. description User defined description. kind Kind of pool. maxConnections Current max number of connections seen at one point. monitorRule Current health monitoring rule applied. Node inventory Metric Description address BIG-IP network address to send to the node. fqdn FQDN of node. kind Type of Node in BIG-IP. maxConnections Current highest number of network connections reported from BIG-IP. monitorRule BIG-IP Health Monitor rule. Pool Member Inventory Metric Description kind Type of Pool member. maxConnections Current highest number of network connections reported from BIG-IP. monitorRule BIG-IP health monitor rule. nodeName Name of the node the pool member is using. poolName Name of the pool the pool member belongs. port Port the pool member listens on. Virtual Server Inventory Metric Description applicationService Current application service assigned. destination Destination address picked up by BIG-IP. kind Type of virtual server. maxConnections Current highest number of network connections reported from BIG-IP. name User defined name. pool Pool the virtual server uses for load balancing. System Inventory Metric Description chassisSerialNumber Chassis Serial Number for the current device. Requires Access Administrator-level user permissions to collect. platform Platform of the current device. Requires Access Administrator-level user permissions to collect. product Product Name for the current device. Requires Access Administrator-level user permissions to collect. Application Inventory Metric Description deviceGroup Device group running application service. kind BIG-IP Defined type. name User defined name. poolToUse Server side pool load balancing requests. template Template applied to application including security and monitoring rules. templateModified Indicator of modifications made to out of the box template. trafficGroup Current traffic group to which service is applied. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 239.49161,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>F5</em> monitoring <em>integration</em>",
        "sections": "<em>F5</em> monitoring <em>integration</em>",
        "tags": "<em>Integrations</em>",
        "body": " for the current status. Inventory data The <em>F5</em> BIG-IP <em>integration</em> also collects configuration data at system, application, pool, pool member, virtual server, and node levels. The data is available on the Infrastructure Inventory page, under the config&#x2F;<em>f5</em> <em>source</em>. For more about inventory data, see"
      },
      "id": "6044e41ce7b9d2f0975799b4"
    },
    {
      "sections": [
        "OpenTelemetry quick start",
        "Step 1. Prerequisites",
        "Step 2. Instrument your service with OpenTelemetry",
        "Tip",
        "Step 3. Send your telemetry data to New Relic",
        "Important",
        "Use the OpenTelemetry collector (recommended)",
        "Use the native OTLP endpoint (pre-release)",
        "Step 4: View your data in the New Relic UI"
      ],
      "title": "OpenTelemetry quick start",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "1b846417a2958b61b047c838db49aea06f09a2a8",
      "image": "https://docs.newrelic.com/static/820ec30261e57dd485d471987fae4a28/0f2bc/collector_introduction_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start/",
      "published_at": "2021-05-05T00:34:35Z",
      "updated_at": "2021-05-05T00:34:35Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up OpenTelemetry with New Relic. Here's an overview of the process, followed by details for each step. Prerequisites Instrument your service with OpenTelemetry Send your telemetry data to New Relic View your data in the New Relic UI In the following sections, we explain some basic architectural approaches, but if you want to explore other implementation options, check out OpenTelemetry architecture recipes. Step 1. Prerequisites First things first: If we dont already know you, sign up for a free New Relic account. Make sure you have an Insights insert key to send spans and metrics to New Relic. Step 2. Instrument your service with OpenTelemetry To get started, you instrument your service with OpenTelemetry. OpenTelemetry has language-specific products and SDKs to help you. Many languages offer out-the-box instrumentation for common libraries and frameworks. Each language also provides an API for further instrumenting your service manually. Tip We recommend that you instrument as many services as possible to get the most benefit from distributed tracing. Go to the repository for your language and follow the instructions to instrument your service. When you're done, return here to complete the next step of sending your telemetry data to New Relic. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...See a complete list of languages in GitHub Step 3. Send your telemetry data to New Relic Choose how you want to export your telemetry data to New Relic: Use the OpenTelemetry collector (recommended) Use the native OTLP endpoint (pre-release) You can use one or both of these approaches in your environment. We'll discuss some highlights of each approach here, but if you need more background, see Introduction to OpenTelemetry. If you are interested in tracing, there are two main options for trace sampling: Configure the head-based, native sampling in OpenTelemetry, which means OpenTelemetry samples traces before they are sent to New Relic. Head-based sampling doesnt analyze all traces, but instead randomly samples traces up front before details about the completed traces are known. Both the OpenTelemetry collector and the native OTLP endpoint support this option. If you want New Relic to analyze all your traces, configure tail-based sampling with New Relic Infinite Tracing, which reroutes traces to our cloud-based trace observer. The trace observer accepts all your traces and sorts through them to find useful ones. If you want to know more about this option, especially if you want to use it in the EU, see Introduction to Infinite Tracing. While Infinite Tracing is not yet compatible with the native OTLP endpoint, it is still possible to configure tail-based sampling via the collector, for more information see Tail Sampling Processor. Important New Relic's language-specific exporters for OpenTelemetry are now deprecated in favor of the OpenTelemetry collector and native OTLP endpoint options described here. If you were previously using a New Relic language-specific exporter consider using the OTLP exporter for your language and send data directly to New Relic's native OTLP endpoint (pre-release). Use the OpenTelemetry collector (recommended) The OpenTelemetry project provides a tool called the OpenTelemetry Collector that you can deploy and use as an intermediate data aggregator. In your service, you use the OpenTelemetry exporter to send telemetry data first to the OpenTelemetry collector. Then, in the OpenTelemetry collector, you enable the New Relic exporter to send data to New Relic. The diagram below shows the flow of data with the collector. To use the collector: Configure your OpenTelemetry collector to export data to New Relic, using our example as a guide. Configure your services OTLP exporter to send data to your collector, following the documentation for your language's OTLP exporter: C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...Find additional OTLP language support in GitHub Use the native OTLP endpoint (pre-release) The example above uses a New Relic exporter, but we have a pre-release program if you want to try out the native OTLP endpoint for sending your data to New Relic. You can either use the OTLP exporter in the OpenTelemetry collector or send us data directly from your service. If you are interested, let us know by completing this form. Step 4: View your data in the New Relic UI Once youve instrumented your service and configured it to export its data to New Relic, you can go to New Relic and view your data. The UI for OpenTelemetry has some similarities to the APM agent UI, so if you are familiar with that, you can go right to the UI. If you need help understanding your OpenTelemetry UI options, see View your OpenTelemetry data in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 223.19522,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> quick start",
        "sections": "<em>OpenTelemetry</em> quick start",
        "tags": "<em>Open</em> <em>source</em> telemetry <em>integrations</em>",
        "body": "<em>Open</em>Telemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up <em>Open</em>Telemetry with New Relic. Here&#x27;s an overview of the process, followed by details for each step. Prerequisites Instrument your service with <em>Open</em>Telemetry Send"
      },
      "id": "6044e5dfe7b9d2aadc5799d4"
    },
    {
      "sections": [
        "On-host integrations metrics",
        "New Relic Integrations Metrics"
      ],
      "title": "On-host integrations metrics",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "fe96c0c4950380504b1a33c3ad861bcb17507cba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/host-integrations-metrics/",
      "published_at": "2021-05-05T03:38:23Z",
      "updated_at": "2021-03-16T15:57:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic Integrations Metrics The following table contains the metrics we collect for our infrastructure integrations. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Agent host.cpuIdlePercent cpuIdlePercent Agent host.cpuIoWaitPercent cpuIOWaitPercent Agent host.cpuPercent cpuPercent Agent host.cpuStealPercent cpuStealPercent Agent host.cpuSystemPercent cpuSystemPercent Agent host.cpuUserPercent cpuUserPercent Agent host.disk.avgQueueLen avgQueueLen Agent host.disk.avgReadQueueLen avgReadQueueLen Agent host.disk.avgWriteQueueLen avgWriteQueueLen Agent host.disk.currentQueueLen currentQueueLen Agent host.disk.freeBytes diskFreeBytes Agent host.disk.freePercent diskFreePercent Agent host.disk.inodesFree inodesFree Agent host.disk.inodesTotal inodesTotal Agent host.disk.inodesUsed inodesUsed Agent host.disk.inodesUsedPercent inodesUsedPercent Agent host.disk.readBytesPerSecond readBytesPerSecond Agent host.disk.readIoPerSecond readIoPerSecond Agent host.disk.readUtilizationPercent readUtilizationPercent Agent host.disk.readWriteBytesPerSecond readWriteBytesPerSecond Agent host.disk.totalBytes diskTotalBytes Agent host.disk.totalUtilizationPercent totalUtilizationPercent Agent host.disk.usedBytes diskUsedBytes Agent host.disk.usedPercent diskUsedPercent Agent host.disk.writeBytesPerSecond writeBytesPerSecond Agent host.disk.writeIoPerSecond writeIoPerSecond Agent host.disk.writeUtilizationPercent writeUtilizationPercent Agent host.diskFreeBytes diskFreeBytes Agent host.diskFreePercent diskFreePercent Agent host.diskReadsPerSecond diskReadsPerSecond Agent host.diskReadUtilizationPercent diskReadUtilizationPercent Agent host.diskTotalBytes diskTotalBytes Agent host.diskUsedBytes diskUsedBytes Agent host.diskUsedPercent diskUsedPercent Agent host.diskUtilizationPercent diskUtilizationPercent Agent host.diskWritesPerSecond diskWritesPerSecond Agent host.diskWriteUtilizationPercent diskWriteUtilizationPercent Agent host.loadAverageFifteenMinute loadAverageFifteenMinute Agent host.loadAverageFiveMinute loadAverageFiveMinute Agent host.loadAverageOneMinute loadAverageOneMinute Agent host.memoryFreeBytes memoryFreeBytes Agent host.memoryFreePercent memoryFreePercent Agent host.memoryTotalBytes memoryTotalBytes Agent host.memoryUsedBytes memoryUsedBytes Agent host.memoryUsedPercent memoryUsedPercent Agent host.net.receiveBytesPerSecond receiveBytesPerSecond Agent host.net.receiveDroppedPerSecond receiveDroppedPerSecond Agent host.net.receiveErrorsPerSecond receiveErrorsPerSecond Agent host.net.receivePacketsPerSecond receivePacketsPerSecond Agent host.net.transmitBytesPerSecond transmitBytesPerSecond Agent host.net.transmitDroppedPerSecond transmitDroppedPerSecond Agent host.net.transmitErrorsPerSecond transmitErrorsPerSecond Agent host.net.transmitPacketsPerSecond transmitPacketsPerSecond Agent host.process.cpuPercent cpuPercent Agent host.process.cpuSystemPercent cpuSystemPercent Agent host.process.cpuUserPercent cpuUserPercent Agent host.process.fileDescriptorCount fileDescriptorCount Agent host.process.ioReadBytesPerSecond ioReadBytesPerSecond Agent host.process.ioReadCountPerSecond ioReadCountPerSecond Agent host.process.ioTotalReadBytes ioTotalReadBytes Agent host.process.ioTotalReadCount ioTotalReadCount Agent host.process.ioTotalWriteBytes ioTotalWriteBytes Agent host.process.ioTotalWriteCount ioTotalWriteCount Agent host.process.ioWriteBytesPerSecond ioWriteBytesPerSecond Agent host.process.ioWriteCountPerSecond ioWriteCountPerSecond Agent host.process.memoryResidentSizeBytes memoryResidentSizeBytes Agent host.process.memoryVirtualSizeBytes memoryVirtualSizeBytes Agent host.process.threadCount threadCount Agent host.swapFreeBytes swapFreeBytes Agent host.swapTotalBytes swapTotalBytes Agent host.swapUsedBytes swapUsedBytes Apache apache.server.busyWorkers server.busyWorkers Apache apache.server.idleWorkers server.idleWorkers Apache apache.server.net.bytesPerSecond net.bytesPerSecond Apache apache.server.net.requestsPerSecond net.requestsPerSecond Apache apache.server.scoreboard.closingWorkers server.scoreboard.closingWorkers Apache apache.server.scoreboard.dnsLookupWorkers server.scoreboard.dnsLookupWorkers Apache apache.server.scoreboard.finishingWorkers server.scoreboard.finishingWorkers Apache apache.server.scoreboard.idleCleanupWorkers server.scoreboard.idleCleanupWorkers Apache apache.server.scoreboard.keepAliveWorkers server.scoreboard.keepAliveWorkers Apache apache.server.scoreboard.loggingWorkers server.scoreboard.loggingWorkers Apache apache.server.scoreboard.readingWorkers server.scoreboard.readingWorkers Apache apache.server.scoreboard.startingWorkers server.scoreboard.startingWorkers Apache apache.server.scoreboard.totalWorkers server.scoreboard.totalWorkers Apache apache.server.scoreboard.writingWorkers server.scoreboard.writingWorkers Cassandra cassandra.node.allMemtablesOffHeapSizeBytes db.allMemtablesOffHeapSizeBytes Cassandra cassandra.node.allMemtablesOnHeapSizeBytes db.allMemtablesOnHeapSizeBytes Cassandra cassandra.node.client.connectedNativeClients client.connectedNativeClients Cassandra cassandra.node.commitLogCompletedTasksPerSecond db.commitLogCompletedTasksPerSecond Cassandra cassandra.node.commitLogPendingTasks db.commitLogPendindTasks Cassandra cassandra.node.commitLogTotalSizeBytes db.commitLogTotalSizeBytes Cassandra cassandra.node.droppedBatchRemoveMessagesPerSecond db.droppedBatchRemoveMessagesPerSecond Cassandra cassandra.node.droppedBatchStoreMessagesPerSecond db.droppedBatchStoreMessagesPerSecond Cassandra cassandra.node.droppedCounterMutationMessagesPerSecond db.droppedCounterMutationMessagesPerSecond Cassandra cassandra.node.droppedHintMessagesPerSecond db.droppedHintMessagesPerSecond Cassandra cassandra.node.droppedMutationMessagesPerSecond db.droppedMutationMessagesPerSecond Cassandra cassandra.node.droppedPagedRangeMessagesPerSecond db.droppedPagedRangeMessagesPerSecond Cassandra cassandra.node.droppedRangeSliceMessagesPerSecond db.droppedRangeSliceMessagesPerSecond Cassandra cassandra.node.droppedReadMessagesPerSecond db.droppedReadMessagesPerSecond Cassandra cassandra.node.droppedReadRepairMessagesPerSecond db.droppedReadRepairMessagesPerSecond Cassandra cassandra.node.droppedRequestResponseMessagesPerSecond db.droppedRequestResponseMessagesPerSecond Cassandra cassandra.node.droppedTraceMessagesPerSecond db.droppedTraceMessagesPerSecond Cassandra cassandra.node.keyCacheCapacityBytes db.keyCacheCapacityBytes Cassandra cassandra.node.keyCacheHitRate db.keyCacheHitRate Cassandra cassandra.node.keyCacheHitsPerSecond db.keyCacheHitsPerSecond Cassandra cassandra.node.keyCacheRequestsPerSecond db.keyCacheRequestsPerSecond Cassandra cassandra.node.keyCacheSizeBytes db.keyCacheSizeBytes Cassandra cassandra.node.liveSsTableCount db.liveSSTableCount Cassandra cassandra.node.loadBytes db.loadBytes Cassandra cassandra.node.query.casReadRequestsPerSecond query.CASReadRequestsPerSecond Cassandra cassandra.node.query.casWriteRequestsPerSecond query.CASWriteRequestsPerSecond Cassandra cassandra.node.query.rangeSliceRequestsPerSecond query.rangeSliceRequestsPerSecond Cassandra cassandra.node.query.rangeSliceTimeoutsPerSecond query.rangeSliceTimeoutsPerSecond Cassandra cassandra.node.query.rangeSliceUnavailablesPerSecond query.rangeSliceUnavailablesPerSecond Cassandra cassandra.node.query.readLatency50ThPercentileMilliseconds query.readLatency50thPercentileMilliseconds Cassandra cassandra.node.query.readLatency75ThPercentileMilliseconds query.readLatency75thPercentileMilliseconds Cassandra cassandra.node.query.readLatency95ThPercentileMilliseconds query.readLatency95thPercentileMilliseconds Cassandra cassandra.node.query.readLatency98ThPercentileMilliseconds query.readLatency98thPercentileMilliseconds Cassandra cassandra.node.query.readLatency999ThPercentileMilliseconds query.readLatency999thPercentileMilliseconds Cassandra cassandra.node.query.readLatency99ThPercentileMilliseconds query.readLatency99thPercentileMilliseconds Cassandra cassandra.node.query.readRequestsPerSecond query.readRequestsPerSecond Cassandra cassandra.node.query.readTimeoutsPerSecond query.readTimeoutsPerSecond Cassandra cassandra.node.query.readUnavailablesPerSecond query.readUnavailablesPerSecond Cassandra cassandra.node.query.viewWriteRequestsPerSecond query.viewWriteRequestsPerSecond Cassandra cassandra.node.query.writeLatency50ThPercentileMilliseconds query.writeLatency50thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency75ThPercentileMilliseconds query.writeLatency75thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency95ThPercentileMilliseconds query.writeLatency95thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency98ThPercentileMilliseconds query.writeLatency98thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency999ThPercentileMilliseconds query.writeLatency999thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency99ThPercentileMilliseconds query.writeLatency99thPercentileMilliseconds Cassandra cassandra.node.query.writeRequestsPerSecond query.writeRequestsPerSecond Cassandra cassandra.node.query.writeTimeoutsPerSecond query.writeTimeoutsPerSecond Cassandra cassandra.node.query.writeUnavailablesPerSecond query.writeUnavailablesPerSecond Cassandra cassandra.node.rowCacheCapacityBytes db.rowCacheCapacityBytes Cassandra cassandra.node.rowCacheHitRate db.rowCacheHitRate Cassandra cassandra.node.rowCacheHitsPerSecond db.rowCacheHitsPerSecond Cassandra cassandra.node.rowCacheRequestsPerSecond db.rowCacheRequestsPerSecond Cassandra cassandra.node.rowCacheSizeBytes db.rowCacheSizeBytes Cassandra cassandra.node.storage.exceptionCount storage.exceptionCount Cassandra cassandra.node.threadPool.antiEntropyStage.activeTasks db.threadpool.internalAntiEntropyStageActiveTasks Cassandra cassandra.node.threadPool.antiEntropyStage.completedTasks db.threadpool.internalAntiEntropyStageCompletedTasks Cassandra cassandra.node.threadPool.antiEntropyStage.currentlyBlockedTasks db.threadpool.internalAntiEntropyStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.antiEntropyStage.pendingTasks db.threadpool.internalAntiEntropyStagePendingTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.activeTasks db.threadpool.internalCacheCleanupExecutorActiveTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.completedTasks db.threadpool.internalCacheCleanupExecutorCompletedTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.currentlyBlockedTasks db.threadpool.internalCacheCleanupExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.pendingTasks db.threadpool.internalCacheCleanupExecutorPendingTasks Cassandra cassandra.node.threadPool.compactionExecutor.activeTasks db.threadpool.internalCompactionExecutorActiveTasks Cassandra cassandra.node.threadPool.compactionExecutor.completedTasks db.threadpool.internalCompactionExecutorCompletedTasks Cassandra cassandra.node.threadPool.compactionExecutor.currentlyBlockedTasks db.threadpool.internalCompactionExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.compactionExecutor.pendingTasks db.threadpool.internalCompactionExecutorPendingTasks Cassandra cassandra.node.threadPool.counterMutationStage.activeTasks db.threadpool.requestCounterMutationStageActiveTasks Cassandra cassandra.node.threadPool.counterMutationStage.completedTasks db.threadpool.requestCounterMutationStageCompletedTasks Cassandra cassandra.node.threadPool.counterMutationStage.currentlyBlockedTasks db.threadpool.requestCounterMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.counterMutationStage.pendingTasks db.threadpool.requestCounterMutationStagePendingTasks Cassandra cassandra.node.threadPool.gossipStage.activeTasks db.threadpool.internalGossipStageActiveTasks Cassandra cassandra.node.threadPool.gossipStage.completedTasks db.threadpool.internalGossipStageCompletedTasks Cassandra cassandra.node.threadPool.gossipStage.currentlyBlockedTasks db.threadpool.internalGossipStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.gossipStage.pendingTasks db.threadpool.internalGossipStagePendingTasks Cassandra cassandra.node.threadPool.hintsDispatcher.activeTasks db.threadpool.internalHintsDispatcherActiveTasks Cassandra cassandra.node.threadPool.hintsDispatcher.completedTasks db.threadpool.internalHintsDispatcherCompletedTasks Cassandra cassandra.node.threadPool.hintsDispatcher.currentlyBlockedTasks db.threadpool.internalHintsDispatcherCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.hintsDispatcher.pendingTasks db.threadpool.internalHintsDispatcherPendingTasks Cassandra cassandra.node.threadPool.internalResponseStage.activeTasks db.threadpool.internalInternalResponseStageActiveTasks Cassandra cassandra.node.threadPool.internalResponseStage.completedTasks db.threadpool.internalInternalResponseStageCompletedTasks Cassandra cassandra.node.threadPool.internalResponseStage.pCurrentlyBlockedTasks db.threadpool.internalInternalResponseStagePCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.internalResponseStage.pendingTasks db.threadpool.internalInternalResponseStagePendingTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.activeTasks db.threadpool.internalMemtableFlushWriterActiveTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.completedTasks db.threadpool.internalMemtableFlushWriterCompletedTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.currentlyBlockedTasks db.threadpool.internalMemtableFlushWriterCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.pendingTasks db.threadpool.internalMemtableFlushWriterPendingTasks Cassandra cassandra.node.threadPool.memtablePostFlush.activeTasks db.threadpool.internalMemtablePostFlushActiveTasks Cassandra cassandra.node.threadPool.memtablePostFlush.completedTasks db.threadpool.internalMemtablePostFlushCompletedTasks Cassandra cassandra.node.threadPool.memtablePostFlush.currentlyBlockedTasks db.threadpool.internalMemtablePostFlushCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtablePostFlush.pendingTasks db.threadpool.internalMemtablePostFlushPendingTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.activeTasks db.threadpool.internalMemtableReclaimMemoryActiveTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.completedTasks db.threadpool.internalMemtableReclaimMemoryCompletedTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.currentlyBlockedTasks db.threadpool.internalMemtableReclaimMemoryCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.pendingTasks db.threadpool.internalMemtableReclaimMemoryPendingTasks Cassandra cassandra.node.threadPool.migrationStage.activeTasks db.threadpool.internalMigrationStageActiveTasks Cassandra cassandra.node.threadPool.migrationStage.completedTasks db.threadpool.internalMigrationStageCompletedTasks Cassandra cassandra.node.threadPool.migrationStage.currentlyBlockedTasks db.threadpool.internalMigrationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.migrationStage.pendingTasks db.threadpool.internalMigrationStagePendingTasks Cassandra cassandra.node.threadPool.miscStage.activeTasks db.threadpool.internalMiscStageActiveTasks Cassandra cassandra.node.threadPool.miscStage.completedTasks db.threadpool.internalMiscStageCompletedTasks Cassandra cassandra.node.threadPool.miscStage.currentlyBlockedTasks db.threadpool.internalMiscStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.miscStage.pendingTasks db.threadpool.internalMiscStagePendingTasks Cassandra cassandra.node.threadPool.mutationStage.activeTasks db.threadpool.requestMutationStageActiveTasks Cassandra cassandra.node.threadPool.mutationStage.completedTasks db.threadpool.requestMutationStageCompletedTasks Cassandra cassandra.node.threadPool.mutationStage.currentlyBlockedTasks db.threadpool.requestMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.mutationStage.pendingTasks db.threadpool.requestMutationStagePendingTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.activeTasks db.threadpool.internalPendingRangeCalculatorActiveTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.completedTasks db.threadpool.internalPendingRangeCalculatorCompletedTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.currentlyBlockedTasks db.threadpool.internalPendingRangeCalculatorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.pendingTasks db.threadpool.internalPendingRangeCalculatorPendingTasks Cassandra cassandra.node.threadPool.readRepairStage.activeTasks db.threadpool.requestReadRepairStageActiveTasks Cassandra cassandra.node.threadPool.readRepairStage.completedTasks db.threadpool.requestReadRepairStageCompletedTasks Cassandra cassandra.node.threadPool.readRepairStage.currentlyBlockedTasks db.threadpool.requestReadRepairStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.readRepairStage.pendingTasks db.threadpool.requestReadRepairStagePendingTasks Cassandra cassandra.node.threadPool.readStage.activeTasks db.threadpool.requestReadStageActiveTasks Cassandra cassandra.node.threadPool.readStage.completedTasks db.threadpool.requestReadStageCompletedTasks Cassandra cassandra.node.threadPool.readStage.currentlyBlockedTasks db.threadpool.requestReadStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.readStage.pendingTasks db.threadpool.requestReadStagePendingTasks Cassandra cassandra.node.threadPool.requestResponseStage.activeTasks db.threadpool.requestRequestResponseStageActiveTasks Cassandra cassandra.node.threadPool.requestResponseStage.completedTasks db.threadpool.requestRequestResponseStageCompletedTasks Cassandra cassandra.node.threadPool.requestResponseStage.currentlyBlockedTasks db.threadpool.requestRequestResponseStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.requestResponseStage.pendingTasks db.threadpool.requestRequestResponseStagePendingTasks Cassandra cassandra.node.threadPool.sampler.activeTasks db.threadpool.internalSamplerActiveTasks Cassandra cassandra.node.threadPool.sampler.completedTasks db.threadpool.internalSamplerCompletedTasks Cassandra cassandra.node.threadPool.sampler.currentlyBlockedTasks db.threadpool.internalSamplerCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.sampler.pendingTasks db.threadpool.internalSamplerPendingTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.activeTasks db.threadpool.internalSecondaryIndexManagementActiveTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.completedTasks db.threadpool.internalSecondaryIndexManagementCompletedTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.currentlyBlockedTasks db.threadpool.internalSecondaryIndexManagementCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.pendingTasks db.threadpool.internalSecondaryIndexManagementPendingTasks Cassandra cassandra.node.threadPool.validationExecutor.activeTasks db.threadpool.internalValidationExecutorActiveTasks Cassandra cassandra.node.threadPool.validationExecutor.completedTasks db.threadpool.internalValidationExecutorCompletedTasks Cassandra cassandra.node.threadPool.validationExecutor.currentlyBlockedTasks db.threadpool.internalValidationExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.validationExecutor.pendingTasks db.threadpool.internalValidationExecutorPendingTasks Cassandra cassandra.node.threadPool.viewMutationStage.activeTasks db.threadpool.requestViewMutationStageActiveTasks Cassandra cassandra.node.threadPool.viewMutationStage.completedTasks db.threadpool.requestViewMutationStageCompletedTasks Cassandra cassandra.node.threadPool.viewMutationStage.currentlyBlockedTasks db.threadpool.requestViewMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.viewMutationStage.pendingTasks db.threadpool.requestViewMutationStagePendingTasks Cassandra cassandra.node.totalHintsInProgress db.totalHintsInProgress Cassandra cassandra.node.totalHintsPerSecond db.totalHintsPerSecond Cassandra cassandra.columnFamily.allMemtablesOffHeapSizeBytes db.allMemtablesOffHeapSizeBytes Cassandra cassandra.columnFamily.allMemtablesOnHeapSizeBytes db.allMemtablesOnHeapSizeBytes Cassandra cassandra.columnFamily.bloomFilterFalseRatio db.bloomFilterFalseRatio Cassandra cassandra.columnFamily.liveDiskSpaceUsedBytes db.liveDiskSpaceUsedBytes Cassandra cassandra.columnFamily.liveSsTableCount db.liveSSTableCount Cassandra cassandra.columnFamily.maxRowSize db.maxRowSize Cassandra cassandra.columnFamily.meanRowSize db.meanRowSize Cassandra cassandra.columnFamily.memtableLiveDataSize db.memtableLiveDataSize Cassandra cassandra.columnFamily.minRowSize db.minRowSize Cassandra cassandra.columnFamily.pendingCompactions db.pendingCompactions Cassandra cassandra.columnFamily.query.readLatency50ThPercentileMilliseconds query.readLatency50thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency75ThPercentileMilliseconds query.readLatency75thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency95ThPercentileMilliseconds query.readLatency95thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency98ThPercentileMilliseconds query.readLatency98thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency999ThPercentileMilliseconds query.readLatency999thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency99ThPercentileMilliseconds query.readLatency99thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readRequestsPerSecond query.readRequestsPerSecond Cassandra cassandra.columnFamily.query.writeLatency50ThPercentileMilliseconds query.writeLatency50thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency75ThPercentileMilliseconds query.writeLatency75thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency95ThPercentileMilliseconds query.writeLatency95thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency98ThPercentileMilliseconds query.writeLatency98thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency999ThPercentileMilliseconds query.writeLatency999thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency99ThPercentileMilliseconds query.writeLatency99thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeRequestsPerSecond query.writeRequestsPerSecond Cassandra cassandra.columnFamily.speculativeRetries db.speculativeRetries Cassandra cassandra.columnFamily.ssTablesPerRead50ThPercentileMilliseconds db.SSTablesPerRead50thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead75ThPercentileMilliseconds db.SSTablesPerRead75thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead95ThPercentileMilliseconds db.SSTablesPerRead95thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead98ThPercentileMilliseconds db.SSTablesPerRead98thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead999ThPercentileMilliseconds db.SSTablesPerRead999thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead99ThPercentileMilliseconds db.SSTablesPerRead99thPercentileMilliseconds Cassandra cassandra.columnFamily.tombstoneScannedHistogram50ThPercentile db.tombstoneScannedHistogram50thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram75ThPercentile db.tombstoneScannedHistogram75thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram95ThPercentile db.tombstoneScannedHistogram95thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram98ThPercentile db.tombstoneScannedHistogram98thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram999ThPercentile db.tombstoneScannedHistogram999thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram99ThPercentile db.tombstoneScannedHistogram99thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogramCount db.tombstoneScannedHistogramCount Consul consul.datacenter.catalog.criticalNodes catalog.criticalNodes Consul consul.datacenter.catalog.passingNodes catalog.passingNodes Consul consul.datacenter.catalog.registeredNodes catalog.registeredNodes Consul consul.datacenter.catalog.upNodes catalog.upNodes Consul consul.datacenter.catalog.warningNodes catalog.warningNodes Consul consul.datacenter.cluster.flaps cluster.flaps Consul consul.datacenter.cluster.suspects cluster.suspects Consul consul.datacenter.raft.commitTime raft.commitTimes Consul consul.datacenter.raft.commitTimeAvgInMilliseconds raft.commitTimeAvgInMilliseconds Consul consul.datacenter.raft.commitTimeMaxInMilliseconds raft.commitTimeMaxInMilliseconds Consul consul.datacenter.raft.completedLeaderElections raft.completedLeaderElections Consul consul.datacenter.raft.initiatedLeaderElections raft.initiatedLeaderElections Consul consul.datacenter.raft.lastContactAvgInMilliseconds raft.lastContactAvgInMilliseconds Consul consul.datacenter.raft.lastContactMaxInMilliseconds raft.lastContactMaxInMilliseconds Consul consul.datacenter.raft.lastContacts raft.lastContacts Consul consul.datacenter.raft.logDispatchAvgInMilliseconds raft.logDispatchAvgInMilliseconds Consul consul.datacenter.raft.logDispatches raft.logDispatches Consul consul.datacenter.raft.logDispatchMaxInMilliseconds raft.logDispatchMaxInMilliseconds Consul consul.datacenter.raft.txns raft.txns Consul consul.agent.aclCacheHitPerSecond agent.aclCacheHit Consul consul.agent.aclCacheMissPerSecond agent.aclCacheMiss Consul consul.agent.client.rpcFailed client.rpcFailed Consul consul.agent.client.rpcLoad client.rpcLoad Consul consul.agent.kvStores agent.kvStoress Consul consul.agent.kvStoresAvgInMilliseconds agent.kvStoresAvgInMilliseconds Consul consul.agent.kvStoresMaxInMilliseconds agent.kvStoresMaxInMilliseconds Consul consul.agent.net.agent.maxLatencyInMilliseconds net.agent.maxLatencyInMilliseconds Consul consul.agent.net.medianLatencyInMilliseconds net.agent.medianLatencyInMilliseconds Consul consul.agent.net.minLatencyInMilliseconds net.agent.minLatencyInMilliseconds Consul consul.agent.net.p25LatencyInMilliseconds net.agent.p25LatencyInMilliseconds Consul consul.agent.net.p75LatencyInMilliseconds net.agent.p75LatencyInMilliseconds Consul consul.agent.net.p90LatencyInMilliseconds net.agent.p90LatencyInMilliseconds Consul consul.agent.net.p95LatencyInMilliseconds net.agent.p95LatencyInMilliseconds Consul consul.agent.net.p99LatencyInMilliseconds net.agent.p99LatencyInMilliseconds Consul consul.agent.peers agent.peers Consul consul.agent.runtime.allocations runtime.allocations Consul consul.agent.runtime.allocationsInBytes runtime.allocationsInBytes Consul consul.agent.runtime.frees runtime.frees Consul consul.agent.runtime.gcCycles runtime.gcCycles Consul consul.agent.runtime.gcPauseInMilliseconds runtime.gcPauseInMilliseconds Consul consul.agent.runtime.goroutines runtime.goroutines Consul consul.agent.runtime.heapObjects runtime.heapObjects Consul consul.agent.runtime.virtualAddressSpaceInBytes runtime.virtualAddressSpaceInBytes Consul consul.agent.staleQueries agent.staleQueries Consul consul.agent.txnAvgInMilliseconds agent.txnAvgInMilliseconds Consul consul.agent.txnMaxInMilliseconds agent.txnMaxInMilliseconds Consul consul.agent.txns agent.txns Couchbase couchbase.bucket.activeItemsEnteringDiskQueuePerSecond bucket.activeItemsEnteringDiskQueuePerSecond Couchbase couchbase.bucket.activeItemsInMemory bucket.activeItemsInMemory Couchbase couchbase.bucket.activeResidentItemsRatio bucket.activeResidentItemsRatio Couchbase couchbase.bucket.averageDiskCommitTimeInMilliseconds bucket.averageDiskCommitTimeInMilliseconds Couchbase couchbase.bucket.averageDiskUpdateTimeInMilliseconds bucket.averageDiskUpdateTimeInMilliseconds Couchbase couchbase.bucket.cacheMisses bucket.cacheMisses Couchbase couchbase.bucket.cacheMissRatio bucket.cacheMissRatio Couchbase couchbase.bucket.casHits bucket.casHits Couchbase couchbase.bucket.casMisses bucket.casMisses Couchbase couchbase.bucket.couchDocsFragmentationPercent bucket.couchDocsFragmentationPercent Couchbase couchbase.bucket.currentConnections bucket.currentConnections Couchbase couchbase.bucket.dataUsedInBytes bucket.dataUsedInBytes Couchbase couchbase.bucket.decrementHitsPerSecond bucket.decrementHitsPerSecond Couchbase couchbase.bucket.decrementMissesPerSecond bucket.decrementMissesPerSecond Couchbase couchbase.bucket.deleteHitsPerSecond bucket.deleteHitsPerSecond Couchbase couchbase.bucket.deleteMissesPerSecond bucket.deleteMissesPerSecond Couchbase couchbase.bucket.diskCreateOperationsPerSecond bucket.diskCreateOperationsPerSecond Couchbase couchbase.bucket.diskFetchesPerSecond bucket.diskFetchesPerSecond Couchbase couchbase.bucket.diskReadsPerSecond bucket.diskReadsPerSecond Couchbase couchbase.bucket.diskUpdateOperationsPerSecond bucket.diskUpdateOperationsPerSecond Couchbase couchbase.bucket.diskUsedInBytes bucket.diskUsedInBytes Couchbase couchbase.bucket.diskWriteQueue bucket.diskWriteQueue Couchbase couchbase.bucket.drainedItemsInQueue bucket.drainedItemsInQueue Couchbase couchbase.bucket.drainedItemsOnDiskQueue bucket.drainedItemsOnDiskQueue Couchbase couchbase.bucket.drainedPendingItemsInQueue bucket.drainedPendingItemsInQueue Couchbase couchbase.bucket.ejectionsPerSecond bucket.ejectionsPerSecond Couchbase couchbase.bucket.evictionsPerSecond bucket.evictionsPerSecond Couchbase couchbase.bucket.getHitsPerSecond bucket.getHitsPerSecond Couchbase couchbase.bucket.getMissesPerSecond bucket.getMissesPerSecond Couchbase couchbase.bucket.hitRatio bucket.hitRatio Couchbase couchbase.bucket.incrementHitsPerSecond bucket.incrementHitsPerSecond Couchbase couchbase.bucket.incrementMissesPerSecond bucket.incrementMissesPerSecond Couchbase couchbase.bucket.itemCount bucket.itemCount Couchbase couchbase.bucket.itemsBeingWritten bucket.itemsBeingWritten Couchbase couchbase.bucket.itemsEjectedFromMemoryToDisk bucket.itemsEjectedFromMemoryToDisk Couchbase couchbase.bucket.itemsOnDiskQueue bucket.itemsOnDiskQueue Couchbase couchbase.bucket.itemsQueuedForStorage bucket.itemsQueuedForStorage Couchbase couchbase.bucket.maximumMemoryUsage bucket.maximumMemoryUsage Couchbase couchbase.bucket.memoryHighWaterMarkInBytes bucket.memoryHighWaterMarkInBytes Couchbase couchbase.bucket.memoryLowWaterMarkInBytes bucket.memoryLowWaterMarkInBytes Couchbase couchbase.bucket.memoryUsedInBytes bucket.memoryUsedInBytes Couchbase couchbase.bucket.metadataInRamInBytes bucket.metadataInRAMInBytes Couchbase couchbase.bucket.missesPerSecond bucket.missesPerSecond Couchbase couchbase.bucket.outOfMemoryErrorsPerSecond bucket.outOfMemoryErrorsPerSecond Couchbase couchbase.bucket.overheadInBytes bucket.overheadInBytes Couchbase couchbase.bucket.pendingItemsInDiskQueue bucket.pendingItemsInDiskQueue Couchbase couchbase.bucket.pendingResidentItemsRatio bucket.pendingResidentItemsRatio Couchbase couchbase.bucket.quotaUtilization bucket.quotaUtilization Couchbase couchbase.bucket.readOperationsPerSecond bucket.readOperationsPerSecond Couchbase couchbase.bucket.readRatePerSecond bucket.readRatePerSecond Couchbase couchbase.bucket.recoverableOutOfMemoryCount bucket.recoverableOutOfMemoryCount Couchbase couchbase.bucket.replicaIndex bucket.replicaIndex Couchbase couchbase.bucket.replicaNumber bucket.replicaNumber Couchbase couchbase.bucket.replicaResidentItemsRatio bucket.replicaResidentItemsRatio Couchbase couchbase.bucket.residentItemsRatio bucket.residentItemsRatio Couchbase couchbase.bucket.temporaryOutOfMemoryErrorsPerSecond bucket.temporaryOutOfMemoryErrorsPerSecond Couchbase couchbase.bucket.threadsNumber bucket.threadsNumber Couchbase couchbase.bucket.totalItems bucket.totalItems Couchbase couchbase.bucket.totalOperationsPerSecond bucket.totalOperationsPerSecond Couchbase couchbase.bucket.viewFragmentationPercent bucket.viewFragmentationPercent Couchbase couchbase.bucket.writeOperationsPerSecond bucket.writeOperationsPerSecond Couchbase couchbase.bucket.writeRatePerSecond bucket.writeRatePerSecond Couchbase couchbase.cluster.autoFailoverCount cluster.autoFailoverCount Couchbase couchbase.cluster.autoFailoverEnabled cluster.autoFailoverEnabled Couchbase couchbase.cluster.databaseFragmentationThreshold cluster.databaseFragmentationThreshold Couchbase couchbase.cluster.diskFreeInBytes cluster.diskFreeInBytes Couchbase couchbase.cluster.diskQuotaTotalInBytes cluster.diskQuotaTotalInBytes Couchbase couchbase.cluster.diskTotalInBytes cluster.diskTotalInBytes Couchbase couchbase.cluster.diskUsedByDataInBytes cluster.diskUsedByDataInBytes Couchbase couchbase.cluster.diskUsedInBytes cluster.diskUsedInBytes Couchbase couchbase.cluster.indexFragmentationThreshold cluster.indexFragmentationThreshold Couchbase couchbase.cluster.maximumBucketCount cluster.maximumBucketCount Couchbase couchbase.cluster.memoryQuotaTotalInBytes cluster.memoryQuotaTotalInBytes Couchbase couchbase.cluster.memoryQuotaTotalPerNodeInBytes cluster.memoryQuotaTotalPerNodeInBytes Couchbase couchbase.cluster.memoryQuotaUsedInBytes cluster.memoryQuotaUsedInBytes Couchbase couchbase.cluster.memoryQuotaUsedPerNodeInBytes cluster.memoryQuotaUsedPerNodeInBytes Couchbase couchbase.cluster.memoryTotalInBytes cluster.memoryTotalInBytes Couchbase couchbase.cluster.memoryUsedByDataInBytes cluster.memoryUsedByDataInBytes Couchbase couchbase.cluster.memoryUsedInBytes cluster.memoryUsedInBytes Couchbase couchbase.cluster.viewFragmentationThreshold cluster.viewFragmentationThreshold Couchbase couchbase.node.backgroundFetches node.backgroundFetches Couchbase couchbase.node.cmdGet node.cmdGet Couchbase couchbase.node.couchDocsActualDiskSizeInBytes node.couchDocsActualDiskSizeInBytes Couchbase couchbase.node.couchDocsDataSizeInBytes node.couchDocsDataSizeInBytes Couchbase couchbase.node.couchSpatialDataSizeInBytes node.couchSpatialDataSizeInBytes Couchbase couchbase.node.couchSpatialDiskSizeInBytes node.couchSpatialDiskSizeInBytes Couchbase couchbase.node.couchViewsActualDiskSizeInBytes node.couchViewsActualDiskSizeInBytes Couchbase couchbase.node.couchViewsDataSizeInBytes node.couchViewsDataSizeInBytes Couchbase couchbase.node.cpuUtilization node.cpuUtilization Couchbase couchbase.node.currentItems node.currentItems Couchbase couchbase.node.currentItemsTotal node.currentItemsTotal Couchbase couchbase.node.getHits node.getHits Couchbase couchbase.node.memoryFreeInBytes node.memoryFreeInBytes Couchbase couchbase.node.memoryTotalInBytes node.memoryTotalInBytes Couchbase couchbase.node.memoryUsedInBytes node.memoryUsedInBytes Couchbase couchbase.node.ops node.ops Couchbase couchbase.node.swapTotalInBytes node.swapTotalInBytes Couchbase couchbase.node.swapUsedInBytes node.swapUsedInBytes Couchbase couchbase.node.uptimeInMilliseconds node.uptimeInMilliseconds Couchbase couchbase.node.vbucketActiveNonResidentItems node.vbucketActiveNonResidentItems Couchbase couchbase.node.vbucketInMemoryItems node.vbucketInMemoryItems Couchbase couchbase.queryengine.activeRequests queryengine.activeRequests Couchbase couchbase.queryengine.averageRequestTimeInMilliseconds queryengine.averageRequestTimeInMilliseconds Couchbase couchbase.queryengine.completedLimit queryengine.completedLimit Couchbase couchbase.queryengine.completedRequests queryengine.completedRequests Couchbase couchbase.queryengine.completedThresholdInMilliseconds queryengine.completedThresholdInMilliseconds Couchbase couchbase.queryengine.cores queryengine.cores Couchbase couchbase.queryengine.garbageCollectionNumber queryengine.garbageCollectionNumber Couchbase couchbase.queryengine.garbageCollectionPaused queryengine.garbageCollectionPaused Couchbase couchbase.queryengine.garbageCollectionTimePausedInMilliseconds queryengine.garbageCollectionTimePausedInMilliseconds Couchbase couchbase.queryengine.medianRequestTimeInMilliseconds queryengine.medianRequestTimeInMilliseconds Couchbase couchbase.queryengine.preparedStatementUtilization queryengine.preparedStatementUtilization Couchbase couchbase.queryengine.requestsLast15MinutesPerSecond queryengine.requestsLast15MinutesPerSecond Couchbase couchbase.queryengine.requestsLast1MinutesPerSecond queryengine.requestsLast1MinutesPerSecond Couchbase couchbase.queryengine.requestsLast5MinutesPerSecond queryengine.requestsLast5MinutesPerSecond Couchbase couchbase.queryengine.requestTime80thPercentileInMilliseconds queryengine.requestTime80thPercentileInMilliseconds Couchbase couchbase.queryengine.requestTime95thPercentileInMilliseconds queryengine.requestTime95thPercentileInMilliseconds Couchbase couchbase.queryengine.requestTime99thPercentileInMilliseconds queryengine.requestTime99thPercentileInMilliseconds Couchbase couchbase.queryengine.systemCpuUtilization queryengine.systemCPUUtilization Couchbase couchbase.queryengine.systemMemoryInBytes queryengine.systemMemoryInBytes Couchbase couchbase.queryengine.totalMemoryInBytes queryengine.totalMemoryInBytes Couchbase couchbase.queryengine.totalThreads queryengine.totalThreads Couchbase couchbase.queryengine.uptimeInMilliseconds queryengine.uptimeInMilliseconds Couchbase couchbase.queryengine.usedMemoryInBytes queryengine.usedMemoryInBytes Couchbase couchbase.queryengine.userCpuUtilization queryengine.userCPUUtilization Docker docker.container.cpuKernelPercent cpuKernelPercent Docker docker.container.cpuLimitCores cpuLimitCores Docker docker.container.cpuPercent cpuPercent Docker docker.container.cpuThrottlePeriods cpuThrottlePeriods Docker docker.container.cpuThrottleTimeMs cpuThrottleTimeMs Docker docker.container.cpuUsedCores cpuUsedCores Docker docker.container.cpuUsedCoresPercent cpuUsedCoresPercent Docker docker.container.cpuUserPercent cpuUserPercent Docker docker.container.ioReadBytesPerSecond ioReadBytesPerSecond Docker docker.container.ioReadCountPerSecond ioReadCountPerSecond Docker docker.container.ioTotalBytes ioTotalBytes Docker docker.container.ioTotalReadBytes ioTotalReadBytes Docker docker.container.ioTotalReadCount ioTotalReadCount Docker docker.container.ioTotalWriteBytes ioTotalWriteBytes Docker docker.container.ioTotalWriteCount ioTotalWriteCount Docker docker.container.ioWriteBytesPerSecond ioWriteBytesPerSecond Docker docker.container.ioWriteCountPerSecond ioWriteCountPerSecond Docker docker.container.memoryCacheBytes memoryCacheBytes Docker docker.container.memoryResidentSizeBytes memoryResidentSizeBytes Docker docker.container.memorySizeLimitBytes memorySizeLimitBytes Docker docker.container.memoryUsageBytes memoryUsageBytes Docker docker.container.memoryUsageLimitPercent memoryUsageLimitPercent Docker docker.container.networkRxBytes networkRxBytes Docker docker.container.networkRxBytesPerSecond networkRxBytesPerSecond Docker docker.container.networkRxDropped networkRxDropped Docker docker.container.networkRxDroppedPerSecond networkRxDroppedPerSecond Docker docker.container.networkRxErrors networkRxErrors Docker docker.container.networkRxErrorsPerSecond networkRxErrorsPerSecond Docker docker.container.networkRxPackets networkRxPackets Docker docker.container.networkRxPacketsPerSecond networkRxPacketsPerSecond Docker docker.container.networkTxBytes networkTxBytes Docker docker.container.networkTxBytesPerSecond networkTxBytesPerSecond Docker docker.container.networkTxDropped networkTxDropped Docker docker.container.networkTxDroppedPerSecond networkTxDroppedPerSecond Docker docker.container.networkTxErrors networkTxErrors Docker docker.container.networkTxErrorsPerSecond networkTxErrorsPerSecond Docker docker.container.networkTxPackets networkTxPackets Docker docker.container.networkTxPacketsPerSecond networkTxPacketsPerSecond Docker docker.container.pids pids Docker docker.container.processCount processCount Docker docker.container.processCountLimit processCountLimit Docker docker.container.restartCount restartCount Docker docker.container.threadCount threadCount Docker docker.container.threadCountLimit threadCountLimit ElasticSearch elasticsearch.cluster.dataNodes cluster.dataNodes ElasticSearch elasticsearch.cluster.nodes cluster.nodes ElasticSearch elasticsearch.cluster.shards.active shards.active ElasticSearch elasticsearch.cluster.shards.initializing shards.initializing ElasticSearch elasticsearch.cluster.shards.primaryActive shards.primaryActive ElasticSearch elasticsearch.cluster.shards.relocating shards.relocating ElasticSearch elasticsearch.cluster.shards.unassigned shards.unassigned ElasticSearch elasticsearch.cluster.tempData temp-data ElasticSearch elasticsearch.index.docs index.docs ElasticSearch elasticsearch.index.docsDeleted index.docsDeleted ElasticSearch elasticsearch.index.primaryShards index.primaryShards ElasticSearch elasticsearch.index.primaryStoreSizeInBytes index.primaryStoreSizeInBytes ElasticSearch elasticsearch.index.replicaShards index.replicaShards ElasticSearch elasticsearch.index.rollup.docsCount primaries.docsnumber ElasticSearch elasticsearch.index.rollup.docsDeleted primaries.docsDeleted ElasticSearch elasticsearch.index.rollup.flushTotal primaries.flushesTotal ElasticSearch elasticsearch.index.rollup.flushTotalTimeInMilliseconds primaries.flushTotalTimeInMilliseconds ElasticSearch elasticsearch.index.rollup.get.documentsExist primaries.get.documentsExist ElasticSearch elasticsearch.index.rollup.get.documentsExistInMilliseconds primaries.get.documentsExistInMilliseconds ElasticSearch elasticsearch.index.rollup.get.documentsMissing primaries.get.documentsMissing ElasticSearch elasticsearch.index.rollup.get.documentsMissingInMilliseconds primaries.get.documentsMissingInMilliseconds ElasticSearch elasticsearch.index.rollup.get.requests primaries.get.requests ElasticSearch elasticsearch.index.rollup.get.requestsCurrent primaries.get.requestsCurrent ElasticSearch elasticsearch.index.rollup.get.requestsInMilliseconds primaries.get.requestsInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyDeleted primaries.index.docsCurrentlyDeleted ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyDeletedInMilliseconds primaries.index.docsCurrentlyDeletedInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyIndexing primaries.index.docsCurrentlyIndexing ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyIndexingInMilliseconds primaries.index.docsCurrentlyIndexingInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsDeleted primaries.index.docsDeleted ElasticSearch elasticsearch.index.rollup.index.docsTotal primaries.index.docsTotal ElasticSearch elasticsearch.index.rollup.indexRefreshesTotal primaries.indexRefreshesTotal ElasticSearch elasticsearch.index.rollup.indexRefreshesTotalInMilliseconds primaries.indexRefreshesTotalInMilliseconds ElasticSearch elasticsearch.index.rollup.merges.current primaries.merges.current ElasticSearch elasticsearch.index.rollup.merges.docsSegmentsCurrentlyMerged primaries.merges.docsSegmentsCurrentlyMerged ElasticSearch elasticsearch.index.rollup.merges.docsTotal primaries.merges.docsTotal ElasticSearch elasticsearch.index.rollup.merges.segmentsCurrentlyMergedInBytes primaries.merges.segmentsCurrentlyMergedInBytes ElasticSearch elasticsearch.index.rollup.merges.segmentsTotal primaries.merges.segmentsTotal ElasticSearch elasticsearch.index.rollup.merges.segmentsTotalInBytes primaries.merges.segmentsTotalInBytes ElasticSearch elasticsearch.index.rollup.merges.segmentsTotalInMilliseconds primaries.merges.segmentsTotalInMilliseconds ElasticSearch elasticsearch.index.rollup.queriesInMilliseconds primaries.queriesInMilliseconds ElasticSearch elasticsearch.index.rollup.queriesTotal primaries.queriesTotal ElasticSearch elasticsearch.index.rollup.queryActive primaries.queryActive ElasticSearch elasticsearch.index.rollup.queryFetches primaries.queryFetches ElasticSearch elasticsearch.index.rollup.queryFetchesInMilliseconds primaries.queryFetchesInMilliseconds ElasticSearch elasticsearch.index.rollup.queryFetchesTotal primaries.queryFetchesTotal ElasticSearch elasticsearch.index.rollup.sizeInBytes primaries.sizeInBytes ElasticSearch elasticsearch.index.storeSizeInBytes index.storeSizeInBytes ElasticSearch elasticsearch.node.activeSearches activeSearches ElasticSearch elasticsearch.node.activeSearchesInMilliseconds activeSearchesInMilliseconds ElasticSearch elasticsearch.node.breakers.estimatedSizeFieldDataCircuitBreakerInBytes breakers.estimatedSizeFieldDataCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.estimatedSizeParentCircuitBreakerInBytes breakers.estimatedSizeParentCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.estimatedSizeRequestCircuitBreakerInBytes breakers.estimatedSizeRequestCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.fieldDataCircuitBreakerTripped breakers.fieldDataCircuitBreakerTripped ElasticSearch elasticsearch.node.breakers.parentCircuitBreakerTripped breakers.parentCircuitBreakerTripped ElasticSearch elasticsearch.node.breakers.requestCircuitBreakerTripped breakers.requestCircuitBreakerTripped ElasticSearch elasticsearch.node.flush.indexRefreshesTotal flush.indexRefreshesTotal ElasticSearch elasticsearch.node.flush.indexRefreshesTotalInMilliseconds flush.indexRefreshesTotalInMilliseconds ElasticSearch elasticsearch.node.fs.bytesAvailableJvmInBytes fs.bytesAvailableJVMInBytes ElasticSearch elasticsearch.node.fs.dataRead fs.bytesReadsInBytes ElasticSearch elasticsearch.node.fs.dataWritten fs.writesInBytes ElasticSearch elasticsearch.node.fs.ioOperations fs.iOOperations ElasticSearch elasticsearch.node.fs.readOperations fs.reads ElasticSearch elasticsearch.node.fs.totalSizeInBytes fs.totalSizeInBytes ElasticSearch elasticsearch.node.fs.unallocatedBytes fs.unallocatedBytesInBYtes ElasticSearch elasticsearch.node.fs.writeOperations fs.writeOperations ElasticSearch elasticsearch.node.get.currentRequestsRunning get.currentRequestsRunning ElasticSearch elasticsearch.node.get.requestsDocumentExists get.requestsDocumentExists ElasticSearch elasticsearch.node.get.requestsDocumentExistsInMilliseconds get.requestsDocumentExistsInMilliseconds ElasticSearch elasticsearch.node.get.requestsDocumentMissing get.requestsDocumentMissing ElasticSearch elasticsearch.node.get.requestsDocumentMissingInMilliseconds get.requestsDocumentMissingInMilliseconds ElasticSearch elasticsearch.node.get.timeGetRequestsInMilliseconds get.timeGetRequestsInMilliseconds ElasticSearch elasticsearch.node.get.totalGetRequests get.totalGetRequests ElasticSearch elasticsearch.node.http.currentOpenConnections http.currentOpenConnections ElasticSearch elasticsearch.node.http.openedConnections http.openedConnections ElasticSearch elasticsearch.node.index.indexingOperationsFailed indices.indexingOperationsFailed ElasticSearch elasticsearch.node.index.indexingWaitedThrottlingInMilliseconds indices.indexingWaitedThrottlingInMilliseconds ElasticSearch elasticsearch.node.index.memoryQueryCacheInBytes indices.memoryQueryCacheInBytes ElasticSearch elasticsearch.node.index.numberIndices indices.numberIndices ElasticSearch elasticsearch.node.index.queryCacheEvictions indices.queryCacheEvictions ElasticSearch elasticsearch.node.index.queryCacheHits indices.queryCacheHits ElasticSearch elasticsearch.node.index.queryCacheMisses indices.queryCacheMisses ElasticSearch elasticsearch.node.index.recoveryOngoingShardSource indices.recoveryOngoingShardSource ElasticSearch elasticsearch.node.index.recoveryOngoingShardTarget indices.recoveryOngoingShardTarget ElasticSearch elasticsearch.node.index.recoveryWaitedThrottlingInMilliseconds indices.recoveryWaitedThrottlingInMilliseconds ElasticSearch elasticsearch.node.index.requestCacheEvictions indices.requestCacheEvictions ElasticSearch elasticsearch.node.index.requestCacheHits indices.requestCacheHits ElasticSearch elasticsearch.node.index.requestCacheMemoryInBytes indices.requestCacheMemoryInBytes ElasticSearch elasticsearch.node.index.requestCacheMisses indices.requestCacheMisses ElasticSearch elasticsearch.node.index.segmentsIndexShard indices.segmentsIndexShard ElasticSearch elasticsearch.node.index.segmentsMemoryUsedDocValuesInBytes indices.segmentsMemoryUsedDocValuesInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedFixedBitSetInBytes indices.segmentsMemoryUsedFixedBitSetInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedIndexSegmentsInBytes indices.segmentsMemoryUsedIndexSegmentsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedIndexWriterInBytes indices.segmentsMemoryUsedIndexWriterInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedNormsInBytes indices.segmentsMemoryUsedNormsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedSegmentVersionMapInBytes indices.segmentsMemoryUsedSegmentVersionMapInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedStoredFieldsInBytes indices.segmentsMemoryUsedStoredFieldsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedTermsInBytes indices.segmentsMemoryUsedTermsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedTermVectorsInBytes indices.segmentsMemoryUsedTermVectorsInBytes ElasticSearch elasticsearch.node.index.translogOperations indices.translogOperations ElasticSearch elasticsearch.node.index.translogOperationsInBytes indices.translogOperationsInBytes ElasticSearch elasticsearch.node.indexing.docsCurrentlyDeleted indexing.docsCurrentlyDeleted ElasticSearch elasticsearch.node.indexing.documentsCurrentlyIndexing indexing.documentsCurrentlyIndexing ElasticSearch elasticsearch.node.indexing.documentsIndexed indexing.documentsIndexed ElasticSearch elasticsearch.node.indexing.timeDeletingDocumentsInMilliseconds indexing.timeDeletingDocumentsInMilliseconds ElasticSearch elasticsearch.node.indexing.timeIndexingDocumentsInMilliseconds indexing.timeIndexingDocumentsInMilliseconds ElasticSearch elasticsearch.node.indexing.totalDocumentsDeleted indexing.totalDocumentsDeleted ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsOldGenerationObjects jvm.gc.majorCollectionsOldGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsYoungGenerationObjects jvm.gc.majorCollectionsYoungGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsYoungGenerationObjectsInMilliseconds jvm.gc.majorCollectionsYoungGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.gc.minorCollectionsYoungGenerationObjects jvm.gc.minorCollectionsYoungGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.mem.heapCommittedInBytes jvm.mem.heapCommittedInBytes ElasticSearch elasticsearch.node.jvm.mem.heapMaxInBytes jvm.mem.heapMaxInBytes ElasticSearch elasticsearch.node.jvm.mem.heapUsed jvm.mem.heapUsed ElasticSearch elasticsearch.node.jvm.mem.heapUsedInBytes jvm.mem.heapUsedInBytes ElasticSearch elasticsearch.node.jvm.mem.maxOldGenerationHeapInBytes jvm.mem.maxOldGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.maxSurvivorSpaceInBytes jvm.mem.maxSurvivorSpaceInBYtes ElasticSearch elasticsearch.node.jvm.mem.maxYoungGenerationHeapInBytes jvm.mem.maxYoungGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.nonHeapCommittedInBytes jvm.mem.nonHeapCommittedInBytes ElasticSearch elasticsearch.node.jvm.mem.nonHeapUsedInBytes jvm.mem.nonHeapUsedInBytes ElasticSearch elasticsearch.node.jvm.mem.usedOldGenerationHeapInBytes jvm.mem.usedOldGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.usedSurvivorSpaceInBytes jvm.mem.usedSurvivorSpaceInBytes ElasticSearch elasticsearch.node.jvm.mem.usedYoungGenerationHeapInBytes jvm.mem.usedYoungGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.threadsActive jvm.ThreadsActive ElasticSearch elasticsearch.node.jvm.threadsPeak jvm.ThreadsPeak ElasticSearch elasticsearch.node.merges.currentActive merges.currentActive ElasticSearch elasticsearch.node.merges.docsSegmentMerges merges.docsSegmentMerges ElasticSearch elasticsearch.node.merges.docsSegmentsMerging merges.docsSegmentsMerging ElasticSearch elasticsearch.node.merges.mergedSegmentsInBytes merges.mergedSegmentsInBytes ElasticSearch elasticsearch.node.merges.segmentMerges merges.segmentMerges ElasticSearch elasticsearch.node.merges.sizeSegmentsMergingInBytes merges.sizeSegmentsMergingInBytes ElasticSearch elasticsearch.node.merges.totalSegmentMergingInMilliseconds merges.totalSegmentMergingInMilliseconds ElasticSearch elasticsearch.node.openFd openFD ElasticSearch elasticsearch.node.queriesTotal queriesTotal ElasticSearch elasticsearch.node.refresh.total refresh.total ElasticSearch elasticsearch.node.refresh.totalInMilliseconds refresh.totalInMilliseconds ElasticSearch elasticsearch.node.searchFetchCurrentlyRunning searchFetchCurrentlyRunning ElasticSearch elasticsearch.node.searchFetches searchFetches ElasticSearch elasticsearch.node.sizeStoreInBytes sizeStoreInBytes ElasticSearch elasticsearch.node.threadpool.activeFetchShardStarted threadpool.activeFetchShardStarted ElasticSearch elasticsearch.node.threadpool.bulkActive threadpool.bulkActive ElasticSearch elasticsearch.node.threadpool.bulkQueue threadpool.bulkQueue ElasticSearch elasticsearch.node.threadpool.bulkRejected threadpool.bulkRejected ElasticSearch elasticsearch.node.threadpool.bulkThreads threadpool.bulkThreads ElasticSearch elasticsearch.node.threadpool.fetchShardStartedQueue threadpool.fetchShardStartedQueue ElasticSearch elasticsearch.node.threadpool.fetchShardStartedRejected threadpool.fetchShardStartedRejected ElasticSearch elasticsearch.node.threadpool.fetchShardStartedThreads threadpool.fetchShardStartedThreads ElasticSearch elasticsearch.node.threadpool.fetchShardStoreActive threadpool.fetchShardStoreActive ElasticSearch elasticsearch.node.threadpool.fetchShardStoreQueue threadpool.fetchShardStoreQueue ElasticSearch elasticsearch.node.threadpool.fetchShardStoreRejected threadpool.fetchShardStoreRejected ElasticSearch elasticsearch.node.threadpool.fetchShardStoreThreads threadpool.fetchShardStoreThreads ElasticSearch elasticsearch.node.threadpool.flushActive threadpool.flushActive ElasticSearch elasticsearch.node.threadpool.flushQueue threadpool.flushQueue ElasticSearch elasticsearch.node.threadpool.flushRejected threadpool.flushRejected ElasticSearch elasticsearch.node.threadpool.flushThreads threadpool.flushThreads ElasticSearch elasticsearch.node.threadpool.forceMergeActive threadpool.forceMergeActive ElasticSearch elasticsearch.node.threadpool.forceMergeQueue threadpool.forceMergeQueue ElasticSearch elasticsearch.node.threadpool.forceMergeRejected threadpool.forceMergeRejected ElasticSearch elasticsearch.node.threadpool.forceMergeThreads threadpool.forceMergeThreads ElasticSearch elasticsearch.node.threadpool.genericActive threadpool.genericActive ElasticSearch elasticsearch.node.threadpool.genericQueue threadpool.genericQueue ElasticSearch elasticsearch.node.threadpool.genericRejected threadpool.genericRejected ElasticSearch elasticsearch.node.threadpool.genericThreads threadpool.genericThreads ElasticSearch elasticsearch.node.threadpool.getActive threadpool.getActive ElasticSearch elasticsearch.node.threadpool.getQueue threadpool.getQueue ElasticSearch elasticsearch.node.threadpool.getRejected threadpool.getRejected ElasticSearch elasticsearch.node.threadpool.getThreads threadpool.getThreads ElasticSearch elasticsearch.node.threadpool.indexActive threadpool.indexActive ElasticSearch elasticsearch.node.threadpool.indexQueue threadpool.indexQueue ElasticSearch elasticsearch.node.threadpool.indexRejected threadpool.indexRejected ElasticSearch elasticsearch.node.threadpool.indexThreads threadpool.indexThreads ElasticSearch elasticsearch.node.threadpool.listenerActive threadpool.listenerActive ElasticSearch elasticsearch.node.threadpool.listenerQueue threadpool.listenerQueue ElasticSearch elasticsearch.node.threadpool.listenerRejected threadpool.listenerRejected ElasticSearch elasticsearch.node.threadpool.listenerThreads threadpool.listenerThreads ElasticSearch elasticsearch.node.threadpool.managementActive threadpool.managementActive ElasticSearch elasticsearch.node.threadpool.managementQueue threadpool.managementQueue ElasticSearch elasticsearch.node.threadpool.managementRejected threadpool.managementRejected ElasticSearch elasticsearch.node.threadpool.managementThreads threadpool.managementThreads ElasticSearch elasticsearch.node.threadpool.refreshActive threadpool.refreshActive ElasticSearch elasticsearch.node.threadpool.refreshQueue threadpool.refreshQueue ElasticSearch elasticsearch.node.threadpool.refreshRejected threadpool.refreshRejected ElasticSearch elasticsearch.node.threadpool.refreshThreads threadpool.refreshThreads ElasticSearch elasticsearch.node.threadpool.searchActive threadpool.searchActive ElasticSearch elasticsearch.node.threadpool.searchQueue threadpool.searchQueue ElasticSearch elasticsearch.node.threadpool.searchRejected threadpool.searchRejected ElasticSearch elasticsearch.node.threadpool.searchThreads threadpool.searchThreads ElasticSearch elasticsearch.node.threadpool.snapshotActive threadpool.snapshotActive ElasticSearch elasticsearch.node.threadpool.snapshotQueue threadpool.snapshotQueue ElasticSearch elasticsearch.node.threadpool.snapshotRejected threadpool.snapshotRejected ElasticSearch elasticsearch.node.threadpool.snapshotThreads threadpool.snapshotThreads ElasticSearch elasticsearch.node.transport.connectionsOpened transport.connectionsOpened ElasticSearch elasticsearch.node.transport.packetsReceived transport.packetsReceived ElasticSearch elasticsearch.node.transport.packetsReceivedInBytes transport.packetsReceivedInBytes ElasticSearch elasticsearch.node.transport.packetsSent transport.packetsSent ElasticSearch elasticsearch.node.transport.packetsSentInBytes transport.packetsSentInBytes F5 f5.node.availabilityState node.availabilityState F5 f5.node.connections node.connections F5 f5.node.connectionsPerSecond node.connectionsPerSecond F5 f5.node.enabled node.enabled F5 f5.node.inDataInBytesPerSecond node.inDataInBytesPerSecond F5 f5.node.monitorStatus node.monitorStatus F5 f5.node.outDataInBytesPerSecond node.outDataInBytesPerSecond F5 f5.node.packetsReceivedPerSecond node.packetsReceivedPerSecond F5 f5.node.packetsSentPerSecond node.packetsSentPerSecond F5 f5.node.requestsPerSecond node.requestsPerSecond F5 f5.node.sessions node.sessions F5 f5.node.sessionStatus node.sessionStatus F5 f5.poolMember.availabilityState member.availabilityState F5 f5.poolMember.connections member.connections F5 f5.poolMember.enabled member.enabled F5 f5.poolMember.inDataInBytesPerSecond member.inDataInBytesPerSecond F5 f5.poolMember.monitorStatus member.monitorStatus F5 f5.poolMember.outDataInBytesPerSecond member.outDataInBytesPerSecond F5 f5.poolMember.packetsReceivedPerSecond member.packetsReceivedPerSecond F5 f5.poolMember.packetsSentPerSecond member.packetsSentPerSecond F5 f5.poolMember.requestsPerSecond member.requestsPerSecond F5 f5.poolMember.sessions member.sessions F5 f5.poolMember.sessionStatus member.sessionStatus F5 f5.pool.activeMembers pool.activeMembers F5 f5.pool.availabilityState pool.availabilityState F5 f5.pool.connections pool.connections F5 f5.pool.connqAgeEdm pool.connqAgeEdm F5 f5.pool.connqAgeEma pool.connqAgeEma F5 f5.pool.connqAgeHead pool.connqAgeHead F5 f5.pool.connqAgeMax pool.connqAgeMax F5 f5.pool.connqAllAgeEdm pool.connqAllAgeEdm F5 f5.pool.connqAllAgeEma pool.connqAllAgeEma F5 f5.pool.connqAllAgeHead pool.connqAllAgeHead F5 f5.pool.connqAllAgeMax pool.connqAllAgeMax F5 f5.pool.connqAllDepth pool.connqAllDepth F5 f5.pool.connqDepth pool.connqDepth F5 f5.pool.currentConnections pool.currentConnections F5 f5.pool.enabled pool.enabled F5 f5.pool.inDataInBytesPerSecond pool.inDataInBytesPerSecond F5 f5.pool.minActiveMembers pool.minActiveMembers F5 f5.pool.outDataInBytesPerSecond pool.outDataInBytesPerSecond F5 f5.pool.packetsReceivedPerSecond pool.packetsReceivedPerSecond F5 f5.pool.packetsSentPerSecond pool.packetsSentPerSecond F5 f5.pool.requestsPerSecond pool.requestsPerSecond F5 f5.pool.sessions pool.sessions F5 f5.system.cpuIdleTicksPerSecond system.cpuIdleTicksPerSecond F5 f5.system.cpuIdleUtilization system.cpuIdleUtilization F5 f5.system.cpuInterruptRequestUtilization system.cpuInterruptRequestUtilization F5 f5.system.cpuIoWaitUtilization system.cpuIOWaitUtilization F5 f5.system.cpuNiceLevelUtilization system.cpuNiceLevelUtilization F5 f5.system.cpuSoftInterruptRequestUtilization system.cpuSoftInterruptRequestUtilization F5 f5.system.cpuStolenUtilization system.cpuStolenUtilization F5 f5.system.cpuSystemTicksPerSecond system.cpuSystemTicksPerSecond F5 f5.system.cpuSystemUtilization system.cpuSystemUtilization F5 f5.system.cpuUserTicksPerSecond system.cpuUserTicksPerSecond F5 f5.system.cpuUserUtilization system.cpuUserUtilization F5 f5.system.memoryFreeInBytes system.memoryFreeInBytes F5 f5.system.memoryTotalInBytes system.memoryTotalInBytes F5 f5.system.memoryUsedInBytes system.memoryUsedInBytes F5 f5.system.otherMemoryFreeInBytes system.otherMemoryFreeInBytes F5 f5.system.otherMemoryTotalInBytes system.otherMemoryTotalInBytes F5 f5.system.otherMemoryUsedInBytes system.otherMemoryUsedInBytes F5 f5.system.swapFreeInBytes system.swapFreeInBytes F5 f5.system.swapTotalInBytes system.swapTotalInBytes F5 f5.system.swapUsedInBytes system.swapUsedInBytes F5 f5.system.tmmMemoryFreeInBytes system.tmmMemoryFreeInBytes F5 f5.system.tmmMemoryTotalInBytes system.tmmMemoryTotalInBytes F5 f5.system.tmmMemoryUsedInBytes system.tmmMemoryUsedInBytes F5 f5.virtualserver.availabilityState virtualserver.availabilityState F5 f5.virtualserver.clientsideConnectionsPerSecond virtualserver.clientsideConnectionsPerSecond F5 f5.virtualserver.connections virtualserver.connections F5 f5.virtualserver.csMaxConnDur virtualserver.csMaxConnDur F5 f5.virtualserver.csMeanConnDur virtualserver.csMeanConnDur F5 f5.virtualserver.csMinConnDur virtualserver.csMinConnDur F5 f5.virtualserver.enabled virtualserver.enabled F5 f5.virtualserver.ephemeralBytesInPerSecond virtualserver.ephemeralBytesInPerSecond F5 f5.virtualserver.ephemeralBytesOutPerSecond virtualserver.ephemeralBytesOutPerSecond F5 f5.virtualserver.ephemeralConnectionsPerSecond virtualserver.ephemeralConnectionsPerSecond F5 f5.virtualserver.ephemeralCurrentConnections virtualserver.ephemeralCurrentConnections F5 f5.virtualserver.ephemeralEvictedConnectionsPerSecond virtualserver.ephemeralEvictedConnectionsPerSecond F5 f5.virtualserver.ephemeralMaxConnections virtualserver.ephemeralMaxConnections F5 f5.virtualserver.ephemeralPacketsReceivedPerSecond virtualserver.ephemeralPacketsReceivedPerSecond F5 f5.virtualserver.ephemeralPacketsSentPerSecond virtualserver.ephemeralPacketsSentPerSecond F5 f5.virtualserver.ephemeralSlowKilledPerSecond virtualserver.ephemeralSlowKilledPerSecond F5 f5.virtualserver.evictedConnsPerSecond virtualserver.evictedConnsPerSecond F5 f5.virtualserver.inDataInBytesPerSecond virtualserver.inDataInBytesPerSecond F5 f5.virtualserver.outDataInBytesPerSecond virtualserver.outDataInBytesPerSecond F5 f5.virtualserver.packetsReceivedPerSecond virtualserver.packetsReceivedPerSecond F5 f5.virtualserver.packetsSentPerSecond virtualserver.packetsSentPerSecond F5 f5.virtualserver.requestsPerSecond virtualserver.requestsPerSecond F5 f5.virtualserver.slowKilledPerSecond virtualserver.slowKilledPerSecond F5 f5.virtualserver.usageRatio virtualserver.usageRatio HAProxy haproxy.backend.activeServers backend.activeServers HAProxy haproxy.backend.averageConnectTimeInSeconds backend.averageConnectTimeInSeconds HAProxy haproxy.backend.averageQueueTimeInSeconds backend.averageQueueTimeInSeconds HAProxy haproxy.backend.averageResponseTimeInSeconds backend.averageResponseTimeInSeconds HAProxy haproxy.backend.averageTotalSessionTimeInSeconds backend.averageTotalSessionTimeInSeconds HAProxy haproxy.backend.backupServers backend.backupServers HAProxy haproxy.backend.bytesInPerSecond backend.bytesInPerSecond HAProxy haproxy.backend.bytesOutPerSecond backend.bytesOutPerSecond HAProxy haproxy.backend.bytesThatBypassedCompressorPerSecond backend.bytesThatBypassedCompressorPerSecond HAProxy haproxy.backend.connectingRequestErrorsPerSecond backend.connectingRequestErrorsPerSecond HAProxy haproxy.backend.connectionRetriesPerSecond backend.connectionRetriesPerSecond HAProxy haproxy.backend.currentQueuedRequestsWithoutServer backend.currentQueuedRequestsWithoutServer HAProxy haproxy.backend.currentSessions backend.currentSessions HAProxy haproxy.backend.dataTransfersAbortedByClientPerSecond backend.dataTransfersAbortedByClientPerSecond HAProxy haproxy.backend.dataTransfersAbortedByServerPerSecond backend.dataTransfersAbortedByServerPerSecond HAProxy haproxy.backend.downtimeInSeconds backend.downtimeInSeconds HAProxy haproxy.backend.http100ResponsesPerSecond backend.http100ResponsesPerSecond HAProxy haproxy.backend.http200ResponsesPerSecond backend.http200ResponsesPerSecond HAProxy haproxy.backend.http300ResponsesPerSecond backend.http300ResponsesPerSecond HAProxy haproxy.backend.http400ResponsesPerSecond backend.http400ResponsesPerSecond HAProxy haproxy.backend.http500ResponsesPerSecond backend.http500ResponsesPerSecond HAProxy haproxy.backend.httpOtherResponsesPerSecond backend.httpOtherResponsesPerSecond HAProxy haproxy.backend.httpRequestsPerSecond backend.httpRequestsPerSecond HAProxy haproxy.backend.httpResponseBytesEmittedByCompressorPerSecond backend.httpResponseBytesEmittedByCompressorPerSecond HAProxy haproxy.backend.httpResponseBytesFedToCompressorPerSecond backend.httpResponseBytesFedToCompressorPerSecond HAProxy haproxy.backend.httpResponsesCompressedPerSecond backend.httpResponsesCompressedPerSecond HAProxy haproxy.backend.interceptedRequestsPerSecond backend.interceptedRequestsPerSecond HAProxy haproxy.backend.maxQueuedRequestsWithoutServer backend.maxQueuedRequestsWithoutServer HAProxy haproxy.backend.maxSessions backend.maxSessions HAProxy haproxy.backend.maxSessionsPerSecond backend.maxSessionsPerSecond HAProxy haproxy.backend.requestRedispatchPerSecond backend.requestRedispatchPerSecond HAProxy haproxy.backend.requestsDenied.securityConcernsPerSecond backend.requestsDenied.securityConcernsPerSecond HAProxy haproxy.backend.responseErrorsPerSecond backend.responseErrorsPerSecond HAProxy haproxy.backend.responsesDenied.securityConcernsPerSecond backend.responsesDenied.securityConcernsPerSecond HAProxy haproxy.backend.serverSelectedPerSecond backend.serverSelectedPerSecond HAProxy haproxy.backend.sessionsPerSecond backend.sessionsPerSecond HAProxy haproxy.backend.timeSinceLastSessionAssignedInSeconds backend.timeSinceLastSessionAssignedInSeconds HAProxy haproxy.backend.timeSinceLastUpDownTransitionInSeconds backend.timeSinceLastUpDownTransitionInSeconds HAProxy haproxy.backend.totalWeight backend.totalWeight HAProxy haproxy.backend.type backend.type HAProxy haproxy.backend.upToDownTransitionsPerSecond backend.upToDownTransitionsPerSecond HAProxy haproxy.frontend.bytesInPerSecond frontend.bytesInPerSecond HAProxy haproxy.frontend.bytesOutPerSecond frontend.bytesOutPerSecond HAProxy haproxy.frontend.connectionsPerSecond frontend.connectionsPerSecond HAProxy haproxy.frontend.currentSessions frontend.currentSessions HAProxy haproxy.frontend.http100ResponsesPerSecond frontend.http100ResponsesPerSecond HAProxy haproxy.frontend.http200ResponsesPerSecond frontend.http200ResponsesPerSecond HAProxy haproxy.frontend.http300ResponsesPerSecond frontend.http300ResponsesPerSecond HAProxy haproxy.frontend.http400ResponsesPerSecond frontend.http400ResponsesPerSecond HAProxy haproxy.frontend.http500ResponsesPerSecond frontend.http500ResponsesPerSecond HAProxy haproxy.frontend.httpOtherResponsesPerSecond frontend.httpOtherResponsesPerSecond HAProxy haproxy.frontend.httpRequests.maxPerSecond frontend.httpRequests.maxPerSecond HAProxy haproxy.frontend.httpRequestsPerSecond frontend.httpRequestsPerSecond HAProxy haproxy.frontend.interceptedRequestsPerSecond frontend.interceptedRequestsPerSecond HAProxy haproxy.frontend.maxConnectionsPerSecond frontend.maxConnectionsPerSecond HAProxy haproxy.frontend.maxSessions frontend.maxSessions HAProxy haproxy.frontend.maxSessionsPerSecond frontend.maxSessionsPerSecond HAProxy haproxy.frontend.requestErrorsPerSecond frontend.requestErrorsPerSecond HAProxy haproxy.frontend.requestsDenied.securityConcernsPerSecond frontend.requestsDenied.securityConcernsPerSecond HAProxy haproxy.frontend.requestsDenied.tcpRequestConnectionRulesPerSecond frontend.requestsDenied.tcpRequestConnectionRulesPerSecond HAProxy haproxy.frontend.requestsDenied.tcpRequestSessionRulesPerSecond frontend.requestsDenied.tcpRequestSessionRulesPerSecond HAProxy haproxy.frontend.responsesDenied.securityConcernsPerSecond frontend.responsesDenied.securityConcernsPerSecond HAProxy haproxy.frontend.sessionsPerSecond frontend.sessionsPerSecond HAProxy haproxy.server.averageConnectTimeInSeconds server.averageConnectTimeInSeconds HAProxy haproxy.server.averageQueueTimeInSeconds server.averageQueueTimeInSeconds HAProxy haproxy.server.averageResponseTimeInSeconds server.averageResponseTimeInSeconds HAProxy haproxy.server.averageTotalSessionTimeInSeconds server.averageTotalSessionTimeInSeconds HAProxy haproxy.server.bytesInPerSecond server.bytesInPerSecond HAProxy haproxy.server.bytesOutPerSecond server.bytesOutPerSecond HAProxy haproxy.server.connectingRequestErrorsPerSecond server.connectingRequestErrorsPerSecond HAProxy haproxy.server.connectionRetriesPerSecond server.connectionRetriesPerSecond HAProxy haproxy.server.currentQueuedRequestsWithoutServer server.currentQueuedRequestsWithoutServer HAProxy haproxy.server.currentSessions server.currentSessions HAProxy haproxy.server.dataTransfersAbortedByClientPerSecond server.dataTransfersAbortedByClientPerSecond HAProxy haproxy.server.dataTransfersAbortedByServerPerSecond server.dataTransfersAbortedByServerPerSecond HAProxy haproxy.server.downtimeInSeconds server.downtimeInSeconds HAProxy haproxy.server.failedChecksPerSecond server.failedChecksPerSecond HAProxy haproxy.server.healthCheckDurationInMilliseconds server.healthCheckDurationInMilliseconds HAProxy haproxy.server.http100ResponsesPerSecond server.http100ResponsesPerSecond HAProxy haproxy.server.http200ResponsesPerSecond server.http200ResponsesPerSecond HAProxy haproxy.server.http300ResponsesPerSecond server.http300ResponsesPerSecond HAProxy haproxy.server.http400ResponsesPerSecond server.http400ResponsesPerSecond HAProxy haproxy.server.http500ResponsesPerSecond server.http500ResponsesPerSecond HAProxy haproxy.server.httpOtherResponsesPerSecond server.httpOtherResponsesPerSecond HAProxy haproxy.server.isActive server.isActive HAProxy haproxy.server.isBackup server.isBackup HAProxy haproxy.server.maxQueuedRequestsWithoutServer server.maxQueuedRequestsWithoutServer HAProxy haproxy.server.maxSessions server.maxSessions HAProxy haproxy.server.maxSessionsPerSecond server.maxSessionsPerSecond HAProxy haproxy.server.requestRedispatchPerSecond server.requestRedispatchPerSecond HAProxy haproxy.server.requestsDenied.securityConcernsPerSecond server.requestsDenied.securityConcernsPerSecond HAProxy haproxy.server.responseErrorsPerSecond server.responseErrorsPerSecond HAProxy haproxy.server.responsesDenied.securityConcernsPerSecond server.responsesDenied.securityConcernsPerSecond HAProxy haproxy.server.serverSelectedPerSecond server.serverSelectedPerSecond HAProxy haproxy.server.serverWeight server.serverWeight HAProxy haproxy.server.sessionsPerSecond server.sessionsPerSecond HAProxy haproxy.server.throttlePercentage server.throttlePercentage HAProxy haproxy.server.timeSinceLastSessionAssignedInSeconds server.timeSinceLastSessionAssignedInSeconds HAProxy haproxy.server.timeSinceLastUpDownTransitionInSeconds server.timeSinceLastUpDownTransitionInSeconds HAProxy haproxy.server.type server.type HAProxy haproxy.server.upToDownTransitionsPerSecond server.upToDownTransitionsPerSecond Kafka kafka.broker.bytesWrittenToTopicPerSecond broker.bytesWrittenToTopicPerSecond Kafka kafka.broker.consumer.requestsExpiredPerSecond consumer.requestsExpiredPerSecond Kafka kafka.broker.follower.requestExpirationPerSecond follower.requestExpirationPerSecond Kafka kafka.broker.ioInPerSecond broker.IOInPerSecond Kafka kafka.broker.ioOutPerSecond broker.IOOutPerSecond Kafka kafka.broker.logFlushPerSecond broker.logFlushPerSecond Kafka kafka.broker.messagesInPerSecond broker.messagesInPerSecond Kafka kafka.broker.net.bytesRejectedPerSecond net.bytesRejectedPerSecond Kafka kafka.broker.replication.isrExpandsPerSecond replication.isrExpandsPerSecond Kafka kafka.broker.replication.isrShrinksPerSecond replication.isrShrinksPerSecond Kafka kafka.broker.replication.leaderElectionPerSecond replication.leaderElectionPerSecond Kafka kafka.broker.replication.uncleanLeaderElectionPerSecond replication.uncleanLeaderElectionPerSecond Kafka kafka.broker.replication.unreplicatedPartitions replication.unreplicatedPartitions Kafka kafka.broker.request.avgTimeFetch request.avgTimeFetch Kafka kafka.broker.request.avgTimeMetadata request.avgTimeMetadata Kafka kafka.broker.request.avgTimeMetadata99Percentile request.avgTimeMetadata99Percentile Kafka kafka.broker.request.avgTimeOffset request.avgTimeOffset Kafka kafka.broker.request.avgTimeOffset99Percentile request.avgTimeOffset99Percentile Kafka kafka.broker.request.avgTimeProduceRequest request.avgTimeProduceRequest Kafka kafka.broker.request.avgTimeUpdateMetadata request.avgTimeUpdateMetadata Kafka kafka.broker.request.avgTimeUpdateMetadata99Percentile request.avgTimeUpdateMetadata99Percentile Kafka kafka.broker.request.clientFetchesFailedPerSecond request.clientFetchesFailedPerSecond Kafka kafka.broker.request.fetchConsumerRequestsPerSecond request.fetchConsumerRequestsPerSecond Kafka kafka.broker.request.fetchFollowerRequestsPerSecond request.fetchFollowerRequestsPerSecond Kafka kafka.broker.request.fetchTime99Percentile request.fetchTime99Percentile Kafka kafka.broker.request.handlerIdle request.handlerIdle Kafka kafka.broker.request.listGroupsRequestsPerSecond request.listGroupsRequestsPerSecond Kafka kafka.broker.request.metadataRequestsPerSecond request.metadataRequestsPerSecond Kafka kafka.broker.request.offsetCommitRequestsPerSecond request.offsetCommitRequestsPerSecond Kafka kafka.broker.request.produceRequestsFailedPerSecond request.produceRequestsFailedPerSecond Kafka kafka.broker.request.produceRequestsPerSecond request.produceRequestsPerSecond Kafka kafka.broker.request.produceTime99Percentile request.produceTime99Percentile Kafka kafka.broker.topic.diskSize topic.diskSize Kafka kafka.topic.bytesInPerSec topic.BytesInPerSec Kafka kafka.topic.bytesOutPerSec topic.BytesOutPerSec Kafka kafka.topic.messagesInPerSec topic.MessagesInPerSec Kafka kafka.topic.partitionsWithNonPreferredLeader topic.partitionsWithNonPreferredLeader Kafka kafka.topic.respondsToMetadataRequests topic.respondsToMetadataRequests Kafka kafka.topic.retentionBytesOrTime topic.retentionBytesOrTime Kafka kafka.topic.underReplicatedPartitions topic.underReplicatedPartitions Kafka kafka.producer.ageMetadataUsedInMilliseconds producer.ageMetadataUsedInMilliseconds Kafka kafka.producer.availableBufferInBytes producer.availableBufferInBytes Kafka kafka.producer.avgBytesSentPerRequestInBytes producer.avgBytesSentPerRequestInBytes Kafka kafka.producer.avgCompressionRateRecordBatches producer.avgCompressionRateRecordBatches Kafka kafka.producer.avgRecordAccumulatorsInMilliseconds producer.avgRecordAccumulatorsInMilliseconds Kafka kafka.producer.avgRecordSizeInBytes producer.avgRecordSizeInBytes Kafka kafka.producer.avgRecordsSentPerSecond producer.avgRecordsSentPerSecond Kafka kafka.producer.avgRecordsSentPerTopicPerSecond producer.avgRecordsSentPerTopicPerSecond Kafka kafka.producer.avgRequestLatency producer.avgRequestLatencyPerSecond Kafka kafka.producer.avgThrottleTime producer.avgThrottleTime Kafka kafka.producer.bufferMemoryAvailableInBytes producer.bufferMemoryAvailableInBytes Kafka kafka.producer.bufferpoolWaitTime producer.bufferpoolWaitTime Kafka kafka.producer.bytesOutPerSecond producer.bytesOutPerSecond Kafka kafka.producer.compressionRateRecordBatches producer.compressionRateRecordBatches Kafka kafka.producer.ioWaitTime producer.ioWaitTime Kafka kafka.producer.maxBytesSentPerRequestInBytes producer.maxBytesSentPerRequestInBytes Kafka kafka.producer.maxRecordSizeInBytes producer.maxRecordSizeInBytes Kafka kafka.producer.maxRequestLatencyInMilliseconds producer.maxRequestLatencyInMilliseconds Kafka kafka.producer.maxThrottleTime producer.maxThrottleTime Kafka kafka.producer.requestPerSecond producer.requestPerSecond Kafka kafka.producer.requestsWaitingResponse producer.requestsWaitingResponse Kafka kafka.producer.responsePerSecond producer.responsePerSecond Kafka kafka.producer.threadsWaiting producer.threadsWaiting Kafka kafka.consumer.avgFetchSizeInBytes consumer.avgFetchSizeInBytes Kafka kafka.consumer.avgRecordConsumedPerTopic consumer.avgRecordConsumedPerTopic Kafka kafka.consumer.avgRecordConsumedPerTopicPerSecond consumer.avgRecordConsumedPerTopicPerSecond Kafka kafka.consumer.bytesInPerSecond consumer.bytesInPerSecond Kafka kafka.consumer.fetchPerSecond consumer.fetchPerSecond Kafka kafka.consumer.hwm consumer.hwm Kafka kafka.consumer.lag consumer.lag Kafka kafka.consumer.maxFetchSizeInBytes consumer.maxFetchSizeInBytes Kafka kafka.consumer.maxLag consumer.maxLag Kafka kafka.consumer.messageConsumptionPerSecond consumer.messageConsumptionPerSecond Kafka kafka.consumer.offset consumer.offset Kafka kafka.consumer.totalLag consumer.totalLag Kafka kafka.consumerGroup.maxLag consumerGroup.maxLag Kafka kafka.consumerGroup.totalLag consumerGroup.totalLag Kubernetes k8s.apiserver.goGoroutines goGoroutines Kubernetes k8s.apiserver.goThreads goThreads Kubernetes k8s.apiserver.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.apiserver.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.controllermanager.goGoroutines goGoroutines Kubernetes k8s.controllermanager.goThreads goThreads Kubernetes k8s.controllermanager.leaderElectionMasterStatus leaderElectionMasterStatus Kubernetes k8s.controllermanager.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.controllermanager.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.etcd.goGoroutines goGoroutines Kubernetes k8s.etcd.goThreads goThreads Kubernetes k8s.etcd.mvccDbTotalSizeInBytes etcdMvccDbTotalSizeInBytes Kubernetes k8s.etcd.networkClientGrpcReceivedBytesRate etcdNetworkClientGrpcReceivedBytesRate Kubernetes k8s.etcd.networkClientGrpcSentBytesRate etcdNetworkClientGrpcSentBytesRate Kubernetes k8s.etcd.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.etcd.process.maxFds processMaxFds Kubernetes k8s.etcd.process.openFds processOpenFds Kubernetes k8s.etcd.process.processFdsUtilization processFdsUtilization Kubernetes k8s.etcd.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.etcd.serverHasLeader etcdServerHasLeader Kubernetes k8s.etcd.serverLeaderChangesSeenDelta etcdServerLeaderChangesSeenDelta Kubernetes k8s.etcd.serverProposalsAppliedDelta etcdServerProposalsAppliedDelta Kubernetes k8s.etcd.serverProposalsAppliedRate etcdServerProposalsAppliedRate Kubernetes k8s.etcd.serverProposalsCommittedDelta etcdServerProposalsCommittedDelta Kubernetes k8s.etcd.serverProposalsCommittedRate etcdServerProposalsCommittedRate Kubernetes k8s.etcd.serverProposalsFailedDelta etcdServerProposalsFailedDelta Kubernetes k8s.etcd.serverProposalsFailedRate etcdServerProposalsFailedRate Kubernetes k8s.etcd.serverProposalsPending etcdServerProposalsPending Kubernetes k8s.scheduler.goGoroutines goGoroutines Kubernetes k8s.scheduler.goThreads goThreads Kubernetes k8s.scheduler.leaderElectionMasterStatus leaderElectionMasterStatus Kubernetes k8s.scheduler.podPreemptionVictims schedulerPodPreemptionVictims Kubernetes k8s.scheduler.preemptionAttemptsDelta schedulerPreemptionAttemptsDelta Kubernetes k8s.scheduler.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.scheduler.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.container.cpuCfsPeriodsDelta containerCpuCfsPeriodsDelta Kubernetes k8s.container.cpuCfsPeriodsTotal containerCpuCfsPeriodsTotal Kubernetes k8s.container.cpuCfsThrottledPeriodsDelta containerCpuCfsThrottledPeriodsDelta Kubernetes k8s.container.cpuCfsThrottledPeriodsTotal containerCpuCfsThrottledPeriodsTotal Kubernetes k8s.container.cpuCfsThrottledSecondsDelta containerCpuCfsThrottledSecondsDelta Kubernetes k8s.container.cpuCfsThrottledSecondsTotal containerCpuCfsThrottledSecondsTotal Kubernetes k8s.container.cpuCoresUtilization cpuCoresUtilization Kubernetes k8s.container.cpuLimitCores cpuLimitCores Kubernetes k8s.container.cpuRequestedCores cpuRequestedCores Kubernetes k8s.container.cpuUsedCores cpuUsedCores Kubernetes k8s.container.fsAvailableBytes fsAvailableBytes Kubernetes k8s.container.fsCapacityBytes fsCapacityBytes Kubernetes k8s.container.fsInodes fsInodes Kubernetes k8s.container.fsInodesFree fsInodesFree Kubernetes k8s.container.fsInodesUsed fsInodesUsed Kubernetes k8s.container.fsUsedBytes fsUsedBytes Kubernetes k8s.container.fsUsedPercent fsUsedPercent Kubernetes k8s.container.isReady isReady Kubernetes k8s.container.memoryLimitBytes memoryLimitBytes Kubernetes k8s.container.memoryMappedFileBytes containerMemoryMappedFileBytes Kubernetes k8s.container.memoryRequestedBytes memoryRequestedBytes Kubernetes k8s.container.memoryUsedBytes memoryUsedBytes Kubernetes k8s.container.memoryUtilization memoryUtilization Kubernetes k8s.container.memoryWorkingSetBytes memoryWorkingSetBytes Kubernetes k8s.container.requestedCpuCoresUtilization requestedCpuCoresUtilization Kubernetes k8s.container.requestedMemoryUtilization requestedMemoryUtilization Kubernetes k8s.container.restartCount restartCount Kubernetes k8s.daemonset.createdAt createdAt Kubernetes k8s.daemonset.metadataGeneration metadataGeneration Kubernetes k8s.daemonset.podsAvailable podsAvailable Kubernetes k8s.daemonset.podsDesired podsDesired Kubernetes k8s.daemonset.podsMisscheduled podsMisscheduled Kubernetes k8s.daemonset.podsReady podsReady Kubernetes k8s.daemonset.podsScheduled podsScheduled Kubernetes k8s.daemonset.podsUnavailable podsUnavailable Kubernetes k8s.daemonset.podsUpdatedScheduled podsUpdatedScheduled Kubernetes k8s.deployment.createdAt createdAt Kubernetes k8s.deployment.podsAvailable podsAvailable Kubernetes k8s.deployment.podsDesired podsDesired Kubernetes k8s.deployment.podsMaxUnavailable podsMaxUnavailable Kubernetes k8s.deployment.podsTotal podsTotal Kubernetes k8s.deployment.podsUnavailable podsUnavailable Kubernetes k8s.deployment.podsUpdated podsUpdated Kubernetes k8s.endpoint.addressAvailable addressAvailable Kubernetes k8s.endpoint.addressNotReady addressNotReady Kubernetes k8s.endpoint.createdAt createdAt Kubernetes k8s.namespace.createdAt createdAt Kubernetes k8s.node.allocatableAttachableVolumes* allocatableAttachableVolumes* Kubernetes k8s.node.allocatableCpuCores allocatableCpuCores Kubernetes k8s.node.allocatableCpuCoresUtilization allocatableCpuCoresUtilization Kubernetes k8s.node.allocatableEphemeralStorageBytes allocatableEphemeralStorageBytes Kubernetes k8s.node.allocatableHugepages* allocatableHugepages* Kubernetes k8s.node.allocatableMemoryBytes allocatableMemoryBytes Kubernetes k8s.node.allocatableMemoryUtilization allocatableMemoryUtilization Kubernetes k8s.node.allocatablePods allocatablePods Kubernetes k8s.node.capacityAttachableVolumes* capacityAttachableVolumes* Kubernetes k8s.node.capacityCpuCores capacityCpuCores Kubernetes k8s.node.capacityEphemeralStorageBytes capacityEphemeralStorageBytes Kubernetes k8s.node.capacityHugepages* capacityHugepages* Kubernetes k8s.node.capacityMemoryBytes capacityMemoryBytes Kubernetes k8s.node.capacityPods capacityPods Kubernetes k8s.node.cpuUsedCoreMilliseconds cpuUsedCoreMilliseconds Kubernetes k8s.node.cpuUsedCores cpuUsedCores Kubernetes k8s.node.fsAvailableBytes fsAvailableBytes Kubernetes k8s.node.fsCapacityBytes fsCapacityBytes Kubernetes k8s.node.fsCapacityUtilization fsCapacityUtilization Kubernetes k8s.node.fsInodes fsInodes Kubernetes k8s.node.fsInodesFree fsInodesFree Kubernetes k8s.node.fsInodesUsed fsInodesUsed Kubernetes k8s.node.fsUsedBytes fsUsedBytes Kubernetes k8s.node.memoryAvailableBytes memoryAvailableBytes Kubernetes k8s.node.memoryMajorPageFaultsPerSecond memoryMajorPageFaultsPerSecond Kubernetes k8s.node.memoryPageFaults memoryPageFaults Kubernetes k8s.node.memoryRssBytes memoryRssBytes Kubernetes k8s.node.memoryUsedBytes memoryUsedBytes Kubernetes k8s.node.memoryWorkingSetBytes memoryWorkingSetBytes Kubernetes k8s.node.netErrorsPerSecond net.errorsPerSecond Kubernetes k8s.node.netRxBytesPerSecond net.rxBytesPerSecond Kubernetes k8s.node.netTxBytesPerSecond net.txBytesPerSecond Kubernetes k8s.node.runtimeAvailableBytes runtimeAvailableBytes Kubernetes k8s.node.runtimeCapacityBytes runtimeCapacityBytes Kubernetes k8s.node.runtimeInodes runtimeInodes Kubernetes k8s.node.runtimeInodesFree runtimeInodesFree Kubernetes k8s.node.runtimeInodesUsed runtimeInodesUsed Kubernetes k8s.node.runtimeUsedBytes runtimeUsedBytes Kubernetes k8s.pod.createdAt createdAt Kubernetes k8s.pod.isReady isReady Kubernetes k8s.pod.isScheduled isScheduled Kubernetes k8s.pod.netErrorsPerSecond net.errorsPerSecond Kubernetes k8s.pod.netRxBytesPerSecond net.rxBytesPerSecond Kubernetes k8s.pod.netTxBytesPerSecond net.txBytesPerSecond Kubernetes k8s.pod.startTime startTime Kubernetes k8s.replicaset.createdAt createdAt Kubernetes k8s.replicaset.observedGeneration observedGeneration Kubernetes k8s.replicaset.podsDesired podsDesired Kubernetes k8s.replicaset.podsFullyLabeled podsFullyLabeled Kubernetes k8s.replicaset.podsMissing podsMissing Kubernetes k8s.replicaset.podsReady podsReady Kubernetes k8s.replicaset.podsTotal podsTotal Kubernetes k8s.service.createdAt createdAt Kubernetes k8s.statefulset.createdAt createdAt Kubernetes k8s.statefulset.currentRevision currentRevision Kubernetes k8s.statefulset.metadataGeneration metadataGeneration Kubernetes k8s.statefulset.observedGeneration observedGeneration Kubernetes k8s.statefulset.podsCurrent podsCurrent Kubernetes k8s.statefulset.podsDesired podsDesired Kubernetes k8s.statefulset.podsReady podsReady Kubernetes k8s.statefulset.podsTotal podsTotal Kubernetes k8s.statefulset.podsUpdated podsUpdated Kubernetes k8s.statefulset.updateRevision updateRevision Kubernetes k8s.volume.fsAvailableBytes fsAvailableBytes Kubernetes k8s.volume.fsCapacityBytes fsCapacityBytes Kubernetes k8s.volume.fsInodes fsInodes Kubernetes k8s.volume.fsInodesFree fsInodesFree Kubernetes k8s.volume.fsInodesUsed fsInodesUsed Kubernetes k8s.volume.fsUsedBytes fsUsedBytes Kubernetes k8s.volume.fsUsedPercent fsUsedPercent Memcached memcached.server.activeSlabs activeSlabs Memcached memcached.server.avgItemSizeInBytes avgItemSizeInBytes Memcached memcached.server.bytesReadServerPerSecond bytesReadServerPerSecond Memcached memcached.server.bytesUsedServerInBytes bytesUsedServerInBytes Memcached memcached.server.bytesWrittenServerPerSecond bytesWrittenServerPerSecond Memcached memcached.server.casHitRatePerSecond casHitRatePerSecond Memcached memcached.server.casMissRatePerSecond casMissRatePerSecond Memcached memcached.server.casWrongRatePerSecond casWrongRatePerSecond Memcached memcached.server.cmdFlushRatePerSecond cmdFlushRatePerSecond Memcached memcached.server.cmdGetRatePerSecond cmdGetRatePerSecond Memcached memcached.server.cmdSetRatePerSecond cmdSetRatePerSecond Memcached memcached.server.connectionRateServerPerSecond connectionRateServerPerSecond Memcached memcached.server.connectionStructuresAllocated connectionStructuresAllocated Memcached memcached.server.currentItemsStoredServer currentItemsStoredServer Memcached memcached.server.deleteCmdNoneRemovedPerSecond deleteCmdNoneRemovedPerSecond Memcached memcached.server.deleteCmdRemovedPerSecond deleteCmdRemovedPerSecond Memcached memcached.server.evictionsPerSecond evictionsPerSecond Memcached memcached.server.getHitPercent getHitPercent Memcached memcached.server.getHitPerSecond getHitPerSecond Memcached memcached.server.getMissPerSecond getMissPerSecond Memcached memcached.server.itemsStoredPerSecond itemsStoredPerSecond Memcached memcached.server.limitBytesStorage limitBytesStorage Memcached memcached.server.limitMaxBytes limitMaxBytes Memcached memcached.server.maxConnectionLimitPerSecond serverMaxConnectionLimitPerSecond Memcached memcached.server.memAllocatedSlabsInBytes memAllocatedSlabsInBytes Memcached memcached.server.openConnectionsServer openConnectionsServer Memcached memcached.server.pointerSize pointerSize Memcached memcached.server.rusageSystem usageRate Memcached memcached.server.rusageUser executionTime Memcached memcached.server.storingItemsPercentMemory storingItemsPercentMemory Memcached memcached.server.threads threads Memcached memcached.server.uptimeInMilliseconds uptimeInMilliseconds Memcached memcached.slab.activeItemsBumpedPerSecond activeItemsBumpedPerSecond Memcached memcached.slab.casBadValPerSecond casBadValPerSecond Memcached memcached.slab.casModifiedSlabPerSecond casModifiedSlabPerSecond Memcached memcached.slab.chunkSizeInBytes chunkSizeInBytes Memcached memcached.slab.chunksPerPage chunksPerPage Memcached memcached.slab.cmdSetRateSlabPerSecond cmdSetRateSlabPerSecond Memcached memcached.slab.decrsModifySlabPerSecond decrsModifySlabPerSecond Memcached memcached.slab.deleteRateSlabPerSecond deleteRateSlabPerSecond Memcached memcached.slab.entriesReclaimedPerSecond entriesReclaimedPerSecond Memcached memcached.slab.evictionsBeforeExpirationPerSecond evictionsBeforeExpirationPerSecond Memcached memcached.slab.evictionsBeforeExplicitExpirationPerSecond evictionsBeforeExplicitExpirationPerSecond Memcached memcached.slab.expiredItemsReclaimedPerSecond expiredItemsReclaimedPerSecond Memcached memcached.slab.freedChunks freedChunks Memcached memcached.slab.freedChunksEnd freedChunksEnd Memcached memcached.slab.getHitRateSlabPerSecond getHitRateSlabPerSecond Memcached memcached.slab.incrsModifySlabPerSecond incrsModifySlabPerSecond Memcached memcached.slab.itemsCold itemsCold Memcached memcached.slab.itemsColdPerSecond itemsColdPerSecond Memcached memcached.slab.itemsDirectReclaimedPerSecond itemsDirectReclaimedPerSecond Memcached memcached.slab.itemsFreedCrawlerPerSecond itemsFreedCrawlerPerSecond Memcached memcached.slab.itemsHot itemsHot Memcached memcached.slab.itemsOldestInMilliseconds itemsOldestInMilliseconds Memcached memcached.slab.itemsRefcountLockedPerSecond itemsRefcountLockedPerSecond Memcached memcached.slab.itemsSlabClass itemsSlabClass Memcached memcached.slab.itemsTimeSinceEvictionInMilliseconds itemsTimeSinceEvictionInMilliseconds Memcached memcached.slab.itemsWarm itemsWarm Memcached memcached.slab.itemsWarmPerSecond itemsWarmPerSecond Memcached memcached.slab.memRequestedSlabInBytesPerSecond memRequestedSlabInBytesPerSecond Memcached memcached.slab.outOfMemoryPerSecond outOfMemoryPerSecond Memcached memcached.slab.selfHealedSlabPerSecond selfHealedSlabPerSecond Memcached memcached.slab.totalChunksSlab totalChunksSlab Memcached memcached.slab.totalPagesSlab totalPagesSlab Memcached memcached.slab.touchHitSlabPerSecond touchHitSlabPerSecond Memcached memcached.slab.usedChunksItems usedChunksItems Memcached memcached.slab.usedChunksPerSecond usedChunksPerSecond Memcached memcached.slab.validItemsEvictedPerSecond validItemsEvictedPerSecond MongoDB mongo.index.accesses collection.indexAccesses MongoDB mongo.index.sizeInBytes collection.indexSizeInBytes MongoDB mongo.collection.avgObjSizeInBytes collection.avgObjSizeInBytes MongoDB mongo.collection.capped collection.capped MongoDB mongo.collection.count collection.count MongoDB mongo.collection.max collection.max MongoDB mongo.collection.maxSizeInBytes collection.maxSizeInBytes MongoDB mongo.collection.nindexes collection.nindexes MongoDB mongo.collection.sizeInBytes collection.sizeInBytes MongoDB mongo.collection.storageSizeInBytes collection.storageSizeInBytes MongoDB mongo.configServer.asserts.messagesPerSecond asserts.messagesPerSecond MongoDB mongo.configServer.asserts.regularPerSecond asserts.regularPerSecond MongoDB mongo.configServer.asserts.rolloversPerSecond asserts.rolloversPerSecond MongoDB mongo.configServer.asserts.userPerSecond asserts.userPerSecond MongoDB mongo.configServer.asserts.warningPerSecond asserts.warningPerSecond MongoDB mongo.configServer.commands.countFailedPerSecond commands.countFailedPerSecond MongoDB mongo.configServer.commands.countPerSecond commands.countPerSecond MongoDB mongo.configServer.commands.createIndexesFailedPerSecond commands.createIndexesFailedPerSecond MongoDB mongo.configServer.commands.createIndexesPerSecond commands.createIndexesPerSecond MongoDB mongo.configServer.commands.deleteFailedPerSecond commands.deleteFailedPerSecond MongoDB mongo.configServer.commands.deletePerSecond commands.deletePerSecond MongoDB mongo.configServer.commands.evalFailedPerSecond commands.evalFailedPerSecond MongoDB mongo.configServer.commands.evalPerSecond commands.evalPerSecond MongoDB mongo.configServer.commands.findAndModifyFailedPerSecond commands.findAndModifyFailedPerSecond MongoDB mongo.configServer.commands.findAndModifyPerSecond commands.findAndModifyPerSecond MongoDB mongo.configServer.commands.insertFailedPerSecond commands.insertFailedPerSecond MongoDB mongo.configServer.commands.insertPerSecond commands.insertPerSecond MongoDB mongo.configServer.commands.updateFailedPerSecond commands.updateFailedPerSecond MongoDB mongo.configServer.commands.updatePerSecond commands.updatePerSecond MongoDB mongo.configServer.connections.available connections.available MongoDB mongo.configServer.connections.current connections.current MongoDB mongo.configServer.connections.totalCreated connections.totalCreated MongoDB mongo.configServer.cursor.openNoTimeout cursor.openNoTimeout MongoDB mongo.configServer.cursor.openPinned cursor.openPinned MongoDB mongo.configServer.cursor.openTotal cursor.openTotal MongoDB mongo.configServer.cursor.timedOutPerSecond cursor.timedOutPerSecond MongoDB mongo.configServer.document.deletedPerSecond document.deletedPerSecond MongoDB mongo.configServer.document.insertedPerSecond document.insertedPerSecond MongoDB mongo.configServer.document.returnedPerSecond document.returnedPerSecond MongoDB mongo.configServer.document.updatedPerSecond document.updatedPerSecond MongoDB mongo.configServer.dur.commits dur.commits MongoDB mongo.configServer.dur.commitsInWriteLock dur.commitsInWriteLock MongoDB mongo.configServer.dur.compression dur.compression MongoDB mongo.configServer.dur.earlyCommits dur.earlyCommits MongoDB mongo.configServer.dur.preparingInMilliseconds dur.preparingInMilliseconds MongoDB mongo.configServer.dur.remappingInMilliseconds dur.remappingInMilliseconds MongoDB mongo.configServer.dur.timeCollectedCommitsInMilliseconds dur.timeCollectedCommitsInMilliseconds MongoDB mongo.configServer.dur.writingDataFilesInMilliseconds dur.writingDataFilesInMilliseconds MongoDB mongo.configServer.dur.writingJournalInMilliseconds dur.writingJournalInMilliseconds MongoDB mongo.configServer.flush.averageInMilliseconds flush.averageInMilliseconds MongoDB mongo.configServer.flush.flushesDisk flush.flushesDisk MongoDB mongo.configServer.flush.lastInMilliseconds flush.lastInMilliseconds MongoDB mongo.configServer.flush.totalInMilliseconds flush.totalInMilliseconds MongoDB mongo.configServer.getlasterror.wtimeMillisPerSecond getlasterror.wtimeMillisPerSecond MongoDB mongo.configServer.getlasterror.wtimeoutsPerSecond getlasterror.wtimeoutsPerSecond MongoDB mongo.configServer.globallock.activeClientsReaders globallock.activeClientsReaders MongoDB mongo.configServer.globallock.activeClientsTotal globallock.activeClientsTotal MongoDB mongo.configServer.globallock.activeClientsWriters globallock.activeClientsWriters MongoDB mongo.configServer.globallock.currentQueueReaders globallock.currentQueueReaders MongoDB mongo.configServer.globallock.currentQueueTotal globallock.currentQueueTotal MongoDB mongo.configServer.globallock.currentQueueWriters globallock.currentQueueWriters MongoDB mongo.configServer.globallock.totalTime globallock.totaltime MongoDB mongo.configServer.locks.collectionAcquireExclusive locks.collectionAcquireExclusive MongoDB mongo.configServer.locks.collectionAcquireIntentExclusive locks.collectionAcquireIntentExclusive MongoDB mongo.configServer.locks.collectionAcquireIntentShared locks.collectionAcquireIntentShared MongoDB mongo.configServer.locks.collectionAcquireWaitCountExclusive locks.collectionAcquireWaitCountExclusive MongoDB mongo.configServer.locks.collectionTimeAcquiringMicrosExclusive locks.collectionTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.databaseAcquireExclusive locks.databaseAcquireExclusive MongoDB mongo.configServer.locks.databaseAcquireIntentExclusive locks.databaseAcquireIntentExclusive MongoDB mongo.configServer.locks.databaseAcquireIntentShared locks.databaseAcquireIntentShared MongoDB mongo.configServer.locks.databaseAcquireShared locks.databaseAcquireShared MongoDB mongo.configServer.locks.databaseAcquireWaitExclusive locks.databaseAcquireWaitExclusive MongoDB mongo.configServer.locks.databaseAcquireWaitIntentExclusive locks.databaseAcquireWaitIntentExclusive MongoDB mongo.configServer.locks.databaseAcquireWaitIntentShared locks.databaseAcquireWaitIntentShared MongoDB mongo.configServer.locks.databaseAcquireWaitShared locks.databaseAcquireWaitShared MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosExclusive locks.databaseTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosIntentExclusive locks.databaseTimeAcquiringMicrosIntentExclusive MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosIntentShared locks.databaseTimeAcquiringMicrosIntentShared MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosShared locks.databaseTimeAcquiringMicrosShared MongoDB mongo.configServer.locks.globalAcquireExclusive locks.globalAcquireExclusive MongoDB mongo.configServer.locks.globalAcquireIntentExclusive locks.globalAcquireIntentExclusive MongoDB mongo.configServer.locks.globalAcquireIntentShared locks.globalAcquireIntentShared MongoDB mongo.configServer.locks.globalAcquireShared locks.globalAcquireShared MongoDB mongo.configServer.locks.globalAcquireWaitExclusive locks.globalAcquireWaitExclusive MongoDB mongo.configServer.locks.globalAcquireWaitIntentExclusive locks.globalAcquireWaitIntentExclusive MongoDB mongo.configServer.locks.globalAcquireWaitIntentShared locks.globalAcquireWaitIntentShared MongoDB mongo.configServer.locks.globalAcquireWaitShared locks.globalAcquireWaitShared MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosExclusive locks.globalTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosIntentExclusive locks.globalTimeAcquiringMicrosIntentExclusive MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosIntentShared locks.globalTimeAcquiringMicrosIntentShared MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosShared locks.globalTimeAcquiringMicrosShared MongoDB mongo.configServer.locks.metadataAcquireExclusive locks.metadataAcquireExclusive MongoDB mongo.configServer.locks.oplogAcquireExclusive locks.oplogAcquireExclusive MongoDB mongo.configServer.locks.oplogAcquireIntentExclusive locks.oplogAcquireIntentExclusive MongoDB mongo.configServer.locks.oplogAcquireIntentShared locks.oplogAcquireIntentShared MongoDB mongo",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 208.53172,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integrations</em> metrics",
        "sections": "On-host <em>integrations</em> metrics",
        "body": " elasticsearch.node.transport.packetsReceivedInBytes transport.packetsReceivedInBytes ElasticSearch elasticsearch.node.transport.packetsSent transport.packetsSent ElasticSearch elasticsearch.node.transport.packetsSentInBytes transport.packetsSentInBytes <em>F5</em> <em>f5</em>.node.availabilityState node.availabilityState <em>F5</em> <em>f5</em>.node.connections"
      },
      "id": "603e8a8a64441f69a34e8841"
    }
  ],
  "/docs/integrations/host-integrations/open-source-host-integrations-list/memcached-open-source-integration": [
    {
      "sections": [
        "New Relic guided install overview",
        "Supported APM agents",
        "Why it matters",
        "Some technical detail",
        "Important",
        "On-host integration (OHI) recipes"
      ],
      "title": "New Relic guided install overview",
      "type": "docs",
      "tags": [
        "Full-Stack Observability",
        "Observe everything",
        "Get started"
      ],
      "external_id": "2058522f6cb1e82dbbe111a176c22ec4aa515ae5",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/full-stack-observability/observe-everything/get-started/new-relic-guided-install-overview/",
      "published_at": "2021-05-04T22:25:26Z",
      "updated_at": "2021-04-12T05:47:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Instrument your systems and send telemetry data to New Relic with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click the Guided install button. If your account reports data through our EU datacenter, click EU Guided install. Guided install EU Guided install Our infrastructure agent discovers the applications and infrastructure and log sources running in your environment, and recommends which ones should be instrumented. The install automates the configuration and deployment of each system you choose to instrument. Supported APM agents If you have a .NET Windows application on IIS, the guided install configures and enables an APM agent. Guided install for .NET EU Guided install for .NET Why it matters With our guided install, you can instrument your applications and infrastructure and start seeing your data in New Relic in minutes. The guided install uses our command line interface (CLI), the infrastructure agent for your host environment, and a library of installation recipes to instrument your applications and infrastructure for you. That means less toil for you. Because our instrumentation recipes are open source, you can modify existing recipes, or build new ones, to suit your needs. Some technical detail The New Relic guided install uses open source installation recipes to instrument on-host integrations. These recipes include installation and setup commands, information about logs, and metadata related to whats being installed. They're collected in a YAML file for each type of system and have all of the installation details necessary to install the infrastructure agent for a specific integration. Important On Windows, our guided install only supports Microsoft SQL Server, logs, and the infrastructure agent. All other integrations are only supported on Linux. On-host integration (OHI) recipes The guided install automates the discovery, configuration, and installation of OHIs. However, there may be times when you want to instrument them one-by-one using the CLI install command. To install any individual on-host integration, run this command: curl -Ls https://raw.githubusercontent.com/newrelic/newrelic-cli/master/scripts/install.sh | bash && sudo NEW_RELIC_API_KEY=API_KEY NEW_RELIC_ACCOUNT_ID=ACCOUNT_ID /usr/local/bin/newrelic install -n INTEGRATION-FLAG Copy For example: curl -Ls https://raw.githubusercontent.com/newrelic/newrelic-cli/master/scripts/install.sh | bash && sudo NEW_RELIC_API_KEY=<API_KEY> NEW_RELIC_ACCOUNT_ID=<ACCOUNT_ID> /usr/local/bin/newrelic install -n apache-open-source-integration Copy The table lists the integrations supported by the guided install CLI command. The specific on-host integration commands are provided for your reference. Our open source integrations send performance metrics and inventory data from your servers and applications to the New Relic platform. You can view pre-built dashboards of your metric data, create alert policies, and create your own custom queries and charts. Integration Command Apache newrelic install -n apache-open-source-integration Cassandra newrelic install -n cassandra-open-source-integration Couchbase newrelic install -n couchbase-open-source-integration ElasticSearch newrelic install -n elasticsearch-open-source-integration HAProxy newrelic install -n haproxy-open-source-integration HashiCorp Consul newrelic install -n hashicorp-consul-open-source-integration JMX newrelic install -n jmx-open-source-integration Memcached newrelic install -n memcached-open-source-integration Microsoft SQL Server (Windows only) newrelic install -n mssql-server-integration-installer MongoDB newrelic install -n mongodb-open-source-integration MySQL newrelic install -n mysql-open-source-integration Nagios newrelic install -n nagios-open-source-integration Nginx newrelic install -n nginx-open-source-integration PostgreSQL newrelic install -n postgres-open-source-integration RabbitMQ newrelic install -n rabbitmq-open-source-integration Redis newrelic install -n redis-open-source-integration Varnish Cache newrelic install -n varnish-cache-open-source-integration",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 588.2147,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "On-host <em>integration</em> (OHI) recipes",
        "body": "-<em>source</em>-<em>integration</em> ElasticSearch newrelic install -n elasticsearch-<em>open</em>-<em>source</em>-<em>integration</em> HAProxy newrelic install -n haproxy-<em>open</em>-<em>source</em>-<em>integration</em> HashiCorp Consul newrelic install -n hashicorp-consul-<em>open</em>-<em>source</em>-<em>integration</em> JMX newrelic install -n jmx-<em>open</em>-<em>source</em>-<em>integration</em> <em>Memcached</em> newrelic"
      },
      "id": "604130a7e7b9d299cb2a07c0"
    },
    {
      "sections": [
        "OpenTelemetry quick start",
        "Step 1. Prerequisites",
        "Step 2. Instrument your service with OpenTelemetry",
        "Tip",
        "Step 3. Send your telemetry data to New Relic",
        "Important",
        "Use the OpenTelemetry collector (recommended)",
        "Use the native OTLP endpoint (pre-release)",
        "Step 4: View your data in the New Relic UI"
      ],
      "title": "OpenTelemetry quick start",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "1b846417a2958b61b047c838db49aea06f09a2a8",
      "image": "https://docs.newrelic.com/static/820ec30261e57dd485d471987fae4a28/0f2bc/collector_introduction_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start/",
      "published_at": "2021-05-05T00:34:35Z",
      "updated_at": "2021-05-05T00:34:35Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up OpenTelemetry with New Relic. Here's an overview of the process, followed by details for each step. Prerequisites Instrument your service with OpenTelemetry Send your telemetry data to New Relic View your data in the New Relic UI In the following sections, we explain some basic architectural approaches, but if you want to explore other implementation options, check out OpenTelemetry architecture recipes. Step 1. Prerequisites First things first: If we dont already know you, sign up for a free New Relic account. Make sure you have an Insights insert key to send spans and metrics to New Relic. Step 2. Instrument your service with OpenTelemetry To get started, you instrument your service with OpenTelemetry. OpenTelemetry has language-specific products and SDKs to help you. Many languages offer out-the-box instrumentation for common libraries and frameworks. Each language also provides an API for further instrumenting your service manually. Tip We recommend that you instrument as many services as possible to get the most benefit from distributed tracing. Go to the repository for your language and follow the instructions to instrument your service. When you're done, return here to complete the next step of sending your telemetry data to New Relic. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...See a complete list of languages in GitHub Step 3. Send your telemetry data to New Relic Choose how you want to export your telemetry data to New Relic: Use the OpenTelemetry collector (recommended) Use the native OTLP endpoint (pre-release) You can use one or both of these approaches in your environment. We'll discuss some highlights of each approach here, but if you need more background, see Introduction to OpenTelemetry. If you are interested in tracing, there are two main options for trace sampling: Configure the head-based, native sampling in OpenTelemetry, which means OpenTelemetry samples traces before they are sent to New Relic. Head-based sampling doesnt analyze all traces, but instead randomly samples traces up front before details about the completed traces are known. Both the OpenTelemetry collector and the native OTLP endpoint support this option. If you want New Relic to analyze all your traces, configure tail-based sampling with New Relic Infinite Tracing, which reroutes traces to our cloud-based trace observer. The trace observer accepts all your traces and sorts through them to find useful ones. If you want to know more about this option, especially if you want to use it in the EU, see Introduction to Infinite Tracing. While Infinite Tracing is not yet compatible with the native OTLP endpoint, it is still possible to configure tail-based sampling via the collector, for more information see Tail Sampling Processor. Important New Relic's language-specific exporters for OpenTelemetry are now deprecated in favor of the OpenTelemetry collector and native OTLP endpoint options described here. If you were previously using a New Relic language-specific exporter consider using the OTLP exporter for your language and send data directly to New Relic's native OTLP endpoint (pre-release). Use the OpenTelemetry collector (recommended) The OpenTelemetry project provides a tool called the OpenTelemetry Collector that you can deploy and use as an intermediate data aggregator. In your service, you use the OpenTelemetry exporter to send telemetry data first to the OpenTelemetry collector. Then, in the OpenTelemetry collector, you enable the New Relic exporter to send data to New Relic. The diagram below shows the flow of data with the collector. To use the collector: Configure your OpenTelemetry collector to export data to New Relic, using our example as a guide. Configure your services OTLP exporter to send data to your collector, following the documentation for your language's OTLP exporter: C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...Find additional OTLP language support in GitHub Use the native OTLP endpoint (pre-release) The example above uses a New Relic exporter, but we have a pre-release program if you want to try out the native OTLP endpoint for sending your data to New Relic. You can either use the OTLP exporter in the OpenTelemetry collector or send us data directly from your service. If you are interested, let us know by completing this form. Step 4: View your data in the New Relic UI Once youve instrumented your service and configured it to export its data to New Relic, you can go to New Relic and view your data. The UI for OpenTelemetry has some similarities to the APM agent UI, so if you are familiar with that, you can go right to the UI. If you need help understanding your OpenTelemetry UI options, see View your OpenTelemetry data in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 223.1951,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> quick start",
        "sections": "<em>OpenTelemetry</em> quick start",
        "tags": "<em>Open</em> <em>source</em> telemetry <em>integrations</em>",
        "body": "<em>Open</em>Telemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up <em>Open</em>Telemetry with New Relic. Here&#x27;s an overview of the process, followed by details for each step. Prerequisites Instrument your service with <em>Open</em>Telemetry Send"
      },
      "id": "6044e5dfe7b9d2aadc5799d4"
    },
    {
      "sections": [
        "Elixir open-source agent",
        "Tip",
        "Get started",
        "For more help"
      ],
      "title": "Elixir open-source agent",
      "type": "docs",
      "tags": [
        "Agents",
        "Open-source licensed agents",
        "Open-source licensed agents"
      ],
      "external_id": "aa03e1693b6ecdd06fa2940ddb99187247743772",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/elixir/elixir-open-source-agent/",
      "published_at": "2021-05-05T18:21:57Z",
      "updated_at": "2021-04-27T11:09:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Monitor Elixir behavior with New Relic using the Elixir open-source agent. The agent: Helps you track transactions, distributed traces, and other parts of your applications behavior Provides an overview of underlying BEAM activity Tip This agent is released as open source on GitHub. A change log is also available there for the latest updates. Get started For requirements, installation, and configuration information, see the Open Source Elixir Agent README on GitHub. Visit New Relics Elixir repository on GitHub for questions about installation, usage, or other topics. Report issues or bugs as an issue in the GitHub repository. For more help Recommendations for learning more: Browse New Relic's Explorers Hub for community discussions about the open-source Elixir agent. Review New Relic's licenses, attributions, data usage limits, and other notices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 180.11786,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elixir <em>open</em>-<em>source</em> agent",
        "sections": "Elixir <em>open</em>-<em>source</em> agent",
        "tags": "<em>Open</em>-<em>source</em> licensed agents",
        "body": "Monitor Elixir behavior with New Relic using the Elixir <em>open</em>-<em>source</em> agent. The agent: Helps you track transactions, distributed traces, and other parts of your applications behavior Provides an overview of underlying BEAM activity Tip This agent is released as <em>open</em> <em>source</em> on GitHub. A change log"
      },
      "id": "6087f0ff28ccbceab351c13f"
    }
  ],
  "/docs/integrations/host-integrations/troubleshooting/not-seeing-host-integration-data": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.82642,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.1604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.1603,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/troubleshooting/pass-infrastructure-agent-parameters-host-integration": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.82642,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.1604,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.1603,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/troubleshooting/run-integrations-manually": [
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.82632,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Elasticsearch monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " for install outside of a package manager. On-<em>host</em> <em>integrations</em> do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration&#x27;s YAML-format configuration is where you can place required login credentials"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    },
    {
      "sections": [
        "VMware Tanzu monitoring integration",
        "Tip",
        "Features",
        "Compatibility and requirements",
        "Install and activate",
        "Find and use data",
        "Important",
        "Set up an alert",
        "Metric data",
        "PCFCounterEvent",
        "PCFHttpStartStop",
        "PCFLogMessage",
        "PCFValueMetric",
        "Fields shared across metric data"
      ],
      "title": "VMware Tanzu monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "92c838d3debb517d3691db6f2c3bd39f31a63e3d",
      "image": "https://docs.newrelic.com/static/770808ce3e9e7fbade510e440fa988c6/c1b63/tanzu-alert-chart.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/vmware-tanzu-monitoring-integration/",
      "published_at": "2021-05-04T16:29:18Z",
      "updated_at": "2021-05-04T16:29:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our VMware Tanzu integration helps you understand the health and performance of your Tanzu environment. Query data from different Tanzu instances and cloud providers, and go from high level views down to the most granular data, such as the last duration of the garbage collector pause. VMware Tanzu data visualized in a New Relic One dashboard. The integration uses Loggregator to collect metrics and events generated by all Tanzu platform components and applications that run on cells. It connects to our platform by instrumenting the VMware Tanzu Application Service (TAS) and the Cloud Foundry Application Runtime (CFAR). Tip To collect data from VMware PKS, use the New Relic Cluster Monitoring integration. Features With the New Relic VMware Tanzu integration you can: Monitor the health of your deployments using our extensive collection of charts and dashboards. Set alerts based on any metrics collected from Firehose. Retrieve logs and metrics related to user apps deployed on the platform. Stream metrics from platform components and health metrics from BOSH-deployed VMs. Filter logs and metrics by configuring the nozzle during and after the installation. Scale the number of instances of the nozzle to support different volumes of data. Use the data retrieved to monitor Key Performance and Key Capacity Scaling indicators. Instrument and monitor multiple VMware Tanzu instances using the same account. Optionally send LogMessage and HttpStartStop envelopes to New Relic Logs, including logs in context support for LogMessage envelopes. Compatibility and requirements Our integration is compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10. BOSH stemcells must be based on Ubuntu Xenial. Before installing the integration, make sure that you need a VMware Tanzu account. Tip This integration sends custom events and logs. If you find you are reaching the custom event data collection and data retention limits of your subscription, please reach out to your New Relic representative. Install and activate The quickest way to install the VMware Tanzu integration is by importing the nr-firehose-nozzle tile into Ops Manager. For more information, see the VMware Tanzu documentation. You can also deploy the nozzle as a standard application, edit the manifest, and run cf push from the command line; see how to build and deploy the integration in our GitHub repository. Find and use data Once you install and activate the VMware Tanzu integration, you can find the data and predefined charts in one.newrelic.com > Infrastructure > Third-party services > VMware Tanzu dashboard. You can query the data to create custom charts and dashboards, and add them to your account. If you collect data from multiple Tanzu environments, use pcf.domain and pcf.IP attributes with WHERE or FACET to discriminate between events from different Tanzu deployments. Important Tanzu metrics are aggregated in order to reduce memory and network consumption. However, you can increase the number of samples acting on the drain interval in the configuration. Tip Many prebuilt dashboards and charts displaying VMware Tanzu data are available upon request. Contact your New Relic representative to get them added to your New Relic account. Set up an alert VMware Tanzu provides a list of indicators on key performance and key capacity scaling, together with warning and critical values that you can monitor using NRQL alert conditions. Here is a sample NRQL query that sets up an alert on memory consumption related to the system space: SELECT average(app.memory.used) FROM PCFContainerMetric WHERE metric.name = 'app.memory' AND app.space.name = 'system' FACET app.instance.uid Copy Here is the resulting chart in New Relic One: For more information on NRQL queries and how to set up different notification channels for alerts, see Create alert conditions for NRQL queries. Important Creating alert conditions from Infrastructure > Settings is currently not supported for this integration. Metric data The VMware Tanzu integration provides the following metric data: PCFContainerMetric PCFCounterEvent PCFHttpStartStop PCFLogMessage PCFValueMetric Shared fields (Aggregation, App, Decoration) PCFContainerMetric Resource usage of an app in a container. Contains all the shared Aggregation, App, and Decoration fields. If the value of metric.name is app.disk, two additional fields are available: Name Description app.disk.quota Total available disk in bytes app.disk.used Disk currently used in percentage If the value of metric.name is app.memory, two additional fields are available: Name Description app.memory.quota Total available memory in bytes app.memory.used Memory currently used as percentage PCFCounterEvent Increment of a counter. Contains all the shared Aggregation and Decoration fields. Name Description total.reported Current value of the counter PCFHttpStartStop The whole lifecycle of an HTTP request. Contains all the shared Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description http.content.length Length of response (in bytes) http.duration Duration of the HTTP request (in milliseconds) http.method Method of the request http.peer.type Role of the emitting process in the request cycle (server or client) http.remote.address Remote address of the request. For a server, this should be the origin of the request http.request.id ID for tracking the lifecycle of the request http.start.timestamp UNIX timestamp (in nanoseconds) when the request was sent (by a client) or received (by a server) http.status Status code returned with the response to the request http.stop.timestamp UNIX timestamp (in nanoseconds) when the request was received http.uri Destination of the request http.user.agent Contents of the UserAgent header on the request PCFLogMessage Log lines and associated metadata. Contains all the shared Aggregation, App, and Decoration fields. These events can optionally be sent to New Relic Logs for visualization in the Logs UI. Name Description log.app.id Application that emitted the message (or to which the application is related) log.message Log message log.message.type Type of the message (OUT or ERR) log.source.instance Instance that emitted the message log.source.type Source of the message. For Cloud Foundry, this can be APP, RTR, DEA, STG, etc. log.timestamp UNIX timestamp (in nanoseconds) when the log was written PCFValueMetric A flat list of key-value pairs fetched from Loggregator. For an extensive list, see the official documentation. Contains all the shared Aggregation and Decoration fields. Fields shared across metric data VMWare Tanzu metrics contain shared data fields in the following categories: Aggregation fields App fields Decoration fields Aggregation fields Fields generated by the aggregation process. Shared by PCFCounterEvent, PCFContainerMetric, and PCFValueMetric. Name Description metric.max Maximum value of the metric recorded by the nozzle from the last aggregated metric sent metric.min Minimum value of the metric recorded by the nozzle from the last aggregated metric sent metric.name Name of the reported metric Note: the field may contain hundreds of different values metric.sample.last.value Last received value of the metric metric.samples.count Number of samples of the metric received by the nozzle since the last aggregated metric sent metric.sum Sum of all the metric values recorded by the nozzle from the last aggregated metric sent metric.type Metric type (for example, integer) metric.unit Metric unit. For example, delta, seconds, or bytes App fields Fields that describe the source of the data. Shared by PCFContainerMetric and PCFLogMessage. Name Description app.instance.state Status of the application app.instance.uid Id of the application instance app.instances.desired Number of instances required app.name Name of the application app.org.name Organization the application belongs to app.space.name Space where the application is running Decoration fields Fields that contain information related to the agent, the PCF environment, and a timestamp. Shared by all data types. Name Description agent.instance Nozzle ID agent.ip Nozzle IP address agent.subscription Agent subscription ID, registered at the firehose agent.version Version of the nozzle bosh.domain API URL of your Tanzu environment pcf.IP IP address (used to uniquely identify source) pcf.deployment Deployment name (used to uniquely identify source) pcf.domain API URL of your Tanzu environment pcf.index Index of job (used to uniquely identify the source) pcf.job Job name (used to uniquely identify the source) pcf.origin Unique description of the origin of the event timestamp UNIX timestamp (in milliseconds) of the event. Example: 1582023990236 pcf.envelope.type Type of wrapped event nr.customEventSource source of the custom event",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.1603,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "VMware Tanzu monitoring <em>integration</em>",
        "sections": "VMware Tanzu monitoring <em>integration</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>"
      },
      "id": "6044e41be7b9d26e4b579a2d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.16019,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Monitor services running <em>on</em> Amazon ECS",
        "sections": "Monitor services running <em>on</em> Amazon ECS",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " in to the <em>host</em> running the infrastructure agent. Via the command line, change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: &#x2F;nginx&#x2F; <em>integrations</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/integrations/host-integrations/understand-use-data/host-integration-data-collection-reporting": [
    {
      "sections": [
        "Remote monitoring in on-host integrations",
        "Important",
        "Effects of activating remote_monitoring",
        "Alert verification",
        "New entity attributes",
        "Changes in recorded metrics",
        "Unrecorded attributes",
        "Updated hostname"
      ],
      "title": "Remote monitoring in on-host integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "Understand and use data"
      ],
      "external_id": "1cfea4c65b855ce9ac5078d2a36ba11b63a6101b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/understand-use-data/remote-monitoring-host-integrations/",
      "published_at": "2021-05-05T18:15:46Z",
      "updated_at": "2021-03-16T06:05:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "From a New Relic perspective, entity is a broad concept. An entity is anything New Relic can identify that has data you can monitor. Integrations can be configured to create their own entity, called a remote entity, by setting the remote_monitoring option to true. If set to false, an integration will be considered a local entity, and the data related to it will be attached to the host entity that the agent creates. Remote monitoring requires infrastructure agent version 1.2.25 or higher. For the Apache, Cassandra, MySQL, NGINX, and Redis integrations, remote monitoring (and multi-tenancy) is enabled by activating the configuration parameter remote_monitoring. Important If your Apache, Cassandra, MySQL, NGINX, or Redis service is located in the same host as the agent, when you activate remote monitoring the resulting entity will be considered as remote, regardless of its actual location. This may affect alerts, alter attributes, and have other effects, as explained here. Effects of activating remote_monitoring By enabling remote_monitoring, the integration becomes a different entity which is no longer attached to the infrastructure agent. As a result, the following items may be affected: Alert verification Enabling remote monitoring can affect your configured alerts in case they are using any of the values that are affected by this new feature. We strongly recommend checking your existing alerts to make sure they keep on working as expected. New entity attributes These attributes are modified in the resulting entity: Display name: New entity unique key (instead of using the display name) Entity GUID: New entity GUID Entity ID: New entity ID Entity key: New entity unique key (instead of using the display name) External key: Using integration entity name (instead of using the agent display) Changes in recorded metrics When remote monitoring is enabled, we will add the hostname and port values to all metrics. If the nricluster name or nriservice are defined in the integration configuration file, they will also be decorated. Unrecorded attributes Since the integration is now an independent entity which is not attached to the agent, the following agent attributes are not collected: agentName agentVersion coreCount criticalViolationCount fullHostname instanceType kernelVersion linuxDistribution entityType operatingSystem processorCount systemMemoryBytes warningViolationCount Your custom attributes Updated hostname For the ApacheSample, RedisSample, CassandraSample, and NginxSample integration metrics, we will use the integration configuration hostname instead of the short hostname from the agent. When the integration hostname is a loopback address, the agent will replace it in order to guarantee uniqueness.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.71552,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Remote monitoring in <em>on</em>-<em>host</em> <em>integrations</em>",
        "sections": "Remote monitoring in <em>on</em>-<em>host</em> <em>integrations</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " will be considered a local entity, and the <em>data</em> related to it will be attached to the <em>host</em> entity that the agent creates. Remote monitoring requires infrastructure agent version 1.2.25 or higher. For the Apache, Cassandra, MySQL, NGINX, and Redis <em>integrations</em>, remote monitoring (and multi-tenancy"
      },
      "id": "603ec000e7b9d216732a07ef"
    },
    {
      "sections": [
        "Find and use your Kubernetes data",
        "Query Kubernetes data",
        "Event types",
        "Manage alerts",
        "Create an alert condition",
        "Use the predefined alert types and thresholds",
        "Select alert notifications",
        "Pod alert notification example",
        "Container resource notification example",
        "Create alert conditions using NRQL",
        "Kubernetes attributes and metrics",
        "Node data",
        "Namespace data",
        "Deployment data",
        "ReplicaSet data",
        "DaemonSet data",
        "StatefulSet data",
        "Pod data",
        "Cluster data",
        "Container data",
        "Volume data",
        "API server data",
        "Controller manager data",
        "Scheduler data",
        "ETCD data",
        "Endpoint data",
        "Service data",
        "Horizontal Pod Autoscaler data",
        "Kubernetes metadata in APM-monitored applications",
        "For more help"
      ],
      "title": "Find and use your Kubernetes data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Understand and use data"
      ],
      "external_id": "d36002ee54b0e3573ec4efef9f9c5ee940f49f96",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/understand-use-data/find-use-your-kubernetes-data/",
      "published_at": "2021-05-05T03:45:27Z",
      "updated_at": "2021-04-12T16:05:48Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can build your own charts and query all your Kubernetes integration data using the query builder and the NerdGraph API. Our integration collects Kubernetes data by instrumenting the container orchestration layer. For a simpler and more visual experience, use the cluster explorer. one.newrelic.com > Dashboards: Using the query builder you can query your Kubernetes data and create clear visualizations. Query Kubernetes data The simplest way to query your Kubernetes data is using the query builder, which accepts NRQL queries in its advanced mode. Alternatively, you can use the NerdGraph API to retrieve Kubernetes data. Event types Kubernetes data is attached to the following event types: Event name Type of Kubernetes data Available since K8sNodeSample Node data v1.0.0 K8sNamespaceSample Namespace data v1.0.0 K8sDeploymentSample Deployment data v1.0.0 K8sReplicasetSample ReplicaSet data v1.0.0 K8sDaemonsetSample DaemonSet data v1.13.0 K8sStatefulsetSample StatefulSet data v1.13.0 K8sPodSample Pod data v1.0.0 K8sClusterSample Cluster data v1.0.0 K8sContainerSample Container data v1.0.0 K8sVolumeSample Volume data v1.0.0 K8sApiServerSample API server data v1.11.0 K8sControllerManagerSample Controller manager data v1.11.0 K8sSchedulerSample Scheduler data v1.11.0 K8sEtcdSample ETCD data v1.11.0 K8sEndpointSample Endpoint data v1.13.0 K8sServiceSample Service data v1.13.0 K8sHpaSample Horizontal Pod Autoscaler data v2.3.0 Manage alerts You can be notified about alert violations for your Kubernetes data: Create an alert condition To create an alert condition for the Kubernetes integration: Go to one.newrelic.com > Infrastructure > Settings > Alerts > Kubernetes, then select Create alert condition. To filter the alert to Kubernetes entities that only have the chosen attributes, select Filter. Select the threshold settings. For more on the Trigger an alert when... options, see Alert types. Select an existing alert policy, or create a new one. Select Create. When an alert condition's threshold is triggered, New Relic sends a notification to the policy's notification channels. Use the predefined alert types and thresholds The Kubernetes integration comes with its own alert policy and alert conditions. To see what the predefined alert conditions are, see Kubernetes integration: Predefined alert policy. In addition, you can create an alert condition for any metric collected by any New Relic integration you use, including the Kubernetes integration: Select the alert type Integrations. From the Select a data source dropdown, select a Kubernetes (K8s) data source. Select alert notifications When an alert condition's threshold is triggered, New Relic sends a message to the notification channel(s) chosen in the alert policy. Depending on the type of notification, you may have the following options: View the incident. Acknowledge the incident. Go to a chart of the incident data by selecting the identifier name. The entity identifier that triggered the alert appears near the top of the notification message. The format of the identifier depends on the alert type: Available pods are less than desired pods alerts: K8s:CLUSTER_NAME:PARENT_NAMESPACE:replicaset:REPLICASET_NAME Copy CPU or memory usage alerts: K8s:CLUSTER_NAME:PARENT_NAMESPACE:POD_NAME:container:CONTAINER_NAME Copy Here are some examples. Pod alert notification example For Available pods are less than desired pods alerts, the ID of the ReplicaSet triggering the issue might look like this: k8s:beam-production:default:replicaset:nginx-deployment-1623441481 Copy This identifier contains the following information: Cluster name: beam-production Parent namespace: default ReplicaSet name: nginx-deployment-1623441481 Container resource notification example For container CPU or memory usage alerts, the entity might look like this: k8s:beam-production:kube-system:kube-state-metrics-797bb87c75-zncwn:container:kube-state-metrics Copy This identifier contains the following information: Cluster name: beam-production Parent namespace: kube-system Pod namespace: kube-state-metrics-797bb87c75-zncwn Container name: kube-state-metrics Create alert conditions using NRQL Follow standard procedures to create alert conditions for NRQL queries. Kubernetes attributes and metrics The Kubernetes integration collects the following metrics and other attributes. Node data Query the K8sNodeSample event for node data: Node attribute Description allocatableCpuCores Node allocatable CPU cores allocatableMemoryBytes Node allocatable memory bytes allocatablePods Node allocatable pods allocatableEphemeralStorageBytes Node allocatable ephemeral-storage bytes capacityCpuCores Node CPU capacity capacityMemoryBytes Node memory capacity (in bytes) capacityPods Pod capacity of the node capacityEphemeralStorageBytes Node ephemeral-storage capacity clusterName Name that you assigned to the cluster when you installed the Kubernetes integration cpuUsedCoreMilliseconds Node CPU usage measured in core milliseconds cpuUsedCores Node CPU usage measured in cores cpuRequestedCores Total amount of CPU cores requested allocatableCpuCoresUtilization Percentage of CPU cores actually used with respect to the CPU cores allocatable fsAvailableBytes Bytes available in the node filesystem fsCapacityBytes Total capacity of the node filesystem in bytes fsInodes Total number of inodes in the node filesystem fsInodesFree Free inodes in the node filesystem fsInodesUsed Used inodes in the node filesystem fsUsedBytes Used bytes in the node filesystem fsCapacityUtilization Percentage of used bytes in the node filesystem with respect to the capacity memoryAvailableBytes Bytes of memory available in the node memoryMajorPageFaultsPerSecond Number of major page faults per second in the node memoryPageFaults Number of page faults in the node memoryRssBytes Bytes of rss memory memoryUsedBytes Bytes of memory used memoryWorkingSetBytes Bytes of memory in the working set memoryRequestedBytes Total amount of requested memory allocatableMemoryUtilization Percentage of bytes of memory in the working set with respect to the node allocatable memory net.errorCountPerSecond Number of errors per second while receiving/transmitting over the network nodeName Host name that the pod is running on runtimeAvailableBytes Bytes available to the container runtime filesystem runtimeCapacityBytes Total capacity assigned to the container runtime filesystem in bytes runtimeInodes Total number of inodes in the container runtime filesystem runtimeInodesFree Free inodes in the container runtime filesystem runtimeInodesUsed Used inodes in the container runtime filesystem runtimeUsedBytes Used bytes in the container runtime filesystem label.LABEL_NAME Labels associated with your node, so you can filter and query for specific nodes Namespace data Query the K8sNamespaceSample event for namespace data: Namespace attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of the namespace when it was created namespace Name of the namespace to be used as an identifier label.LABEL_NAME Labels associated with your namespace, so you can filter and query for specific namespaces status Current status of the namespace. The value can be Active or Terminated Deployment data Query the K8sDeploymentSample event for deployment data: Deployment attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the deployment was created deploymentName Name of the deployment to be used as an identifier namespace Name of the namespace that the deployment belongs to label.LABEL_NAME Labels associated with your deployment, so you can filter and query for specific deployments podsAvailable Number of replicas that are currently available podsDesired Number of replicas that you defined in the deployment podsTotal Total number of replicas that are currently running podsUnavailable Number of replicas that are currently unavailable podsUpdated Number of replicas that have been updated to achieve the desired state of the deployment podsMissing Total number of replicas that are missing (number of desired replicas, podsDesired, minus the total number of replicas, podsTotal) ReplicaSet data Query the K8sReplicasetSample event for ReplicaSet data: Replica attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the ReplicaSet was created deploymentName Name of the deployment to be used as an identifier namespace Name of the namespace that the ReplicaSet belongs to observedGeneration Integer representing generation observed by the ReplicaSet podsDesired Number of replicas that you defined in the deployment podsFullyLabeled Number of pods that have labels that match the ReplicaSet pod template labels podsReady Number of replicas that are ready for this ReplicaSet podsTotal Total number of replicas that are currently running podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) replicasetName Name of the ReplicaSet to be used as an identifier DaemonSet data Query the K8sDaemonsetSample event for DaemonSet data: DaemonSet attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the DaemonSet was created namespaceName Name of the namespace that the DaemonSet belongs to label.LABEL_NAME Labels associated with your DaemonSet, so you can filter and query for specific DaemonSet daemonsetName Name associated with the DaemonSet podsDesired The number of nodes that should be running the daemon pod podsScheduled The number of nodes running at least one daemon pod and are supposed to podsAvailable The number of nodes that should be running the daemon pod and have one or more of the daemon pod running and available podsReady The number of nodes that should be running the daemon pod and have one or more of the daemon pod running and ready podsUnavailable The number of nodes that should be running the daemon pod and have none of the daemon pod running and available podsMisscheduled The number of nodes running a daemon pod but are not supposed to podsUpdatedScheduled The total number of nodes that are running updated daemon pod podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) metadataGeneration Sequence number representing a specific generation of the desired state StatefulSet data Query the K8sStatefulsetSample event for StatefulSet data: StatefulSet attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the StatefulSet was created namespaceName Name of the namespace that the StatefulSet belongs to label.LABEL_NAME Labels associated with your StatefulSet, so you can filter and query for specific StatefulSet statefulsetName Name associated with the StatefulSet podsDesired Number of desired pods for a StatefulSet podsReady The number of ready replicas per StatefulSet podsCurrent The number of current replicas per StatefulSet podsTotal The number of replicas per StatefulSet podsUpdated The number of updated replicas per StatefulSet podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) observedGeneration The generation observed by the StatefulSet controller metadataGeneration Sequence number representing a specific generation of the desired state for the StatefulSet currentRevision Indicates the version of the StatefulSet used to generate pods in the sequence. Value range: between 0 and podsCurrent updateRevision Indicates the version of the StatefulSet used to generate pods in the sequence. Value range: between podsDesired-podsUpdated and podsDesired Pod data Query the K8sPodSample event for pod data: Pod attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the pod was created in epoch seconds createdBy Name of the Kubernetes object that created the pod. For example, newrelic-infra createdKind Kind of Kubernetes object that created the pod. For example, DaemonSet. deploymentName Name of the deployment to be used as an identifier isReady Boolean representing whether or not the pod is ready to serve requests isScheduled Boolean representing whether or not the pod has been scheduled to run on a node label.LABEL_NAME Labels associated with your pod, so you can filter and query for specific pods message Details related to the last pod status change namespace Name of the namespace that the pod belongs to net.errorCountPerSecond Number of errors per second while receiving/transmitting over the network net.errorsPerSecond Number of errors per second net.rxBytesPerSecond Number of bytes per second received over the network net.txBytesPerSecond Number of bytes per second transmitted over the network nodeIP Host IP address that the pod is running on nodeName Host name that the pod is running on podName Name of the pod to be used as an identifier reason Reason why the pod is in the current status startTime Timestamp of when the pod started running in epoch seconds status Current status of the pod. Value can be Pending, Running, Succeeded, Failed, Unknown Cluster data Query the K8sClusterSample event to see cluster data: Cluster attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration clusterK8sVersion Kubernetes version that the cluster is running Container data Query the K8sContainerSample event for container data: Container attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration containerID Unique ID associated with the container. If you are running Docker, this is the Docker container id containerImage Name of the image that the container is running containerImageID Unique ID associated with the image that the container is running containerName Name associated with the container cpuLimitCores Integer representing limit CPU cores defined for the container in the pod specification cpuRequestedCores Requested CPU cores defined for the container in the pod specification cpuUsedCores CPU cores actually used by the container cpuCoresUtilization Percentage of CPU cores actually used by the container with respect to the CPU limit specified. This percentage is based on this calculation: (cpuUsedCores / cpuLimitCores) * 100 requestedCpuCoresUtilization Percentage of CPU cores actually used by the container with respect to the CPU request specified deploymentName Name of the deployment to be used as an identifier isReady Boolean. Whether or not the container's readiness check succeeded label.LABEL_NAME Labels associated with your container, so you can filter and query for specific containers memoryLimitBytes Integer representing limit bytes of memory defined for the container in the pod specification memoryRequestedBytes Integer. Requested bytes of memory defined for the container in the pod specification memoryUsedBytes Integer. Bytes of memory actually used by the container memoryUtilization Percentage of memory actually used by the container with respect to the memory limit specified requestedMemoryUtilization Percentage of memory actually used by the container with respect to the memory request specified memoryWorkingSetBytes Integer. Bytes of memory in the working set memoryWorkingSetUtilization Percentage of working set memory actually used by the container with respect to the memory limit specified requestedMemoryWorkingSetUtilization Percentage of working set memory actually used by the container with respect to the memory request specified namespace Name of the namespace that the container belongs to nodeIP Host IP address the container is running on nodeName Host name that the container is running on podName Name of the pod that the container is in, to be used as an identifier reason Provides a reason why the container is in the current status restartCount Number of times the container has been restarted status Current status of the container. Value can be Running, Terminated, or Unknown containerCpuCfsPeriodsDelta Delta change of elapsed enforcement period intervals containerCpuCfsThrottledPeriodsDelta Delta change of throttled period intervals containerCpuCfsThrottledSecondsDelta Delta change of duration the container has been throttled, in seconds containerCpuCfsPeriodsTotal Total number of elapsed enforcement period intervals containerCpuCfsThrottledPeriodsTotal Total number of throttled period intervals containerCpuCfsThrottledSecondsTotal Total time duration the container has been throttled, in seconds containerMemoryMappedFileBytes Total size of memory mapped files used by this container, in bytes Volume data Query the K8sVolumeSample event for volume data: Volume attribute Description volumeName Name that you assigned to the volume at creation clusterName Cluster where the volume is configured namespace Namespace where the volume is configured podName The pod that the volume is attached to. The Kubernetes monitoring integration lists Volumes that are attached to a pod persistent If this is a persistent volume, this value is set to true pvcNamespace Namespace where the Persistent Volume Claim is configured pvcName Name that you assigned to the Persistent Volume Claim at creation fsCapacityBytes Capacity of the volume, in bytes fsUsedBytes Usage of the volume, in bytes fsAvailableBytes Capacity available of the volume, in bytes fsUsedPercent Usage of the volume in percentage fsInodes Total inodes of the volume fsInodesUsed inodes used in the volume fsInodesFree inodes available in the volume Volume data is available for volume plugins that implement the MetricsProvider interface: AWSElasticBlockStore AzureDisk AzureFile Cinder Flexvolume Flocker GCEPersistentDisk GlusterFS iSCSI StorageOS VsphereVolume API server data Query the K8sApiServerSample event in New Relic Insights to see API Server data. For more information, see Configure control plane monitoring: API server attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent, in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist apiserverRequestDelta_verb_VERB_code_CODE Difference of the number of apiserver requests, broken out for each verb and HTTP response code apiserverRequestRate_verb_VERB_code_CODE Rate of apiserver requests, broken out for each verb and HTTP response code restClientRequestsDelta_code_CODE_method_METHOD Difference of the number of HTTP requests, partitioned by method and code restClientRequestsRate_code_CODE_method_METHOD Rate of the number of HTTP requests, partitioned by method and code etcdObjectCounts_resource_RESOURCE-KIND Number of stored objects at the time of last check, split by kind Controller manager data Query the K8sControllerManagerSample event in New Relic Insights to see Controller manager data. For more information, see Configure control plane monitoring: Controller manager attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist workqueueAddsDelta_name_WORK-QUEUE-NAME Difference of the total number of adds handled by workqueue workqueueDepth_name_WORK-QUEUE-NAME Current depth of workqueue workqueueRetriesDelta_name_WORK-QUEUE-NAME Difference of the total number of retries handled by workqueue leaderElectionMasterStatus Gauge of if the reporting system is master of the relevant lease, 0 indicates backup, 1 indicates master Scheduler data Query the K8sSchedulerSample event in New Relic Insights to see Scheduler data. For more information, see Configure control plane monitoring: Scheduler attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist leaderElectionMasterStatus Gauge of if the reporting system is master of the relevant lease, 0 indicates backup, 1 indicates master httpRequestDurationMicroseconds_handler_HANDLER_quantile_QUANTILE The HTTP request latencies in microseconds, per quantile httpRequestDurationMicroseconds_handler_HANDLER_sum The sum of the HTTP request latencies, in microseconds httpRequestDurationMicroseconds_handler_HANDLER_count The number of observed HTTP requests events restClientRequestsDelta_code_CODE_host_HOST_method_METHOD Difference of the number of HTTP requests, partitioned by status code, method, and host restClientRequestsRate_code_CODE_host_HOST_method_METHOD Rate of the number of HTTP requests, partitioned by status code, method, and host schedulerScheduleAttemptsDelta_result_RESULT Difference of the number of attempts to schedule pods, by the result. unschedulable means a pod could not be scheduled, while error means an internal scheduler problem schedulerScheduleAttemptsRate_result_RESULT Rate of the number of attempts to schedule pods, by the result. unschedulable means a pod could not be scheduled, while error means an internal scheduler problem schedulerSchedulingDurationSeconds_operation_OPERATION_quantile_QUANTILE Scheduling latency in seconds split by sub-parts of the scheduling operation schedulerSchedulingDurationSeconds_operation_OPERATION_sum The sum of scheduling latency in seconds split by sub-parts of the scheduling operation schedulerSchedulingDurationSeconds_operation_OPERATION_count The number of observed events of schedulings split by sub-parts of the scheduling operation. schedulerPreemptionAttemptsDelta Difference of the total preemption attempts in the cluster till now schedulerPodPreemptionVictims Number of selected preemption victims ETCD data Query the K8sEtcdSample event in New Relic Insights to see ETCD data. For more information, see Configure control plane monitoring: ETCD attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist etcdServerHasLeader Whether or not a leader exists. 1 is existence, 0 is not etcdServerLeaderChangesSeenDelta Difference of the number of leader changes seen etcdMvccDbTotalSizeInBytes Total size of the underlying database physically allocated, in bytes etcdServerProposalsCommittedDelta Difference of the total number of consensus proposals committed etcdServerProposalsCommittedRate Rate of the total number of consensus proposals committed etcdServerProposalsAppliedDelta Difference of the total number of consensus proposals applied etcdServerProposalsAppliedRate Rate of the total number of consensus proposals applied etcdServerProposalsPending The current number of pending proposals to commit etcdServerProposalsFailedDelta Difference of the total number of failed proposals seen etcdServerProposalsFailedRate Rate of the total number of failed proposals seen processOpenFds Number of open file descriptors processMaxFds Maximum number of open file descriptors processFdsUtilization Percentage open file descriptors with respect to the maximum number that can be opened etcdNetworkClientGrpcReceivedBytesRate Rate of the total number of bytes received from gRPC clients etcdNetworkClientGrpcSentBytesRate Rate of the total number of bytes sent to gRPC clients Endpoint data Query the K8sEndpointSample event in New Relic Insights for endpoint data: Endpoint attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the endpoint was created namespaceName Name of the namespace that the endpoint belongs to endpointName Name associated with the endpoint label.LABEL_NAME Labels associated with your endpoint, so you can filter and query for specific endpoints addressAvailable Number of addresses available in endpoint addressNotReady Number of addresses not ready in endpoint Service data Query the K8sServiceSample event in New Relic Insights for service data: Service attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the service was created namespaceName Name of the namespace that the service belongs to label.LABEL_NAME Labels associated with your service, so you can filter and query for specific service serviceName Name associated with the service loadBalancerIP The IP of the external load balancer, if Spectype is LoadBalancer. externalName The external name value, if Spectype is ExternalName clusterIP The internal cluster IP, if Spectype is ClusterIP specType Type of the service selector.LABEL_NAME The label selector that this service targets Horizontal Pod Autoscaler data Query the K8sHpaSample event in New Relic Insights for Horizontal Pod Autoscaler data: HPA attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration label.LABEL_NAME Labels associated with your HPA, so you can filter and query for specific autoscaler currentReplicas Current number of replicas of pods managed by this autoscaler desiredReplicas Desired number of replicas of pods managed by this autoscaler minReplicas Lower limit for the number of pods that can be set by the autoscaler, 1 by default maxReplicas Upper limit for the number of pods that can be set by the autoscaler; cannot be smaller than minReplicas targetMetric The metric specifications used by this autoscaler when calculating the desired replica count isAble Boolean representing whether or not the autoscaler is able to fetch and update scales, as well as whether or not any backoff-related conditions would prevent scaling isActive Boolean representing whether or not the autoscaler is enabled (if it's able to calculate the desired scales) isLimited Boolean representing whether or not the autoscaler is capped, either up or down, by the maximum or minimum replicas configured labels Number of Kubernetes labels converted to Prometheus labels metadataGeneration The generation observed by the HorizontalPodAutoscaler controller Kubernetes metadata in APM-monitored applications By linking your applications with Kubernetes, the following attributes are added to application trace and distributed trace: nodeName containerName podName clusterName deploymentName namespaceName For more help",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 193.87389,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Find <em>and</em> <em>use</em> your Kubernetes <em>data</em>",
        "sections": "Find <em>and</em> <em>use</em> your Kubernetes <em>data</em>",
        "tags": "<em>Understand</em> <em>and</em> <em>use</em> <em>data</em>",
        "body": " collected by any New Relic integration you <em>use</em>, including the Kubernetes integration: Select the alert type <em>Integrations</em>. From the Select a <em>data</em> source dropdown, select a Kubernetes (K8s) <em>data</em> source. Select alert notifications When an alert condition&#x27;s threshold is triggered, New Relic sends a message"
      },
      "id": "603eb9a4196a678bfca83dbb"
    },
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.86641,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Find <em>and</em> <em>use</em> <em>data</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " structure of on-<em>host</em> integration configuration, see Configuration. Find and <em>use</em> <em>data</em> <em>Data</em> from this service is reported to an integration dashboard. Elasticsearch <em>data</em> is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    }
  ],
  "/docs/integrations/host-integrations/understand-use-data/remote-monitoring-host-integrations": [
    {
      "sections": [
        "On-host integration data collection and reporting",
        "Data collection and reporting process",
        "File structure and specifications"
      ],
      "title": "On-host integration data collection and reporting",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "Understand and use data"
      ],
      "external_id": "76942c8b7c37f1eaf368770c80177e3a43d8ca4c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/understand-use-data/host-integration-data-collection-reporting/",
      "published_at": "2021-05-05T18:14:08Z",
      "updated_at": "2021-03-16T06:04:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Read on to learn how New Relic on-host integrations collect and report data to New Relic. Data collection and reporting process This is how an infrastructure on-host integration sends data to New Relic: On startup, the infrastructure agent scans the directory that contains the integration's definition files. The infrastructure agent registers every integration executable defined in the definition file. The agent scans a dedicated directory for integration configuration files. If those config files specify integrations that have been registered with the infrastructure agent, the agent sets up and schedules the integrations. At the scheduled interval (the default is 15 seconds), the agent harvests the data from the integration and prepares it for transmission. Every 60 seconds, it sends that data to New Relic, along with any other infrastructure data. After a successful collection pass, the integration executable exits. File structure and specifications Understanding the file structure of New Relic on-host integrations can help you customize your integration, understand and use your data, and troubleshoot problems. On-host integrations adhere to a set of open source specifications, allowing anyone to build their own infrastructure on-host integration. For an explanation of these file specifications, see File specs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 221.26584,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>On</em>-<em>host</em> <em>integration</em> <em>data</em> collection <em>and</em> reporting",
        "sections": "<em>On</em>-<em>host</em> <em>integration</em> <em>data</em> collection <em>and</em> reporting",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": ", the integration executable exits. File structure and specifications Understanding the file structure of New Relic on-<em>host</em> <em>integrations</em> can help you customize your integration, <em>understand</em> and <em>use</em> your <em>data</em>, and troubleshoot problems. On-<em>host</em> <em>integrations</em> adhere to a set of open source specifications, allowing anyone to build their own infrastructure on-<em>host</em> integration. For an explanation of these file specifications, see File specs."
      },
      "id": "603e8553196a67ca20a83dd5"
    },
    {
      "sections": [
        "Find and use your Kubernetes data",
        "Query Kubernetes data",
        "Event types",
        "Manage alerts",
        "Create an alert condition",
        "Use the predefined alert types and thresholds",
        "Select alert notifications",
        "Pod alert notification example",
        "Container resource notification example",
        "Create alert conditions using NRQL",
        "Kubernetes attributes and metrics",
        "Node data",
        "Namespace data",
        "Deployment data",
        "ReplicaSet data",
        "DaemonSet data",
        "StatefulSet data",
        "Pod data",
        "Cluster data",
        "Container data",
        "Volume data",
        "API server data",
        "Controller manager data",
        "Scheduler data",
        "ETCD data",
        "Endpoint data",
        "Service data",
        "Horizontal Pod Autoscaler data",
        "Kubernetes metadata in APM-monitored applications",
        "For more help"
      ],
      "title": "Find and use your Kubernetes data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Understand and use data"
      ],
      "external_id": "d36002ee54b0e3573ec4efef9f9c5ee940f49f96",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/understand-use-data/find-use-your-kubernetes-data/",
      "published_at": "2021-05-05T03:45:27Z",
      "updated_at": "2021-04-12T16:05:48Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can build your own charts and query all your Kubernetes integration data using the query builder and the NerdGraph API. Our integration collects Kubernetes data by instrumenting the container orchestration layer. For a simpler and more visual experience, use the cluster explorer. one.newrelic.com > Dashboards: Using the query builder you can query your Kubernetes data and create clear visualizations. Query Kubernetes data The simplest way to query your Kubernetes data is using the query builder, which accepts NRQL queries in its advanced mode. Alternatively, you can use the NerdGraph API to retrieve Kubernetes data. Event types Kubernetes data is attached to the following event types: Event name Type of Kubernetes data Available since K8sNodeSample Node data v1.0.0 K8sNamespaceSample Namespace data v1.0.0 K8sDeploymentSample Deployment data v1.0.0 K8sReplicasetSample ReplicaSet data v1.0.0 K8sDaemonsetSample DaemonSet data v1.13.0 K8sStatefulsetSample StatefulSet data v1.13.0 K8sPodSample Pod data v1.0.0 K8sClusterSample Cluster data v1.0.0 K8sContainerSample Container data v1.0.0 K8sVolumeSample Volume data v1.0.0 K8sApiServerSample API server data v1.11.0 K8sControllerManagerSample Controller manager data v1.11.0 K8sSchedulerSample Scheduler data v1.11.0 K8sEtcdSample ETCD data v1.11.0 K8sEndpointSample Endpoint data v1.13.0 K8sServiceSample Service data v1.13.0 K8sHpaSample Horizontal Pod Autoscaler data v2.3.0 Manage alerts You can be notified about alert violations for your Kubernetes data: Create an alert condition To create an alert condition for the Kubernetes integration: Go to one.newrelic.com > Infrastructure > Settings > Alerts > Kubernetes, then select Create alert condition. To filter the alert to Kubernetes entities that only have the chosen attributes, select Filter. Select the threshold settings. For more on the Trigger an alert when... options, see Alert types. Select an existing alert policy, or create a new one. Select Create. When an alert condition's threshold is triggered, New Relic sends a notification to the policy's notification channels. Use the predefined alert types and thresholds The Kubernetes integration comes with its own alert policy and alert conditions. To see what the predefined alert conditions are, see Kubernetes integration: Predefined alert policy. In addition, you can create an alert condition for any metric collected by any New Relic integration you use, including the Kubernetes integration: Select the alert type Integrations. From the Select a data source dropdown, select a Kubernetes (K8s) data source. Select alert notifications When an alert condition's threshold is triggered, New Relic sends a message to the notification channel(s) chosen in the alert policy. Depending on the type of notification, you may have the following options: View the incident. Acknowledge the incident. Go to a chart of the incident data by selecting the identifier name. The entity identifier that triggered the alert appears near the top of the notification message. The format of the identifier depends on the alert type: Available pods are less than desired pods alerts: K8s:CLUSTER_NAME:PARENT_NAMESPACE:replicaset:REPLICASET_NAME Copy CPU or memory usage alerts: K8s:CLUSTER_NAME:PARENT_NAMESPACE:POD_NAME:container:CONTAINER_NAME Copy Here are some examples. Pod alert notification example For Available pods are less than desired pods alerts, the ID of the ReplicaSet triggering the issue might look like this: k8s:beam-production:default:replicaset:nginx-deployment-1623441481 Copy This identifier contains the following information: Cluster name: beam-production Parent namespace: default ReplicaSet name: nginx-deployment-1623441481 Container resource notification example For container CPU or memory usage alerts, the entity might look like this: k8s:beam-production:kube-system:kube-state-metrics-797bb87c75-zncwn:container:kube-state-metrics Copy This identifier contains the following information: Cluster name: beam-production Parent namespace: kube-system Pod namespace: kube-state-metrics-797bb87c75-zncwn Container name: kube-state-metrics Create alert conditions using NRQL Follow standard procedures to create alert conditions for NRQL queries. Kubernetes attributes and metrics The Kubernetes integration collects the following metrics and other attributes. Node data Query the K8sNodeSample event for node data: Node attribute Description allocatableCpuCores Node allocatable CPU cores allocatableMemoryBytes Node allocatable memory bytes allocatablePods Node allocatable pods allocatableEphemeralStorageBytes Node allocatable ephemeral-storage bytes capacityCpuCores Node CPU capacity capacityMemoryBytes Node memory capacity (in bytes) capacityPods Pod capacity of the node capacityEphemeralStorageBytes Node ephemeral-storage capacity clusterName Name that you assigned to the cluster when you installed the Kubernetes integration cpuUsedCoreMilliseconds Node CPU usage measured in core milliseconds cpuUsedCores Node CPU usage measured in cores cpuRequestedCores Total amount of CPU cores requested allocatableCpuCoresUtilization Percentage of CPU cores actually used with respect to the CPU cores allocatable fsAvailableBytes Bytes available in the node filesystem fsCapacityBytes Total capacity of the node filesystem in bytes fsInodes Total number of inodes in the node filesystem fsInodesFree Free inodes in the node filesystem fsInodesUsed Used inodes in the node filesystem fsUsedBytes Used bytes in the node filesystem fsCapacityUtilization Percentage of used bytes in the node filesystem with respect to the capacity memoryAvailableBytes Bytes of memory available in the node memoryMajorPageFaultsPerSecond Number of major page faults per second in the node memoryPageFaults Number of page faults in the node memoryRssBytes Bytes of rss memory memoryUsedBytes Bytes of memory used memoryWorkingSetBytes Bytes of memory in the working set memoryRequestedBytes Total amount of requested memory allocatableMemoryUtilization Percentage of bytes of memory in the working set with respect to the node allocatable memory net.errorCountPerSecond Number of errors per second while receiving/transmitting over the network nodeName Host name that the pod is running on runtimeAvailableBytes Bytes available to the container runtime filesystem runtimeCapacityBytes Total capacity assigned to the container runtime filesystem in bytes runtimeInodes Total number of inodes in the container runtime filesystem runtimeInodesFree Free inodes in the container runtime filesystem runtimeInodesUsed Used inodes in the container runtime filesystem runtimeUsedBytes Used bytes in the container runtime filesystem label.LABEL_NAME Labels associated with your node, so you can filter and query for specific nodes Namespace data Query the K8sNamespaceSample event for namespace data: Namespace attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of the namespace when it was created namespace Name of the namespace to be used as an identifier label.LABEL_NAME Labels associated with your namespace, so you can filter and query for specific namespaces status Current status of the namespace. The value can be Active or Terminated Deployment data Query the K8sDeploymentSample event for deployment data: Deployment attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the deployment was created deploymentName Name of the deployment to be used as an identifier namespace Name of the namespace that the deployment belongs to label.LABEL_NAME Labels associated with your deployment, so you can filter and query for specific deployments podsAvailable Number of replicas that are currently available podsDesired Number of replicas that you defined in the deployment podsTotal Total number of replicas that are currently running podsUnavailable Number of replicas that are currently unavailable podsUpdated Number of replicas that have been updated to achieve the desired state of the deployment podsMissing Total number of replicas that are missing (number of desired replicas, podsDesired, minus the total number of replicas, podsTotal) ReplicaSet data Query the K8sReplicasetSample event for ReplicaSet data: Replica attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the ReplicaSet was created deploymentName Name of the deployment to be used as an identifier namespace Name of the namespace that the ReplicaSet belongs to observedGeneration Integer representing generation observed by the ReplicaSet podsDesired Number of replicas that you defined in the deployment podsFullyLabeled Number of pods that have labels that match the ReplicaSet pod template labels podsReady Number of replicas that are ready for this ReplicaSet podsTotal Total number of replicas that are currently running podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) replicasetName Name of the ReplicaSet to be used as an identifier DaemonSet data Query the K8sDaemonsetSample event for DaemonSet data: DaemonSet attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the DaemonSet was created namespaceName Name of the namespace that the DaemonSet belongs to label.LABEL_NAME Labels associated with your DaemonSet, so you can filter and query for specific DaemonSet daemonsetName Name associated with the DaemonSet podsDesired The number of nodes that should be running the daemon pod podsScheduled The number of nodes running at least one daemon pod and are supposed to podsAvailable The number of nodes that should be running the daemon pod and have one or more of the daemon pod running and available podsReady The number of nodes that should be running the daemon pod and have one or more of the daemon pod running and ready podsUnavailable The number of nodes that should be running the daemon pod and have none of the daemon pod running and available podsMisscheduled The number of nodes running a daemon pod but are not supposed to podsUpdatedScheduled The total number of nodes that are running updated daemon pod podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) metadataGeneration Sequence number representing a specific generation of the desired state StatefulSet data Query the K8sStatefulsetSample event for StatefulSet data: StatefulSet attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the StatefulSet was created namespaceName Name of the namespace that the StatefulSet belongs to label.LABEL_NAME Labels associated with your StatefulSet, so you can filter and query for specific StatefulSet statefulsetName Name associated with the StatefulSet podsDesired Number of desired pods for a StatefulSet podsReady The number of ready replicas per StatefulSet podsCurrent The number of current replicas per StatefulSet podsTotal The number of replicas per StatefulSet podsUpdated The number of updated replicas per StatefulSet podsMissing Total number of replicas that are currently missing (number of desired replicas, podsDesired, minus the number of ready replicas, podsReady) observedGeneration The generation observed by the StatefulSet controller metadataGeneration Sequence number representing a specific generation of the desired state for the StatefulSet currentRevision Indicates the version of the StatefulSet used to generate pods in the sequence. Value range: between 0 and podsCurrent updateRevision Indicates the version of the StatefulSet used to generate pods in the sequence. Value range: between podsDesired-podsUpdated and podsDesired Pod data Query the K8sPodSample event for pod data: Pod attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the pod was created in epoch seconds createdBy Name of the Kubernetes object that created the pod. For example, newrelic-infra createdKind Kind of Kubernetes object that created the pod. For example, DaemonSet. deploymentName Name of the deployment to be used as an identifier isReady Boolean representing whether or not the pod is ready to serve requests isScheduled Boolean representing whether or not the pod has been scheduled to run on a node label.LABEL_NAME Labels associated with your pod, so you can filter and query for specific pods message Details related to the last pod status change namespace Name of the namespace that the pod belongs to net.errorCountPerSecond Number of errors per second while receiving/transmitting over the network net.errorsPerSecond Number of errors per second net.rxBytesPerSecond Number of bytes per second received over the network net.txBytesPerSecond Number of bytes per second transmitted over the network nodeIP Host IP address that the pod is running on nodeName Host name that the pod is running on podName Name of the pod to be used as an identifier reason Reason why the pod is in the current status startTime Timestamp of when the pod started running in epoch seconds status Current status of the pod. Value can be Pending, Running, Succeeded, Failed, Unknown Cluster data Query the K8sClusterSample event to see cluster data: Cluster attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration clusterK8sVersion Kubernetes version that the cluster is running Container data Query the K8sContainerSample event for container data: Container attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration containerID Unique ID associated with the container. If you are running Docker, this is the Docker container id containerImage Name of the image that the container is running containerImageID Unique ID associated with the image that the container is running containerName Name associated with the container cpuLimitCores Integer representing limit CPU cores defined for the container in the pod specification cpuRequestedCores Requested CPU cores defined for the container in the pod specification cpuUsedCores CPU cores actually used by the container cpuCoresUtilization Percentage of CPU cores actually used by the container with respect to the CPU limit specified. This percentage is based on this calculation: (cpuUsedCores / cpuLimitCores) * 100 requestedCpuCoresUtilization Percentage of CPU cores actually used by the container with respect to the CPU request specified deploymentName Name of the deployment to be used as an identifier isReady Boolean. Whether or not the container's readiness check succeeded label.LABEL_NAME Labels associated with your container, so you can filter and query for specific containers memoryLimitBytes Integer representing limit bytes of memory defined for the container in the pod specification memoryRequestedBytes Integer. Requested bytes of memory defined for the container in the pod specification memoryUsedBytes Integer. Bytes of memory actually used by the container memoryUtilization Percentage of memory actually used by the container with respect to the memory limit specified requestedMemoryUtilization Percentage of memory actually used by the container with respect to the memory request specified memoryWorkingSetBytes Integer. Bytes of memory in the working set memoryWorkingSetUtilization Percentage of working set memory actually used by the container with respect to the memory limit specified requestedMemoryWorkingSetUtilization Percentage of working set memory actually used by the container with respect to the memory request specified namespace Name of the namespace that the container belongs to nodeIP Host IP address the container is running on nodeName Host name that the container is running on podName Name of the pod that the container is in, to be used as an identifier reason Provides a reason why the container is in the current status restartCount Number of times the container has been restarted status Current status of the container. Value can be Running, Terminated, or Unknown containerCpuCfsPeriodsDelta Delta change of elapsed enforcement period intervals containerCpuCfsThrottledPeriodsDelta Delta change of throttled period intervals containerCpuCfsThrottledSecondsDelta Delta change of duration the container has been throttled, in seconds containerCpuCfsPeriodsTotal Total number of elapsed enforcement period intervals containerCpuCfsThrottledPeriodsTotal Total number of throttled period intervals containerCpuCfsThrottledSecondsTotal Total time duration the container has been throttled, in seconds containerMemoryMappedFileBytes Total size of memory mapped files used by this container, in bytes Volume data Query the K8sVolumeSample event for volume data: Volume attribute Description volumeName Name that you assigned to the volume at creation clusterName Cluster where the volume is configured namespace Namespace where the volume is configured podName The pod that the volume is attached to. The Kubernetes monitoring integration lists Volumes that are attached to a pod persistent If this is a persistent volume, this value is set to true pvcNamespace Namespace where the Persistent Volume Claim is configured pvcName Name that you assigned to the Persistent Volume Claim at creation fsCapacityBytes Capacity of the volume, in bytes fsUsedBytes Usage of the volume, in bytes fsAvailableBytes Capacity available of the volume, in bytes fsUsedPercent Usage of the volume in percentage fsInodes Total inodes of the volume fsInodesUsed inodes used in the volume fsInodesFree inodes available in the volume Volume data is available for volume plugins that implement the MetricsProvider interface: AWSElasticBlockStore AzureDisk AzureFile Cinder Flexvolume Flocker GCEPersistentDisk GlusterFS iSCSI StorageOS VsphereVolume API server data Query the K8sApiServerSample event in New Relic Insights to see API Server data. For more information, see Configure control plane monitoring: API server attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent, in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist apiserverRequestDelta_verb_VERB_code_CODE Difference of the number of apiserver requests, broken out for each verb and HTTP response code apiserverRequestRate_verb_VERB_code_CODE Rate of apiserver requests, broken out for each verb and HTTP response code restClientRequestsDelta_code_CODE_method_METHOD Difference of the number of HTTP requests, partitioned by method and code restClientRequestsRate_code_CODE_method_METHOD Rate of the number of HTTP requests, partitioned by method and code etcdObjectCounts_resource_RESOURCE-KIND Number of stored objects at the time of last check, split by kind Controller manager data Query the K8sControllerManagerSample event in New Relic Insights to see Controller manager data. For more information, see Configure control plane monitoring: Controller manager attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist workqueueAddsDelta_name_WORK-QUEUE-NAME Difference of the total number of adds handled by workqueue workqueueDepth_name_WORK-QUEUE-NAME Current depth of workqueue workqueueRetriesDelta_name_WORK-QUEUE-NAME Difference of the total number of retries handled by workqueue leaderElectionMasterStatus Gauge of if the reporting system is master of the relevant lease, 0 indicates backup, 1 indicates master Scheduler data Query the K8sSchedulerSample event in New Relic Insights to see Scheduler data. For more information, see Configure control plane monitoring: Scheduler attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist leaderElectionMasterStatus Gauge of if the reporting system is master of the relevant lease, 0 indicates backup, 1 indicates master httpRequestDurationMicroseconds_handler_HANDLER_quantile_QUANTILE The HTTP request latencies in microseconds, per quantile httpRequestDurationMicroseconds_handler_HANDLER_sum The sum of the HTTP request latencies, in microseconds httpRequestDurationMicroseconds_handler_HANDLER_count The number of observed HTTP requests events restClientRequestsDelta_code_CODE_host_HOST_method_METHOD Difference of the number of HTTP requests, partitioned by status code, method, and host restClientRequestsRate_code_CODE_host_HOST_method_METHOD Rate of the number of HTTP requests, partitioned by status code, method, and host schedulerScheduleAttemptsDelta_result_RESULT Difference of the number of attempts to schedule pods, by the result. unschedulable means a pod could not be scheduled, while error means an internal scheduler problem schedulerScheduleAttemptsRate_result_RESULT Rate of the number of attempts to schedule pods, by the result. unschedulable means a pod could not be scheduled, while error means an internal scheduler problem schedulerSchedulingDurationSeconds_operation_OPERATION_quantile_QUANTILE Scheduling latency in seconds split by sub-parts of the scheduling operation schedulerSchedulingDurationSeconds_operation_OPERATION_sum The sum of scheduling latency in seconds split by sub-parts of the scheduling operation schedulerSchedulingDurationSeconds_operation_OPERATION_count The number of observed events of schedulings split by sub-parts of the scheduling operation. schedulerPreemptionAttemptsDelta Difference of the total preemption attempts in the cluster till now schedulerPodPreemptionVictims Number of selected preemption victims ETCD data Query the K8sEtcdSample event in New Relic Insights to see ETCD data. For more information, see Configure control plane monitoring: ETCD attribute Description processResidentMemoryBytes Resident memory size, in bytes processCpuSecondsDelta Difference of the user and system CPU time spent in seconds goThreads Number of OS threads created goGoroutines Number of goroutines that currently exist etcdServerHasLeader Whether or not a leader exists. 1 is existence, 0 is not etcdServerLeaderChangesSeenDelta Difference of the number of leader changes seen etcdMvccDbTotalSizeInBytes Total size of the underlying database physically allocated, in bytes etcdServerProposalsCommittedDelta Difference of the total number of consensus proposals committed etcdServerProposalsCommittedRate Rate of the total number of consensus proposals committed etcdServerProposalsAppliedDelta Difference of the total number of consensus proposals applied etcdServerProposalsAppliedRate Rate of the total number of consensus proposals applied etcdServerProposalsPending The current number of pending proposals to commit etcdServerProposalsFailedDelta Difference of the total number of failed proposals seen etcdServerProposalsFailedRate Rate of the total number of failed proposals seen processOpenFds Number of open file descriptors processMaxFds Maximum number of open file descriptors processFdsUtilization Percentage open file descriptors with respect to the maximum number that can be opened etcdNetworkClientGrpcReceivedBytesRate Rate of the total number of bytes received from gRPC clients etcdNetworkClientGrpcSentBytesRate Rate of the total number of bytes sent to gRPC clients Endpoint data Query the K8sEndpointSample event in New Relic Insights for endpoint data: Endpoint attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the endpoint was created namespaceName Name of the namespace that the endpoint belongs to endpointName Name associated with the endpoint label.LABEL_NAME Labels associated with your endpoint, so you can filter and query for specific endpoints addressAvailable Number of addresses available in endpoint addressNotReady Number of addresses not ready in endpoint Service data Query the K8sServiceSample event in New Relic Insights for service data: Service attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration createdAt Timestamp of when the service was created namespaceName Name of the namespace that the service belongs to label.LABEL_NAME Labels associated with your service, so you can filter and query for specific service serviceName Name associated with the service loadBalancerIP The IP of the external load balancer, if Spectype is LoadBalancer. externalName The external name value, if Spectype is ExternalName clusterIP The internal cluster IP, if Spectype is ClusterIP specType Type of the service selector.LABEL_NAME The label selector that this service targets Horizontal Pod Autoscaler data Query the K8sHpaSample event in New Relic Insights for Horizontal Pod Autoscaler data: HPA attribute Description clusterName Name that you assigned to the cluster when you installed the Kubernetes integration label.LABEL_NAME Labels associated with your HPA, so you can filter and query for specific autoscaler currentReplicas Current number of replicas of pods managed by this autoscaler desiredReplicas Desired number of replicas of pods managed by this autoscaler minReplicas Lower limit for the number of pods that can be set by the autoscaler, 1 by default maxReplicas Upper limit for the number of pods that can be set by the autoscaler; cannot be smaller than minReplicas targetMetric The metric specifications used by this autoscaler when calculating the desired replica count isAble Boolean representing whether or not the autoscaler is able to fetch and update scales, as well as whether or not any backoff-related conditions would prevent scaling isActive Boolean representing whether or not the autoscaler is enabled (if it's able to calculate the desired scales) isLimited Boolean representing whether or not the autoscaler is capped, either up or down, by the maximum or minimum replicas configured labels Number of Kubernetes labels converted to Prometheus labels metadataGeneration The generation observed by the HorizontalPodAutoscaler controller Kubernetes metadata in APM-monitored applications By linking your applications with Kubernetes, the following attributes are added to application trace and distributed trace: nodeName containerName podName clusterName deploymentName namespaceName For more help",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 193.87387,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Find <em>and</em> <em>use</em> your Kubernetes <em>data</em>",
        "sections": "Find <em>and</em> <em>use</em> your Kubernetes <em>data</em>",
        "tags": "<em>Understand</em> <em>and</em> <em>use</em> <em>data</em>",
        "body": " collected by any New Relic integration you <em>use</em>, including the Kubernetes integration: Select the alert type <em>Integrations</em>. From the Select a <em>data</em> source dropdown, select a Kubernetes (K8s) <em>data</em> source. Select alert notifications When an alert condition&#x27;s threshold is triggered, New Relic sends a message"
      },
      "id": "603eb9a4196a678bfca83dbb"
    },
    {
      "sections": [
        "Elasticsearch monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Tip",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux",
        "Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Example configuration",
        "Find and use data",
        "Metric data",
        "Elasticsearch cluster metrics",
        "Elasticsearch node metrics",
        "Elasticsearch common metrics",
        "Elasticsearch index metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Elasticsearch monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "434d522dd3732e7683eb50743879d2fe4a3d9de8",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/elasticsearch-monitoring-integration/",
      "published_at": "2021-05-04T16:33:15Z",
      "updated_at": "2021-05-04T16:33:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Elasticsearch integration collects and sends inventory and metrics from your Elasticsearch cluster to our platform, where you can see the health of your Elasticsearch environment. We collect metrics at the cluster, node, and index level so you can more easily find the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Elasticsearch 5.x through 7.x If Elasticsearch is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Elasticsearch. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Quick start Instrument your Elasticsearch cluster quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Guided install EU Guided install Learn more Tip If you're hosted in the EU, use our EU guided install. Install and activate To install the Elasticsearch integration, follow the instructions for your environment: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux Follow the instructions for installing an integration, using the file name nri-elasticsearch. Change directory to the integrations folder: cd /etc/newrelic-infra/integrations.d Copy Copy the sample configuration file: sudo cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.yml file as described in the configuration settings. Restart the infrastructure agent. Windows Download the nri-elasticsearch .MSI installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-elasticsearch/nri-elasticsearch-amd64.msi To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\nri-elasticsearch-amd64.msi Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp elasticsearch-config.yml.sample elasticsearch-config.yml Copy Edit the elasticsearch-config.ymlfile as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: Integrations are also available in tarball format to allow for install outside of a package manager. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, elasticsearch-config.yml. Config options are below. For an example, see the example config file on GitHub. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands commands: all: captures inventory for the local Elasticsearch node, and metrics for the Elasticsearch cluster. inventory: captures only the configuration for the local Elasticsearch node. labels: The env label controls the environment attribute. The default value is production. A typical agent deployment consists of one agent installed on each node in an Elasticsearch cluster. The agent configuration should be one of these options: Only one node agent using the all command, as metrics are collected for the whole cluster. The rest of agents use the inventory command. All nodes using the all command with master_only set to true, so only the elected master collects the metrics. The rest of agents collect only the inventory. Arguments The all and inventory commands accept the following arguments: hostname: the hostname or IP of the node. Default: localhost. local_hostname: the hostname or IP of the Elasticsearch node from which inventory data is collected. Should only be set if you don't want to collect inventory data against localhost. Default is localhost. port: the port on which the Elasticsearch API is listening. Default: 9200. username: the username to connect to the API with, if the X-Pack security add-on is installed. password: the password to connect to the API with, if the X-Pack security add-on is installed. use_ssl: whether or not to connect using SSL. Default: false. ca_bundle_dir: location of SSL certificate on the host. Only required if use_ssl is true. ca_bundle_file: location of SSL certificate on the host. Only required if use_ssl is true. timeout: the timeout for API requests, in seconds. Default: 30. ssl_alternative_hostname: an alternative server hostname that the integration will accept as valid for the purposes of SSL negotiation. timeout: the timeout for API requests, in seconds. Default: 30. config_path: the path to the Elasticsearch configuration file. Default: /etc/elasticsearch/elasticsearch.yml. collect_indices: true or false to collect indices metrics. If true collect indices, else do not. indices_regex: can be used to filter which indices are collected. If left blank it will be ignored. collect_primaries: true or false to collect primaries metrics. If true collect primaries, else do not. master_only: true or false. If true the node only collects metrics if it's an elected master. Example configuration For an example config, see the example config file on GitHub. For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Elasticsearch data is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample ElasticsearchIndexSample You can query this data for troubleshooting purposes or to create custom charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Elasticsearch integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as cluster. or shards.. Elasticsearch cluster metrics These attributes are attached to the ElasticsearchClusterSample event type: Metric Description cluster.dataNodes The number of data nodes in the cluster. cluster.nodes The number of nodes in the cluster. cluster.status The Elasticsearch cluster health: red, yellow, or green. shards.active The number of active shards in the cluster. shards.initializing The number of shards that are currently initializing. shards.primaryActive The number of active primary shards in the cluster. shards.relocating The number of shards that are relocating from one node to another. shards.unassigned The number of shards that are unassigned to a node. Elasticsearch node metrics These attributes are attached to the ElasticsearchNodeSample event type: Metric Description activeSearches The number of active searches. activeSearchesInMilliseconds The time spent on the search fetch. breakers.estimatedSizeFieldDataCircuitBreakerInBytes The estimated size of the field data circuit breaker, in bytes. breakers.estimatedSizeParentCircuitBreakerInBytes The estimated size of the parent circuit breaker, in bytes. breakers.estimatedSizeRequestCircuitBreakerInBytes The estimated size of the request circuit breaker, in bytes. breakers.fieldDataCircuitBreakerTripped The number of times the field data circuit breaker has tripped. breakers.parentCircuitBreakerTripped The number of times the parent circuit breaker has tripped. breakers.requestCircuitBreakerTripped The number of times the request circuit breaker has tripped. cache.cacheSizeIDInBytes The size of the id cache, in bytes. flush.indexFlushDisk The number of index flushes to disk since start. flush.timeFlushIndexDiskInSeconds The time spent flushing the index to disk. fs.bytesAvailableJVMInBytes Bytes available to this Java virtual machine on this file store, in bytes. fs.bytesReadsInBytes The total bytes read from the file store, in bytes. fs.bytesUserIoOperationsInBytes The total bytes used for all I/O operations on the file store, in bytes. fs.iOOperations The total I/O operations on the file store. fs.reads The total number of reads from the file store. fs.totalSizeInBytes The total size of the file store, in bytes. fs.unallocatedBytesInBytes The total number of unallocated bytes in the file store, in bytes. fs.writes The total number of writes to the file store. fs.writesInBytes The total bytes written to the file store, in bytes. get.currentRequestsRunning The number of get requests currently running. get.requestsDocumentExists The number of get requests where the document existed. get.requestsDocumentExistsInMilliseconds The time spent on get requests where the document existed. get.requestsDocumentMissing The number of get requests where the document was missing. get.requestsDocumentMissingInMilliseconds The time spent on get requests where the document was missing. get.timeGetRequestsInMilliseconds The time spent on get requests. get.totalGetRequests The number of get requests. http.currentOpenConnections The number of current open HTTP connections. http.openedConnections The number of opened HTTP connections. indexing.docsCurrentlyDeleted The number of documents currently being deleted from an index. indexing.documentsCurrentlyIndexing The number of documents currently being indexed to an index. indexing.documentsIndexed The number of documents indexed to an index. indexing.timeDeletingDocumentsInMilliseconds The time spent deleting documents from an index. indexing.timeIndexingDocumentsInMilliseconds The time spent indexing documents to an index. indexing.totalDocumentsDeleted The number of documents deleted from an index. indices.indexingOperationsFailed The number of failed indexing operations. indices.indexingWaitedThrottlingInMilliseconds The time indexing waited due to throttling. indices.memoryQueryCacheInBytes The memory used by the query cache, in bytes. indices.numberIndices The number of documents across all primary shards assigned to the node. indices.queryCacheEvictions The number of query cache evictions. indices.queryCacheHits The number of query cache hits. indices.queryCacheMisses The number of query cache misses. indices.recoveryOngoingShardSource The number of ongoing recoveries for which a shard serves as a source. indices.recoveryOngoingShardTarget The number of ongoing recoveries for which a shard serves as a target. indices.recoveryWaitedThrottlingInMilliseconds The total time recoveries waited due to throttling. indices.requestCacheEvictions The number of request cache evictions. indices.requestCacheHits The number of request cache hits. indices.requestCacheMemoryInBytes The memory used by the request cache, in bytes. indices.requestCacheMisses The number of request cache misses. indices.segmentsIndexShard The number of segments in an index shard. indices.segmentsMaxMemoryIndexWriterInBytes The maximum memory used by the index writer, in bytes. indices.segmentsMemoryUsedDocValuesInBytes The memory used by doc values, in bytes. indices.segmentsMemoryUsedFixedBitSetInBytes The memory used by fixed bit set, in bytes. indices.segmentsMemoryUsedIndexSegmentsInBytes The memory used by index segments, in bytes. indices.segmentsMemoryUsedIndexWriterInBytes The memory used by the index writer, in bytes. indices.segmentsMemoryUsedNormsInBytes The memory used by norm, in bytes. indices.segmentsMemoryUsedSegmentVersionMapInBytes The memory used by the segment version map, in bytes. indices.segmentsMemoryUsedStoredFieldsInBytes The memory used by stored fields, in bytes. indices.segmentsMemoryUsedTermsInBytes The memory used by terms, in bytes. indices.segmentsMemoryUsedTermVectorsInBytes The memory used by term vectors, in bytes. indices.translogOperations The number of operations in the transaction log. indices.translogOperationsInBytes The size of the transaction log, in bytes. jvm.gc.collections The number of garbage collections run by the JVM. jvm.gc.collectionsInMilliseconds The time spent on garbage collection in the JVM. jvm.gc.concurrentMarkSweep The number of concurrent mark & sweep GCs in the JVM. jvm.gc.concurrentMarkSweepInMilliseconds The time spent on concurrent mark & sweep GCs in the JVM. jvm.gc.majorCollectionsOldGenerationObjects The number of major GCs in the JVM that collect old generation objects. jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds The time spent in major GCs in the JVM that collect old generation objects. jvm.gc.minorCollectionsYoungGenerationObjects The number of minor GCs in the JVM that collects young generation objects. jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds The time spent in minor GCs in the JVM that collects young generation objects. jvm.gc.parallelNewCollections The number of parallel new GCs in the JVM. jvm.gc.parallelNewCollectionsInMilliseconds The time spent on parallel new GCs in the JVM. jvm.mem.heapCommittedInBytes The amount of memory guaranteed to be available to the JVM heap, in bytes. jvm.mem.heapMaxInBytes The maximum amount of memory that can be used by the JVM heap, in bytes. jvm.mem.heapUsed The percentage of memory currently used by the JVM heap as a value between 0 and 1. jvm.mem.heapUsedInBytes The amount of memory currently used by the JVM heap, in bytes. jvm.mem.maxOldGenerationHeapInBytes The maximum amount of memory that can be used by the old generation heap, in bytes. jvm.mem.maxSurvivorSpaceInBytes The maximum amount of memory that can be used by the survivor space, in bytes. jvm.mem.maxYoungGenerationHeapInBytes The maximum amount of memory that can be used by the young generation heap, in bytes. jvm.mem.nonHeapCommittedInBytes The amount of memory guaranteed to be available to JVM non-heap, in bytes. jvm.mem.nonHeapUsedInBytes The amount of memory currently used by the JVM non-heap, in bytes. jvm.mem.usedOldGenerationHeapInBytes The amount of memory currently used by the old generation heap, in bytes. jvm.mem.usedSurvivorSpaceInBytes The amount of memory currently used by the survivor space, in bytes. jvm.mem.usedYoungGenerationHeapInBytes The amount of memory currently used by the young generation heap, in bytes. jvm.ThreadsActive The number of active threads in the JVM. jvm.ThreadsPeak The peak number of threads used by the JVM. merges.currentActive The number of currently active segment merges. merges.docsSegmentsMerging The number of documents across segments currently being merged. merges.docsSegmentMerges The number of documents across all merged segments. merges.mergedSegmentsInBytes The size of all merged segments, in bytes. merges.segmentMerges The number of segment merges. merges.sizeSegmentsMergingInBytes The size of the segments currently being merged, in bytes. merges.totalSegmentMergingInMilliseconds The time spent on segment merging. openFD The number of opened file descriptors associated with the current process, or-1 if not supported. queriesTotal The number of queries. refresh.total The number of index refreshes. refresh.totalInMilliseconds The time spent on index refreshes. searchFetchCurrentlyRunning The number of search fetches currently running. searchFetches The number of search fetches. sizeStoreInBytes The size of the store, in bytes. threadpool.bulk.Queue The number of queued threads in the bulk pool. threadpool.bulkActive The number of active threads in the bulk pool. threadpool.bulkRejected The number of rejected threads in the bulk pool. threadpool.bulkThreads The number of threads in the bulk pool. threadpool.fetchShardStartedQueue The number of queued threads in the fetch shard started pool. threadpool.fetchShardStartedRejected The number of rejected threads in the fetch shard started pool. threadpool.fetchShardStartedThreads The number of threads in the fetch shard started pool. threadpool.fetchShardStoreActive The number of active threads in the fetch shard store pool. threadpool.fetchShardStoreQueue The number of queued threads in the fetch shard store pool. threadpool.fetchShardStoreRejected The number of rejected threads in the fetch shard store pool. threadpool.fetchShardStoreThreads The number of threads in the fetch shard store pool. threadpool.flushActive The number of active threads in the flush queue. threadpool.flushQueue The number of queued threads in the flush pool. threadpool.flushRejected The number of rejected threads in the flush pool. threadpool.flushThreads The number of threads in the flush pool. threadpool.forceMergeActive The number of active threads for force merge operations. threadpool.forceMergeQueue The number of queued threads for force merge operations. threadpool.forceMergeRejected The number of rejected threads for force merge operations. threadpool.forceMergeThreads The number of threads for force merge operations. threadpool.genericActive The number of active threads in the generic pool. threadpool.genericQueue The number of queued threads in the generic pool. threadpool.genericRejected The number of rejected threads in the generic pool. threadpool.genericThreads The number of threads in the generic pool. threadpool.getActive The number of active threads in the get pool. threadpool.getQueue The number of queued threads in the get pool. threadpool.getRejected The number of rejected threads in the get pool. threadpool.getThreads The number of threads in the get pool. threadpool.indexActive The number of active threads in the index pool. threadpool.indexQueue The number of queued threads in the index pool. threadpool.indexRejected The number of rejected threads in the index pool. threadpool.indexThreads The number of threads in the index pool. threadpool.listenerActive The number of active threads in the listener pool. threadpool.listenerQueue The number of queued threads in the listener pool. threadpool.listenerRejected The number of rejected threads in the listener pool. threadpool.listenerThreads The number of threads in the listener pool. threadpool.managementActive The number of active threads in the management pool. threadpool.managementQueue The number of queued threads in the management pool. threadpool.managementRejected The number of rejected threads in the management pool. threadpool.managementThreads The number of threads in the management pool. threadpool.mergeActive The number of active threads in the merge pool. threadpool.mergeQueue The number of queued threads in the merge pool. threadpool.mergeRejected The number of rejected threads in the merge pool. threadpool.mergeThreads The number of threads in the merge pool. threadpool.percolateActive The number of active threads in the percolate pool. threadpool.percolateQueue The number of queued threads in the percolate pool. threadpool.percolateRejected The number of rejected threads in the percolate pool. threadpool.percolateThreads The number of threads in the percolate pool. threadpool.refreshActive The number of active threads in the refresh pool. threadpool.refreshQueue The number of queued threads in the refresh pool. threadpool.refreshRejected The number of rejected threads in the refresh pool. threadpool.refreshThreads The number of threads in the refresh pool. threadpool.searchActive The number of active threads in the search pool. threadpool.searchQueue The number of queued threads in the search pool. threadpool.searchRejected The number of rejected threads in the search pool. threadpool.searchThreads The number of threads in the search pool. threadpool.snapshotActive The number of active threads in the snapshot pool. threadpool.snapshotQueue The number of queued threads in the snapshot pool. threadpool.snapshotRejected The number of rejected threads in the snapshot pool. threadpool.snapshotThreads The number of threads in the snapshot pool. threadpool.activeFetchShardStarted The number of active threads in the fetch shard started pool. transport.connectionsOpened The number of connections opened for cluster communication. transport.packetsReceived The number of packets received in cluster communication. transport.packetsReceivedInBytes The size of data received in cluster communication, in bytes. transport.packetsSent The number of packets sent in cluster communication. transport.packetsSentInBytes The size of data sent in cluster communication, in bytes. Elasticsearch common metrics These attributes are attached to the ElasticsearchCommonSample event type: primaries.docsDeleted The number of documents deleted from the primary shards. primaries.docsnumber The number of documents in the primary shards. primaries.flushesTotal The number of index flushes to disk from the primary shards since start. primaries.flushTotalTimeInMilliseconds The time spent flushing the index to disk from the primary shards. primaries.get.documentsExist The number of get requests on primary shards where the document existed. primaries.get.documentsExistInMilliseconds The time spent on get requests from the primary shards where the document existed. primaries.get.documentsMissing The number of get requests from the primary shards where the document was missing. primaries.get.documentsMissingInMilliseconds The time spent on get requests from the primary shards where the document was missing. primaries.get.requests The number of get requests from the primary shards. primaries.get.requestsCurrent The number of get requests currently running on the primary shards. primaries.get.requestsInMilliseconds The time spent on get requests from the primary shards. primaries.index.docsCurrentlyDeleted The number of documents currently being deleted from an index on the primary shards. primaries.index.docsCurrentlyDeletedInMilliseconds The time spent deleting documents from an index on the primary shards. primaries.index.docsCurrentlyIndexing The number of documents currently being indexed to an index on the primary shards. primaries.index.docsCurrentlyIndexingInMilliseconds The time spent indexing documents to an index on the primary shards. primaries.index.docsDeleted The number of documents deleted from an index on the primary shards. primaries.index.docsTotal The number of documents indexed to an index on the primary shards. primaries.indexRefreshesTotal The number of index refreshes on the primary shards. primaries.indexRefreshesTotalInMilliseconds The time spent on index refreshes on the primary shards. primaries.merges.current The number of currently active segment merges on the primary shards. primaries.merges.docsSegmentsCurrentlyMerged The number of documents across segments currently being merged on the primary shards. primaries.merges.docsTotal The number of documents across all merged segments on the primary shards. primaries.merges.SegmentsCurrentlyMergedInBytes The size of the segments currently being merged on the primary shards, in bytes. primaries.merges.SegmentsTotal The number of segment merges on the primary shards. primaries.merges.segmentsTotalInBytes The size of all merged segments on the primary shards, in bytes. primaries.merges.segmentsTotalInMilliseconds The time spent on segment merging on the primary shards. primaries.queriesInMilliseconds The time spent querying on the primary shards. primaries.queriesTotal The number of queries to the primary shards. primaries.queryActive The number of currently active queries on the primary shards. primaries.queryFetches The number of query fetches currently running on the primary shards. primaries.queryFetchesInMilliseconds The time spent on query fetches on the primary shards. primaries.queryFetchesTotal The number of query fetches on the primary shards. primaries.sizeInBytes The size of all the primary shards, in bytes. Elasticsearch index metrics These attributes are attached to the ElasticsearchIndexSample event type: index.docs The number of documents in the index. index.docsDeleted The number of deleted documents in the index. index.health The status of the index: red, yellow, or green. index.primaryShards The number of primary shards in the index. index.primaryStoreSizeInBytes The store size of primary shards in the index. index.replicaShards The number of replica shards in the index. index.storeSizeInBytes The store size of primary and replica shards in the index, in bytes. Inventory data The Elasticsearch integration captures the configuration parameters of the Elasticsearch node, as specified in the YAML config file. It also collects node configuration information from the \" _ nodes/ _ local\" endpoint. The data is available on the Inventory page, under the config/elasticsearch source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.8663,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elasticsearch monitoring <em>integration</em>",
        "sections": "Find <em>and</em> <em>use</em> <em>data</em>",
        "tags": "<em>On</em>-<em>host</em> <em>integrations</em>",
        "body": " structure of on-<em>host</em> integration configuration, see Configuration. Find and <em>use</em> <em>data</em> <em>Data</em> from this service is reported to an integration dashboard. Elasticsearch <em>data</em> is attached to the following event types: ElasticsearchClusterSample ElasticsearchNodeSample ElasticsearchCommonSample"
      },
      "id": "6044e41c28ccbc65ee2c6070"
    }
  ],
  "/docs/integrations/infrastructure-integrations/cloud-integrations/cloud-integrations-account-status-dashboard": [
    {
      "sections": [
        "Metric data gaps with cloud integrations",
        "Problem",
        "Solution",
        "Amazon (AWS)",
        "Microsoft Azure",
        "Google Cloud Platform (GCP)",
        "Tip",
        "Cause"
      ],
      "title": "Metric data gaps with cloud integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Cloud integrations"
      ],
      "external_id": "ee3473d0cbc9059b6b36503949d08d1e76c7fc06",
      "image": "https://docs.newrelic.com/static/dfa79b9e3086b81f216d306ba0afe557/c1b63/screen-metric-gap.png",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/cloud-integrations/metric-data-gaps-cloud-integrations/",
      "published_at": "2021-05-05T18:15:46Z",
      "updated_at": "2021-03-29T21:18:28Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You've set up your AWS, Azure, or GCP integration and are monitoring your metrics. However, you notice gaps in your metric data charts. This screenshot shows a metric data chart with gaps. Solution Heres a list of metrics which might show gaps in your metric data. If possible, avoid setting up alerts for these metrics because we know they can generate false positives. Amazon (AWS) Integration Provider Event Type Metric SNS SnsTopic QueueSample provider.subscriptionsConfirmed SnsTopic QueueSample provider.subscriptionsPending SnsTopic QueueSample provider.subscriptionsDeleted EFS EfsFileSystem BlockDeviceSample provider.lastKnownSizeInBytes ECS EcsCluster ComputeSample provider.registeredContainerInstancesCount EcsCluster ComputeSample provider.activeServicesCount EcsCluster ComputeSample provider.pendingTasksCount EcsCluster ComputeSample provider.runningTasksCount EcsService ComputeSample provider.pendingCount EcsService ComputeSample provider.runningCount EcsService ComputeSample provider.desiredCount DynamoDB DynamoDbTable DatastoreSample provider.itemCount DynamoDbTable DatastoreSample provider.tableSizeBytes AutoScaling AutoScalingInstance AutoScalingInstanceSample healthStatus Billing BillingBudget FinanceSample provider.actualAmount Billingbudget FinanceSample provider.forecastedAmount BillingBudget FinanceSample provider.limitAmount Microsoft Azure Integration Provider Event Type Metric SQL AzureSqlDatabase AzureSqlDatabaseSample databaseSizeCurrentBytes AzureSqlDatabase AzureSqlDatabaseSample databaseSizeLimitBytes AzureSqlServer AzureSqlServerSample dtuCurrent AzureSqlServer AzureSqlServerSample dtuLimit Google Cloud Platform (GCP) Tip We're currently reviewing the GCP metrics that can cause data gaps. Tip This list isn't complete. We're currently reviewing the full list of metrics that can cause data gaps. Cause Some metrics arent present in the usual cloud provider APIs (CloudWatch, Stackdriver, Azure Monitor) and are fetched from the service APIs instead. Each cloud service provider has a unique service API that processes data and interacts with the service. For example, if a metric isnt present in AWS CloudWatch, New Relic will fetch the metric from the AWS ECS service API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 141.14552,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Metric data gaps with <em>cloud</em> <em>integrations</em>",
        "sections": "Metric data gaps with <em>cloud</em> <em>integrations</em>",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": " AzureSqlDatabase AzureSqlDatabaseSample databaseSizeLimitBytes AzureSqlServer AzureSqlServerSample dtuCurrent AzureSqlServer AzureSqlServerSample dtuLimit Google <em>Cloud</em> Platform (GCP) Tip We&#x27;re currently reviewing the GCP metrics that can cause data gaps. Tip This list isn&#x27;t complete. We&#x27;re currently"
      },
      "id": "603e821e196a67a042a83df3"
    },
    {
      "sections": [
        "Configure polling frequency and data collection for cloud integrations",
        "Tip",
        "Overview of settings",
        "Caution",
        "Change polling frequency",
        "Specify data to be fetched",
        "Data collection",
        "Filters",
        "Potential impact on alerts and charts"
      ],
      "title": "Configure polling frequency and data collection for cloud integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Cloud integrations"
      ],
      "external_id": "b900b7545f9032201c212449be114e10176bf789",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/cloud-integrations/configure-polling-frequency-data-collection-cloud-integrations/",
      "published_at": "2021-05-05T18:14:56Z",
      "updated_at": "2021-03-16T06:05:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our cloud integrations get data from cloud provider APIs. In New Relic, you can change some of the data collection-related settings for your cloud integrations. Read on to see what changes you can make and the reasons for making them. Tip To use integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Overview of settings New Relic cloud integrations get data from cloud providers' APIs. Data is generally collected from monitoring APIs such as AWS CloudWatch, Azure Monitor, and GCP Stackdriver, and inventory metadata is collected from the specific services' APIs. You can use the account status dashboard to see how your cloud integrations are handling data from a cloud service provider. If you want to report more or less data from your cloud integrations, or if you need to control the use of the cloud providers' APIs to prevent reaching rate and throttling limits in your cloud account, you can change the configuration settings to modify the amount of data they report. The two main controls are: Change polling frequency Change what data is reported Examples of business reasons for wanting to change your polling frequency include: Billing: If you need to manage your AWS CloudWatch bill, you may want to decrease the polling frequency. Before you do this, make sure that any alert conditions set for your cloud integrations are not affected by this reduction. New services: If you are deploying a new service or configuration and you want to collect data more often, you may want to increase the polling frequency temporarily. Caution Changing the configuration settings for your integrations may impact alert conditions and chart trends. Change polling frequency The polling frequency configuration determines how often New Relic reports data from your cloud provider for each service. By default, the polling frequency is set to the maximum frequency that is available for each service. To change the polling frequency for a cloud integration: Go to one.newrelic.com > Infrastructure. Select the tab that corresponds to your cloud service provider. Select Configure next to the integration. Use the dropdowns next to Data polling interval every to select how frequently you want New Relic to capture your cloud integration data. Specify data to be fetched You can specify which information you want captured for your cloud integration by enabling the collection of additional data and by applying multiple filters to each integration. To change this settings for your cloud integration: Go to one.newrelic.com > Infrastructure. Select the tab that corresponds to your cloud service provider. Select Configure next to the integration. Under Data collections and filters, turn the toggles you want On. For filters, select or enter the values that you want included in your reported data. Data collection For some cloud integrations, an additional number of calls to the cloud provider APIs are needed in order to collect data. For example, to fetch tags for AWS Elastic Map Reduce clusters, an additional call to the service API is required. Therefore, in order to better control the amount of API calls that are sent to your cloud account for these integrations, New Relic allows you to specify if you need these data to be collected. There are different data collection toggles available, depending on the integration. Toggle Description Collect tags Some integrations require additional API calls to the cloud provider to report tags. Tag collection is enabled by default. Switch this to Off if you don't want the integration to collect your cloud resource tags and thus reduce the volume of API calls. Collect extended inventory Some integrations can collect extended inventory metadata about your cloud resources by making additional API calls to the cloud provider. The metadata that is included within extended inventory for each cloud integration is described in the integration documentation. Extended inventory collection is disabled by default. Switch this to On if you want to monitor extended inventory. This will increase the volume of API calls. Collect shards data Available for AWS Kinesis Streams integration. By default, New Relic does not report shard metrics. Switch this to On if you want to monitor shard metrics in addition to data stream metrics. Collect Lambda@Edge data Available for AWS CloudFront integration. By default, New Relic does not report Lambda@Edge data. Switch this to On if you're using Lambda@Edge in AWS CloudFront and want to get Lambda execution location metadata. Collect nodes data Available for AWS Elasticsearch integration. By default, New Relic does not report Elasticsearch node metrics. Switch this to On if you want to monitor node metrics in addition to cluster metrics. Collect NAT Gateway data and Collect VPN data Available for AWS VPC integration. By default, New Relic does not report NAT Gateway nor VPN metrics. Switch these to On if you want to monitor NAT Gateway and VPN metrics and inventory, in addition to other VPC related entities inventory. Collect IP addresses Available for AWS EC2 integration. By default, New Relic collects EC2 instance metadata that includes public and private IP addresses, and network interface details. Switch this to Off if you don't want New Relic to store and display these IP data. Filters When a filter is On, you specify the data that you want to be collected; for example, if the Limit to AWS region is On, the regions that you select will be the ones that data will be collected for. There are different filters available, depending on the integration: Filter Description Region Select the regions that include the resources that you want to monitor. Queue prefixes Available for AWS SQS integration. Enter each name or prefix for the queues that you want to monitor. Filter values are case-sensitive. Load balancer prefixes Available for AWS ALB integration. Enter each name or prefix for the application load balancers that you want to monitor. Filter values are case-sensitive. Stage name prefixes Available for AWS API Gateway integration. Enter each name or prefix for the stages that you want to monitor. Filter values are case-sensitive. Tag key Enter one tag key that is associated with the resources that you want to monitor. Filter values are case-sensitive, and you can use this filter in combination with tag value filter. Tag value Enter one tag value that is associated with the resources that you want to monitor. Filter values are case-sensitive, and you can use this filter in combination with tag key. Resource group Select the resource groups that are associated with the resources that you want to monitor. Potential impact on alerts and charts If you change an integration's configuration, it can impact alert conditions and charts. Here are some things to consider: If you change this setting... It may have this impact... Any configuration setting When you change the configuration settings, the data that New Relic displays in infrastructure charts, on the inventory page, and in the events feed changes as well. Any filters When you create alert conditions after you set filters, make sure that your alerts are not triggered by resources that you filtered out. Filter for regions If you filter for specific regions, it may lower the amount of data reported to New Relic, which could trigger an alert. If you create an alert condition for a specific region and then filter that region out, the region would no longer report data and would never trigger the alert. Polling frequency When you create an alert, make sure that you define the threshold for a time period that is longer than the polling frequency. Tags and extended inventory If you turn on tags and/or extended inventory, New Relic makes more API calls to the cloud provider, which could increase your cloud provider API usage bill.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 138.33432,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure polling frequency and data collection for <em>cloud</em> <em>integrations</em>",
        "sections": "Configure polling frequency and data collection for <em>cloud</em> <em>integrations</em>",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": "Our <em>cloud</em> <em>integrations</em> get data from <em>cloud</em> provider APIs. In New Relic, you can change some of the data collection-related settings for your <em>cloud</em> <em>integrations</em>. Read on to see what changes you can make and the reasons for making them. Tip To use <em>integrations</em> and the rest of our observability"
      },
      "id": "603e8eef64441fcc7e4e8853"
    },
    {
      "sections": [
        "Introduction to infrastructure integrations",
        "Tip",
        "Types of infrastructure integrations",
        "Cloud integrations",
        "On-host integrations",
        "Install instructions",
        "Features",
        "Types of integration data"
      ],
      "title": "Introduction to infrastructure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Get started"
      ],
      "external_id": "98b6a0d19418b67c315b3757a1acc905b2fc53bf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/get-started/introduction-infrastructure-integrations/",
      "published_at": "2021-05-04T16:40:37Z",
      "updated_at": "2021-03-13T03:24:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers various integrations for reporting data to our platform. One category of integrations is our Infrastructure integrations. Tip To use integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Types of infrastructure integrations New Relic has two main categories of infrastructure integrations: cloud and on-host. Cloud integrations Cloud integrations collect data from cloud services and accounts. There's no installation process for cloud integrations, you simply connect your New Relic account to your cloud provider account. Integrations Description Amazon Web Services (AWS) cloud-based integrations Connect your Amazon Web Services (AWS) account to monitor and report data to New Relic. See the list of AWS integrations. Microsoft Azure cloud-based integrations Connect your Microsoft Azure account to monitor and report data to New Relic. See the list of Azure integrations. Google Cloud Platform (GCP) cloud-based integrations Connect your Google Cloud Platform (GCP) account to monitor and report data to New Relic. See the list of GCP integrations. On-host integrations On-host integrations are what we call integrations that you can run directly on your host or server. They typically connect to core services in your servers: Integrations Description Kubernetes integration Connect your account to gain visibility of your Kubernetes environment, explore your clusters, and manage alerts. On-host integrations Monitor and report data from many popular services, including Kubernetes, Redis, Apache, RabbitMQ, and many more. Build your own To create your own lightweight integration, use our Flex integration tool. Install instructions To enable cloud integrations or install on-host integrations, see: Cloud integrations: AWS procedures, Azure procedures, Google Cloud Platform procedures Kubernetes: Kubernetes procedures On-host integrations: See an integration's documentation for install procedures SDK integrations: Procedures to create a custom integration Features After an infrastructure integration is activated, you can: Filter and analyze the metrics and configuration data in our Infrastructure UI. Query your data and create custom charts and dashboards. Create alert conditions to monitor problems with your services' performance. For cloud integrations, configure data collection settings. Types of integration data Infrastructure integrations generate some basic types of data that you can use in New Relic. Integration data Description Metrics Numeric measurement data. Examples: Number of requests in a queue Number of hits on a database per minute Percentage of CPU being used Cloud-based and on-host integrations include pre-built dashboards that display important metrics. Inventory Live system state and configuration information. Examples: Host name AWS region or availability zone Port being used Changes in inventory generate events in New Relic, so you can easily figure out when performance issues were caused by a change in the system. Events Important activity on a system. Examples: Service starting Version update New table being created Changes to inventory are a type of event. Attributes Key-value pairs generated by some integrations. Examples: Certain inventory data Additional data attached to events Any data that is not considered metrics or inventory Depending on the integration, other types of information may be reported as attributes. Our integrations are data agnostic; they have no knowledge of whether reported data contains personal information. For more information about New Relic's security measures, see our security and privacy documentation, or visit the New Relic security website.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 117.353195,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>infrastructure</em> <em>integrations</em>",
        "sections": "Introduction to <em>infrastructure</em> <em>integrations</em>",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": " account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Types of <em>infrastructure</em> <em>integrations</em> New Relic has two main categories of <em>infrastructure</em> <em>integrations</em>: <em>cloud</em> and on-host. <em>Cloud</em> <em>integrations</em> <em>Cloud</em> <em>integrations</em> collect data from <em>cloud</em> services and accounts"
      },
      "id": "60450a39e7b9d2de845799cd"
    }
  ],
  "/docs/integrations/infrastructure-integrations/cloud-integrations/configure-polling-frequency-data-collection-cloud-integrations": [
    {
      "sections": [
        "Metric data gaps with cloud integrations",
        "Problem",
        "Solution",
        "Amazon (AWS)",
        "Microsoft Azure",
        "Google Cloud Platform (GCP)",
        "Tip",
        "Cause"
      ],
      "title": "Metric data gaps with cloud integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Cloud integrations"
      ],
      "external_id": "ee3473d0cbc9059b6b36503949d08d1e76c7fc06",
      "image": "https://docs.newrelic.com/static/dfa79b9e3086b81f216d306ba0afe557/c1b63/screen-metric-gap.png",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/cloud-integrations/metric-data-gaps-cloud-integrations/",
      "published_at": "2021-05-05T18:15:46Z",
      "updated_at": "2021-03-29T21:18:28Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You've set up your AWS, Azure, or GCP integration and are monitoring your metrics. However, you notice gaps in your metric data charts. This screenshot shows a metric data chart with gaps. Solution Heres a list of metrics which might show gaps in your metric data. If possible, avoid setting up alerts for these metrics because we know they can generate false positives. Amazon (AWS) Integration Provider Event Type Metric SNS SnsTopic QueueSample provider.subscriptionsConfirmed SnsTopic QueueSample provider.subscriptionsPending SnsTopic QueueSample provider.subscriptionsDeleted EFS EfsFileSystem BlockDeviceSample provider.lastKnownSizeInBytes ECS EcsCluster ComputeSample provider.registeredContainerInstancesCount EcsCluster ComputeSample provider.activeServicesCount EcsCluster ComputeSample provider.pendingTasksCount EcsCluster ComputeSample provider.runningTasksCount EcsService ComputeSample provider.pendingCount EcsService ComputeSample provider.runningCount EcsService ComputeSample provider.desiredCount DynamoDB DynamoDbTable DatastoreSample provider.itemCount DynamoDbTable DatastoreSample provider.tableSizeBytes AutoScaling AutoScalingInstance AutoScalingInstanceSample healthStatus Billing BillingBudget FinanceSample provider.actualAmount Billingbudget FinanceSample provider.forecastedAmount BillingBudget FinanceSample provider.limitAmount Microsoft Azure Integration Provider Event Type Metric SQL AzureSqlDatabase AzureSqlDatabaseSample databaseSizeCurrentBytes AzureSqlDatabase AzureSqlDatabaseSample databaseSizeLimitBytes AzureSqlServer AzureSqlServerSample dtuCurrent AzureSqlServer AzureSqlServerSample dtuLimit Google Cloud Platform (GCP) Tip We're currently reviewing the GCP metrics that can cause data gaps. Tip This list isn't complete. We're currently reviewing the full list of metrics that can cause data gaps. Cause Some metrics arent present in the usual cloud provider APIs (CloudWatch, Stackdriver, Azure Monitor) and are fetched from the service APIs instead. Each cloud service provider has a unique service API that processes data and interacts with the service. For example, if a metric isnt present in AWS CloudWatch, New Relic will fetch the metric from the AWS ECS service API.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 141.14552,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Metric data gaps with <em>cloud</em> <em>integrations</em>",
        "sections": "Metric data gaps with <em>cloud</em> <em>integrations</em>",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": " AzureSqlDatabase AzureSqlDatabaseSample databaseSizeLimitBytes AzureSqlServer AzureSqlServerSample dtuCurrent AzureSqlServer AzureSqlServerSample dtuLimit Google <em>Cloud</em> Platform (GCP) Tip We&#x27;re currently reviewing the GCP metrics that can cause data gaps. Tip This list isn&#x27;t complete. We&#x27;re currently"
      },
      "id": "603e821e196a67a042a83df3"
    },
    {
      "sections": [
        "Cloud integrations: Account status dashboard",
        "Why it matters",
        "Understand dashboard data",
        "Find account status dashboard"
      ],
      "title": "Cloud integrations: Account status dashboard",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Cloud integrations"
      ],
      "external_id": "3457b96bec1ab5f0252c061f4ef82bc15f9b61e7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/cloud-integrations/cloud-integrations-account-status-dashboard/",
      "published_at": "2021-05-05T18:14:55Z",
      "updated_at": "2021-03-16T06:05:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Each cloud account you link to New Relic infrastructure monitoring (for example, AWS or Azure) has an account status dashboard that shows how our platform is handling data from that cloud service provider. Why it matters Reasons to monitor this dashboard include: Understand API call usage. We use cloud APIs to collect metrics and inventory data. The dashboards show counts and frequencies for those API calls. You may want to monitor these for two reasons: API calls can cost money, and your API usage may be throttled at certain usage levels. To decrease usage, you can configure integration settings to reduce the number of components you monitor or how frequently they're monitored. Troubleshoot missing data or other data issues. The dashboard displays information that affects how integrations report data, including: Errors that affect New Relic collecting data, like permission errors or quota exhaustion errors. Changes to integration configuration, like polling interval changes, or tag-collection being disabled or enabled. Understand dashboard data The specific data displayed on the account status dashboard will differ by cloud service provider. Common charts include: Data updates: Shows updates to metric data or inventory data (updates shown as a 1 value). Account changes: Actions affecting how the integration works. For example: renaming or unlinking a cloud account, changing polling intervals, and other configuration options. Data freshness: Timestamp of the last data point collected for each integration. Fetching errors: These indicate issues with collecting data. (This may be a problem on the cloud-provider side.) Options for better understanding chart data include: Mouse over a charts icon to see a chart description (if available). View the chart's underlying NRQL query. Find account status dashboard To find the account status dashboard for a cloud service provider: From one.newrelic.com > Infrastructure, and select a cloud service provider (for example, AWS). Select Account status dashboard for the cloud account.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 135.00824,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Cloud</em> <em>integrations</em>: Account status dashboard",
        "sections": "<em>Cloud</em> <em>integrations</em>: Account status dashboard",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": "Each <em>cloud</em> account you link to New Relic <em>infrastructure</em> monitoring (for example, AWS or Azure) has an account status dashboard that shows how our platform is handling data from that <em>cloud</em> service provider. Why it matters Reasons to monitor this dashboard include: Understand API call usage. We use"
      },
      "id": "603eb14764441f1cda4e8874"
    },
    {
      "sections": [
        "Introduction to infrastructure integrations",
        "Tip",
        "Types of infrastructure integrations",
        "Cloud integrations",
        "On-host integrations",
        "Install instructions",
        "Features",
        "Types of integration data"
      ],
      "title": "Introduction to infrastructure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Get started"
      ],
      "external_id": "98b6a0d19418b67c315b3757a1acc905b2fc53bf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/get-started/introduction-infrastructure-integrations/",
      "published_at": "2021-05-04T16:40:37Z",
      "updated_at": "2021-03-13T03:24:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers various integrations for reporting data to our platform. One category of integrations is our Infrastructure integrations. Tip To use integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Types of infrastructure integrations New Relic has two main categories of infrastructure integrations: cloud and on-host. Cloud integrations Cloud integrations collect data from cloud services and accounts. There's no installation process for cloud integrations, you simply connect your New Relic account to your cloud provider account. Integrations Description Amazon Web Services (AWS) cloud-based integrations Connect your Amazon Web Services (AWS) account to monitor and report data to New Relic. See the list of AWS integrations. Microsoft Azure cloud-based integrations Connect your Microsoft Azure account to monitor and report data to New Relic. See the list of Azure integrations. Google Cloud Platform (GCP) cloud-based integrations Connect your Google Cloud Platform (GCP) account to monitor and report data to New Relic. See the list of GCP integrations. On-host integrations On-host integrations are what we call integrations that you can run directly on your host or server. They typically connect to core services in your servers: Integrations Description Kubernetes integration Connect your account to gain visibility of your Kubernetes environment, explore your clusters, and manage alerts. On-host integrations Monitor and report data from many popular services, including Kubernetes, Redis, Apache, RabbitMQ, and many more. Build your own To create your own lightweight integration, use our Flex integration tool. Install instructions To enable cloud integrations or install on-host integrations, see: Cloud integrations: AWS procedures, Azure procedures, Google Cloud Platform procedures Kubernetes: Kubernetes procedures On-host integrations: See an integration's documentation for install procedures SDK integrations: Procedures to create a custom integration Features After an infrastructure integration is activated, you can: Filter and analyze the metrics and configuration data in our Infrastructure UI. Query your data and create custom charts and dashboards. Create alert conditions to monitor problems with your services' performance. For cloud integrations, configure data collection settings. Types of integration data Infrastructure integrations generate some basic types of data that you can use in New Relic. Integration data Description Metrics Numeric measurement data. Examples: Number of requests in a queue Number of hits on a database per minute Percentage of CPU being used Cloud-based and on-host integrations include pre-built dashboards that display important metrics. Inventory Live system state and configuration information. Examples: Host name AWS region or availability zone Port being used Changes in inventory generate events in New Relic, so you can easily figure out when performance issues were caused by a change in the system. Events Important activity on a system. Examples: Service starting Version update New table being created Changes to inventory are a type of event. Attributes Key-value pairs generated by some integrations. Examples: Certain inventory data Additional data attached to events Any data that is not considered metrics or inventory Depending on the integration, other types of information may be reported as attributes. Our integrations are data agnostic; they have no knowledge of whether reported data contains personal information. For more information about New Relic's security measures, see our security and privacy documentation, or visit the New Relic security website.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 117.353195,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>infrastructure</em> <em>integrations</em>",
        "sections": "Introduction to <em>infrastructure</em> <em>integrations</em>",
        "tags": "<em>Infrastructure</em> <em>integrations</em>",
        "body": " account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Types of <em>infrastructure</em> <em>integrations</em> New Relic has two main categories of <em>infrastructure</em> <em>integrations</em>: <em>cloud</em> and on-host. <em>Cloud</em> <em>integrations</em> <em>Cloud</em> <em>integrations</em> collect data from <em>cloud</em> services and accounts"
      },
      "id": "60450a39e7b9d2de845799cd"
    }
  ]
}