---
title: Update New Relic OTLP handling of long attributes
freshnessValidatedDate: never
---

We’re adjusting our OTLP ingest validation logic to be less strict when encountering long attribute values.

The New Relic OTLP endpoint has a variety of [attribute limits](/docs/opentelemetry/best-practices/opentelemetry-otlp/#attribute-limits). Currently, if you send New Relic an OTLP span, metric, or trace with a string attribute which exceeds the 4095 character length limit, a [NrIntegrationError](https://docs.newrelic.com/docs/data-apis/manage-data/nrintegrationerror/) is generated and the entire record is dropped.

This is the single most common cause for missing data. Luckily, there’s an easy fix:

**Instead of dropping records which violate this limit, we’re simply going to truncate long attributes to 4095 characters.**

We’ll be rolling out this change on **June 2, 2025**.

## How will this affect me [#how-will-this-affect-me]

We believe this will be a welcome relief in almost all cases. Missing data can be painful to track down and diagnose, especially in environments with a large number of deployments managed by many individual teams. With this change, the New Relic OTLP endpoint will better embody our philosophy of "store what you send", and have one less common pitfall.

However, given New Relic's [usage based pricing model](https://newrelic.com/pricing), this change means that records that were previously dropped for exceeding limits will now be stored and contribute to data usage for your account.

If you’re following our OTLP endpoint documentation on [attribute limits](/docs/opentelemetry/best-practices/opentelemetry-otlp/#attribute-limits), **there is nothing to do**. Conforming to the attribute limits means that no data is currently being dropped, and therefore no additional data will be stored.

To check if any data in your account is currently being dropped due to exceeding the attribute length limit, run the following NRQL query:

```
FROM NrIntegrationError SELECT * WHERE message like 'One or more OTLP data point(s) was dropped because the length of an attribute value was over the limit.
```

If this query returns results, you may see a change in usage and therefore billing following this change. The exact amount depends on the frequency with which records violate the limits. See [mitigation](#mitigation) for advice on how to offset any additional data usage you may incur.

## Mitigation [#mitigation]

Part of OpenTelemetry’s core value proposition is in providing tools to take control of your telemetry data pipeline. As such, there are a variety of tools available to help mitigate a change in data usage. For a full discussion, see [Manage OpenTelemetry data ingest volume](/docs/opentelemetry/best-practices/opentelemetry-manage-data-ingest-volume/). A few strategies are particularly relevant here:

* [Truncate long attributes](#truncate-long-attributes)
* [Drop offending attributes](#drop-offending-attributes)
* [Send fewer records with sampling](#send-fewer-records-with-sampling)

### Truncate long attributes [#truncate-long-attributes]

Leveraging the collector [transform](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/transformprocessor) processor and the [truncate_all](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/pkg/ottl/ottlfuncs#truncate_all) editor, truncate all attributes to some known limit, as demonstrated [here](https://github.com/newrelic/newrelic-opentelemetry-examples/blob/036108c49b20f0e05ca80366a488d60cd02c5e1d/other-examples/collector/nr-config/otel-config.yaml#L14-L24). This is what the New Relic OTLP endpoint will be doing after this change. However, you can leverage this technique to offset a change in usage by setting a lower limit than New Relic platform limits. For example, you could set a limit of 1000 if you think you’ll only need the first 1000 characters for your observability use case.

```yaml
transform:
  trace_statements:
    - truncate_all(span.attributes, 4095)
    - truncate_all(resource.attributes, 4095)
  log_statements:
    - truncate_all(log.attributes, 4095)
    - truncate_all(resource.attributes, 4095)
  metric_statements:
    - truncate_all(datapoint.attributes, 4095)
    - truncate_all(resource.attributes, 4095)
```

### Drop offending attributes [#drop-offending-attributes]

Leveraging the collector [transform](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/transformprocessor) processor and the [delete_key](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/pkg/ottl/ottlfuncs#delete_key) editor, delete attributes which are not valuable:

```yaml
transform:
  trace_statements:
    - delete_key(span.attributes, "attribute.key.to.drop")
  log_statements:
    - delete_key(log.attributes, "attribute.key.to.drop")
  metric_statements:
    - delete_key(datapoint.attributes, "attribute.key.to.drop")
```

You may choose to drop attribute keys which are particularly long and which are therefore outsized contributors to usage, or attributes which are short but simply not typically useful. For reference, the following list summarizes the 10 most common attributes that violate the length limit:

* `exception.stactrace`
* `other` - a catch-all for custom attributes not defined in the OpenTelemetry semantic conventions
* `db.statement`
* `process.command_line`
* `graphql.document`
* `db.operation`
* `db.query.text`
* `http.url`
* `exception.message`
* `http.target`

### Send fewer records with sampling [#send-fewer-records-with-sampling]

Offset the additional data by adjusting your sampling rate using any of the strategies discussed [here](/docs/opentelemetry/best-practices/opentelemetry-manage-data-ingest-volume/#concept-sampling).
