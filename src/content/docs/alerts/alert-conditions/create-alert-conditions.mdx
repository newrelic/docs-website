---
title: Create alert conditions
tags:
  - Alerts
  - Alert conditions
translate:
  - jp
metaDescription: "Use the conditions page to identify what triggers an alert policy's notification, starting with the product and type of metric or service."
redirects:
  - /docs/alerts-applied-intelligence/new-relic-alerts/get-started/your-first-nrql-condition
  - /docs/alerts/alert-policies/configuring-alerts/managing-your-alerts
  - /docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/alert-conditions
  - /docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/select-product-targets-alert-condition
  - /docs/alerts/create-alert/create-alert-condition/create-alert-conditions
  - /docs/alerts/create-alert/create-alert-condition/update-or-disable-policies-conditions
  - /docs/alerts/new-relic-alerts-beta/configuring-alert-policies/define-alert-conditions
  - /docs/alerts/create-alert/create-alert-condition/alert-conditions
freshnessValidatedDate: 2024-10-28
---

An alert condition is the core element that defines when an [incident](/docs/new-relic-solutions/get-started/glossary/#alert-incident) is created. It acts as the essential starting point for building any meaningful alert. Alert conditions contain the parameters or thresholds met before you're informed.  They can mitigate excessive alerting or tell your team when new or unusual behavior appears.

An alert condition is a continuously running query that measures a given set of events against a defined threshold and opens an [incident](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/specify-when-alerts-create-incidents/) when the threshold is met for a specified window of time.

There are a lot of ways to create an alert condition. You can create an alert condition from:

* A [chart](#create-chart)
* Alert [policies](#create-policy)
* The [<DNT>**Alert coverage gaps**</DNT>](/docs/alerts/alert-conditions/alert-coverage-gaps/#create-an-alert) option
* The [<DNT>**Use guided mode**</DNT>](#create-guided-mode) option in the UI
* The [<DNT>**Write your own query**</DNT>](#create-own-query) option in the UI


<Steps>
  <Step>
    <Tabs>
      <TabsBar>
        <TabsBarItem id="create-chart">
          Create from a chart
        </TabsBarItem>

        <TabsBarItem id="create-policy">
          Create from alert policies
        </TabsBarItem>

        <TabsBarItem id="create-guided-mode">
          Create from the guided mode
        </TabsBarItem>

        <TabsBarItem id="create-own-query">
          Create writing your own query
        </TabsBarItem>
      </TabsBar>

      <TabsPages>
        <TabsPageItem id="create-chart">
        You can create an alert condition from pre-existing NRQL queries that are part of a chart.

        To create a new alert condition from a chart, follow these steps:

        1. Go to <DNT>**[one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > Dashboards**</DNT> and select a dashboard.

        2. Find a chart you want to use to create your alert condition, click the <Icon name="fe-more-horizontal"/> icon on the right corner of the chart, and select <DNT>**Create alert condition**</DNT>.

        <img
          width="70%;"
          title="Create an alert condition from a chart"
          alt="Create an alert condition from a chart"
          src="/images/alerts_screenshot-crop_chart-create-alert-condition.webp"
        />

        3. The create new alert condition page opens having a specific query. <DNT>**Run**</DNT>.

        4. Review your NRQL query and click <DNT>**Next**</DNT>.      

        </TabsPageItem>

        <TabsPageItem id="create-policy">

        You can create a new alert condition from alert policies.

        To create an alert condition from alert policies, follow these steps:

        1. Go to <DNT>**[one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > Alerts**</DNT>.

        2. Select <DNT>**Alert Policies**</DNT> in the left navigation.

        3. Click <DNT>**+ New alert condition**</DNT>.

        <img
          title="Create an alert condition from alert policies"
          alt="Create an alert condition from alert policies"
          src="/images/alerts_screenshot-crop_alert-policies-create-alert-condition.webp"
        />

        4. Select one of these options:

            * <DNT>**[Use guided mode](#create-guided-mode)**</DNT>

            * <DNT>**[Write your own query](#create-own-query)**</DNT>

        </TabsPageItem>

        <TabsPageItem id="create-guided-mode">
        When you create a new alert condition using the guided mode, you'll have several options that you'll need to choose. From your selected options, we'll build your query. We recommend this option becauseâ€¦.

        To create an alert condition using the guided mode, follow these steps:

        1. Go to <DNT>**[one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > Alerts**</DNT>.

        2. Select <DNT>**Alert Conditions**</DNT> in the left navigation.

        3. Click <DNT>**+ New alert condition**</DNT>.

        4. Select <DNT>**Use guided mode**</DNT>.
                
        <img
          width="50%;"
          title="Create an alert condition using the guided mode"
          alt="Create an alert condition using the guided mode"
          src="/images/alerts_screenshot-crop_conditions-guided-mode-option.webp"
        />

        5. Select the part or parts of your system you want to include in your alert condition.

        6. Click <DNT>**Next**</DNT>.    

        7. Select the entities to watch.

        8. Select a metric to monitor. Depending on the selected parts of your system, you'll see different metrics. These are the usual:

            * [Golden metrics](/docs/apis/nerdgraph/examples/golden-metrics-entities-nerdgraph-api-tutorial/)

            * Other metrics
      
          If you selected Host, you'll see these metrics:
            * Golden metrics
            * Host metrics
            * Host not reporting
            * Storage metrics
            * Network metrics
            * Process metrics

          If you selected Synthetic monitors, you'll see these metrics:
            * Median duration (s)
            * Failures

        9. Review your NRQL query.

        10. Click <DNT>**Next**</DNT>.

      </TabsPageItem>

      <TabsPageItem id="create-own-query">
      This option allows you to use NRQL to define your alert from scratch. 
      
      <Callout variant="tip">
        See [NRQL alert conditions tips](/docs/alerts/alert-conditions/nrql-tips) to get more info about formatting and configuring your NRQL alert conditions.
      </Callout>
      
      To create an alert condition writing your own query, follow these steps:

      1. Go to <DNT>**[one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > Alerts**</DNT>.

      2. Select <DNT>**Alert Conditions**</DNT> in the left navigation.

      3. Click <DNT>**+ New alert condition**</DNT>.

      4. Select <DNT>**Write your own query**</DNT>.

      <img
          width="50%;"
          title="Create an alert condition writing your own query"
          alt="Create an alert condition writing your own query"
          src="/images/alerts_screenshot-crop_conditions-writing-own-query.webp"
      />

      5. Select the part or parts of your system you want to include in your alert condition.

      6. Click <DNT>**Next**</DNT>.

      7. Write your query and click <DNT>**Run**</DNT>.

      8. Review your NRQL query and click <DNT>**Next**</DNT>.

      </TabsPageItem>
      </TabsPages>
    </Tabs>
  </Step>

  <Step>

    ### Set thresholds for alert conditions [#thresholds]

    A [threshold](/docs/new-relic-solutions/get-started/glossary/#alert-threshold) is a value that you define in your alert condition. Thresholds are the rules each alert condition must follow. When this defined value is reached for a specified window of time, an [incident](/docs/new-relic-solutions/get-started/glossary/#alert-incident) is created. An incident means there is a problem with your system and you should investigate.

      <img
          title="Set thresholds for alert conditions"
          alt="Set thresholds for alert conditions"
          src="/images/alerts_screenshot-crop_conditions-set-thresholds.webp"
      />

    <CollapserGroup>
      <Collapser
        id="window-duration"
        title="Window duration"
      >

      Setting the [window duration](/docs/alerts/alert-conditions/streaming-alerts-key-terms-concepts/#window-duration) for your alert condition tells New Relic how to group your data. If you're creating an alert condition for a data set that sends a signal to New Relic once every hour, you'd want to set the window duration to something closer to sixty minutes because it'll help spot patterns and unusual behavior. But, if you're creating an alert condition for web transaction time and New Relic collects a signal for that data every minute, we'd recommend setting the window duration to one minute.

      For your first alert we recommend sticking with our default settings, but the more you get familiar with creating an alert condition we encourage you to customize these fields based on your own experience.

      </Collapser>

      <Collapser
        id="sliding-window"
        title="Use sliding window aggregation"
      >
        Throughout the day, data streams from your application into New Relic. Instead of evaluating that data immediately for incidents, alert conditions collect the data over a period of time known as the aggregation window. To make loss of signal detection more effective and to reduce unnecessary notifications, you can customize aggregation windows to the duration that you need. An additional delay allows for slower data points to arrive before the window is aggregated.

        Sliding windows are helpful when you need to smooth out "spiky" charts. One common use case is to use sliding windows to smooth line graphs that have a lot of variation over short periods of time in cases where the rolling aggregate is more important than aggregates from narrow windows of time.

        You can set your aggregation window to anything between <DNT>**30 seconds**</DNT> and <DNT>**6 hours**</DNT>. The default is <DNT>**1 minute**</DNT>.

        We recommend using our sliding window aggregation if you're not expecting to have a steady and consistent stream of data but are expecting some dips and spikes in data. Once enabled, set the slide by interval to control how much overlap time your aggregated windows have. The interval must be shorter than the aggregation window while also dividing evenly into it.

        You can learn more about sliding window aggregation in [this tutorial](/docs/query-your-data/nrql-new-relic-query-language/nrql-query-tutorials/create-smoother-charts-sliding-windows/).

      </Collapser>

      <Collapser
        id="streaming-method"
        title="Streaming method"
      >
        In general, we recommend using the <DNT>**event flow**</DNT> streaming method. This is best for data that comes into your system frequently and steadily. There are specific cases where <DNT>**event timer**</DNT> might be a better method to choose, but for your first alert we recommend our default, <DNT>**event flow**</DNT>. To better understand which streaming method to choose, see [Streaming alerts: key terms and concepts](/docs/alerts/alert-conditions/streaming-alerts-key-terms-concepts/#streaming-method).

    </Collapser>

    <Collapser
      id="timer"
      title="Delay and timer"
    >
      This delay and timer setting indicates how long the condition need to wait before aggregating the data in the aggregation window. The event flow and cadence methods use delay. Event timer uses timer.

      Note that if your timer is much shorter than your window duration and your data flow is inconsistent, your alerts may not be accurate.

      The delay default is <DNT>**2 minutes**</DNT>. The timer default is <DNT>**1 minute**</DNT>, has a minimum value of <DNT>**5 seconds**</DNT>, and a maximum value of <DNT>**20 minutes**</DNT>.

    </Collapser>

    <Collapser
      id="gap-filling-strategy"
      title="Gap filling strategy"
    >

    Gap filling lets you customize the values to use when your signals don't have any data. You can fill gaps in your data streams with the last value received, a static value, or else do nothing and leave the gap there. The default is `None`.

    * <DNT>**None**</DNT>: (Default) Choose this if you don't want to take any action on empty aggregation windows. On evaluation, an empty aggregation window will reset the threshold duration timer. For example, if a condition says that all aggregation windows must have data points above the threshold for 5 minutes, and 1 of the 5 aggregation windows is empty, then the condition won't be an incident.

    * <DNT>**Custom static value**</DNT>: Choose this if you'd like to insert a custom static value into the empty aggregation windows before they're evaluated. This option has an additional, required parameter of `fillValue` (as named in the API) that specifies what static value should be used. This defaults to `0`.
    
    * <DNT>**Last known value**</DNT>: This option inserts the last seen value before evaluation occurs. We maintain the state of the last seen value for a minimum of 2 hours. If the configured threshold duration is longer than 2 hours, this value is kept for that duration instead.

    </Collapser>

    <Collapser
      id="evaluation-delay"
      title="Evaluation delay"
    >
    Evaluation delay is how long we wait before we start evaluating a signal agains the thresholds in this condition. You can enable the <DNT>**Use evaluation delay**</DNT> option and set up to 120 minutes to delay the evalution of incoming signals.

    When new entities are first deployed, resource utilization on the entity is often unusually high. In autoscale environments this can easily create a lot of false alerts. By delaying the start of alert detection on signals emitted from new entities you can significantly reduce the number of false alarms associated with deployments in orchestrated or autoscale environments.

    </Collapser>

    <Collapser
      id="condition-threshold"
      title="Set condition threshold"
    >

    You can set static or anomaly thresholds. A static threshold will open an incident whenever your system behaves differently than the criteria that you set. Static alert thresholds are much more customizable, and we recommend them if you've a strong sense of your data and what you're looking for.

    Anomaly thresholds are ideal when you're more concerned about deviations from expected patterns than specific numerical values. They enable you to monitor for unusual activity without needing to set predefined limits. New Relic's anomaly detection dynamically analyzes your data over time, adapting thresholds to reflect evolving system behavior. You can choose the threshold direction:

      * <DNT>**Upper and lower**</DNT> to be alerted about any higher and lower deviations than expected.
      * <DNT>**Lower only**</DNT>: To focus solely on unusually low values.
      * <DNT>**Upper only**</DNT>: To focus solely on unusually high values.

      You can also check our documentation about [anomaly threshold](/docs/alerts/alert-conditions/anomaly-detection/).

      <Callout variant="important">
        There is a maximum of 2 thresholds per condition.
      </Callout>

      </Collapser>

      <Collapser
        id="severity-level"
        title="Severity level"
      >

      You can assign a security level:
        * <DNT>**Critical**</DNT>: It will open a critical priority level incident and send notifications depending on the policy's issue [creation preference setting](/docs/alerts/organize-alerts/specify-when-alerts-create-incidents/) and any workflow you may have configured. Lost signal thresholds, when triggered, also open critical priority level incidents.
        * <DNT>**Warning**</DNT>: It will open a high priority leve incident and may send notifications depending on the policy's issue [creation preference setting](/docs/alerts/organize-alerts/specify-when-alerts-create-incidents/) and any workflow you may have configured. Use a warning threshold if you want to monitor when a system behavior is concerning or noteworthy but not important enough to require a critical-level threshold.

      </Collapser>

      <Collapser
        id="query-returns"
        title="When a query returns a value outside the threshold"
      >

      Set when you want your query returns a value. We recommend using our default settings and adjusting to your needs as necessary. So, leave the settings to open an incident <DNT>**When a query returns a value outside the threshold**</DNT> deviates from the predicted value: <DNT>**for at least 2 minutes**</DNT> by <DNT>**3 standard deviation(s)**</DNT>".

      </Collapser>
      <Collapser
        id="lost-signal"
        title="Add lost signal threshold"
      >
      You can use the <DNT>**Consider the signal lost after**</DNT> option to adjust the time window from 30 seconds to 48 hours.
        
        <Callout variant="important">
        The loss of signal feature requires a signal to be present before it can detect that the loss of signal. If you enable a condition while a signal isn't present, it won't detect a loss of signal and this feature won't activate.
      </Callout>
        
      The lost signal threshold determines how long to wait before considering a missing signal lost. If the signal doesn't return within that time, you can choose to open a new incident or close any related ones. You can also choose to skip opening an incident when a signal is expected to terminate. Set the threshold based on your system's expected behavior and data collection frequency. For example, if a website experiences a complete loss of traffic, or throughput, the corresponding telemetry data sent to New Relic will also cease. Monitoring for this loss of signal can serve as an early warning system for such outages.

      Loss of signal settings include a time duration and a few actions.

        * Signal loss expiration time:
            * GraphQL Node: [`expiration.expirationDuration`](/docs/apis/nerdgraph/examples/nerdgraph-api-loss-signal-gap-filling/#loss-of-signal).

            * Expiration duration is a timer that starts and resets when we receive a data point in the streaming alerts pipeline. If we don't receive another data point before your expiration time expires, we consider that signal to be lost. This can be because no data is being sent to New Relic or the `WHERE` clause of your query is filtering that data out before it's streamed to the alerts pipeline. Note that when you've a faceted query, each facet is a signal. So if any one of those signals ends during the duration specified, that will be considered a loss of signal.

            * The loss of signal expiration time is independent of the threshold duration and triggers as soon as the timer expires.

            * The maximum expiration duration is 48 hours. This is helpful when monitoring for the execution of infrequent jobs. The minimum is 30 seconds, but we recommend using at least 3-5 minutes.

        * Loss of signal actions: Once a signal is considered lost, you've a few options:
          * <DNT>**Close all current open incidents**</DNT>: This closes all open incidents that are related to a specific signal. It won't necessarily close all incidents for a condition. If you're alerting on an ephemeral service, or on a sporadic signal, you'll want to choose this action to ensure that incidents are closed properly. The GraphQL node name for this is [`closeViolationsOnExpiration`](/docs/apis/nerdgraph/examples/nerdgraph-api-loss-signal-gap-filling/#loss-of-signal).

          * <DNT>**Open new "lost signal" incident**</DNT>: This will open a new incident when the signal is considered lost. These incidents will indicate that they are due to a loss of signal. Based on your incident preferences, this should trigger a notification. The graphQL node name for this is [`openViolationOnExpiration`](/docs/apis/nerdgraph/examples/nerdgraph-api-loss-signal-gap-filling/#loss-of-signal).

          * When you enable both of the above actions, we'll close all open incidents first, and then open a new incident for loss of signal.

          * <DNT>**Do not open lost signal incidents on expected termination**</DNT>. When a signal is expected to terminate, you can choose not to open a new incident. This is useful when you know that a signal will be lost at a certain time, and you don't want to open a new incident for that signal loss. The GraphQL node name for this is [`ignoreOnExpectedTermination`](/docs/apis/nerdgraph/examples/nerdgraph-api-loss-signal-gap-filling/#loss-of-signal).

      <Callout variant="important">
        In order to prevent a loss of signal incident from opening when the option <DNT>**Do not open "lost signal" incident on expected termination**</DNT> is enabled, the tag `termination: expected` is added to the entity. This tag tells us the signal was expected to terminate. See [how to add the tag directly to the entity](/docs/new-relic-solutions/new-relic-one/core-concepts/use-tags-help-organize-find-your-data/#add-tags).
      </Callout>

    Incidents open due to loss of signal close when:

      * the signal comes back. Newly opened lost signal incidents will close immediately when new data is evaluated.
      * the condition they belong to expires. By default, conditions expire after 3 days.
      * you manually close the incident with the <DNT>**Close all current open incidents**</DNT> option.

      <Callout variant="tip">
        Loss of signal detection doesn't work on NRQL queries that use nested aggregation or sub-queries.
      </Callout>

      </Collapser>
    </CollapserGroup>    
    
    </Step>
    <Step>
    ### Add alert condition details [#add-details]

      <img
          title="Add alert condition details"
          alt="Add alert condition details"
          src="/images/alerts_screenshot-crop_condition-add-details.webp"
      />

    <CollapserGroup>
      <Collapser
        id="name-your-condition"
        title="Name your alert condition"
      >
        A best practice for condition naming involves a structured format that conveys essential information at a glance. Include the following elements in your condition names:

        * <DNT>**Priority**</DNT>: Indicate the severity or urgency of the alert, like P1, P2, P3.
        * <DNT>**Signal**</DNT>: Specify the metric or condition being monitored, like High Avg Latency or Low Throughput.
        * <DNT>**Entity**</DNT>: Identify the affected system, application, or component, like WebPortal App or Database Server.


      An example of a well-formed condition name following this structure would be `P2 | High Avg Latency | WebPortal App`.
      </Collapser>

      <Collapser
        id="connect-condition-policy"
        title="Connect this condition to a policy"
      >
        If you already have a policy you want to connect to an alert condition, then select the existing policy. Learn more about policies [here](/docs/alerts/organize-alerts/create-edit-or-find-alert-policy/).

        If you prefer to create a new policy, you'll have these options:

        * <DNT>**Policy name**</DNT>: Type a [meaningful name](/docs/alerts/organize-alerts/create-edit-or-find-alert-policy/#best-practices-policies) for the policy (maximum 64 characters).

        * <DNT>**Group incidents into issues**</DNT>: You have to choose an issue preference option. See [Issue preference options](/docs/alerts/organize-alerts/specify-when-alerts-create-incidents/#preference-options) for more information about it.


      Check the box <DNT>**Correlate and suppress noise**</DNT> to enable [correlation](/docs/alerts/organize-alerts/change-applied-intelligence-correlation-logic-decisions/#configure-correlation) for the alert policy and only get notified when you need to take action.

      </Collapser>
      <Collapser
        id="close-open-incidents"
        title="Close open incidents after"
      >
        An incident automatically closes when the targeted signal returns to a non-breaching state for the period indicated in the condition's thresholds. This wait time is called the recovery period.

        When an incident closes automatically:

        1. The closing timestamp is backdated to the start of the recovery period.
        2. The evaluation resets and restarts from when the previous incident ended.


      All conditions have an incident time limit setting that automatically force-close a long-lasting incident. New Relic automatically defaults to 3 days and recommends that you use our default settings for your first alert. Another way to close an open incident when the signal does not return data is by configuring a [`loss of signal`](/docs/alerts/alert-conditions/create-alert-conditions/#lost-signal) threshold.

      </Collapser>

      <Collapser
        id="customize-incidents"
        title="Customize incidents from this condition"
      >

      A <DNT>**[Title template](/docs/alerts/create-alert/condition-details/title-template)**</DNT> is used when incidents are opened by the condition. It overrides the default title. Your title should use handlebars for incident event attributes. For example, `{{conditionName}}` targeting `{{targetName}}` incident.

      You can use the <DNT>**[Description template](/docs/alerts/alert-conditions/description-template)**</DNT> field to define a description template with tags and custom attributes such as host name, owning team, and product, to consistently pass useful information downstream.
    </Collapser>
      <Collapser
        id="runbook"
        title="Runbook URL"
      >
        If you'd like to link to a [runbook](/docs/new-relic-solutions/get-started/glossary/#alert-runbook) for the condition that triggered the incident, you can add the URL in the runbook URL field.
      </Collapser>
    </CollapserGroup>    

    </Step>
    <Step>
    ### Save your alert condition

    Once you've finished, click <DNT>**Save condition**</DNT>. You'll see a summary of your alert condition.
    </Step>
</Steps>

