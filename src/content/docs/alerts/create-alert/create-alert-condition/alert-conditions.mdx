---
title: Alert conditions
tags:
    - Alerts
    - Alert conditions
translate:
    - jp
metaDescription: "Use the conditions page to identify what triggers an alert policy's notification, starting with the product and type of metric or service."
redirects:
    - /docs/alerts-applied-intelligence/new-relic-alerts/get-started/your-first-nrql-condition
    - /docs/alerts/alert-policies/configuring-alerts/managing-your-alerts
    - /docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/alert-conditions
    - /docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/select-product-targets-alert-condition
    - /docs/alerts/create-alert/create-alert-condition/create-alert-conditions
    - /docs/alerts/create-alert/create-alert-condition/update-or-disable-policies-conditions
freshnessValidatedDate: never
---

An alert condition is the core element that defines when an [incident](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/incident-event-attributes/#definition) is created. It acts as the essential starting point for building any meaningful alert. Alert conditions contain the parameters or thresholds met before you're informed.  They can mitigate excessive alerting or tell your team when new or unusual behavior appears.

<img
  width="80%;"
  title="A diagram showing some basic concepts and terms for New Relic alerting"
  alt="A diagram showing some basic concepts and terms for New Relic alerting"
  src="/images/accounts_diagram_alerting-concepts.webp"
/>

## Create a new alert condition [#create-alert-condition]

An alert condition is a continuously running query that measures a given set of events against a defined threshold and opens an [incident](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/specify-when-alerts-create-incidents/) when the threshold is met for a specified window of time.

This example demonstrates manually creating a new alert condition using the <DNT>**Alert condition details**</DNT> page. But there are a lot of ways to create an alert condition. You can create an alert condition from:

* [A chart](/docs/tutorial-create-alerts/create-an-alert/)
* [A policy page](https://one.newrelic.com/nr1-core/condition-builder/policy-entity)
* [The <DNT>**Alert coverage gaps**</DNT> page](https://one.newrelic.com/alerts-ai/detection-gaps/)

You can also use one of our alert builders:

* Use <DNT>**Write your own query**</DNT> to build alerts from scratch
* <DNT>
    **Use guided mode**
  </DNT>

For all methods except for our guided mode, the process for creating an alert condition will be _exactly_ the same as described in the steps below.

<Steps>
  <Step>
    ### Set your signal behavior

    In this example, imagine that your team manages the `WebPortal` application for an ecommerce site. You want to be alerted of any latency issues.

    To create a new alert condition:

    * Go to [one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > <DNT>**Alerts**</DNT>.
    * Select <DNT>**Alert Conditions**</DNT> in the left navigation.
    * Then select <DNT>**New alert condition**</DNT>.
    * Select <DNT>**Write your own query**</DNT>.

    <CollapserGroup>
      <Collapser
        id="set-your-query"
        title="Set your signal behavior"
      >
        You can use a NRQL query to define the signals you want an alert condition to use as the foundation for your alert. For this example, you will be using this query:

        ```
        SELECT average(duration)
        FROM PageView
        WHERE appName = 'WebPortal'
        ```

        Using this query for your alert condition tells New Relic you want to know the average latency, or duration, to load pages within your `WebPortal` application. Proactive alerting on latency, a core golden signal, provides early warnings of potential degradation.

        You can learn more about how to use NRQL, New Relic's query language, see our [NRQL documentation](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/).
      </Collapser>
    </CollapserGroup>
  </Step>

  <Step>
    ### Fine-tune advanced signal settings [#advanced-signal-settings]

    After you've defined your signal, click <DNT>**Run**</DNT>. A chart will appear and display the parameters that you've set.

    <img
      title="set a query"
      alt="A screenshot showing a user how to set the signal behavior for an alert condition."
      src="/images/alerts_screenshot-crop_set-a-query.webp"
    />

    For this example, the chart will show the average latency for your `WebPortal` application. Click <DNT>**Next**</DNT> and begin configuring your alert condition.

    For this example, you will customize these advanced signal settings for the condition you created to monitor latency in your `WebPortal` application.

    <CollapserGroup>
      <Collapser
        id="window-duration"
        title="Window duration"
      >
        <img
          title="fine tune alert settings"
          alt="A screenshot depicting advanced signal settings."
          src="/images/alerts_screenshot-crop_fine-tune-alert-signals.webp"
        />

        The window duration defines how New Relic groups your data for analysis in an alert condition. Choosing the right setting depends on your data's frequency and your desired level of detail:

        * <DNT>**High-frequency data (for example, pageviews every minute)**</DNT>: Set the window duration to match the data frequency (1 minute in this case) for real-time insights into fluctuations and trends.
        * <DNT>**Low-frequency data (for example, hourly signals)**</DNT>: Choose a window duration that captures enough data to reveal patterns and anomalies (for example, 60 minutes for hourly signals).

        Remember, you can customize the window duration based on your needs and experience. We recommend using the defaults when starting and experimenting as you become more comfortable creating alert conditions.
      </Collapser>

      <Collapser
        id="sliding-window"
        title="Use sliding window aggregation"
      >
        Traditional aggregation methods can fall short when dealing with data that's sparsely populated or exhibits significant fluctuations between intervals. Here's how to use sliding window aggregation to analyze such data and trigger timely alerts effectively:

        1. <DNT>**Smooth out the noise**</DNT>: Start by creating a large aggregation window. This window (for example, 5 minutes) acts as a buffer, smoothing out the inherent "noise" or variability in your data. This helps prevent spurious alerts triggered by isolated spikes or dips.
        2. <DNT>**Avoid lag with a sliding window**</DNT>: While a large window helps in data analysis, if you wait for the entire interval to elapse before checking thresholds, you can experience significant delays in alert notifications. We recommend smaller sliding windows (for example, one minute). Imagine this sliding window as a moving frame scanning your data within the larger aggregation window. Each time the frame advances by its smaller interval, it calculates an aggregate value (for example, average).
        3. <DNT>**Set your threshold duration**</DNT>: Now, you can define your alert threshold within the context of the smaller sliding window. This allows you to trigger alerts quickly when the aggregate value in the current frame deviates significantly from the desired range without sacrificing the smoothing effect of the larger window.

           You can learn more about sliding window aggregation in [this NRQL tutorial](/docs/query-your-data/nrql-new-relic-query-language/nrql-query-tutorials/create-smoother-charts-with-sliding-windows/).
      </Collapser>

      <Collapser
        id="streaming-method"
        title="Streaming method"
      >
        In general, we recommend using the <DNT>**event flow**</DNT> streaming method. This is best for data that comes into your system frequently and steadily. There are specific cases where <DNT>**event timer**</DNT> might be a better method to choose, but for your first alert, we recommend our default, <DNT>**event flow**</DNT>. We recommend watching this brief video (approx. 5:31 minutes) to understand better which streaming method to choose.

        <Video
          type="wistia"
          id="n6nei987ln"
        />
      </Collapser>

      <Collapser
        id="delay"
        title="Delay"
      >
        The delay feature in alert conditions safeguards against potential issues arising from inconsistent data collection. It acts as a buffer, allowing extra time for data to arrive and be processed before triggering an alert. This helps prevent false positives and ensures more accurate incident creation.

        How it works:

        The appropriate delay setting is determined by evaluating the consistency of your incoming data:

        * Consistent data: A lower delay setting is sufficient if data points consistently arrive with timestamps within a single minute.
        * Inconsistent data: If data points arrive with timestamps spanning multiple minutes in the past or future, a higher delay setting is necessary to accommodate the inconsistency.

        Creating a buffer:

        * The selected delay setting introduces a waiting period before the alert condition assesses data against defined thresholds.
        * This buffer allows time for data discrepancies to settle, reducing the likelihood of misleading alerts.
      </Collapser>

      <Collapser
        id="gap-filling-strategy"
        title="Gap-filling strategy"
      >
        You're creating an alert condition to notify your team of any latency issues with the `WebPortal` application. In this example, your application consistently sends New Relic data. There is a constant stream of signals being sent from your application to New Relic, and there is no expected gap in signal, so you won't need to select a gap-filling strategy.

        Gap-filling strategies address scenarios where data collection might be intermittent or incomplete. They provide a method for substituting missing data points with estimated values, ensuring that alert conditions can still function effectively even with gaps in the data stream.

        When to leave gap-filling off:

        * <DNT>**Consistent data flow**</DNT>: If your application consistently sends data to New Relic without expected gaps, as in the case of the WebPortal application, gap-filling is generally unnecessary. Leaving the gap-filling strategy set to none is often the most appropriate approach in such cases.

        Key considerations:

        * <DNT>**Popular use case**</DNT>: A common use of gap filling is to insert a value of 0 for windows with no data received.
        * <DNT>**Anomaly thresholds**</DNT>: The gap-filling value is interpreted as the number of standard deviations from the last observed value when using anomaly thresholds. For example, filling gaps with a value of 0 would replicate the last value seen, effectively assuming no change.

          Learn more about gap-filling strategies in our [lost signal docs](/docs/apis/nerdgraph/examples/nerdgraph-api-loss-signal-gap-filling/).
      </Collapser>
    </CollapserGroup>
  </Step>

  <Step>
    ### Set thresholds for alert conditions [#thresholds]

    If an alert condition is a container, then thresholds are the rules each alert condition must follow. As data streams into your system, the alert condition searches for any incidents of these rules. If the alert condition sees data from your system that has met all the conditions you've set, it will create an incident. An incident signals that something is off in your system, and you should look.

    <img
      title="set a threshold"
      alt="A screenshot depicting how to set the threshold for an alert condition."
      src="/images/alerts_screenshot-crop_set-a-threshold-for-an-alert-condition.webp"
    />

    <CollapserGroup>
      <Collapser
        id="anomaly-threshold"
        title="Anomaly threshold"
      >
        Anomaly thresholds are ideal when you're more concerned about deviations from expected patterns than specific numerical values. They enable you to monitor for unusual activity without needing to set predefined limits. New Relic's anomaly detection dynamically analyzes your data over time, adapting thresholds to reflect evolving system behavior.

        Setting up anomaly detection:

        1. Choose upper or lower:
           * Select upper and lower to be alerted about any higher and lower deviations than expected.
           * Select upper only to focus solely on unusually high values.
        2. Assign priority:
           * Set the priority level to critical for your initial alert to ensure prompt attention to potential issues.
        3. Define breach criteria:
           * Start with the default settings: open an incident when a query returns a value that deviates from the predicted value for at least five minutes by three standard deviations.
           * Customize these settings as needed to align with your specific application and alerting requirements.

        You can learn more about priority levels in our [alert condition docs](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/set-thresholds-alert-condition#threshold-levels).

        Learn more about anomaly threshold and model behaviors in our anomaly [documentation](/docs/alerts-applied-intelligence/applied-intelligence/anomaly-detection/anomaly-detection-applied-intelligence/).
      </Collapser>

      <Collapser
        id="static-threshold"
        title="Static threshold"
      >
        Unlike anomaly thresholds, a static threshold doesn't look at your data set as a whole and determines what behavior is unusual based on your system's history. Instead, a static threshold will open an incident whenever your system behaves differently than the criteria that you set.

        You need to set the priority level for both anomaly and static thresholds.  See the section above for more details.
      </Collapser>

      <Collapser
        id="lost-signal"
        title="Lost signal threshold (optional)"
      >
        The [lost signal threshold](/docs/alerts/create-alert/create-alert-condition/create-nrql-alert-conditions/#signal-loss) determines how long to wait before considering a missing signal lost. If the signal doesn't return within that time, you can choose to open a new incident or close any related ones. You can also choose to skip opening an incident when a signal is expected to terminate. Set the threshold based on your system's expected behavior and data collection frequency. For example, if a website experiences a complete loss of traffic, or throughput, the corresponding telemetry data sent to New Relic will also cease. Monitoring for this loss of signal can serve as an early warning system for such outages.
      </Collapser>
    </CollapserGroup>
  </Step>

  <Step>
    ### Add alert condition details [#add-details]

    At this point in the process, you now have a fully defined condition and set all the rules to ensure an incident is opened when you want it to be. Based on the settings above, if your alert condition recognizes this behavior in your system that breaches the thresholds that you've set, it will create an incident. Now, all you need to do is to name this condition and attach it to a policy.

    The policy is the sorting system for the incident. When you create a policy, you create the tool that organizes all your incoming incidents. You can connect policies to <DNT>**[workflows](/docs/alerts-applied-intelligence/applied-intelligence/incident-workflows/incident-workflows/)**</DNT> that tell New Relic where you want all this incoming information to go, how often you want it to be sent, and where.

    <img
      title="name an alert condition "
      alt="A screenshot demonstrating how you can new alert condition."
      src="/images/alerts_screenshot-crop_name-your-alert-condition.webp"
    />

    <CollapserGroup>
      <Collapser
        id="name-your-condition"
        title="Name your condition"
      >
        A best practice for condition naming involves a structured format that conveys essential information at a glance. Include the following elements in your condition names:

        * **Priority**: Indicate the severity or urgency of the alert, like P1, P2, P3.
        * **Signal**: Specify the metric or condition being monitored, like High Avg Latency or Low Throughput.
        * **Entity**: Identify the affected system, application, or component, like WebPortal App or Database Server.

          An example of a well-formed condition name following this structure would be `P2 | High Avg Latency | WebPortal App`.
      </Collapser>

      <Collapser
        id="select-an-existing-policy"
        title="Select an existing policy"
      >
        If you already have a policy you want to connect to an alert condition, then select the existing policy.

        Learn more about creating policies [here](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/specify-when-alerts-create-incidents/).
      </Collapser>

      <Collapser
        id="create-a-new-policy"
        title="Create a new policy"
      >
        Balancing responsiveness and fatigue in your alerting strategy is crucial, and you've laid out the key considerations regarding `pageview` monitoring for your `WebPortal` application. Let's explore the policy options:

        1. One issue per policy (default):
           * Pros: Reduces noise and ensures immediate action.
           * Cons: Groups all incidents under one issue, even if triggered by different conditions. It's not ideal for multiple pageview concerns.

        2. One issue per condition:
           * Pros: Creates separate issues for each condition, ideal for isolating and addressing specific latency issues.
           * Cons: Can generate more alerts, potentially leading to fatigue.

        3. An issue for every incident:

           * Pros: Provides granular detail for external systems but is not optimal for internal consumption due to potential overload.
           * Cons: It is the noisiest option, and it is challenging to track broader trends and prioritize effectively.

           Learn more about creating policies [here](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/specify-when-alerts-create-incidents/).
      </Collapser>

      <Collapser
        id="close-open-incidents"
        title="Close open incidents"
      >
        An incident automatically closes when the targeted signal returns to a non-breaching state for the period indicated in the condition's thresholds. This wait time is called the recovery period.

        For example, if you're measuring latency and the breaching behavior is that `duration` to load pages in your `WebPortal` application has increased to more than 3 seconds, the incident will automatically close when `duration` is equal to or lower than 3 seconds for 5 consecutive minutes.

        When an incident closes automatically:

        1. The closing timestamp is backdated to the start of the recovery period.
        2. The evaluation resets and restarts from when the previous incident ended.

           All conditions have an incident time limit setting that automatically force-close a long-lasting incident.

           New Relic automatically defaults to 3 days and recommends that you use our default settings for your first alert.

           Another way to close an open incident when the signal does not return data is by configuring a `loss of signal` threshold. Refer to the lost signal threshold section above for more details.
      </Collapser>

      <Collapser
        id="custom-incident-description"
        title="Set a custom incident description"
      >
        Since you're creating an alert condition that lets you know if there are any latency issues with your `WebPortal` application, you want to make sure your developers have all the information they need when notified about this incident. You will use workflows to notify a team Slack channel when an incident is created.

        Learn more about custom incident descriptions in our [docs](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/alert-custom-incident-descriptions).
      </Collapser>

      <Collapser
        id="title-template"
        title="Use the title template"
      >
        <img
          title="alert details page"
          alt="A screenshot depicting the final step of creating an alert condition- the alert details page"
          src="/images/alerts_screenshot-crop_adding-alert-details-to-an-alert-condition.webp"
        />

        Using the title template is optional but we recommend it. An alert condition defines a set of thresholds you want to monitor. If any of those thresholds are breached, an incident is created. Meaningful title templates help you pinpoint issues and resolve outages faster.

        Learn more about title templates in our [docs](//docs/alerts/create-alert/condition-details/title-template/).
      </Collapser>

      <Collapser
        id="runbook"
        title="Add runbook URL"
      >
        An operations runbook detailing investigation, triage, or remediation steps is often linked in this field.
      </Collapser>
    </CollapserGroup>
  </Step>
</Steps>

## Edit an existing alert condition [#edit-existing-alert-condition]

If you want to edit an alert condition you've already created, you can:

1. Go to [one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > <DNT>**Alerts**</DNT>.
2. Select <DNT>**Alert Conditions**</DNT> in the left navigation.
3. Click on the alert condition you want to edit.

From there, you will see the <DNT>**Alert conditions details**</DNT> page. This page contains all the elements you set when you created your condition. You can edit specific aspects of the alert condition by clicking the pencil in the top right of each section.

### Signal history [#signal-coverage]

Under <DNT>**Signal history**</DNT>, you can see the most recent results for the NRQL query you used to create your alert condition. For this example, you would see the average latency on the `WebPortal` app for the specific time frame you've set.

For all alert conditions built with NRQL queries, the <DNT>**Signal history**</DNT> will be presented with a line chart.

Any alert condition built with a synthetic monitor will be a bit different. This is because synthetic monitors allow you to ping your application from multiple locations, which can produce positive or negative results each time the monitor runs. This data can only be presented with a table.

## Types of conditions [#condition-types]

The primary and recommended condition type is a NRQL alert condition, but there are other types of conditions. We've included a complete list of our condition types below.

<CollapserGroup>
  <Collapser
    id="nrql-conditions"
    title="NRQL query conditions"
  >
    Use the UI or [NerdGraph API](/docs/alerts/alerts-nerdgraph/nerdgraph-examples/nerdgraph-api-alerts-nrql-conditions) to create NRQL conditions for [basic NRQL queries that return a number](/docs/new-relic-alerts-alert-nrql-queries).
  </Collapser>

  <Collapser
    id="apm-conditions"
    title="APM metric alert conditions"
  >
    See [APM metric alert conditions](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/apm-metric-alert-conditions) for tips about creating <InlinePopover type="apm"/> conditions using NRQL.
  </Collapser>

  <Collapser
    id="anomaly-conditions"
    title="Anomaly conditions"
  >
    [Anomaly alerting](/docs/alerts/new-relic-alerts/configuring-alert-policies/create-anomaly-alert-conditions) allows you to create conditions that dynamically adjust to changing data and trends, such as weekly or seasonal patterns. This feature is available for [<InlinePopover type="apm"/>](/docs/apm/new-relic-apm/getting-started/welcome-new-relic-apm) and [<InlinePopover type="browser"/>](/docs/browser/new-relic-browser/getting-started/new-relic-browser) apps, as well as [NRQL queries](/docs/alerts/new-relic-alerts/defining-conditions/create-alert-conditions-nrql-queries).
  </Collapser>

  <Collapser
    id="synthetics-multi-location"
    title="Synthetic monitoring multi-location conditions"
  >
    With [multi-location Synthetic monitoring conditions](/docs/alerts/new-relic-alerts/defining-conditions/multi-location-synthetics-alert-conditions), you can set up a monitor to notify you when a specific number of locations are failing at the same time.
  </Collapser>

  <Collapser
    id="key-transactions"
    title="Key transaction metrics conditions"
  >
    For APM, you can set up conditions for [key transactions](/docs/apm/transactions/key-transactions/introduction-key-transactions).
  </Collapser>

  <Collapser
    id="instance-incidents"
    title="Java instance conditions"
  >
    You can set thresholds that open an incident when they are breached by any of your Java app's instance metrics.

    By [scoping thresholds to specific instances](/docs/alerts/new-relic-alerts/configuring-alert-policies/scope-alert-thresholds-specific-instances), you can more quickly identify where potential problems are originating. This is useful, for example, to detect anomalies that are occurring only in a subset of your app's instances. These sorts of anomalies are easy to miss for apps that aggregate metrics across a large number of instances.
  </Collapser>

  <Collapser
    id="jvm-metrics"
    title="JVM health metric conditions (Java apps)"
  >
    For Java apps monitored by APM, you can set [thresholds](/docs/using-new-relic/welcome-new-relic/get-started/glossary#alert-threshold) that open an incident when the heap size or number of threads for a single JVM is out of the expected operating range.

    We evaluate alerting threshold breaches individually for each of the app's [selected instances](#instance-incidents). When creating your condition, select <DNT>**JVM health metric**</DNT> as the type of condition for your Java app's alert policy, then select any of the available thresholds:

    * Deadlocked threads
    * Heap memory usage
    * CPU utilization time
    * Garbage collection CPU time

      Incidents will automatically close when the inverse of the threshold is met, but by using the UI you can also change the time when an [incident](/docs/new-relic-solutions/get-started/glossary/#alert-incident) force-closes for a JVM health metric. Default is 24 hours.
  </Collapser>

  <Collapser
    id="app-response-percentiles"
    title="Web transaction percentile conditions"
  >
    We include the option to define a percentile as the [threshold](/docs/using-new-relic/welcome-new-relic/get-started/glossary#alert-threshold) for your condition when your web app's response time is above, below, or equal to this value. This is useful, for example, when Operations personnel want to alert on a percentile for an app server's <DNT>**overall**</DNT> [web transaction](/docs/using-new-relic/welcome-new-relic/getting-started/glossary#transaction) response time rather than the <DNT>**average**</DNT> web response time.

    <Callout variant="tip">
      If you want to set an arbitrary threshold in a condition for a [non-web app transaction](/docs/using-new-relic/welcome-new-relic/getting-started/glossary#non-web-transaction), use the [NRQL queries](#nrql-conditions) feature.
    </Callout>

    To define the percentile threshold:

    1. Select <DNT>**Web transactions percentiles**</DNT> as the type of condition for your <InlinePopover type="apm"/> app's condition, then select a single app. (To alert on more than one app, create an individual <DNT>**Web transactions percentiles**</DNT> condition for each.)
    2. To define the thresholds that open the incident, type the <DNT>**Percentile nth**</DNT> response time value, then select its frequency (above, below, or equal to this value).

       We store the transaction time in milliseconds, although the user interface shows the Critical and Warning values as seconds. If you want to define milliseconds, be sure to include the decimal point in your value.
  </Collapser>

  <Collapser
    id="dynamic-targeting-labels"
    title="Dynamic targeting with labels for apps"
  >
    By applying [labels](/docs/data-analysis/user-interface-functions/organize-your-data/labels-categories-organize-apps-servers-monitors#labels) to applications, you can automatically link these [entities](/docs/using-new-relic/welcome-new-relic/get-started/glossary#alert-entity) to your condition. This makes it easy to manage all the applications within a dynamic environment. We recommend using the [agent configuration file](/docs/data-analysis/user-interface-functions/organize-your-data/labels-categories-organize-apps-servers-monitors#config) to best maintain entity labels.

    A single label identifies <DNT>**all**</DNT> entities associated with that label (maximum 10,000 entities). Multiple labels only identify entities which share all the selected labels.

    Using dynamic targeting with your condition also requires that you set an [incident close timer](/docs/alerts-applied-intelligence/new-relic-alerts/alert-incidents/how-alert-condition-incidents-are-closed#time-limit).

    To add, edit, or remove up to ten labels for a condition:

    1. Select <DNT>**APM > Application metric**</DNT> as the product type.
    2. When identifying entities, select the <DNT>**Labels**</DNT> tab. Search for a label by name, or select a label from the list of categories.

       You can also create conditions directly within the context of what you're monitoring with [Infrastructure](/docs/infrastructure/new-relic-infrastructure/configuration/infrastructure-alerts-add-edit-or-view-host-alert-information).
  </Collapser>

  <Collapser
    id="infrastructure"
    title="Infrastructure conditions"
  >
    You can [create conditions for your resources](/docs/infrastructure/new-relic-infrastructure/configuration/infrastructure-alerts-add-edit-or-view-host-alert-information) directly from our infrastructure monitoring UI.

    For example, to get a notification when an infrastructure agent stops receiving data, use the [host not reporting](/docs/infrastructure/new-relic-infrastructure/configuration/create-infrastructure-host-not-reporting-condition) condition type. This allows you to dynamically alert on filtered groups of hosts and configure the time window from 5 to 60 minutes.
  </Collapser>
</CollapserGroup>

## Add conditions to a policy [#add-conditions]

To [add more conditions to a policy](/docs/alerts/new-relic-alerts-beta/configuring-alert-policies/define-alert-conditions):

1. Go to <DNT>**[one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > Alerts > Alert Policies**</DNT>
2. Detect a policy.
3. Click <DNT>**Add a condition**</DNT>.

To [create a new NRQL condition](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions):

1. Go to <DNT>**[one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > Alerts > Alert Conditions**</DNT>
2. Click <DNT>**Add a condition**</DNT>.

## Copy a condition [#copy-condition]

To copy an existing condition, including its [targets](/docs/apm/new-relic-apm/getting-started/glossary#alert-target) and [thresholds](/docs/apm/new-relic-apm/getting-started/glossary#alert-threshold), and add it to another policy for the selected account:

1. Go to <DNT>**[one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > Alerts > Alert conditions**</DNT>.
2. From the list of alert conditions, click on the three dots icon of the alert you want to copy and select <DNT>**Duplicate condition**</DNT>.
3. From the <DNT>**Copy alert condition**</DNT>, search or scroll the list to select the policy where you want to add this condition.
4. Optional: Change the condition's name if necessary.
5. Optional: Click the toggle switch to <DNT>**Enable on save**</DNT>
6. Select <DNT>**Copy condition**</DNT>.

   By default, the selected alert policy will add the copied condition in the **Disabled** state. Follow standard procedures to [add](/docs/alerts/new-relic-alerts-beta/configuring-alert-policies/define-alert-conditions) or copy more conditions to the alert policy, and then [**Enable** the condition](/docs/alerts/new-relic-alerts-beta/updating-alert-policies/disable-or-delete-alert-policies-conditions#disable-conditions) as needed. Additionally, the new condition will not copy any [tags](/docs/new-relic-solutions/new-relic-one/core-concepts/use-tags-help-organize-find-your-data/#add-via-ui-api) added to the original condition.

## Enable/disable a condition [#disable-conditions]

To disable or re-enable a condition:

1. Go to <DNT>**[one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > Alerts > Alert Conditions**</DNT>. Then, from the list of <DNT>**Alert conditions**</DNT>, use the toggle to enable or disable the condition.
2. Click the <DNT>**On/Off**</DNT> switch to toggle it.

If you [copy a condition](/docs/alerts/new-relic-alerts-beta/updating-alert-policies/copy-alert-conditions), it automatically saves it in the new policy as disabled (<DNT>**Off**</DNT>), even if the condition was enabled (<DNT>**On**</DNT>) in the original policy.

## Delete a condition [#delete-conditions]

To turn a condition off but keep it with the policy, [disable](/docs/alerts/new-relic-alerts-beta/updating-alert-policies/disable-or-delete-alert-policies-conditions#condition-on-off) it. To delete one or more conditions:

1. Go to <DNT>**[one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > Alerts > Alert Conditions**</DNT>.
2. From the list of <DNT>**Alert conditions**</DNT>, select a condition, then click <DNT>**Delete**</DNT> from the ellipses menu (...).

   <Callout variant="tip">
     If you don't see the delete button, your account admin may have disabled condition deletion for your organization.
   </Callout>

## Troubleshoot the alert condition page [#troubleshoot]

If you see an empty signal in the history chart on the **Alert Condition** page, consider one of the following:

* <DNT>**Review the condition's settings**</DNT>: Double-check that all elements are configured correctly.
* <DNT>**Inspect NRQL queries**</DNT>: Ensure they target valid metrics or events and return data.
* <DNT>**Examine entity configuration**</DNT>: Confirm that the entity is set up correctly to send signals.
* <DNT>**Consult New Relic documentation**</DNT>: Refer to relevant guides for assistance with specific issues.

## What's next? [#whats-next]

<DocTiles>
  <DocTile
    title="Create your first New Relic alert"
    path="/docs/tutorial-create-alerts/create-new-relic-alerts/"
    label={{text: "Start here", color: "orange"}}
  >
    A crash course in alerts for beginners
  </DocTile>

  <DocTile
    title="Advanced alert conditions"
    path="/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-alert-conditions/"
  >
    If you've already set up your alert conditions, dig deeper with advanced settings
  </DocTile>

  <DocTile
    title="Get notified"
    path="/docs/alerts-applied-intelligence/applied-intelligence/incident-workflows/incident-workflows/"
  >
    Use workflows to get notified about any unusual behavior in your stack
  </DocTile>
</DocTiles>
