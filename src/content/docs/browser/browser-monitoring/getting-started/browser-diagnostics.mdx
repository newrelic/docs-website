---
title: "What's causing my app's high latency?"
tags:
- Learning paths
- Customer experience
- Browser monitoring
- Latency
metaDescription: "How to find, diagnose, and fix, an instance of your abnormal or poor app latency."
redirects:
signupBanner:
  text: Monitor and improve your entire stack. 100GB free. Forever.
---

import solutionsOmaUprPatternNormal from 'images/solutions_screenshot-full_oma-upr-pattern-normal.png'

import solutionsNormalPercentilePattern from 'images/solutions_screenshot-full_normal-percentile-pattern.png'

import solutionsPatternAbnormal from 'images/solutions_screenshot-full_pattern-abnormal.png'

import solutionsPatternAbnormalCompare from 'images/solutions_screenshot-full_pattern-abnormal-compare.png'

Your app's performance is one of the most important aspects of whether or not it will succeed. [According to Google's research](https://think.storage.googleapis.com/images/micromoments-guide-to-winning-shift-to-mobile-download.pdf), 29% of mobile users will avoid using a site or an application if it's too slow or too difficult to find information, with 70% of those users doing so because the site is too slow. Diagnosing and fixing performance issues helps your organization attract new users while retaining existing ones, and New Relic is just the tool to help you do it. Once you've created <InlinePopover type="dashboards" /> for your services, you can use them to find issues causing high latency.

## Diagnostic steps [#diagnostic-steps]
<Steps>

<Step>
First, you should establish a direct problem statement to help you identify the issue and the best way to solve it. A good problem statement answers the following questions:

1. What is the problem that the end-user is experiencing? In this case, high latency when using your app or site.
2. What is it that the end-user should be experiencing? In the case of high latency, the user should experience little to none.
3. What is the source of the issue that the user is experiencing? This is the part that you can use New Relic to identify. 
</Step>

<Step>
There are three primary categories that help in identifying the source of a problem. Understanding these significantly reduces your time spent finding the cause of the issue. Once you've found the issue, you can begin to create a solution to the problem.

<CollapserGroup>
  <Collapser
    id="output-perf"
    title="Output performance"
  >

**This requires**: <InlinePopover type="apm" />

Output performance is the ability of your internal technology stack to deliver the expected responses (output) to the end-user. This is traditionally referred to as the "back-end" services. Most of the time, output performance is measured by the speed and quality of the response. The end-user describes the service as either slow, not working, or inaccessible. Use [APM dashboards](https://one.newrelic.com/nr1-core?filters=%28domain%20IN%20%28%27APM%27%2C%20%27EXT%27%29%20AND%20type%20IN%20%28%27APPLICATION%27%2C%20%27SERVICE%27%29%29&state=62bc4f1a-cea5-dbdd-6c12-ab2653bdba94) to identify issues in output performance.
</Collapser>
  <Collapser
    id="input-perf"
    title="Input performance"
  >

**This requires**: <InlinePopover type="synthetics" />

Input performance is the ability of your services to receive requests from the client. Errors result when something between the client and your services is breaking the request-response lifecycle, even when your output performance could be exceeding expected performance levels. This could occur at any point between the client and your services. [Synthetics dashboards](https://one.newrelic.com/synthetics-nerdlets) will show you issues with input performance of your services in New Relic.
</Collapser>
  <Collapser
    id="client-perf"
    title="Client performance"
  >
**This requires**: <InlinePopover type="browser" /> and/or <InlinePopover type="mobile" />

Client performance is the ability for a browser and/or mobile application to make requests and render responses. You can identify browser and/or mobile issues as the source of the problem once both output (back-end) and input performance (synthetics) have been ruled out. Due to the depth of diagnostics in input and output diagnostic, browser and mobile will be covered in an advanced diagnostics guide in the future. You can use both [browser](https://one.newrelic.com/nr1-core?filters=%28domain%3D%27BROWSER%27%20AND%20type%3D%27APPLICATION%27%29&state=1f7bae2d-dd3a-0027-3079-b60bcd19ded9) and [mobile](https://one.newrelic.com/nr1-core?filters=%28domain%3D%27MOBILE%27%20AND%20type%3D%27APPLICATION%27%29) monitoring dashboards to find errors in client performance.
</Collapser>
</CollapserGroup>

Defining your performance metrics within these categories, also called [service levels](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide), significantly improves your ability to find the source of a problem your customers are experiencing.
</Step>

<Step>
The next step after identifying the source of a problem is to identify any anomalies. Identifying pattern anomalies will improve your ability to identify what and where the direct cause of problems may be. The key to identifying these is realizing that you don't need to know how the service should be performing, but only that recent behavior has changed. 

The examples below use response time or latency as the metric, but you can apply the same analysis to almost any dataset, such as errors, throughput, hardware resource metrics, queue depths, and many more.

<CollapserGroup>
  <Collapser
    className="freq-link"
    id="normal"
    title="Normal"
  >
Here you can see an example of a seemingly volatile response time chart in APM using <InlinePopover type="dashboards" />. Upon close inspection, you can see that the behavior of the response time is repetitive: there's no radical change in the behavior across a 7-day period. The spikes are repetitive and not unusual compared to the rest of the timeline.

<img
  alt="normal pattern"
  title="Normal pattern"
  src={solutionsOmaUprPatternNormal}
/>

If you change the view of the data from **average over time** to **percentiles over time**, it becomes even more clear how "regular" the changes in response time are.

<img
  alt="normal pattern with percentile"
  title="Normal pattern with percentile"
  src={solutionsNormalPercentilePattern}
/>
  </Collapser>

  <Collapser
    className="freq-link"
    id="abnormal"
    title="Abnormal"
  >
Compared to the example above, this chart shows an application response time that seems to have increased in an unusual way compared to recent behavior.

<img
  alt="abnormal pattern"
  title="Abnormal pattern"
  src={solutionsPatternAbnormal}

/>

You can use a week-over-week comparison to confirm this. When you do, you can see the pattern has changed and that it appears to be worsening from last week's comparison.

<img
  alt="abnormal pattern week-over-week"
  title="Abnormal pattern week-over-week comparison"
  src={solutionsPatternAbnormalCompare}
/>
  </Collapser>
</CollapserGroup>

Examples of other common anomalies are: 

1. Throughput (traffic)
2. Code (deployments)
3. Resources (hardware allocation)
4. Upstream or downstream dependency changes
5. Data volume
</Step>
</Steps>

Let's look at an example problem. Let's say you deploy a new product, and a significant increase in requests causes unacceptable response times. Using dashboards to identify anomalies, the source is discovered in the login middleware service. The problem is a jump in TCP queue times.

Here's a breakdown of this situation:

* **Category**: output performance
* **Source**: login middleware
* **Direct cause**: TCP queue times from additional request load
* **Solution**: increased TCP connection limit and scaled resources
* **Root-cause**: insufficient capacity planning and quality assurance testing on downstream service impacting login middleware