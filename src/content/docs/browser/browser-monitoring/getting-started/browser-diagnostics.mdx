---
title: "What's causing my app's high latency?"
tags:
- Learning paths
- Customer experience
- Browser monitoring
- Latency
metaDescription: "How to find, diagnose, and fix, an instance of your abnormal or poor app latency."
redirects:
---

import solutionsOmaUprPatternNormal from 'images/solutions_screenshot-full_oma-upr-pattern-normal.png'

import solutionsNormalPercentilePattern from 'images/solutions_screenshot-full_normal-percentile-pattern.png'

import solutionsPatternAbnormal from 'images/solutions_screenshot-full_pattern-abnormal.png'

import solutionsPatternAbnormalCompare from 'images/solutions_screenshot-full_pattern-abnormal-compare.png'

Your app's performance is one of the most important aspects of whether or not it will succeed. [According to Google's research](https://think.storage.googleapis.com/images/micromoments-guide-to-winning-shift-to-mobile-download.pdf), 29% of mobile users will avoid using a site or an application if it's too slow or too difficult to find information, with 70% of those users doing so because the site is too slow. Diagnosing and fixing performance issues can help your organization attract new users while retaining existing ones, and New Relic is just the tool to help you do it. 

The terms "performance", "latency", and "response time" encompass multiple issues. The majority of "slowness" problems occur due to an output issue stemming from backend services. New Relic's service level management measures this slowness for both "output" and "client" categories. 

## Diagnostic steps [#diagnostic-steps]
<Steps>

<Step>
First, you should establish the problem statement in as simple and effective a way as possible. A good problem statement answers the following questions:

1. What is the problem that the end-user is experiencing?
2. What is it that the end-user should be experiencing?
3. What is the technical assessment of what the user is experiencing?
</Step>

<Step>
There are three primary breakpoint categories to improve your response times in identifying the source of an issue:

1. **Output**
2. **Input**
3. **Client**

Defining your performance metrics within these categories, also called [service levels](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide), significantly improves your response time finding the source of a problem. We cover measuring these categories in [our service level management guide](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide).
</Step>

<Step>
The next step after identifying the source of a problem is to identify what changed. This helps you determine how to resolve the problem quickly. Examples of common changes are:

1. Throughput (traffic)
2. Code (deployments)
3. Resources (hardware allocation)
4. Upstream or downstream dependency changes
5. Data volume
</Step>
</Steps>

Let's look at an example problem. Let's say you deploy a new product, and a significant increase in requests causes unacceptable response times. The source is discovered in the login middleware service. The problem is a jump in TCP queue times.

Here's a breakdown of this situation:

* **Category**: output performance
* **Source**: login middleware
* **Direct cause**: TCP queue times from additional request load
* **Solution**: increased TCP connection limit and scaled resources
* **Root-cause**: insufficient capacity planning and quality assurance testing on downstream service impacting login middleware

## Understand performance categories [#performance-categories]

As previously mentioned, there are three primary performance categories that jump-start your diagnostic journey. Understanding these significantly reduces time spent finding the source of the problem.

<CollapserGroup>
  <Collapser
    id="output-perf"
    title="Output performance"
  >

**This requires**: <InlinePopover type="apm" />

Output performance is the ability of your internal technology stack to deliver the expected responses (output) to the end-user. This is traditionally referred to as the "back-end" services. Most of the time, output performance is measured by the speed and quality of the response. The end-user describes the service as either slow, not working, or inaccessible.

The most common obstacle with output performance issues is the ability to respond to end-user requests in a timely **and** successful manner, but you can address this by identifying a latency anomaly or error anomaly.
</Collapser>
  <Collapser
    id="input-perf"
    title="Input performance"
  >

**This requires**: [synthetics](https://one.newrelic.com/synthetics-nerdlets)

Input performance is the ability of your services to receive requests from the client. Errors result when something between the client and your services is breaking the request-response lifecycle, even when your output performance could be exceeding expected performance levels. This could occur at any point between the client and your services.
</Collapser>
  <Collapser
    id="client-perf"
    title="Client performance"
  >
**This requires**: <InlinePopover type="browser" /> and/or <InlinePopover type="mobile" />

Client performance is the ability for a browser and/or mobile application to make requests and render responses. You can identify browser and/or mobile issues as the source of the problem once both output (back-end) and input performance (synthetics) have been ruled out. Due to the depth of diagnostics in input and output diagnostic, browser and mobile will be covered in an advanced diagnostics guide in the future.
</Collapser>
</CollapserGroup>

## Identify performance pattern anomalies [#pattern-anomalies]

Identifying pattern anomalies will improve your ability to identify what and where the direct cause of problems may be. The key to identifying these is realizing that you don't need to know how the service should be performing, but only that recent behavior has changed.

The examples below use response time or latency as the metric, but you can apply the same analysis to almost any dataset, such as errors, throughput, hardware resource metrics, queue depths, and many more.

<CollapserGroup>
  <Collapser
    className="freq-link"
    id="normal"
    title="Normal"
  >
Here you can see an example of a seemingly volatile response time chart in APM using <InlinePopover type="dashboards" />. Upon close inspection, you can see that the behavior of the response time is repetitive: there's no radical change in the behavior across a 7-day period. The spikes are repetitive and not unusual compared to the rest of the timeline.

<img
  alt="normal pattern"
  title="Normal pattern"
  src={solutionsOmaUprPatternNormal}
/>

If you change the view of the data from **average over time** to **percentiles over time**, it becomes even more clear how "regular" the changes in response time are.

<img
  alt="normal pattern with percentile"
  title="Normal pattern with percentile"
  src={solutionsNormalPercentilePattern}
/>
  </Collapser>

  <Collapser
    className="freq-link"
    id="abnormal"
    title="Abnormal"
  >
Compared to the example above, this chart shows an application response time that seems to have increased in an unusual way compared to recent behavior.

<img
  alt="abnormal pattern"
  title="Abnormal pattern"
  src={solutionsPatternAbnormal}

/>

You can use a week-over-week comparison to confirm this. When you do, you can see the pattern has changed and that it appears to be worsening from last week's comparison.

<img
  alt="abnormal pattern week-over-week"
  title="Abnormal pattern week-over-week comparison"
  src={solutionsPatternAbnormalCompare}
/>
  </Collapser>
</CollapserGroup>