---
title: Kafka monitoring integration
contentType: page
template: basicDoc
topics:
  - Integrations
  - On-host integrations
  - On-host integrations list
japaneseVersion: ''
---

The New Relic Kafka [on-host integration](/docs/integrations/host-integrations/getting-started/introduction-host-integrations) reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics.

Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see [Instrument Kafka message queues](/docs/agents/java-agent/instrumentation/instrument-kafka-message-queues).

## Compatibility and requirements

Our integration is compatible with Kafka versions 0.8 or higher.

Before installing the integration, make sure that you meet the following requirements:

* If Kafka is **not** running on Kubernetes or Amazon ECS, you must [install the infrastructure agent](/docs/infrastructure/install-infrastructure-agent/get-started/install-infrastructure-agent-new-relic) on a host that's running Kafka. Otherwise:
  * If running on Kubernetes, see [these requirements](https://docs.newrelic.com/docs/monitor-service-running-kubernetes#requirements).
  * If running on ECS, see [these requirements](https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs).
* Java 8 or higher
* JMX enabled on all brokers, Java consumers, and Java producers that you want monitored
* Total number of monitored topics must be fewer than 300

For Kafka running on Kubernetes, see [the Kubernetes requirements](http://docs.newrelic.com/docs/monitor-service-running-kubernetes#requirements).

## Install and activate

To install the Kafka integration, choose your setup:

<CollapserGroup>
  <Collapser
    id="ecs-install"
    title="ECS"
  >
    See [Monitor service running on ECS](/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs).
  </Collapser>

  <Collapser
    id="k8s-install"
    title="Kubernetes"
  >
    See [Monitor service running on Kubernetes](/docs/monitor-service-running-kubernetes).
  </Collapser>

  <Collapser
    id="linux-install"
    title="Linux installation"
  >
    1. Follow the instructions for [installing an integration](/docs/install-integrations-package), using the file name `nri-kafka`.
    2. Change the directory to the integrations configuration folder:

       ```
       cd /etc/newrelic-infra/integrations.d
       ```
    3. Copy of the sample configuration file:

       ```
       sudo cp kafka-config.yml.sample kafka-config.yml
       ```
    4. Edit the `kafka-config.yml` file as described in the [configuration settings](#config).
    5. [Restart](/docs/infrastructure/new-relic-infrastructure/configuration/start-stop-restart-check-infrastructure-agent-status) the Infrastructure agent.
  </Collapser>

  <Collapser
    id="windows-install"
    title="Windows installation"
  >
    1. Download the `nri-kafka` .MSI installer image from:

       [http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64.msi](http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64.msi)
    2. To install from the Windows command prompt, run:

       ```
       msiexec.exe /qn /i PATH\TO\nri-kafka-amd64.msi
       ```
    3. In the Integrations directory, `C:\Program Files\New Relic\newrelic-infra\integrations.d\`, create a copy of the sample configuration file by running:

       ```
       cp kafka-config.yml.sample kafka-config.yml
       ```
    4. Edit the `kafka-config.yml` configuration as described in the [configuration settings](#config).
    5. [Restart the infrastructure agent](https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/configuration/start-stop-restart-check-infrastructure-agent-status).
  </Collapser>
</CollapserGroup>

Additional notes:

* **Advanced:** It's also possible to [install the integration from a tarball file](/docs/integrations/host-integrations/installation/install-host-integrations-built-new-relic#tarball). This gives you full control over the installation and configuration process.
* **On-host integrations do not automatically update.** For best results, regularly [update the integration package](/docs/integrations/host-integrations/installation/update-infrastructure-host-integration-package) and [the infrastructure agent](/docs/infrastructure/new-relic-infrastructure/installation/update-infrastructure-agent).

## Configure the integration

An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment.

There are several ways to configure the integration, depending on how it was installed:

* If enabled via Kubernetes: see [Monitor services running on Kubernetes](/docs/monitor-service-running-kubernetes).
* If enabled via Amazon ECS: see [Monitor services running on ECS](/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs).
* If installed on-host: edit the config in the integration's YAML config file, `kafka-config.yml`.

For examples of typical configurations, see the [example configurations](#config-examples).

<Callout variant="important">
  With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see [Secrets management](https://docs.newrelic.com/docs/integrations/host-integrations/installation/secrets-management).
</Callout>

### Commands

The configuration accepts the following commands:

* `inventory`: collects configuration status
* `metrics`: collects performance metrics
* `consumer_offset`: collects consumer group offset data

### Arguments

The configuration accepts the following arguments:

**General arguments:**

* `cluster_name`: user-defined name to uniquely identify the cluster being monitored. Required.
* `kafka_version`: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to `1.0.0`. Versions older than 1.0.0 may be missing some features.
* `autodiscover_strategy`: the method of discovering brokers. Options are `zookeeper` or `bootstrap`. Defaults to `zookeeper`

**Zookeeper autodiscovery arguments** (only relevant when `autodiscover_strategy` is `zookeeper`):

* `zookeeper_hosts`: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected.
* `zookeeper_auth_scheme`: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is `digest`. If omitted, no authentication is used.
* `zookeeper_auth_secret`: the ZooKeeper authentication secret that is used to connect. Should be of the form `username:password`. Only required if `zookeeper_auth_scheme` is specified.
* `zookeeper_path`: the Zookeeper node under which the Kafka configuration resides. Defaults to `/`.
* `preferred_listener`: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are `PLAINTEXT`, `SASL_PLAINTEXT`, `SSL`, and `SASL_SSL`. Note: The `SASL_*` protocols only support Kerberos (GSSAPI) authentication.

**Bootstrap broker discovery arguments** (only relevant when `autodiscover_strategy` is `bootstrap`):

* `bootstrap_broker_host`: the host for the bootstrap broker.
* `bootstrap_broker_kafka_port`: the Kafka port for the bootstrap broker.
* `bootstrap_broker_kafka_protocol`: the protocol to use to connect to the bootstrap broker. Supported values are `PLAINTEXT`, `SASL_PLAINTEXT`, `SSL`, and `SASL_SSL`. Note: The `SASL_*` protocols only support Kerberos (GSSAPI) authentication. Default: `PLAINTEXT`.
* `bootstrap_broker_jmx_port`: the JMX port to use for collection.
* `bootstrap_broker_jmx_user`: the JMX user to use for collection.
* `bootstrap_broker_jmx_password`: the JMX password to use for collection.

**Producer and consumer collection:**

* `producers`: producers to collect. For each provider a `name`, `hostname`, `port`, `username`, and `password` can be provided in JSON form. `name` is the producer’s name as it appears in Kafka. `hostname`, `port`, `username`, and `password` are optional and use the default if unspecified.
* `consumers`: consumers to collect. For each consumer a `name`, `hostname`, `port`, `username`, and `password` can be specified in JSON form. `name` is the consumer’s name as it appears in Kafka. `hostname`, `port`, `username`, and `password` are optional and use the default if unspecified.

**JMX connection options:**

* `default_jmx_host`: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used.
* `default_jmx_port`: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used.
* `default_jmx_user`: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used.
* `default_jmx_password`: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used.
* `key_store`: the filepath of the keystore containing the JMX client's SSL certificate.
* `key_store_password`: the password for the JMX SSL key store.
* `trust_store`: the filepath of the trust keystore containing the JMX server's SSL certificate.
* `trust_store_password`: the password for the JMX trust store.
* `timeout`: the timeout for individual JMX queries in milliseconds. Default: `10000`.

**Broker connection options:**

* `tls_ca_file`: the certificate authority file for SSL and SASL_SSL listeners.
* `tls_cert_file`: the client certificate file for SSL and SASL_SSL listeners.
* `tls_key_file`: the client key file for SSL and SASL_SSL listeners.
* `tls_insecure_skip_verify`: skip verifying the server's certificate chain and host name
* `sasl_mechanism`: the type of SASL authentication to use. Supported options are `SCRAM-SHA-512`, `SCRAM-SHA-256`, `PLAIN`, and `GSSAPI`.
* `sasl_gssapi_realm`: kerberos realm. Required for `SASL_SSL` or `SASL_PLAINTEXT`
* `sasl_gssapi_service_name`: kerberos service name. Required for `SASL_SSL` or `SASL_PLAINTEXT`
* `sasl_gssapi_username`: kerberos username. Required for `SASL_SSL` or `SASL_PLAINTEXT`
* `sasl_gssapi_key_tab_path`: path to the kerberos keytab. Required for `SASL_SSL` or `SASL_PLAINTEXT`
* `sasl_gssapi_kerberos_config_path`: path to the kerberos config file. Default: `/etc/krb5.conf`

**Collection filtering:**

* `collect_broker_topic_data`: signals if broker and topic metrics are collected. Options are `true` or `false`, defaults to `true`. Should only be set to `false` when monitoring only producers and consumers, and `topic_mode` is set to `all`.
* `local_only_collection`: collect only the metrics related to the configured bootstrap broker. Only used if `autodiscover_strategy` is `bootstrap`. Default: `false`
* `consumer_group_regex`: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: `consumer_groups` has been deprecated, use this argument instead.
* `topic_mode`: determines how many topics we collect. Options are `all`, `none`, `list`, or `regex`.
* `collect_topic_size`: collect the metric Topic size. Options are `true` or `false`, defaults to `false`. `topic_size` is a resource-intensive metric to collect.
* `topic_list`: array of topic names to monitor. Only in effect if `topic_mode` is set to `list`.
* `topic_regex`: regex pattern that matches the topic names to monitor. Only in effect if `topic_mode` is set to `regex`.
* `topic_bucket`: used to split topic collection across multiple instances. Should be of the form `<bucket number>/<number of buckets>`. Default: `1/1`.

### Labels

Labels are optional tags which help to identify collection data. Some examples are included below.

* `env`: label to identify the environment. For example: `production`.
* `role`: label to identify which role is accessing the data.

### Example configuration

<Callout variant="tip">
  For more details on configuration parameters, see the [kafka-config.yml.sample config file](https://github.com/newrelic/nri-kafka/blob/master/kafka-config.yml.sample) on GitHub.
</Callout>

<CollapserGroup id="config-examples">
  <Collapser
    id="single-agent"
    title="Example: Single agent deployment"
  >
    Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node.

    * Brokers
    * Single ZooKeeper node
    * Single producer:

      * Name: `my-producer`
      * Host: `my-producer.my.localnet`
      * JMX Port: `9989`
    * Single consumer:

      * Name: `my-consumer`
      * Host: `my-consumer.my.localnet`
      * JMX Port: `9987`

    Example `kafka-config.yml` config file for this environment:

    ```
    integration_name: com.newrelic.kafka

    instances:
      - name: kafka-metrics
        command: metrics
        arguments:
          zookeeper_hosts: '[{"host": "localhost", "port": 2181}]'
          producers: '[{"name": "my-producer", "host": "my-producer.my.localnet", "port": 9989}]'
          consumers: '[{"name": "my-consumer", "host": "my-consumer.my.localnet", "port": 9987}]'
          topic_mode: List
          collect_topic_size: false
          topic_list: '["topic_1", "topic_2"]'
        labels:
          env: production
          role: kafka

      - name: kafka-inventory
        command: inventory
        arguments:
          zookeeper_hosts: '[{"host": "localhost", "port": 2181}]'
          topic_mode: Regex
          topic_regex: 'topic_[0-9]+'
        labels:
          env: production
          role: kafka
    ```
  </Collapser>

  <Collapser
    id="multiple-agents"
    title="Example: Multiple agent deployment"
  >
    Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node.

    * Brokers
    * Single ZooKeeper node
    * Single producer:

      * Name: `my-producer`
      * Host: `my-producer.my.localnet`
      * JMX Port: `9989`
    * Single consumer:

      * Name: `my-consumer`
      * Host: `my-consumer.my.localnet`
      * JMX Port: `9987`

    Example `kafka-config.yml` config file for this environment:

    **ZooKeeper node configuration:**

    ```
    integration_name: com.newrelic.kafka

    instances:
      - name: kafka-metrics
        command: metrics
        arguments:
          zookeeper_hosts: '[{"host": "localhost", "port": 2181}]'
          topic_mode: List
          collect_topic_size: false
          topic_list: '["topic_1", "topic_2"]'
        labels:
          env: production
          role: kafka

      - name: kafka-inventory
        command: inventory
        arguments:
          zookeeper_hosts: '[{"host": "localhost", "port": 2181}]'
          topic_mode: List
          topic_list: '["topic_1", "topic_2"]'
        labels:
          env: production
          role: kafka
    ```

    **Producer node configuration:**

    ```
    integration_name: com.newrelic.kafka

    instances:
      - name: kafka-metrics
        command: metrics
        arguments:
          producers: '[{"name": "my-producer", "host": "my-producer.my.localnet", "port": 9989}]'
          topic_mode: List
          topic_list: '["topic_1", "topic_2"]'
        labels:
          env: production
          role: kafka
    ```

    **Consumer node configuration:**

    ```
    integration_name: com.newrelic.kafka

    instances:
      - name: kafka-metrics
        command: metrics
        arguments:
          consumers: '[{"name": "my-consumer", "host": "my-consumer.my.localnet", "port": 9987}]'
          topic_mode: List
          topic_list: '["topic_1", "topic_2"]'
        labels:
          env: production
          role: kafka
    ```
  </Collapser>

  <Collapser
    id="offset-collection"
    title="Example: Offset collection"
  >
    Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node.

    <Callout variant="important">
      Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node.
    </Callout>

    * Brokers
    * Single ZooKeeper node
    * Consumers
    * Consumer Groups
      * consumer_group_a1
      * consumer_group_a2
      * consumer_group_b1

    For this example environment, if you want to monitor offsets for only `consumer_group_a1` and `consumer_group_a2`, a sample config might look like this:

    ```
    integration_name: com.newrelic.kafka

    - name: kafka-consumer-offsets
      command: consumer_offset
      arguments:
        zookeeper_hosts: '[{"host": "localhost", "port": 2181}]'
        consumer_group_regex: 'consumer_group_a.'
      labels:
        env: production
        role: kafka
    ```
  </Collapser>
</CollapserGroup>

For more about the general structure of on-host integration configuration, see [Configuration](/docs/integrations/integrations-sdk/file-specifications/host-integration-configuration-overview).

## Find and use data

Data from this service is reported to an [integration dashboard](/docs/integrations/new-relic-integrations/getting-started/infrastructure-integration-dashboards-charts).

Kafka data is attached to the following [event types](/docs/using-new-relic/data/understand-data/new-relic-data-types#events-new-relic):

* [`KafkaBrokerSample`](#broker-sample)
* [`KafkaTopicSample`](#topic-sample)
* [`KafkaProducerSample`](#producer-sample)
* [`KafkaConsumerSample`](#consumer-sample)
* [`KafkaOffsetSample`](#offset-sample)

You can [query this data](/docs/using-new-relic/data/understand-data/query-new-relic-data) for troubleshooting purposes or to create charts and dashboards.

For more on how to find and use your data, see [Understand integration data](/docs/infrastructure/integrations/find-use-infrastructure-integration-data).

## Metric data

The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as `broker.` or `consumer.`.

### KafkaBrokerSample event

<Table>
  <thead>
    <tr>
      <th style={{ width: "350px" }}>
        Metric
      </th>

      <th>
        Description
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        `broker.bytesWrittenToTopicPerSecond`
      </td>

      <td>
        Number of bytes written to a topic by the broker per second.
      </td>
    </tr>

    <tr>
      <td>
        `broker.IOInPerSecond`
      </td>

      <td>
        Network IO into brokers in the cluster in bytes per second.
      </td>
    </tr>

    <tr>
      <td>
        `broker.IOOutPerSecond`
      </td>

      <td>
        Network IO out of brokers in the cluster in bytes per second.
      </td>
    </tr>

    <tr>
      <td>
        `broker.logFlushPerSecond`
      </td>

      <td>
        Log flush rate.
      </td>
    </tr>

    <tr>
      <td>
        `broker.messagesInPerSecond`
      </td>

      <td>
        Incoming messages per second.
      </td>
    </tr>

    <tr>
      <td>
        `follower.requestExpirationPerSecond`
      </td>

      <td>
        Rate of request expiration on followers in evictions per second.
      </td>
    </tr>

    <tr>
      <td>
        `net.bytesRejectedPerSecond`
      </td>

      <td>
        Rejected bytes per second.
      </td>
    </tr>

    <tr>
      <td>
        `replication.isrExpandsPerSecond`
      </td>

      <td>
        Rate of replicas joining the ISR pool.
      </td>
    </tr>

    <tr>
      <td>
        `replication.isrShrinksPerSecond`
      </td>

      <td>
        Rate of replicas leaving the ISR pool.
      </td>
    </tr>

    <tr>
      <td>
        `replication.leaderElectionPerSecond`
      </td>

      <td>
        Leader election rate.
      </td>
    </tr>

    <tr>
      <td>
        `replication.uncleanLeaderElectionPerSecond`
      </td>

      <td>
        Unclean leader election rate.
      </td>
    </tr>

    <tr>
      <td>
        `replication.unreplicatedPartitions`
      </td>

      <td>
        Number of unreplicated partitions.
      </td>
    </tr>

    <tr>
      <td>
        `request.avgTimeFetch`
      </td>

      <td>
        Average time per fetch request in milliseconds.
      </td>
    </tr>

    <tr>
      <td>
        `request.avgTimeMetadata`
      </td>

      <td>
        Average time for metadata request in milliseconds.
      </td>
    </tr>

    <tr>
      <td>
        `request.avgTimeMetadata99Percentile`
      </td>

      <td>
        Time for metadata requests for 99th percentile in milliseconds.
      </td>
    </tr>

    <tr>
      <td>
        `request.avgTimeOffset`
      </td>

      <td>
        Average time for an offset request in milliseconds.
      </td>
    </tr>

    <tr>
      <td>
        `request.avgTimeOffset99Percentile`
      </td>

      <td>
        Time for offset requests for 99th percentile in milliseconds.
      </td>
    </tr>

    <tr>
      <td>
        `request.avgTimeProduceRequest`
      </td>

      <td>
        Average time for a produce request in milliseconds.
      </td>
    </tr>

    <tr>
      <td>
        `request.avgTimeUpdateMetadata`
      </td>

      <td>
        Average time for a request to update metadata in milliseconds.
      </td>
    </tr>

    <tr>
      <td>
        `request.avgTimeUpdateMetadata99Percentile`
      </td>

      <td>
        Time for update metadata requests for 99th percentile in milliseconds.
      </td>
    </tr>

    <tr>
      <td>
        `request.clientFetchesFailedPerSecond`
      </td>

      <td>
        Client fetch request failures per second.
      </td>
    </tr>

    <tr>
      <td>
        `request.fetchTime99Percentile`
      </td>

      <td>
        Time for fetch requests for 99th percentile in milliseconds.
      </td>
    </tr>

    <tr>
      <td>
        `request.handlerIdle`
      </td>

      <td>
        Average fraction of time the request handler threads are idle.
      </td>
    </tr>

    <tr>
      <td>
        `request.produceRequestsFailedPerSecond`
      </td>

      <td>
        Failed produce requests per second.
      </td>
    </tr>

    <tr>
      <td>
        `request.produceTime99Percentile`
      </td>

      <td>
        Time for produce requests for 99th percentile.
      </td>
    </tr>
  </tbody>
</Table>

### KafkaConsumerSample event

<Table>
  <thead>
    <tr>
      <th style={{ width: "350px" }}>
        Metric
      </th>

      <th>
        Description
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        `consumer.avgFetchSizeInBytes`
      </td>

      <td>
        Average number of bytes fetched per request for a specific topic.
      </td>
    </tr>

    <tr>
      <td>
        `consumer.avgRecordConsumedPerTopic`
      </td>

      <td>
        Average number of records in each request for a specific topic.
      </td>
    </tr>

    <tr>
      <td>
        `consumer.avgRecordConsumedPerTopicPerSecond`
      </td>

      <td>
        Average number of records consumed per second for a specific topic in records per second.
      </td>
    </tr>

    <tr>
      <td>
        `consumer.bytesInPerSecond`
      </td>

      <td>
        Consumer bytes per second.
      </td>
    </tr>

    <tr>
      <td>
        `consumer.fetchPerSecond`
      </td>

      <td>
        The minimum rate at which the consumer sends fetch requests to a broke in requests per second.
      </td>
    </tr>

    <tr>
      <td>
        `consumer.maxFetchSizeInBytes`
      </td>

      <td>
        Maximum number of bytes fetched per request for a specific topic.
      </td>
    </tr>

    <tr>
      <td>
        `consumer.maxLag`
      </td>

      <td>
        Maximum consumer lag.
      </td>
    </tr>

    <tr>
      <td>
        `consumer.messageConsumptionPerSecond`
      </td>

      <td>
        Rate of consumer message consumption in messages per second.
      </td>
    </tr>

    <tr>
      <td>
        `consumer.offsetKafkaCommitsPerSecond`
      </td>

      <td>
        Rate of offset commits to Kafka in commits per second.
      </td>
    </tr>

    <tr>
      <td>
        `consumer.offsetZooKeeperCommitsPerSecond`
      </td>

      <td>
        Rate of offset commits to ZooKeeper in writes per second.
      </td>
    </tr>

    <tr>
      <td>
        `consumer.requestsExpiredPerSecond`
      </td>

      <td>
        Rate of delayed consumer request expiration in evictions per second.
      </td>
    </tr>
  </tbody>
</Table>

### KafkaProducerSample event

<Table>
  <thead>
    <tr>
      <th style={{ width: "350px" }}>
        Metric
      </th>

      <th>
        Description
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        `producer.ageMetadataUsedInMilliseconds`
      </td>

      <td>
        Age in seconds of the current producer metadata being used.
      </td>
    </tr>

    <tr>
      <td>
        `producer.availableBufferInBytes`
      </td>

      <td>
        Total amount of buffer memory that is not being used in bytes.
      </td>
    </tr>

    <tr>
      <td>
        `producer.avgBytesSentPerRequestInBytes`
      </td>

      <td>
        Average number of bytes sent per partition per-request.
      </td>
    </tr>

    <tr>
      <td>
        `producer.avgCompressionRateRecordBatches`
      </td>

      <td>
        Average compression rate of record batches.
      </td>
    </tr>

    <tr>
      <td>
        `producer.avgRecordAccumulatorsInMilliseconds`
      </td>

      <td>
        Average time in ms record batches spent in the record accumulator.
      </td>
    </tr>

    <tr>
      <td>
        `producer.avgRecordSizeInBytes`
      </td>

      <td>
        Average record size in bytes.
      </td>
    </tr>

    <tr>
      <td>
        `producer.avgRecordsSentPerSecond`
      </td>

      <td>
        Average number of records sent per second.
      </td>
    </tr>

    <tr>
      <td>
        `producer.avgRecordsSentPerTopicPerSecond`
      </td>

      <td>
        Average number of records sent per second for a topic.
      </td>
    </tr>

    <tr>
      <td>
        `producer.AvgRequestLatencyPerSecond`
      </td>

      <td>
        Producer average request latency.
      </td>
    </tr>

    <tr>
      <td>
        `producer.avgThrottleTime`
      </td>

      <td>
        Average time that a request was throttled by a broker in milliseconds.
      </td>
    </tr>

    <tr>
      <td>
        `producer.bufferMemoryAvailableInBytes`
      </td>

      <td>
        Maximum amount of buffer memory the client can use in bytes.
      </td>
    </tr>

    <tr>
      <td>
        `producer.bufferpoolWaitTime`
      </td>

      <td>
        Faction of time an appender waits for space allocation.
      </td>
    </tr>

    <tr>
      <td>
        `producer.bytesOutPerSecond`
      </td>

      <td>
        Producer bytes per second out.
      </td>
    </tr>

    <tr>
      <td>
        `producer.compressionRateRecordBatches`
      </td>

      <td>
        Average compression rate of record batches for a topic.
      </td>
    </tr>

    <tr>
      <td>
        `producer.iOWaitTime`
      </td>

      <td>
        Producer I/O wait time in milliseconds.
      </td>
    </tr>

    <tr>
      <td>
        `producer.maxBytesSentPerRequestInBytes`
      </td>

      <td>
        Max number of bytes sent per partition per-request.
      </td>
    </tr>

    <tr>
      <td>
        `producer.maxRecordSizeInBytes`
      </td>

      <td>
        Maximum record size in bytes.
      </td>
    </tr>

    <tr>
      <td>
        `producer.maxRequestLatencyInMilliseconds`
      </td>

      <td>
        Maximum request latency in milliseconds.
      </td>
    </tr>

    <tr>
      <td>
        `producer.maxThrottleTime`
      </td>

      <td>
        Maximum time a request was throttled by a broker in milliseconds.
      </td>
    </tr>

    <tr>
      <td>
        `producer.messageRatePerSecond`
      </td>

      <td>
        Producer messages per second.
      </td>
    </tr>

    <tr>
      <td>
        `producer.responsePerSecond`
      </td>

      <td>
        Number of producer responses per second.
      </td>
    </tr>

    <tr>
      <td>
        `producer.requestPerSecond`
      </td>

      <td>
        Number of producer requests per second.
      </td>
    </tr>

    <tr>
      <td>
        `producer.requestsWaitingResponse`
      </td>

      <td>
        Current number of in-flight requests awaiting a response.
      </td>
    </tr>

    <tr>
      <td>
        `producer.threadsWaiting`
      </td>

      <td>
        Number of user threads blocked waiting for buffer memory to enqueue their records.
      </td>
    </tr>
  </tbody>
</Table>

### KafkaTopicSample event

<Table>
  <thead>
    <tr>
      <th style={{ width: "350px" }}>
        Metric
      </th>

      <th>
        Description
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        `topic.diskSize`
      </td>

      <td>
        Current topic disk size per broker in bytes.
      </td>
    </tr>

    <tr>
      <td>
        `topic.partitionsWithNonPreferredLeader`
      </td>

      <td>
        Number of partitions per topic that are not being led by their preferred replica.
      </td>
    </tr>

    <tr>
      <td>
        `topic.respondMetaData`
      </td>

      <td>
        Number of topics responding to meta data requests.
      </td>
    </tr>

    <tr>
      <td>
        `topic.retentionSizeOrTime`
      </td>

      <td>
        Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time.
      </td>
    </tr>

    <tr>
      <td>
        `topic.underReplicatedPartitions`
      </td>

      <td>
        Number of partitions per topic that are under-replicated.
      </td>
    </tr>
  </tbody>
</Table>

### KafkaOffsetSample event

<Table>
  <thead>
    <tr>
      <th style={{ width: "350px" }}>
        Metric
      </th>

      <th>
        Description
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        `consumer.offset`
      </td>

      <td>
        The last consumed offset on a partition by the consumer group.
      </td>
    </tr>

    <tr>
      <td>
        `consumer.lag`
      </td>

      <td>
        The difference between a broker's high water mark and the consumer's offset (`consumer.hwm` - `consumer.offset`).
      </td>
    </tr>

    <tr>
      <td>
        `consumer.hwm`
      </td>

      <td>
        The offset of the last message written to a partition (high water mark).
      </td>
    </tr>

    <tr>
      <td>
        `consumer.totalLag`
      </td>

      <td>
        The sum of lags across partitions consumed by a consumer.
      </td>
    </tr>

    <tr>
      <td>
        `consumerGroup.totalLag`
      </td>

      <td>
        The sum of lags across all partitions consumed by a `consumerGroup`.
      </td>
    </tr>

    <tr>
      <td>
        `consumerGroup.maxLag`
      </td>

      <td>
        The maximum lag across all partitions consumed by a `consumerGroup`.
      </td>
    </tr>
  </tbody>
</Table>

## Inventory data

The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the [Inventory UI page](/docs/infrastructure/new-relic-infrastructure/infrastructure-ui-pages/infrastructure-inventory-page-search-your-entire-infrastructure) under the `config/kafka` source.

## Troubleshooting

Troubleshooting tips:

<CollapserGroup>
  <Collapser
    id="duplicate-info"
    title="Duplicate data being reported"
  >
    For agents monitoring producers and/or consumers, and that have `Topic mode` set to `All`:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the [configuration option](#config) `Collect topic size` is set to false.
  </Collapser>

  <Collapser
    id="zookeeper-node-not-found"
    title="Integration is logging errors 'zk: node not found'"
  >
    Ensure that `zookeeper_path` is set correctly in the [configuration file](#config).
  </Collapser>
</CollapserGroup>

## Check the source code

This integration is open source software. That means you can [browse its source code](https://github.com/newrelic/nri-kafka "Link opens in a new window.") and send improvements, or create your own fork and build it.
