---
title: Best practices for OpenTelemetry with New Relic
tags:
  - Integrations
  - Open source telemetry integrations
  - OpenTelemetry
redirects:
  - /docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-logs/
---

Here are some best practices based on how OpenTelemetry works with New Relic.

## Resources [#resources]

A resource in OpenTelemetry represents information about an entity generating telemetry data. All telemetry data sent to New Relic is expected to be associated with a resource so that it can be linked with the appropriate entity in New Relic. The [OpenTelemetry Resource SDK specification](https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/resource/sdk.md) defines the functionality implemented by all language SDKs for defining a resource.

<Callout variant="important">
At a minimum, the `service.name` attribute must be present on your resource for data to be associated with an entity in New Relic.
</Callout>

## Traces [#traces]

Make sure you are familiar with the options for sampling trace spans.

### Sampling [#sampling]

 Trace data is the most mature OpenTelemetry data type. Because of this, New Relic's OpenTelemetry user experience is largely based on trace data and is therefore influenced by your sampling strategy.

You can configure sampling in a number of places:

* **Service:** Use the OpenTelemetry SDK for your language.
* **Collector:** If you're running your own instance of the OpenTelemtry collector, you can configure it to do more sophisticated forms of sampling, such as tail-based sampling.

Check out this documentation about how to configure different types of sampling:

<CollapserGroup>
  <Collapser
    className="freq-link"
    id="built-in"
    title="OpenTelemetry built-in samplers"
  >
  [Built-in samplers](https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/trace/sdk.md#built-in-samplers) implemented by the OpenTelemetry SDK for each language.
  </Collapser>
  <Collapser
    className="freq-link"
    id="ot-tail-based"
    title="OpenTelemetry tail-based samplers"
  >
    The OpenTelemtry collector's [tail-based sampling processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/tailsamplingprocessor). We have an [example](https://github.com/newrelic/newrelic-opentelemetry-examples/tree/main/collector/k8s-collector-tail-sampling) demonstrating the use of the tail-based sampling processor.
  </Collapser>
  <Collapser
    className="freq-link"
    id="infinite-tracing"
    title="New Relic tail-based sampling with Infinite Tracing"
  >
  Infinite Tracing is New Relic's tail-based sampling option. You can use this in conjunction with your OpenTelemetry instrumented services. In setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer:

  <Callout variant="important">
  Currently, Infinite Tracing does not support OTLP ingest. You must run your own instance of the OpenTelemetry Collector and configure it to use the [New Relic exporter](/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-legacy-new-relic-exporters).
  </Callout>

  1. Follow the steps in [Set up the trace observer](/docs/distributed-tracing/infinite-tracing/set-trace-observer/) to get the value for <var>YOUR_TRACE_OBSERVER_URL</var>.
  2. Use the value of <var>YOUR_TRACE_OBSERVER_URL</var> to configure your integration.
  3. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the `AlwaysOn` sampler.
  </Collapser>
</CollapserGroup>

## Batching [#batching]

<Callout variant="caution">
  Avoid getting rate limited! You should batch requests sent to the OTLP endpoint as described in this section.
</Callout>

By default, the OpenTelemetry SDKs and Collector send one (1) data point per request. Using these defaults, it is likely your account will be rate limited.

All OpenTelemetry SDKs and Collectors provide a `BatchProcessor`, which batches data points in memory. This batching allows requests to be sent with more than one (1) data point.

<table>
  <thead>
      <tr>
        <th style={{ width: "200px" }}>Component</th>
        <th>Batch Processor</th>
      </tr>
  </thead>
  <tbody>
    <tr>
      <td>Collector</td>
      <td>[Batch Processor](https://github.com/open-telemetry/opentelemetry-collector/blob/main/processor/batchprocessor/README.md)</td>
    </tr>
    <tr>
      <td>Go SDK</td>
      <td>[BatchSpanProcessor](https://pkg.go.dev/go.opentelemetry.io/otel/sdk/trace#NewBatchSpanProcessor)</td>
    </tr>
    <tr>
      <td>JS SDK</td>
      <td>[BatchSpanProcessor](https://open-telemetry.github.io/opentelemetry-js/classes/_opentelemetry_tracing.batchspanprocessor.html)</td>
    </tr>
    <tr>
      <td>Python SDK</td>
      <td>[BatchExportSpanProcessor](https://open-telemetry.github.io/opentelemetry-python/sdk/trace.export.html#opentelemetry.sdk.trace.export.BatchExportSpanProcessor)</td>
    </tr>
  </tbody>
</table>

## Metrics [#metrics]

Metric support with OpenTelemetry is actively being developed and is rapidly evolving. Not all language SDKs have metric support yet, and those that do are subject to change a lot in the coming months.

### Temporality [#temporality]

OpenTelemetry metrics have a notion of [temporality](https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/metrics/datamodel.md#temporality) indicating whether reported values incorporate previous measurements (cumulative temporality) or not (delta temporality).

At the moment, language SDKs that have metric support produce metrics with a cumulative temporality by default. However, New Relic best supports metrics sent with a delta temporality. You will need to refer to the OpenTelemetry SDK for your language to determine how to configure metrics to be emitted with a delta temporality.

Here is an example demonstraing how to configure aggregation temporality using the [Java](https://github.com/newrelic/newrelic-opentelemetry-examples/tree/main/java/sdk-nr-config) and [Go](https://github.com/newrelic/newrelic-opentelemetry-examples/tree/main/go/go-metrics) OpenTelemetry SDKs.

### Metric types [#metric-types]

The OpenTelemetry [data model](https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/metrics/datamodel.md#metric-points) for metrics defines a number of different metric types: sum, gauge, histogram, and summary.

Currently, New Relic does not support ingesting OpenTelemetry histogram metrics.

Here is a summary of New Relic's support by metric type and aggregation temporatlity:

<table>
  <thead>
    <tr>
      <th style={{ width: "200px" }}>Metric Type</th>
      <th>Aggregation Temporality</th>
      <th>Supported</th>
    </tr>
  </thead>
    <tbody>
    <tr>
      <td>Gauge</td>
      <td>&nbsp;</td>
      <td>‚úÖ</td>
    </tr>
    <tr>
      <td>Summary</td>
      <td>&nbsp;</td>
      <td>‚úÖ</td>
    </tr>
    <tr>
      <td>Sum</td>
      <td>**Delta**</td>
     <td>‚úÖ</td>
    </tr>
    <tr>
      <td>Sum</td>
      <td>**Cumulative**</td>
      <td>üü° &nbsp;New Relic converts these to Gauge metrics </td>
    </tr>
    <tr>
      <td>Histogram</td>
      <td>&nbsp;</td>
      <td>‚ùå</td>
    </tr>
    </tbody>
</table>

## Logs [#logs]

Logs generated from your applications and environment are an important piece of telemetry. They may represent application logs, machine generated events, or system logs. OpenTelemetry has defined a [log data model](https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/logs/data-model.md) for representing log data.

Let's look at how to send logs using OpenTelemetry tooling, correlate them with applications, and view them in New Relic.

### Send logs to New Relic [#send-logs]

The [OpenTelemtry Collector](https://github.com/open-telemetry/opentelemetry-collector) and [OpenTelemetry Collector Contrib](https://github.com/open-telemetry/opentelemetry-collector-contrib) repositories contain a number of components for consuming log data. The general pattern is to configure the collector to:

1. Receive logs from any of the log receivers. Some of the receiver options include [Filelog Receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/filelogreceiver), [Fluent Forward Receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/fluentforwardreceiver), and [Syslog Receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/syslogreceiver).
2. Process logs, potentially annotating them with resource information. Some of the processor options include [Resource Detection Processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourcedetectionprocessor) and [Resource Processor](https://github.com/open-telemetry/opentelemetry-collector/tree/main/processor/resourceprocessor).
3. Export logs to New Relic via the OTLP exporter.

### Application log correlation [#log-correlation]

Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry [semantic convention for services](https://github.com/open-telemetry/opentelemetry-specification/tree/main/specification/resource/semantic_conventions#service) specifies `service.name` as a required field. All application metric, trace, and log data sent to New Relic with the same `service.name` are associated with the same [entity](/docs/new-relic-one/use-new-relic-one/core-concepts/what-entity-new-relic).

The specifics of how logs get annotated with the `service.name` resource attribute depends on the application's environment:

* Applications may produce structured JSON logs, which you can configure to include `service.name` as another field.
* You can deploy applications alongside a dedicated [Collector Agent](https://opentelemetry.io/docs/collector/getting-started/#agent) instance, which you can configure with a [Resource Processor](https://github.com/open-telemetry/opentelemetry-collector/tree/main/processor/resourceprocessor) to annotate logs with the `service.name` attribute.

Optionally, additional application [trace context](https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/logs/overview.md#log-correlation) (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified [trace context fields](https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/logs/data-model.md#trace-context-fields) on available log messages.

The [Logs in Context with Log4j2 example in GitHub](https://github.com/newrelic/newrelic-opentelemetry-examples/tree/main/java/logs-in-context-log4j2) demonstrates an end-to-end working example for a simple Java application using Log4j2.

### View OpenTelemetry logs [#view-logs]

Here are two ways you can view logs:

* Look in the New Relic [Logs UI](/docs/logs/log-management/ui-data/use-logs-ui/).
* If your logs are correlated with an application, view them in the [context of the application](/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic#logs).
