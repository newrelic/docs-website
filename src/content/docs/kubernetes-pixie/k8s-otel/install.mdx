---
title: Install OpenTelemetry Collector on Kubernetes
tags:
    - Kubernetes integration
    - New Relic K8S OpenTelemetry integration
    - New Relic OpenTelemetry Collector
    - NRDOT
    - New Relic OpenTelemetry
    - OpenTelemetry Collector
    - OpenTelemetry Kubernetes
    - OpenTelemetry Helm chart
    - New Relic OpenTelemetry installation
metaDescription: "Instrument your Kubernetes Cluster in New Relic using OpenTelemetry Collectors"
freshnessValidatedDate: 2025-06-30
---

You can monitor your Kubernetes cluster using OpenTelemetry, which provides a standardized way to collect and send telemetry data to New Relic. This integration allows you to gain insights into your Kubernetes environment, including metrics, events, and logs. 

New Relic recommends using the [New Relic distribution of OpenTelemetry (NRDOT) for Kubernetes](https://github.com/newrelic/nrdot-collector-releases/tree/main/distributions/nrdot-collector-k8s). This distribution is pre-configured to work seamlessly with New Relic and includes all necessary components and configurations to effectively monitor your Kubernetes cluster. However, if you prefer to use your own OpenTelemetry Collector, you can do so by ensuring it includes the required components for Kubernetes monitoring. 


## Requirements [#requirements]

To send Kubernetes telemetry data to New Relic, you need an OpenTelemetry Collector. Our [NRDOT](https://github.com/newrelic/nrdot-collector-releases/tree/main/distributions/nrdot-collector-k8s) is configured to automatically monitor your Kubernetes cluster. It deploys all necessary components through our [`nr-k8s-otel-collector`](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector) Helm chart.

If you opt for a different OpenTelemetry Collector, ensure it includes all the key components for comprehensive Kubernetes monitoring:

<CollapserGroup>

    <Collapser
        id="otel-components"
        title="Required OpenTelemetry collectors components"
    >

    * [Attributes processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/attributesprocessor)
    * [Filter processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/filterprocessor)
    * [Filelog receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/filelogreceiver)
    * [GroupByAttrs processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/groupbyattrsprocessor)
    * [Hostmetrics receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/hostmetricsreceiver)
    * [K8sAttributes processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/k8sattributesprocessor)
    * [K8sevents receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/k8seventsreceiver)
    * [Kubelet receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/kubeletstatsreceiver)
    * [MetricsTransform processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/metricstransformprocessor)
    * [Prometheus receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/prometheusreceiver)
    * [ResourceDetection processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourcedetectionprocessor)
    * [Resource processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourceprocessor)
    * [Transform processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/transformprocessor)

    </Collapser>

</CollapserGroup>


### Supported Environments [#supported-envs]

Our OpenTelemetry monitoring for Kubernetes provides robust support across various deployment environments. Supported platforms include:

* **Cloud vendors:**

  * Amazon EKS
  * Microsoft AKS
  * Google GKE
  * Red Hat OpenShift

* **On-Premise clusters:** We offer support for on-premise Kubernetes clusters.
* **Kubernetes versions:** Support aligns with the Kubernetes versions currently supported by each vendor, ensuring compatibility and effective monitoring solutions across these environments.

<Callout variant="important">

    Windows nodes are not supported.

</Callout>


## Install your Kubernetes cluster with OpenTelemetry [#install]

You can install OpenTelemetry for Kubernetes using one of the following methods:

<Callout variant="tip">

    For advanced configuration options, refer to [Advanced configuration for OpenTelemetry Kubernetes](/docs/kubernetes-pixie/k8s-otel/advance-config).

</Callout>

<CollapserGroup>

    <Collapser
        id="helm-install"
        title="Helm install"
    >

## Helm install [#helm-install]

The Helm installation method is the recommended approach for deploying OpenTelemetry on Kubernetes. This method simplifies the deployment process and allows for easy configuration management. You can use either the NRDOT or your own OpenTelemetry Collector.

New Relic recommends using the NRDOT for Kubernetes, which is pre-configured to work seamlessly with New Relic. This distribution includes all necessary components and configurations to effectively monitor your Kubernetes cluster. However, if you prefer to use your own OpenTelemetry Collector, you can do so by ensuring it includes the required components for Kubernetes monitoring.

**To install Helm chart:**

1. Download the [Helm chart `values.yaml` file](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector/values.yaml#L20-L24) and adapt it to meet your specific requirements. 

   * The cluster name and <InlinePopover type="licenseKey"/> are mandatory.

   * For more information, refer to [configuration parameters](#config-params).

2. Install the [Helm chart](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector) using your `values.yaml` file.

    ```shell
    helm repo add newrelic https://helm-charts.newrelic.com
    helm upgrade nr-k8s-otel-collector newrelic/nr-k8s-otel-collector -f your-custom-values.yaml -n newrelic --create-namespace --install
    ```

3. Ensure the pods have successfully spun up.

    ```shell
        kubectl get pods -n newrelic --watch
    ```

    You should see pods with names such as `nr-k8s-otel-collector-<hash>` in the `newrelic` namespace.

4. Verify that New Relic is receiving the necessary data, including metrics, events, and logs, by running the following queries. For more information, refer to [Introduction to the query builder](/docs/query-your-data/explore-query-data/query-builder/introduction-query-builder/).

    ```sql
    FROM Metric SELECT * WHERE k8s.cluster.name='<CLUSTER_NAME>'
    FROM OtlpInfrastructureEvent SELECT * WHERE k8s.cluster.name='<CLUSTER_NAME>'
    FROM Log SELECT * WHERE k8s.cluster.name='<CLUSTER_NAME>'
    ```

<Callout variant="tip">

    If you want to use a different OpenTelemetry Collector to collect Kubernetes telemetry data, update the `image` settings in your `values.yaml` file to specify your desired OpenTelemetry Collector:

    ```yaml
    image:
        repository: otel/opentelemetry-collector-contrib # Example: Using the contrib distro
        tag: "latest" # Or a specific stable version like "0.98.0"
    ```

</Callout>

    </Collapser>

  <Collapser
    id="manifest-install"
    title="Manifests install"
  >

The Manifest installation method provides a way to set up OpenTelemetry for Kubernetes without using Helm. This method is suitable for users who prefer a more hands-on approach or have specific requirements that necessitate manual configuration.

**To perform a manifest installation:**

1. Copy the contents of the `nr-k8s-otel-collector`'s [rendered examples directory](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector/examples/k8s/rendered) to your local workspace. This directory contains the Kubernetes manifest files.

2. Update the [`secret.yaml`](https://github.com/newrelic/helm-charts/blob/master/charts/nr-k8s-otel-collector/examples/k8s/rendered/secret.yaml) file in your local `rendered` directory. Replace `<Your Base64 encoded License key>` with your New Relic License Key, encoded in Base64.
    ```yaml
    data:
        licenseKey: <Your Base64 encoded License key>
    ```

    To `Base64` encode your license key:
      * **Linux/macOS:** Use `echo -n "YOUR_LICENSE_KEY" | base64`.
      * **Windows (PowerShell):** Use `[System.Convert]::ToBase64String([System.Text.Encoding]::UTF8.GetBytes("YOUR_LICENSE_KEY"))`.

3. Manually update your cluster name in both [`daemonset-configmap.yaml`](https://github.com/newrelic/helm-charts/blob/master/charts/nr-k8s-otel-collector/examples/k8s/rendered/daemonset-configmap.yaml) and [`deployment-configmap.yaml`](https://github.com/newrelic/helm-charts/blob/master/charts/nr-k8s-otel-collector/examples/k8s/rendered/deployment-configmap.yaml) files within your local `rendered` directory. Locate instances of `k8s.cluster.name` and replace `<cluster_name>` with the desired name for your cluster.

    ```yaml
    - key: k8s.cluster.name
        action: upsert
        value: <cluster_name>
    ```

4. After updating these required fields, create the `newrelic` namespace and deploy the manifests to your cluster using `kubectl`.

    ```bash
    kubectl create namespace newrelic
    kubectl apply -n newrelic -R -f rendered
    ```

<Callout variant="tip">
  
    When deploying without Helm, components deployed via the rendered manifests might include a prefix, such as `example-`. This prefix is typically used in Helm chart templates for dynamic naming based on the Helm release name. If you prefer cleaner naming conventions, you can adjust these prefixes directly in the manifest files before applying them.

</Callout>

  </Collapser>

  </CollapserGroup>


## Configuration parameters in `values.yaml` [#config-params]

The `values.yaml` file in the [`nr-k8s-otel-collector`](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector) Helm chart repository contains all the configuration parameters you can use to customize your OpenTelemetry Collector deployment. This file allows you to set various options, such as cluster name, license key, and other collector settings.

<CollapserGroup>

  <Collapser
    id="values-yaml"
    title="Configuration parameters"
  >

  <table>
    <thead>
        <tr>
            <th>Parameter Name</th>
            <th>Data Type</th>
            <th>Description</th>
            <th>Example Value</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>`kube-state-metrics.enabled`</td>
            <td>Boolean</td>
            <td>Specifies whether to install the [`kube-state-metrics` Helm chart](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics). This parameter is mandatory if `infrastructure.enabled` is set to `true` and you do not provide your own instance of Kube State Metrics (KSM) version 1.8 or later. KSM versions 2.0 and above disable labels and annotations metrics by default; you can enable these by using the `metricLabelsAllowlist` or `metricAnnotationsAllowList` options in your KSM configuration.</td>
            <td>`true`</td>
        </tr>
        <tr>
            <td>`kube-state-metrics.prometheusScrape`</td>
            <td>Boolean</td>
            <td>Disables Prometheus from automatically discovering KSM endpoints. This prevents potential duplication of scraped data if KSM is already being monitored by another Prometheus instance.</td>
            <td>`false`</td>
        </tr>
        <tr>
            <td>`provider`</td>
            <td>String</td>
            <td>Specifies the cloud vendor or environment where your Kubernetes cluster is deployed. This parameter sets known configuration constraints specific to your provider. Currently, supported values are `"GKE_AUTOPILOT"` for Google Kubernetes Engine Autopilot clusters and `"OPEN_SHIFT"` for Red Hat OpenShift.</td>
            <td>`"GKE_AUTOPILOT"`</td>
        </tr>
        <tr>
            <td>`image.repository`</td>
            <td>String</td>
            <td>Defines the Docker image repository for the OpenTelemetry Collector. You can specify your own collector image, provided it meets the necessary requirements for Kubernetes monitoring.</td>
            <td>`"newrelic/nrdot-collector-k8s"`</td>
        </tr>
        <tr>
            <td>`image.pullPolicy`</td>
            <td>String</td>
            <td>Determines the image pull policy. The default is `IfNotPresent`, which skips pulling an image if it already exists locally. If this parameter is defined without a specific value, it defaults to `Always`.</td>
            <td>`"IfNotPresent"`</td>
        </tr>
        <tr>
            <td>`image.tag`</td>
            <td>String</td>
            <td>Overrides the default image tag. By default, the tag is set to the chart's `appVersion`. Use this parameter to specify a particular version of the collector image.</td>
            <td>`"1.1.0"`</td>
        </tr>
        <tr>
            <td>`cluster`</td>
            <td>String</td>
            <td>Defines the name of the Kubernetes cluster being monitored. This is a mandatory parameter. You can also configure it using `global.cluster`.</td>
            <td>`"my-eks-cluster"`</td>
        </tr>
        <tr>
            <td>`licenseKey`</td>
            <td>String</td>
            <td>Specifies your New Relic License Key for authentication. This is a mandatory parameter. You can also configure it using `global.licenseKey`.</td>
            <td>`"NRAL-ABCDEFGHIJKLMN"`</td>
        </tr>
        <tr>
            <td>`customSecretName`</td>
            <td>String</td>
            <td>Specifies the name of a user-created Kubernetes Secret that holds the New Relic License Key. Use this option if you prefer not to include the license key directly in your `values.yaml` file. You can also configure it using `global.customSecretName`.</td>
            <td>`"my-nr-license-secret"`</td>
        </tr>
        <tr>
            <td>`customSecretLicenseKey`</td>
            <td>String</td>
            <td>Specifies the key within the Kubernetes Secret (defined by `customSecretName`) where the license key is located. Use this option when obtaining the license key from a Secret. You can also configure it using `global.customSecretLicenseKey`.</td>
            <td>`"licenseKey"`</td>
        </tr>
        <tr>
            <td>`proxy`</td>
            <td>String</td>
            <td>Configures the OpenTelemetry Collector instances (both Daemonset and Deployment) to send all telemetry data through the specified HTTP/HTTPS proxy.</td>
            <td>`"http://your-proxy.example.com:3128"`</td>
        </tr>
        <tr>
            <td>`podLabels`</td>
            <td>Map (key-value)</td>
            <td>Specifies additional labels to be added to all chart pods. These labels can be used for organizing and selecting Kubernetes resources.</td>
            <td>`{app.kubernetes.io/component: "otel-collector"}`</td>
        </tr>
        <tr>
            <td>`labels`</td>
            <td>Map (key-value)</td>
            <td>Specifies additional labels to be added to all Kubernetes objects created by the chart. These labels apply broadly to deployments, services, and other resources.</td>
            <td>`{environment: "production"}`</td>
        </tr>
        <tr>
            <td>`priorityClassName`</td>
            <td>String</td>
            <td>Sets the `priorityClassName` for all pods deployed by the chart. This controls the scheduling priority of the pods. You can also configure it using `global.priorityClassName`.</td>
            <td>`"system-cluster-critical"`</td>
        </tr>
        <tr>
            <td>`dnsConfig`</td>
            <td>Map (key-value)</td>
            <td>Sets the `dnsConfig` for all pods deployed by the chart. This allows for custom DNS settings for the pods. You can also configure it using `global.dnsConfig`.</td>
            <td>`{nameservers: ["8.8.8.8"]}`</td>
        </tr>
        <tr>
            <td>`daemonset.nodeSelector`</td>
            <td>Map (key-value)</td>
            <td>Sets the node selector for Daemonset pods, controlling which nodes they are scheduled on. This parameter overrides global node selectors.</td>
            <td>`{kubernetes.io/os: "linux"}`</td>
        </tr>
        <tr>
            <td>`daemonset.tolerations`</td>
            <td>List</td>
            <td>Specifies tolerations for Daemonset pods, allowing them to be scheduled on nodes with matching taints. This parameter overrides global tolerations.</td>
            <td>`[{key: "node-role.kubernetes.io/master", effect: "NoSchedule"}]`</td>
        </tr>
        <tr>
            <td>`daemonset.affinity`</td>
            <td>Map (key-value)</td>
            <td>Configures affinities for Daemonset pods, influencing their scheduling based on node or pod labels. This parameter overrides global affinities.</td>
            <td>`{nodeAffinity: {requiredDuringSchedulingIgnoredDuringExecution: {nodeSelectorTerms: [{matchExpressions: [{key: "disktype", operator: "In", values: ["ssd"]}]}]}}}`</td>
        </tr>
        <tr>
            <td>`daemonset.podAnnotations`</td>
            <td>Map (key-value)</td>
            <td>Specifies additional annotations to be added to the Daemonset pods.</td>
            <td>`{prometheus.io/scrape: "true"}`</td>
        </tr>
        <tr>
            <td>`daemonset.podSecurityContext`</td>
            <td>Map (key-value)</td>
            <td>Sets the security context at the pod level for the Daemonset. This parameter overrides global pod security contexts.</td>
            <td>`{runAsUser: 1000}`</td>
        </tr>
        <tr>
            <td>`daemonset.containerSecurityContext`</td>
            <td>Map (key-value)</td>
            <td>Sets the security context at the container level for the Daemonset. This parameter overrides global container security contexts. This chart defaults to `privileged: true` for host monitoring capabilities.</td>
            <td>`{privileged: true}`</td>
        </tr>
        <tr>
            <td>`daemonset.resources`</td>
            <td>Map (key-value)</td>
            <td>Defines the compute resources (CPU and memory requests and limits) for the Daemonset containers.</td>
            <td>`{limits: {cpu: "500m", memory: "512Mi"}, requests: {cpu: "200m", memory: "256Mi"}}`</td>
        </tr>
        <tr>
            <td>`daemonset.envs`</td>
            <td>List</td>
            <td>Specifies additional environment variables to be set for the Daemonset containers.</td>
            <td>`[{name: "DEBUG_MODE", value: "true"}]`</td>
        </tr>
        <tr>
            <td>`daemonset.envsFrom`</td>
            <td>List</td>
            <td>Specifies additional environment variable sources for the Daemonset containers, typically from Secrets or ConfigMaps.</td>
            <td>`[{secretRef: {name: "my-daemonset-secret"}}]`</td>
        </tr>
        <tr>
            <td>`daemonset.configMap.overrideConfig`</td>
            <td>Map (key-value)</td>
            <td>Provides a complete OpenTelemetry configuration for the Daemonset. If set, this parameter overrides the default configuration and disables other configuration parameters specific to the Daemonset.</td>
            <td>`receivers: {hostmetrics: {collection_interval: 30s}}`</td>
        </tr>
        <tr>
            <td>`daemonset.configMap.extraConfig`</td>
            <td>Map (key-value)</td>
            <td>Specifies additional OpenTelemetry configuration for the Daemonset. If set, this parameter extends the default configuration by adding more receivers, processors, exporters, connectors, or pipelines.</td>
            <td>`receivers: {custom_receiver: {}}`</td>
        </tr>
        <tr>
            <td>`deployment.nodeSelector`</td>
            <td>Map (key-value)</td>
            <td>Sets the node selector for Deployment pods, controlling which nodes they are scheduled on. This parameter overrides global node selectors.</td>
            <td>`{kubernetes.io/hostname: "control-plane-node"}`</td>
        </tr>
        <tr>
            <td>`deployment.tolerations`</td>
            <td>List</td>
            <td>Specifies tolerations for Deployment pods, allowing them to be scheduled on nodes with matching taints. This parameter overrides global tolerations.</td>
            <td>`[{key: "CriticalAddonsOnly", operator: "Exists"}]`</td>
        </tr>
        <tr>
            <td>`deployment.affinity`</td>
            <td>Map (key-value)</td>
            <td>Configures affinities for Deployment pods, influencing their scheduling based on node or pod labels. This parameter overrides global affinities.</td>
            <td>`{podAntiAffinity: {requiredDuringSchedulingIgnoredDuringExecution: [{labelSelector: {matchLabels: {app: "my-app"}}, topologyKey: "kubernetes.io/hostname"}]}}`</td>
        </tr>
        <tr>
            <td>`deployment.podAnnotations`</td>
            <td>Map (key-value)</td>
            <td>Specifies additional annotations to be added to the Deployment pods.</td>
            <td>`{sidecar.istio.io/inject: "false"}`</td>
        </tr>
        <tr>
            <td>`deployment.podSecurityContext`</td>
            <td>Map (key-value)</td>
            <td>Sets the security context at the pod level for the Deployment. This parameter overrides global pod security contexts.</td>
            <td>`{fsGroup: 2000}`</td>
        </tr>
        <tr>
            <td>`deployment.containerSecurityContext`</td>
            <td>Map (key-value)</td>
            <td>Sets the security context at the container level for the Deployment. This parameter overrides global container security contexts.</td>
            <td>`{runAsNonRoot: true}`</td>
        </tr>
        <tr>
            <td>`deployment.resources`</td>
            <td>Map (key-value)</td>
            <td>Defines the compute resources (CPU and memory requests and limits) for the Deployment containers.</td>
            <td>`{limits: {cpu: "200m", memory: "256Mi"}, requests: {cpu: "100m", memory: "128Mi"}}`</td>
        </tr>
        <tr>
            <td>`deployment.envs`</td>
            <td>List</td>
            <td>Specifies additional environment variables to be set for the Deployment containers.</td>
            <td>`[{name: "ENV_VAR", value: "some_value"}]`</td>
        </tr>
        <tr>
            <td>`deployment.envsFrom`</td>
            <td>List</td>
            <td>Specifies additional environment variable sources for the Deployment containers, typically from Secrets or ConfigMaps.</td>
            <td>`[{configMapRef: {name: "my-deployment-config"}}]`</td>
        </tr>
        <tr>
            <td>`deployment.configMap.overrideConfig`</td>
            <td>Map (key-value)</td>
            <td>Provides a complete OpenTelemetry configuration for the Deployment. If set, this parameter overrides the default configuration and disables other configuration parameters specific to the Deployment.</td>
            <td><pre>`receivers: {k8s_events: {collection_interval: 5s}}`</pre></td>
        </tr>
        <tr>
            <td>`deployment.configMap.extraConfig`</td>
            <td>Map (key-value)</td>
            <td>Specifies additional OpenTelemetry configuration for the Deployment. If set, this parameter extends the default configuration by adding more receivers, processors, exporters, connectors, or pipelines.</td>
            <td><pre>`exporters: {log: {verbosity: "detailed"}}`</pre></td>
        </tr>
        <tr>
            <td>`nodeSelector`</td>
            <td>Map (key-value)</td>
            <td>Sets the node selector for all pods deployed by the chart, affecting both Daemonset and Deployment. This parameter can also be configured using `global.nodeSelector`.</td>
            <td>`{disktype: "ssd"}`</td>
        </tr>
        <tr>
            <td>`tolerations`</td>
            <td>List</td>
            <td>Sets tolerations for all pods deployed by the chart, allowing them to be scheduled on nodes with matching taints. This parameter can also be configured using `global.tolerations`.</td>
            <td>`[{key: "taint-key", operator: "Exists", effect: "NoSchedule"}]`</td>
        </tr>
        <tr>
            <td>`affinity`</td>
            <td>Map (key-value)</td>
            <td>Configures affinities for all pods deployed by the chart, influencing their scheduling based on node or pod labels. This parameter can also be configured using `global.affinity`.</td>
            <td>`{podAffinity: {requiredDuringSchedulingIgnoredDuringExecution: [{labelSelector: {matchLabels: {app: "otel-collector"}}, topologyKey: "kubernetes.io/hostname"}]}}`</td>
        </tr>
        <tr>
            <td>`podSecurityContext`</td>
            <td>Map (key-value)</td>
            <td>Sets the security context at the pod level for all pods deployed by the chart. This parameter can also be configured using `global.securityContext.pod`.</td>
            <td>`{runAsGroup: 3000}`</td>
        </tr>
        <tr>
            <td>`containerSecurityContext`</td>
            <td>Map (key-value)</td>
            <td>Sets the security context at the container level for all containers in pods deployed by the chart. This parameter can also be configured using `global.securityContext.container`.</td>
            <td>`{allowPrivilegeEscalation: false}`</td>
        </tr>
        <tr>
            <td>`rbac.create`</td>
            <td>Boolean</td>
            <td>Specifies whether Role-Based Access Control (RBAC) resources, such as Service Accounts, Roles, and RoleBindings, should be created by the Helm chart.</td>
            <td>`true` or `false`</td>
        </tr>
        <tr>
            <td>`serviceAccount.create`</td>
            <td>Boolean</td>
            <td>Specifies whether a Kubernetes ServiceAccount should be created for the collector pods. If set to `true` and `serviceAccount.name` is not specified, a name is generated using the full name template.</td>
            <td>`true` or `false`</td>
        </tr>
        <tr>
            <td>`serviceAccount.name`</td>
            <td>String</td>
            <td>Specifies a custom name for the ServiceAccount. If not set and `serviceAccount.create` is `true`, the chart generates a name.</td>
            <td>`"my-otel-sa"`</td>
        </tr>
        <tr>
            <td>`serviceAccount.annotations`</td>
            <td>Map (key-value)</td>
            <td>Specifies any additional annotations to add to the created ServiceAccount.</td>
            <td>`{eks.amazonaws.com/role-arn: "arn:aws:iam::123456789012:role/MyEKSFargateRole"}`</td>
        </tr>
        <tr>
            <td>`verboseLog`</td>
            <td>Boolean</td>
            <td>Enables debug-level logging for this integration. If set globally (using `global.verboseLog`), it enables debug logs for all integrations.</td>
            <td>`true` or `false`</td>
        </tr>
        <tr>
            <td>`nrStaging`</td>
            <td>Boolean</td>
            <td>Directs the collector to send all data to the New Relic staging backend. This option requires a valid staging license key. This parameter can also be configured using `global.nrStaging`.</td>
            <td>`true` or `false`</td>
        </tr>
        <tr>
            <td>`receivers.prometheus.enabled`</td>
            <td>Boolean</td>
            <td>Specifies whether the `prometheus` receiver is enabled for data collection.</td>
            <td>`true` or `false`</td>
        </tr>
        <tr>
            <td>`receivers.prometheus.scrapeInterval`</td>
            <td>String</td>
            <td>Sets the scrape interval for the `prometheus` receiver, determining how frequently metrics are collected.</td>
            <td>`1m`</td>
        </tr>
        <tr>
            <td>`receivers.k8sEvents.enabled`</td>
            <td>Boolean</td>
            <td>Specifies whether the `k8sEvents` receiver is enabled for collecting Kubernetes events.</td>
            <td>`true` or `false`</td>
        </tr>
        <tr>
            <td>`receivers.hostmetrics.enabled`</td>
            <td>Boolean</td>
            <td>Specifies whether the `hostmetrics` receiver is enabled for collecting host-level metrics.</td>
            <td>`true` or `false`</td>
        </tr>
        <tr>
            <td>`receivers.hostmetrics.scrapeInterval`</td>
            <td>String</td>
            <td>Sets the scrape interval for the `hostmetrics` receiver.</td>
            <td>`1m`</td>
        </tr>
        <tr>
            <td>`receivers.kubeletstats.enabled`</td>
            <td>Boolean</td>
            <td>Specifies whether the `kubeletstats` receiver is enabled for collecting metrics from the Kubelet.</td>
            <td>`true` or `false`</td>
        </tr>
        <tr>
            <td>`receivers.kubeletstats.scrapeInterval`</td>
            <td>String</td>
            <td>Sets the scrape interval for the `kubeletstats` receiver.</td>
            <td>`1m`</td>
        </tr>
        <tr>
            <td>`receivers.filelog.enabled`</td>
            <td>Boolean</td>
            <td>Specifies whether the `filelog` receiver is enabled for collecting logs from files, typically container logs.</td>
            <td>`true` or `false`</td>
        </tr>
        <tr>
            <td>`lowDataMode`</td>
            <td>Boolean</td>
            <td>Enables a mode that sends only the essential metrics required to illuminate the New Relic Kubernetes user interface. This helps reduce data ingestion volume.</td>
            <td>`true` or `false`</td>
        </tr>
    </tbody>
</table>

  </Collapser>
</CollapserGroup>


## Alerts [#alerts]

You can install essential alert policies by going through the guided installation flow in <DNT>Integrations & Agents</DNT>. This automatically sets up alert policy named **Kubernetes (OpenTelemetry) alert policy** in your New Relic account with multiple alert conditions designed for Kubernetes observability. Customize these conditions to suit your specific monitoring needs.

<img
    title="Kubernetes OpenTelemetry alert policy"
    alt="Kubernetes OpenTelemetry alert policy"
    src="/images/alert-k8s-policy.webp"
/>


## Find and use data [#find]

Check out these documents to learn more on how to find data:

* [Explore your Kubernetes cluster](/docs/kubernetes-pixie/kubernetes-integration/understand-use-data/kubernetes-cluster-explorer/) to know the status of your cluster, from the control plane to nodes and pods.

* [Kubernetes APM summary page](/docs/apm/apm-ui-pages/monitoring/kubernetes-summary-page/) which offers insights into your Kubernetes integration alongside your monitored applications.




## Metrics [#metrics]

* The `LowDataMode` is the default mode. To view the full list of metrics refer to [`LowDataMode` list](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector/docs/metrics-lowDataMode.md).
* [Metrics - Full list](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector/docs/metrics-full.md)



## Uninstall [#uninstall]

### Helm uninstall [#helm-uninstall]

To stop monitoring a Kubernetes cluster with OpenTelemetry, run this command:

```shell
helm uninstall nr-k8s-otel-collector -n newrelic
```

### Manifests uninstall [#manifests-uninstall]

To stop monitoring a Kubernetes cluster with OpenTelemetry, run this command:

```shell
kubectl delete -R -f rendered
kubectl delete namespaces newrelic
```

## Support [#support]

If you have issues with the OpenTelemetry observability for Kubernetes, refer to:

* [Troubleshooting guide](/docs/kubernetes-pixie/k8s-otel/troubleshooting/) 
* [Issues section on GitHub](https://github.com/newrelic/helm-charts/issues) for any similar problems or consider opening a new issue.


## Related articles [#related-docs]

<DocTiles>
    <DocTile title="Advanced configuration for OpenTelemetry Kubernetes" path="/docs/kubernetes-pixie/k8s-otel/advance-config/">
        Customize your OpenTelemetry Collector configuration for Kubernetes in New Relic.
    </DocTile>
    <DocTile title="Troubleshooting OpenTelemetry for Kubernetes" path="/docs/kubernetes-pixie/k8s-otel/troubleshooting/">
        Learn how to troubleshoot issues with the OpenTelemetry Collector in Kubernetes.
    </DocTile>
</DocTiles>