---
title: 'Install the Kubernetes integration'
tags:
- Integrations
- Kubernetes integration
- Installation
translate:
- jp
- kr
metaDescription: "New Relic's Kubernetes integration: How to install and activate the integration, and what data is reported."
redirects:
- /docs/integrations/kubernetes-integration/installation/kubernetes-integration-install-configure
- /docs/kubernetes-monitoring-integration-beta
- /docs/kubernetes-integration-beta
- /docs/kubernetes-integration
- /docs/kubernetes-integration-new-relic-infrastructure
- /docs/kubernetes-monitoring-integration
- /docs/integrations/host-integrations/host-integrations-list/kubernetes-monitoring-integration
- /docs/integrations/kubernetes-integration/installation/kubernetes-monitoring-integration
- /docs/integrations/kubernetes-integration/installation/kubernetes-monitoring-installation
- /docs/integrations/kubernetes-integration/installation/kubernetes-installation-configuration
- /docs/integrations/kubernetes-integration/installation
- /docs/kubernetes-pixie/kubernetes-integration/installation/install-kubernetes-integration-using-helm/
signupBanner:
text: Monitor and improve your entire stack. 100GB free. Forever.
freshnessValidatedDate: never
---

import kubernetesAks from 'images/kubernetes_logo_aks.webp'

import kubernetesOpenshift from 'images/kubernetes_logo_openshift.webp'

import kubernetesCke from 'images/kubernetes_logo_cke.webp'

import kubernetesEks from 'images/kubernetes_logo_eks.webp'

import pixieLiveDebugging from 'images/pixie_screenshot-full_live-debugging.webp'

import pixieServiceOtelMap from 'images/pixie_screenshot-full_service-otel-map.webp'

import kubernetesFargateOverview from 'images/kubernetes_diagram_fargate-overview.svg'

import kubernetesFargateWorkflow from 'images/kubernetes_diagram_fargate-workflow.svg'

import kubernetesFargateUi from 'images/kubernetes_screenshot-crop_fargate-ui.webp'

The New Relic Kubernetes integration gives you full observability into the health and performance of your environment by leveraging the New Relic infrastructure agent. This agent collects telemetry data from your cluster using several New Relic integrations such as the [Kubernetes events integration](/docs/integrations/kubernetes-integration/kubernetes-events/install-kubernetes-events-integration), the [Prometheus Agent](/docs/infrastructure/prometheus-integrations/install-configure-prometheus-agent/install-prometheus-agent/), and the [New Relic Logs Kubernetes plugin](/docs/logs).

## Installation options

To install our Kubernetes integration, we recommend that you follow the instructions here for our guided install experience. We recommend this interactive installation tool for servers, VMs, and [unprivileged](/docs/infrastructure/install-infrastructure-agent/linux-installation/linux-agent-running-modes) environments. 

<Tabs>
  <TabsBar>
    <TabsBarItem id="guided-install">Guided install (recommended) </TabsBarItem>
    <TabsBarItem id="windows-install">In Windows</TabsBarItem>
    <TabsBarItem id="eks-fargate">In EKS Fargate</TabsBarItem>
    <TabsBarItem id="man-helm">Manual Helm</TabsBarItem>
    <TabsBarItem id="gke-autopilot">In GKE Autopilot</TabsBarItem>
  </TabsBar>

  <TabsPages>
    <TabsPageItem id="guided-install">
    The guided install experience simplifies the installation process for the New Relic Kubernetes integration, and gives you control over which features are enabled and what data is collected. It also offers a quickstart option that includes some optional, pre-built resources such as dashboards and alerts alongside the Kubernetes integration so that you can gain instant visibility into your Kubernetes clusters. 
    
    You can choose from one of the following three options: 
    
    1. New Relic CLI
    2. A Helm command with pre-populated required values
    3. A plain manifest


    ## Navigating the Kubernetes integration guided install [#kubernetes-install-navigation]

    Once you start the guided install, use the following information to help you make decisions about the configurations.

    <Callout variant="tip">
      The steps that follow skip the preliminary steps for the quickstart. If you chose the guided install with the quickstart, just click through the pages <DoNotTranslate>**Confirm your Kubernetes quickstart installation**</DoNotTranslate> and <DoNotTranslate>**Installation plan**</DoNotTranslate> to reach the main guided install pages described below.
    </Callout>

    <Steps>
      <Step>

## Prepare to install

Prepare your Kubernetes system for the guided install:

* If custom manifests have been used instead of Helm, you will need to first remove the old installation using `kubectl delete -f previous-manifest-file.yml`, and then proceed through the guided installer again. This will generate an updated set of manifests that can be deployed using `kubectl apply -f manifest-file.yml`.
* Make sure you're using the supported Kubernetes versions and make sure to check out the preliminary notes for your managed services or platforms on our [compatibility and requirements page](/docs/kubernetes-pixie/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements).
* Make sure you have your New Relic <InlinePopover type="licenseKey" />. You can set up a free account&mdash;no credit card required.
* Make sure the newrelic dockerhub (`https://hub.docker.com/u/newrelic`) and Google's registry (`registry.k8s.io`) domains are added to your allow list. This is where the installation will pull container images from. Note, you may need to follow the [commands](https://kubernetes.io/blog/2023/03/10/image-registry-redirect/#how-can-i-check-if-i-am-impacted) to identify the additional Google registry domains to be added to your white list, because `registry.k8s.io` typically redirects to your local registry domain (e.g., `asia-northeast1-docker.pkg.dev`) based on your region.

    If you're installing our integration on a managed cloud, please take a look at these [preliminary notes](#cloud-platforms) before proceeding:

    <CollapserGroup>
      <Collapser
        className="freq-link"
        id="install-amazon-eks"
        title={<><img src={kubernetesEks} alt="EKS" style={{ verticalAlign: 'middle' }}/>Amazon EKS / EKS Anywhere / EKS Anywhere on bare metal</>}
      >
        The Kubernetes integration only monitors worker nodes into Amazon EKS as Amazon abstracts the management of master nodes away from the Kubernetes platform.

        Before using our guided install to deploy the Kubernetes integration in Amazon EKS,  make sure to install `eksctl`, the [command line tool](https://docs.aws.amazon.com/eks/latest/userguide/eksctl.html) for managing Kubernetes clusters on Amazon EKS.
      </Collapser>

      <Collapser
        className="freq-link"
        id="install-google-kubernetes-engine"
        title={<><img src={kubernetesCke} alt="CKE" style={{ verticalAlign: 'middle' }}/>Google Kubernetes Engine (GKE Standard)</>}
      >
        The Kubernetes integration only monitors worker nodes in GKE as Google abstracts the management of master nodes away from the Kubernetes platform.

        Before starting our guided install to deploy the Kubernetes integration on GKE, ensure you have sufficient permissions:

        1. Go to [console.cloud.google.com/iam-admin/iam](https://console.cloud.google.com/iam-admin/iam) and find your username.
        2. Click <DoNotTranslate>**edit**</DoNotTranslate>.
        3. Ensure you have permissions to create `Roles` and `ClusterRoles`: If you're not sure, add the <DoNotTranslate>**Kubernetes Engine Cluster Admin**</DoNotTranslate> role. If you cannot edit your user role, ask the owner of the GCP project to give you the necessary permissions.
      </Collapser>

      <Collapser
        className="freq-link"
        id="install-openshift-container-platform"
        title={<><img src={kubernetesOpenshift} alt="OpenShift" style={{ verticalAlign: 'middle' }}/> OpenShift container platform</>}
      >
        To deploy the Kubernetes integration with [OpenShift](https://learn.openshift.com):

        1. Add the service accounts used by the integration to your privileged [Security Context Constraints](https://docs.openshift.com/enterprise/3.0/admin_guide/manage_scc.html):

          ```
          oc adm policy add-scc-to-user privileged system:serviceaccount:<namespace>:<release_name>-newrelic-infrastructure
          oc adm policy add-scc-to-user privileged system:serviceaccount:<namespace>:<release_name>-nrk8s-controlplane
          oc adm policy add-scc-to-user privileged system:serviceaccount:<namespace>:<release_name>-kube-state-metrics
          oc adm policy add-scc-to-user privileged system:serviceaccount:<namespace>:<release_name>-newrelic-logging
          oc adm policy add-scc-to-user privileged system:serviceaccount:<namespace>:<release_name>-nri-kube-events
          oc adm policy add-scc-to-user privileged system:serviceaccount:<namespace>:<release_name>-nri-metadata-injection-admission
          oc adm policy add-scc-to-user privileged system:serviceaccount:<namespace>:<release_name>-nrk8s-controlplane
          oc adm policy add-scc-to-user privileged system:serviceaccount:<namespace>:default
          ```

          <Callout variant="tip">
            The installer provides `newrelic-bundle` as default `release_name` and `newrelic` as default `namespace`.
          </Callout>

        2. Complete the steps in our [guided install](https://one.newrelic.com/nr1-core?state=51fbbd48-c8ca-ead9-bb90-af96e18d82a7).
        3. If you're using signed certificates, make sure they are properly configured by using the following variables in the `DaemonSet` portion of your manifest. Set the `.pem` file:
          ```yaml
          env:
            - name: NRIA_CA_BUNDLE_DIR
              value: YOUR_CA_BUNDLE_DIR
            - name: NRIA_CA_BUNDLE_FILE
              value: YOUR_CA_BUNDLE_NAME
          ```
        4. Set your YAML key path to `spec.template.spec.containers.name.env`.
        5. Save your changes.

      </Collapser>

      <Collapser
        className="freq-link"
        id="install-azure-aks"
        title={<><img src={kubernetesAks} alt="AKS" style={{ verticalAlign: 'middle' }}/> Azure Kubernetes Service (AKS)</>}
      >
        The Kubernetes integration only monitors worker nodes in the Azure Kubernetes Service as Azure abstracts the management of master nodes away from the Kubernetes platform.

      </Collapser>

    </CollapserGroup>
      </Step>

      <Step>
## Begin the guided install
Begin your guided install by clicking one of the options below:

<table>
  <thead>
    <tr>
      <th style={{ width: "200px" }}>
        Guided install option
      </th>
      <th>
        Description
      </th>
    </tr>
  </thead>
    <tbody>
    <tr>
      <td>
        [Guided install](https://onenr.io/0oR861DvMQG)
      </td>
      <td>
        Use this if your New Relic organization does **not** use the [EU](/docs/using-new-relic/welcome-new-relic/get-started/our-eu-us-region-data-centers) data center, and you don't need the bonus dashboards and alerts from the quickstart.
      </td>
    </tr>
    <tr>
      <td>
        [Guided install (EU)](https://onenr.io/0VwgOqNAZwJ)
      </td>
      <td>
        Use this if your New Relic organization uses the [EU](/docs/using-new-relic/welcome-new-relic/get-started/our-eu-us-region-data-centers) data center, and you don't need the bonus dashboards and alerts from the quickstart.
      </td>
    </tr>
    <tr>
      <td>
        [Guided install with quickstart](https://one.newrelic.com/launcher/catalog-pack-details.launcher/?pane=eyJuZXJkbGV0SWQiOiJjYXRhbG9nLXBhY2stZGV0YWlscy5jYXRhbG9nLXBhY2stY29udGVudHMiLCJxdWlja3N0YXJ0SWQiOiI4OGE3OWY1Mi04MWMxLTRmYTItOTlmOC0zY2I1YjAxMmYxNjAifQ==)
      </td>
      <td>
        Use this option if your New Relic organization does **not** use the [EU](/docs/using-new-relic/welcome-new-relic/get-started/our-eu-us-region-data-centers) data center, and you also want to install some bonus dashboards and alerts from the quickstart.
      </td>
    </tr>
    </tbody>
</table>   

</Step>
      <Step>

        ## Configure your install

        On the page <DoNotTranslate>**Configure the Kubernetes Integration**</DoNotTranslate> complete the following fields:

        <table>
          <thead>
            <tr>
              <th style={{ width: "200px" }}>
                Field
              </th>
              <th>
                Description
              </th>
            </tr>
          </thead>
            <tbody>
            <tr>
              <td>
                We'll send your data to this account
              </td>
              <td>
                Choose the New Relic account that you want your Kubernetes data written to.
              </td>
            </tr>
            <tr>
              <td>
                Cluster name
              </td>
              <td>
                Cluster name is the name we will use to tag your Kubernetes data with so that you can filter for the data specific to the cluster you're installing this integration in. This is important if you choose to connect multiple clusters to your New Relic account so choose a name that you'll recognize.
              </td>
            </tr>
            <tr>
              <td>
                Namespace for the integration
              </td>
              <td>
                Namespace for the integration is the namespace we will use to house the Kubernetes integration in your cluster. We recommend using the default namespace of `newrelic`.
              </td>
            </tr>
            </tbody>
        </table>
      </Step>

      <Step>

## Select additional data

        On the page <DoNotTranslate>**Select the additional data you want to gather**</DoNotTranslate>, choose the options that are right for you:

        ### Scrape Prometheus endpoints [#scrape-endpoints]

        By selecting this option, we will install Prometheus in agent mode to collect metrics from the Prometheus endpoints exposed in your cluster. Expand the collapsers to see details about each option:

        <CollapserGroup>
          <Collapser
            className="freq-link"
            id="scrape-all-except-ksm"
            title="Scrape all Prometheus endpoints except core Kubernetes system metrics (recommended)"
          >
            We recommend this configuration because various other components of the Kubernetes integration, such as [`kube-state-metrics`, `newrelic-infrastructure`, and `nri-prometheus`](https://github.com/newrelic/helm-charts/tree/master/charts/nri-bundle#bundled-charts) will already collect these metrics and configuring Prometheus to exclude those metrics will save your data ingest costs by removing any metric redundancies.

            This configuration will filter out any metrics prefixed with [`kube_`, `container_`, `machine_`, and `cadvisor_`](https://github.com/newrelic/newrelic-prometheus-configurator/blob/64af9453f4b20d4aab88a4d1afda55cf9a6e63c4/charts/newrelic-prometheus-agent/static/lowdatamodedefaults.yaml).

            Here's an example from `newrelic-prometheus-configurator/charts/newrelic-prometheus-agent/static/lowdatamodedefaults.yaml`:

            ```yaml
            low_data_mode:
            - action: drop
              source_labels: [__name__]
              regex: "kube_.+|container_.+|machine_.+|cadvisor_.+"
            ```

          </Collapser>
          <Collapser
            className="freq-link"
            id="scrape-all-endpoints"
            title="Scrape all Prometheus endpoints"
          >
            Select <DoNotTranslate>**Scrape all Prometheus endpoints**</DoNotTranslate> if you prefer to preserve Prometheus' metric naming conventions across all Prometheus metrics regardless of any metric redundancies.
          </Collapser>
          <Collapser
            className="freq-link"
            id="scrape-with-quickstarts"
            title="Scrape only Prometheus endpoints with quickstarts"
          >
            New Relic provides [quickstarts](https://newrelic.com/instant-observability/?category=prometheus&search=), which are pre-made dashboards, alerts, and entities for various services. Select this option to have Prometheus only scrape for [services which have a pre-made quickstart](https://github.com/newrelic/newrelic-prometheus-configurator/blob/main/charts/newrelic-prometheus-agent/values.yaml#L214-L228) and are ready to go for instant observability.

            Here's an example from `newrelic-prometheus-configurator/charts/newrelic-prometheus-agent/values.yaml` showing in the `app_values` field which services will be scraped for the Prometheus quickstart option:

            ```yaml
            kubernetes:
                # NewRelic provides a list of Dashboards, alerts and entities for several Services. The integrations_filter configuration
                # allows to scrape only the targets having this experience out of the box.
                # If integrations_filter is enabled, then the jobs scrape merely the targets having one of the specified labels matching
                # one of the values of app_values.
                # Under the hood, a relabel_configs with 'action=keep' are generated, consider it in case any custom extra_relabel_config is needed.
                integrations_filter:
                  # -- enabling the integration filters, merely the targets having one of the specified labels matching
                  #    one of the values of app_values are scraped. Each job configuration can override this default.
                  enabled: true
                  # -- source_labels used to fetch label values in the relabel config added by the integration filters configuration
                  source_labels: ["app.kubernetes.io/name", "app.newrelic.io/name", "k8s-app"]
                  # -- app_values used to create the regex used in the relabel config added by the integration filters configuration.
                  # Note that a single regex will be created from this list, example: '.*(?i)(app1|app2|app3).*'
                  app_values: ["redis", "traefik", "calico", "nginx", "coredns", "kube-dns", "etcd", "cockroachdb", "velero", "harbor", "argocd"]
            ```
          </Collapser>
          <Collapser
            className="freq-link"
            id="custom-app-labels"
            title="Scrape only the certain labels"
          >
            You'll find this option useful if you're an advanced user who has a good idea of what services you want to see Prometheus metrics from. Enter a comma-separated list of services you want Prometheus to scrape, and Prometheus will perform a wildcard match on the service name in order to find you metrics from your desired endpoint.

            This option will *only* provide metrics from the services that match the submitted list, so be careful to validate the entry for correctness. To learn more about custom app labels, see [Advanced configuration for the Prometheus agent](/docs/infrastructure/prometheus-integrations/install-configure-prometheus-agent/advanced-configuration/#enable-disable-integrations).

            The services you add to the submitted list will overwrite the data in `app_values` below, and Prometheus will <DoNotTranslate>**only**</DoNotTranslate> scrape metrics from those services.

            Here is an example from `newrelic-prometheus-configurator/charts/newrelic-prometheus-agent/values.yaml`:

            ```yaml
              kubernetes:
                # NewRelic provides a list of Dashboards, alerts and entities for several Services. The integrations_filter configuration
                # allows to scrape only the targets having this experience out of the box.
                # If integrations_filter is enabled, then the jobs scrape merely the targets having one of the specified labels matching
                # one of the values of app_values.
                # Under the hood, a relabel_configs with 'action=keep' are generated, consider it in case any custom extra_relabel_config is needed.
                integrations_filter:
                  # -- enabling the integration filters, merely the targets having one of the specified labels matching
                  #    one of the values of app_values are scraped. Each job configuration can override this default.
                  enabled: true
                  # -- source_labels used to fetch label values in the relabel config added by the integration filters configuration
                  source_labels: ["app.kubernetes.io/name", "app.newrelic.io/name", "k8s-app"]
                  # -- app_values used to create the regex used in the relabel config added by the integration filters configuration.
                  # Note that a single regex will be created from this list, example: '.*(?i)(app1|app2|app3).*'
                  app_values: ["redis", "traefik", "calico", "nginx", "coredns", "kube-dns", "etcd", "cockroachdb", "velero", "harbor", "argocd"]
            ```

          </Collapser>
        </CollapserGroup>

        ### Gather log data [#gather-logs]

        You can customize the detail of log data within the install UI:

        <CollapserGroup>
          <Collapser
          className="freq-link"
          id="full-enrichment"
          title="Forward all logs with full enrichment"
          >
            If you prefer more robust data, select this option to fully enrich your logs by adding label and annotation data.

            Here's an example of a log with full data enrichment:

            ```json
            [
              {
                "cluster_name": "api-test",
                "kubernetes": {
                  "annotations": {
                    "kubernetes.io/psp": "eks.privileged"
                  },
                  "container_hash": "fryckbos/test@sha256:5b098eaf3c7d5b3585eb10cebee63665b6208bea31ef31a3f0856c5ffdda644b",
                  "container_image": "fryckbos/test:latest",
                  "container_name": "newrelic-logging",
                  "docker_id": "134e1daf63761baa15e035b08b7aea04518a0f0e50af4215131a50c6a379a072",
                  "host": "ip-192-168-17-123.ec2.internal",
                  "labels": {
                    "app": "newrelic-logging",
                    "app.kubernetes.io/name": "newrelic-logging",
                    "controller-revision-hash": "84db95db86",
                    "pod-template-generation": "1",
                    "release": "nri-bundle"
                  },
                  "namespace_name": "nrlogs",
                  "pod_id": "54556e3e-719c-46b5-af69-020b75d69bf1",
                  "pod_name": "nri-bundle-newrelic-logging-jxnbj"
                },
                "message": "[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\n",
                "plugin": {
                  "source": "kubernetes",
                  "type": "fluent-bit",
                  "version": "1.8.1"
                },
                "stream": "stderr",
                "time": "2021-09-14T12:30:49.138824971Z",
                "timestamp": 1631622649138
              }
            ]
            ```
          </Collapser>
          <Collapser
          className="freq-link"
          id="min-enrichment"
          title="Forward all logs with minimal enrichment (low data mode)"
          >
            If you want to prioritize data ingest costs, you can choose to gather log data with minimal enrichment, also known as low data mode. This option drops labels and annotations from your logs and only shares standard Kubernetes log data such as the name of the cluster, container, namespace, and pod, along with the message and timestamp.

            When selecting the minimal enrichment mode, only the following log attributes are retained: `cluster_name`, `container_name`, `namespace_name`, `pod_name`, `stream`, `message` and `log`.

            Here's an example of a log with minimal data enrichment:

            ```json
            [
              {
                "cluster_name": "api-test",
                "container_name": "newrelic-logging",
                "namespace_name": "nrlogs",
                "pod_name": "nri-bundle-newrelic-logging-jxnbj",
                "message": "[2021/09/14 12:30:49] [ info] [engine] started (pid=1)\n",
                "stream": "stderr",
                "timestamp": 1631622649138
              }
            ]
            ```
        </Collapser>

        </CollapserGroup>

        ### Enable service-level insights, full-body requests, and application profiles through Pixie [#enable-pixie]

        [Pixie](https://docs.px.dev/about-pixie/what-is-pixie/) is an open source observability tool for Kubernetes applications that uses eBPF to automatically collect telemetry data. If you don't have Pixie installed on your cluster, but want to leverage Pixie's powerful telemetry data collection and visualization on the [New Relic platform](/docs/kubernetes-pixie/auto-telemetry-pixie/get-started-auto-telemetry-pixie/), check <DoNotTranslate>**Enable service-level insights, full-body requests, and application profiles through Pixie**</DoNotTranslate>.

        If you're already using Community Cloud, select <DoNotTranslate>**Community Cloud hosted Pixie is already running on this cluster**</DoNotTranslate>. Keep the following in mind about the different ways [Pixie can be hosted](https://docs.px.dev/installing-pixie/install-guides/#title). New Relic provides a different level of integration support for each Pixie hosting option.

        <CollapserGroup>
          <Collapser
            className="freq-link"
            id="community-cloud-pixie"
            title="Community Cloud Pixie"
          >
            If you're already leveraging Pixie's Community Cloud, you can provide an API key to connect Pixie to New Relic. This approach will embed Pixie's live UI into your New Relic account for easy access (via Pixie's Live Debugging tool), as well as write Pixie data into New Relic through the New Relic OpenTelemetry endpoint.

            <img
              title="service graph in live debugger"
              alt="service-graph"
              src={pixieLiveDebugging}
            />
          </Collapser>
          <Collapser
            className="freq-link"
            id="self-hosted-pixie"
            title="Self-hosted Pixie"
          >
            If you're using Pixie with a self-hosted Pixie Cloud, you can also connect Pixie to New Relic. This approach will enable the export of Pixie telemetry data into New Relic via the OpenTelemetry endpoint for long-term data retention and visibility. Unfortunately, if you're self-hosting your Pixie Cloud, New Relic does not support embedding Pixie's Live UI.

            If you're self-hosting Pixie Cloud and would like to enable the export of Pixie telemetry data into New Relic, simply enable Pixie in the Kubernetes Integration without checking the <DoNotTranslate>**Community Cloud hosted Pixie option**</DoNotTranslate>. The Kubernetes Integration will detect that Pixie is running in your cluster and enable the data export for instant data visibility and insight.

            <img
              title="The OpenTelemetry <DoNotTranslate>**Service map**</DoNotTranslate> view shows helps visualize your application's dependencies."
              alt="The OpenTelemetry <DoNotTranslate>**Service map**</DoNotTranslate> view shows helps visualize your application's dependencies."
              src={pixieServiceOtelMap}
            />
          </Collapser>
        </CollapserGroup>
      </Step>

      <Step>

      ## Finish your install
      
        Finalize the Kubernetes installation setup by choosing one of the following installation methods in the last step of the guided install:

        * <DoNotTranslate>**Guided Install (recommended)**</DoNotTranslate>: This option will automatically download and use the [`newrelic-cli`](https://developer.newrelic.com/automate-workflows/get-started-new-relic-cli/) CLI to install and configure the Kubernetes integration.
        * <DoNotTranslate>**Helm 3**</DoNotTranslate>: Use this option if you prefer using [Helm](https://helm.sh/) to install and configure the Kubernetes integration. This option installs the [`nri-bundle` Helm chart](/docs/kubernetes-pixie/kubernetes-integration/installation/install-kubernetes-integration-using-helm/#install-k8-helm), which you can further configure with the options described [here](/docs/kubernetes-pixie/kubernetes-integration/installation/install-kubernetes-integration-using-helm/#configure). 
        * <DoNotTranslate>**Manifest**</DoNotTranslate>: Select this option if you prefer generating a Kubernetes manifest in YAML format and manually installing it with [`kubectl`](https://kubernetes.io/docs/reference/kubectl/).

        <Callout variant="tip">
          Not seeing data? If you completed the steps above and are still not seeing data, check out [this troubleshooting page](/docs/kubernetes-pixie/kubernetes-integration/troubleshooting/kubernetes-integration-troubleshooting-not-seeing-data/).
        </Callout>
      </Step>
    </Steps>

    </TabsPageItem>
    
    <TabsPageItem id="windows-install">
    
    Use this option when you have a Windows-based Kubernetes system. Note that there are [various limitations](/docs/kubernetes-pixie/kubernetes-integration/installation/install-version2-kubernetes-integration-windows/#k8-windows-limitations) to the Windows integration. 


<Callout title="preview">
This feature is currently in preview.
</Callout>

## Compatibility and requirements [#k8-windows-req]

Before you install the [Kubernetes integration](/docs/integrations/kubernetes-integration/get-started/introduction-kubernetes-integration), review the [compatibility and requirements](/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements).

<Callout variant="important">
When using containers in Windows, the container host version and the container image version must be the same. Our Kubernetes integration can run on Windows versions LTSC 2019 (1809), 20H2, and LTSC 2022.
</Callout>

To check your Windows version:

1. Open a command window.
2. Run the following command:
    ```shell
    Reg Query "HKLM\SOFTWARE\Microsoft\Windows NT\CurrentVersion" /v
    ReleaseIdcmd.exe
    ```

### Example: Get Kubernetes for Windows from a BusyBox container [#example-k8s-windows-busybox]

Run this command:

```shell
kubectl exec -it busybox1-766bb4d6cc-rmsnj -- Reg Query
"HKLM\SOFTWARE\Microsoft\Windows NT\CurrentVersion" /v ReleaseId
```

You should see something like this:

```shell
HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows NT\CurrentVersion
ReleaseId	REG_SZ	1809
```

For a useful mapping between release IDs and OS versions, see [here](https://hub.docker.com/_/microsoft-windows-servercore).

## Install [#k8-windows-install]

You can install the Kubernetes integration for Windows using Helm. See an example on how to install the integration in a cluster with nodes having different build versions of Windows (1809 and 2004):

1. Add the New Relic Helm charts repo:

    ```shell
    helm repo add newrelic https://helm-charts.newrelic.com
    ```

2. Create a namespace for newrelic:

    ```shell
    kubectl create namespace newrelic
    ```

3. Install kube-state-metrics.

    ```shell
    helm repo add ksm https://kubernetes.github.io/kube-state-metrics
    helm install ksm ksm/kube-state-metrics --version 2.13.2
    ```

    <Callout variant="important">
    This command is for installing kube-state-metrics, a mandatory dependency of the integration, on a Linux node. We don't support installing this for non-Linux nodes, and if you install it on a non-Linux node, deployment might fail. We recommend using `nodeSelector` to choose a Linux node. This can be done by editing kube-state-metrics deployment. 
    </Callout>

4. Create a `values-newrelic.yaml` file with the follow data to be used by Helm:

    ```yaml
    global:
      licenseKey: _YOUR_NEW_RELIC_LICENSE_KEY_
      cluster: _K8S_CLUSTER_NAME_
    
    enableLinux: true        # Set to true if your cluster also has linux nodes
    enableWindows: true
    windowsOsList:
      - version: 2019            # Human-readable version identifier
        imageTag: 2-windows-1809-alpha  # Tag to be used for nodes running the windows version above
        buildNumber: 10.0.17763         # Build number for your nodes running the version above. Used as a selector.
      - version: 20h2
        imageTag: 2-windows-20H2-alpha
        buildNumber: 10.0.19042
      - version: 2022
        imageTag: 2-windows-ltsc2022-alpha
        buildNumber: 10.0.20348
    nodeSelector:
      kubernetes.io/os: linux         # Selector for Linux installation.
    windowsNodeSelector:
      kubernetes.io/os: windows       # Selector for Windows installation.
    ```

5. Install the integration with:

    ```shell
    helm upgrade --install newrelic newrelic/newrelic-infrastructure \
    --namespace newrelic --create-namespace \
    --version 2.7.2 \
    -f values-newrelic.yaml
    ```

6. Check that pods are being deployed and reach a stable state:

    ```shell
    kubectl -n newrelic get pods -w
    ```

The Helm chart will create one DaemonSet per each version of Windows that is in the list and use NodeSelector to deploy the corresponding Pod per Node.



## Limitations [#k8-windows-limitations]

The following limitations apply to the Kubernetes integration for Windows:

* The Windows agent only sends the [Kubernetes samples](/docs/integrations/kubernetes-integration/understand-use-data/understand-use-data#event-types) (`K8sNodeSample`, `K8sPodSample`, etc.)
    * `SystemSample`, `StorageSample`, `NetworkSample`, and `ProcessSample` are not generated.
    * Some [Kubernetes metrics](/docs/integrations/kubernetes-integration/understand-use-data/understand-use-data#metrics) are missing because the Windows kubelet doesn’t have them:
* Node:
    * `fsInodes`: not sent
    * `fsInodesFree`: not sent
    * `fsInodesUsed`: not sent
    * `memoryMajorPageFaultsPerSecond`: always returns zero as a value
    * `memoryPageFaults`: always returns zero as a value
    * `memoryRssBytes`: always returns zero as a value
    * `runtimeInodes`: not sent
    * `runtimeInodesFree`: not sent
    * `runtimeInodesUsed`: not sent
* Pod:
    * `net.errorsPerSecond`: not sent
    * `net.rxBytesPerSecond`: not sent
    * `net.txBytesPerSecond`: not sent
* Container:
    * `containerID`: not sent
    * `containerImageID`: not sent
    * `memoryUsedBytes`: in the UI, this is displayed in the pod card that appears when you click on a pod, and will show no data. We will soon fix this by updating our charts to use `memoryWorkingSetBytes` instead.
* Volume:
    * `fsUsedBytes`: zero, so `fsUsedPercent` is zero

## Known issues with the Windows Kubelet [#k8-windows-limitations]

There are a couple of issues with the Windows version of Kubelet that can prevent the integration from fetching data:

* [Issue 90554:](https://github.com/kubernetes/kubernetes/pull/90554) This issue makes the Kubelet return 500 errors when the integration makes a request to the `/stats/summary` endpoint. It will be included in the Kubernetes 1.19 release and has been backported to the releases 1.16.11, 1.17.7, and 1.18.4. There is no solution on the integration side for this problem, we advise you to update to one of the patch versions as soon as possible. You can see if you're being affected by this problem by [enabling verbose logs](/docs/integrations/kubernetes-integration/troubleshooting/get-logs-version#verbose) and looking for messages of the type:

```shell
error querying Kubelet. Get "https://<KUBELET_IP>/stats/summary": error calling kubelet endpoint. Got status code: 500
```

* [Issue 87730:](https://github.com/kubernetes/kubernetes/pull/87730) This issue makes the Kubelet metrics very slow when running minimal load. It makes the integration fail with a timeout error. A patch for this issue has been added for Kubernetes 1.18 and backported to 1.15.12, 1.16.9, and 1.17.5. We advise you to update to one of the patch versions as soon as possible. To mitigate this issue you can increase the integration timeout with the [`TIMEOUT` config option](/docs/integrations/kubernetes-integration/installation/kubernetes-integration-install-configure#kube-state-metrics-timeout-change). You can see if you're being affected by this problem by [enabling verbose logs](/docs/integrations/kubernetes-integration/troubleshooting/get-logs-version#verbose) and looking for messages of the type:

```shell
error querying Kubelet. Get "https://<KUBELET_IP>/stats/summary": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
```

</TabsPageItem>

<TabsPageItem id="eks-fargate">
Use this option when monitoring Kubernetes workloads on EKS Fargate. This integration automatically injects a sidecar containing the infrastructure agent and the nri-kubernetes integration in each pod that needs to be monitored.

<Callout title="Preview">
This feature is currently in preview.
</Callout>

New Relic supports monitoring Kubernetes workloads on EKS Fargate by automatically injecting a sidecar containing the infrastructure agent and the `nri-kubernetes` integration in each pod that needs to be monitored.

If the same Kubernetes cluster also contains EC2 nodes, our solution will also be deployed as a `DaemonSet` in all of them. No sidecar will be injected into pods scheduled in EC2 nodes, and no `DaemonSet` will be deployed to Fargate nodes. Here's an example of a hybrid instance with both Fargate and EC2 nodes:

<img
title="Diagram showing an EKS cluster with Fargate and EC2 nodes"
alt="Diagram showing an EKS cluster with Fargate and EC2 nodes"
src={kubernetesFargateOverview}
/>

<figcaption>
In a mixed environment, the integration only uses a sidecar for Fargate nodes.
</figcaption>

New Relic collects all the supported metrics for all Kubernetes objects regardless of where they are scheduled, whether it's Fargate or EC2 nodes. Please note that, due to the limitations imposed by Fargate, the New Relic integration is limited to running in [unprivileged](/docs/integrations/kubernetes-integration/installation/kubernetes-integration-install-configure/#unprivileged) mode on Fargate nodes. This means that metrics that are usually fetched from the host directly, like running processes, will not be available for Fargate nodes.

The agent in both scenarios will scrape data from Kube State Metrics (KSM), Kubelet, and cAdvisor and send data in the same format.

<Callout variant="important">
Just like for any other Kubernetes cluster, our solution still requires you to deploy and monitor a Kube State Metrics (KSM) instance. Our Helm Chart and/or installer will do so automatically by default, although this behavior can be disabled if your cluster already has a working instance of KSM. This KSM instance will be monitored as any other workload: By injecting a sidecar if it gets scheduled in a Fargate node or with the local instance of the `DaemonSet` if it gets scheduled on an EC2 node.
</Callout>

Other components of the New Relic solution for Kubernetes, such as `nri-prometheus`, `nri-metadata-injection`, and `nri-kube-events`, do not have any particularities and will be deployed by our Helm Chart normally as they would in non-Fargate environments.

## Installation [#installation]

You can choose between two alternatives for installing New Relic full observability in your EKS Fargate cluster:

* [Automatic injection (recommended)](#automatic)
* [Manual injection](#manual)

Regardless of the approach you choose, the experience is exactly the same after it's installed. The only difference is how the container is injected. We do recommend setting up automatic injection with the New Relic infrastructure monitoring operator because it will eliminate the need to manually edit each deployment you want to monitor.

### Automatic injection (recommended) [#automatic]

By default, when Fargate support is enabled, New Relic will deploy an operator to the cluster (`newrelic-infra-operator`). Once deployed, this operator will automatically inject the monitoring sidecar to pods that are scheduled into Fargate nodes, while also managing the creation and the update of `Secrets`, `ClusterRoleBindings`, and any other related resources.

This operator accepts a variety of advanced configuration options that can be used to narrow or widen the scope of the injection, through the use of label selectors for both pods and namespaces.

#### What the operator does [#what-operator-does]

Behind the scenes, the operator sets up a `MutatingWebhookConfiguration`, which allows it to modify the pod objects that are about to be created in the cluster. On this event, and when the pod being created matches the user’s configuration, the operator will:

1. Add a sidecar container to the pod containing the New Relic Kubernetes integration.
2. If a secret doesn't exist, create one in the same namespace as the pod containing the New Relic <InlinePopover type="licenseKey" />, which is needed for the sidecar to report data.
3. Add the pod's service account to a `ClusterRoleBinding` previously created by the operator chart, which will grant this sidecar the required permissions to hit the Kubernetes metrics endpoints.

The `ClusterRoleBinding` grants the following permissions to the pod being injected:

```yml

rules:
- apiGroups: [""]
  resources:
  - "nodes"
  - "nodes/metrics"
  - "nodes/stats"
  - "nodes/proxy"
  - "pods"
  - "services"
  - "namespaces"
  verbs: ["get", "list"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]

```

<Callout variant="tip">
In order for the sidecar to be injected, and therefore to get metrics from pods deployed before the operator has been installed, you need to manually perform a rollout (restart) of the affected deployments. This way, when the pods are created, the operator will be able to inject the monitoring sidecar. New Relic has chosen not to do this automatically in order to prevent unexpected service disruptions and resource usage spikes.
</Callout>

<Callout variant="important">
Remember to create a Fargate profile with a selector that declares the `newrelic` namespace (or the namespace you choose for the installation).
</Callout>

Here's the injection workflow:

<img
title="Diagram showing the workflow of sidecar injection"
alt="Diagram showing the workflow of sidecar injection"
src={kubernetesFargateWorkflow}
/>

#### Automatic injection installation [#auto-injection-install]

<Callout variant="tip">
The following steps are for a default setup. Before completing these, we suggest you take a look at the [Configuration](#config-auto) section below to see if you want to modify any aspects of the automatic injection.
</Callout>

First, add the New Relic Helm repository if you have not done so before:

```shell
helm repo add newrelic https://helm-charts.newrelic.com
```

Then, in order to install the operator in charge of injecting the infrastructure sidecar, please create a file named `values.yaml`, which will be used to define your configuration:

```yaml
## Global values
global:
  # -- The cluster name for the Kubernetes cluster.
  cluster: "_YOUR_K8S_CLUSTER_NAME_"

  # -- The license key for your New Relic Account. This will be preferred configuration option if both `licenseKey` and `customSecret` are specified.
  licenseKey: "_YOUR_NEW_RELIC_LICENSE_KEY_"

  # -- (bool) In each integration it has different behavior. Enables operating system metric collection on each EC2 K8s node. Not applicable to Fargate nodes.
  # @default -- false
  privileged: true

  # -- (bool) Must be set to `true` when deploying in an EKS Fargate environment
  # @default -- false
  fargate: true

## Enable nri-bundle sub-charts

newrelic-infra-operator:
  # Deploys the infrastructure operator, which injects the monitoring sidecar into Fargate pods
  enabled: true
  tolerations: 
  - key: "eks.amazonaws.com/compute-type"
    operator: "Equal"
    value: "fargate"
    effect: "NoSchedule"
  config:
    ignoreMutationErrors: true
    infraAgentInjection:
      # Injection policies can be defined here.  See [values file](https://github.com/newrelic/newrelic-infra-operator/blob/main/charts/newrelic-infra-operator/values.yaml#L114-L125) for more detail.
      policies:
      - namespaceName: namespace-a
      - namespaceName: namespace-b

newrelic-infrastructure:
  # Deploys the Infrastructure Daemonset to EC2 nodes.  Disable for Fargate-only clusters.
  enabled: true

nri-metadata-injection:
  # Deploy our mutating admission webhook to link APM and Kubernetes entities
  enabled: true

kube-state-metrics:
  # Deploys Kube State Metrics.  Disable if you are already running KSM in your cluster.
  enabled: true

nri-kube-events:
  # Deploy the Kubernetes events integration.
  enabled: true

newrelic-logging:
  # Deploys the New Relic's Fluent Bit daemonset to EC2 nodes.  Disable for Fargate-only clusters.
  enabled: true

newrelic-prometheus-agent:
  # Deploys the Prometheus agent for scraping Prometheus endpoints.
  enabled: true
  config:
    kubernetes:
      integrations_filter:
        enabled: true
        source_labels: ["app.kubernetes.io/name", "app.newrelic.io/name", "k8s-app"]
        app_values: ["redis", "traefik", "calico", "nginx", "coredns", "kube-dns", "etcd", "cockroachdb", "velero", "harbor", "argocd", "istio"]
```

Finally, after creating and tweaking the file, you can deploy the solution using the following Helm command:

```shell
helm upgrade --install newrelic-bundle newrelic/nri-bundle -n newrelic --create-namespace -f values.yaml
```

<Callout variant="important">
When deploying the solution on a hybrid cluster (with both EC2 and Fargate nodes), please make sure that the solution is not selected by any Fargate profiles; otherwise, the `DaemonSet` instances will be stuck in a pending state. For fargate-only environments this is not a concern because no `DaemonSet` instances are created.
</Callout>

#### Configuration [#config-auto]

You can configure different aspects of the automatic injection. By default, the operator will inject the monitoring sidecar to all pods deployed in Fargate nodes which are not part of a `Job` or a `BatchJob`.

This behavior can be changed through configuration options. For example, you can define selectors to narrow or widen the selection of pods that are injected, assign resources to the operator, and tune the sidecar. Also, you can add other attributes, labels, and environment variables. Please refer to the chart [README.md](https://github.com/newrelic/helm-charts/blob/master/charts/newrelic-infra-operator/README.md) and [values.yaml](https://github.com/newrelic/helm-charts/blob/master/charts/newrelic-infra-operator/values.yaml).

<Callout variant="important">
Specifying your own custom injection rules will discard the default ruleset that prevents sidecar injection on pods that are not scheduled in Fargate. Please ensure that your custom rules have the same effect; otherwise, on hybrid clusters which also have the `DaemonSet` deployed, pods scheduled in EC2 will be monitored twice, leading to incorrect or duplicate data.
</Callout>

#### Update to the latest version or to a new configuration [#update-auto-install-version]

To update to the latest version of the EKS Fargate integration, upgrade the Helm repository using `helm repo update newrelic` and reinstall the bundle by simply running again the command above.

To update the configuration of the infrastructure agent injected or the operator itself, modify the `values-newrelic.yaml` and upgrade the Helm release with the new configuration. The operator is updated immediately, and your workloads will be instrumented with the new version on their next restart. If you wish to upgrade them immediately, you can force a restart of your workloads by running:

```shell
kubectl rollout restart deployment YOUR_APP
```

#### Uninstall the Fargate integration [#uninstall-auto-injection]

In order to uninstall the sidecar performing the automatic injection but keep the rest of the New Relic solution, using Helm, disable the infra-operator by setting `infra-operator.enabled` to `false`, either in the `values.yaml` file or in the command line (`--set`), and re-run the installation command above.

We strongly recommend keeping the `--set global.fargate=true` flag, since it does not enable automatic injection but makes other components of the installation Fargate-aware, preventing unwanted behavior.

To uninstall the whole solution:

1. Completely uninstall the Helm release.
2. Rollout the pods in order to remove the sidecar:
    ```shell
    kubectl rollout restart deployment YOUR_APP
    ```
3. Garbage collect the secrets:
    ```shell
    kubectl delete secrets -n YOUR_NAMESPACE -l newrelic/infra-operator-created=true
    ```

#### Known limitations: automatic injection [#known-limitations]

Here are some issues to be aware of when using automatic injection:

1. Currently there is no controller that watches the whole cluster to make sure that secrets that are no longer needed are garbage collected. However, all objects share the same label that you can use to remove all resources, if needed. We inject the label `newrelic/infra-operator-created: true`, which you can use to delete resources with a single command.

2. At the moment, it's not possible to use the injected sidecar to monitor services running in the pod. The sidecar will only monitor Kubernetes itself. However, advanced users might want to exclude these pods from automatic injection and manually inject a customized version of the sidecar with on-host integrations enabled by configuring them and mounting their configurations in the proper place. For help, see this [tutorial](/docs/integrations/kubernetes-integration/link-apps-services/tutorial-monitor-redis-running-kubernetes/).

### Manual injection [#manual]

If you have  any concerns about the automatic injection, you can inject the sidecar manually directly by modifying the manifests of the workloads scheduled that are going to be scheduled on Fargate nodes. Please note that adding the sidecar into deployments scheduled into EC2 nodes may lead into incorrect or duplicate data, especially if those nodes are already being monitored with the `DaemonSet`.

The following objects are required for the sidecar to successfully report data:

* The `ClusterRole` providing the permission needed by the `nri-kubernetes` integration
* A `ClusterRoleBinding` linking the `ClusterRole` and the service account of the pod
* The secret storing the New Relic `licenseKey` in each Fargate namespace
* The sidecar container in the spec template of the monitored workload

#### Manual injection installation [#manual-injection-install]

<Callout variant="tip">
These manual setup steps are for a generic installation. Before completing these, take a look at the [Configuration](#config-manual) section below to see if you want to modify any aspects of the automatic injection.
</Callout>

Complete the following for manual injection:

1. If `ClusterRole` doesn't exist, create it and grant the permissions required to hit the metrics endpoints. This only needs to be done once, even for monitoring multiple applications in the same cluster.

    <CollapserGroup>
        <Collapser
        id="cluster-role"
        title="ClusterRole"
        >
        You can use this snippet as it appears below, without any changes:

        ```yml
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRole
        metadata:
          labels:
            app: newrelic-infrastructure
          name: newrelic-newrelic-infrastructure-infra-agent
        rules:
        - apiGroups:
          - ""
          resources:
          - nodes
          - nodes/metrics
          - nodes/stats
          - nodes/proxy
          - pods
          - services
          verbs:
          - get
          - list
        - nonResourceURLs:
          - /metrics
          verbs:
          - get
        ```
        </Collapser>
    </CollapserGroup>

2. For each workload you want to monitor, add an additional sidecar container for the `newrelic/infrastructure-k8s` image. Here is an example of an injected sidecar.

    <CollapserGroup>
        <Collapser
        id="container-to-inject"
        title="ContainerToInject"
        >
        Take the container of the following snippet and inject it in the workload you want to monitor, specifying the name of your `FargateProfile` in the `customAttributes` variable. Note that the volumes can be defined as `emptyDir: {}`.

        <Callout variant="tip">
        In the special case of a KSM deployment, you also need to remove the `DISABLE_KUBE_STATE_METRICS` environment variable and increase the resources requests and limits.
        </Callout>

        ```yml
        apiVersion: apps/v1
        kind: Deployment
        spec:
        template:
        spec:
        containers:
        - name: newrelic-infrastructure
          env:
          - name: NRIA_LICENSE_KEY
            valueFrom:
              secretKeyRef:
                key: license
                name: newrelic-newrelic-infrastructure-config
          - name: NRIA_VERBOSE
            value: "1"
          - name: DISABLE_KUBE_STATE_METRICS
            value: "true"
          - name: CLUSTER_NAME
            value: testing-injection
          - name: COMPUTE_TYPE
            value: serverless
          - name: NRK8S_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: NRIA_DISPLAY_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: NRIA_CUSTOM_ATTRIBUTES
            value: '{"clusterName":"$(CLUSTER_NAME)", "computeType":"$(COMPUTE_TYPE)", "fargateProfile":"[YOUR FARGATE PROFILE]"}'
          - name: NRIA_PASSTHROUGH_ENVIRONMENT
            value: KUBERNETES_SERVICE_HOST,KUBERNETES_SERVICE_PORT,CLUSTER_NAME,CADVISOR_PORT,NRK8S_NODE_NAME,KUBE_STATE_METRICS_URL,KUBE_STATE_METRICS_POD_LABEL,TIMEOUT,ETCD_TLS_SECRET_NAME,ETCD_TLS_SECRET_NAMESPACE,API_SERVER_SECURE_PORT,KUBE_STATE_METRICS_SCHEME,KUBE_STATE_METRICS_PORT,SCHEDULER_ENDPOINT_URL,ETCD_ENDPOINT_URL,CONTROLLER_MANAGER_ENDPOINT_URL,API_SERVER_ENDPOINT_URL,DISABLE_KUBE_STATE_METRICS,DISCOVERY_CACHE_TTL
          image: newrelic/infrastructure-k8s:2.4.0-unprivileged
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 100M
              cpu: 200m
            requests:
              cpu: 100m
              memory: 50M
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/db/newrelic-infra/data
            name: tmpfs-data
          - mountPath: /var/db/newrelic-infra/user_data
            name: tmpfs-user-data
          - mountPath: /tmp
            name: tmpfs-tmp
          - mountPath: /var/cache/nr-kubernetes
            name: tmpfs-cache
        [...]
        ```
        </Collapser>
    </CollapserGroup>

3. Create a `ClusterRoleBinding`, or add to a previously created one the `ServiceAccount` of the application that is going to be monitored. All the workloads may share the same `ClusterRoleBinding`, but the `ServiceAccount` of each one must be added to it.

    <CollapserGroup>
        <Collapser
        id="cluster-role-binding"
        title="ClusterRoleBinding"
        >
        Create the following `ClusterRoleBinding` that has as subjects the service account of the pods you want to monitor.

        <Callout variant="tip">
        You don't need to repeat the same service account twice. Each time you want to monitor a pod with a service account that isn't included yet, just add it to the list.
        </Callout>

        ```yml
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRoleBinding
        metadata:
          name: newrelic-newrelic-infrastructure-infra-agent
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: newrelic-newrelic-infrastructure-infra-agent
        subjects:
        - kind: ServiceAccount
          name: [INSERT_SERVICE_ACCOUNT_NAME_OF_WORKLOAD]
          namespace: [INSERT_SERVICE_ACCOUNT_NAMESPACE_OF_WORKLOAD]
        ```
        </Collapser>
    </CollapserGroup>


4. Create a secret containing the New Relic <InlinePopover type="licenseKey" />. Each namespace needs its own secret.

    <CollapserGroup>
        <Collapser
        id="secret"
        title="Secret"
        >
        Create the following `Secret` that has a license with the Base64 encoded value of your <InlinePopover type="licenseKey" />. One secret is needed in each namespace where a pod you want to monitor is running.

        ```yml
        apiVersion: v1
        data:
          license: INSERT_YOUR_NEW_RELIC_LICENSE_ENCODED_IN_BASE64
          kind: Secret
        metadata:
          name: newrelic-newrelic-infrastructure-config
          namespace: [INSERT_NAMESPACE_OF_WORKLOAD]
          type: Opaque
        ```
        </Collapser>
    </CollapserGroup>

#### Configuration [#config-manual]

When adding the manifest of the sidecar agent manually, you can use any agent configuration option to configure the agent behavior. For help, see [Infrastructure agent configuration settings](/docs/infrastructure/install-infrastructure-agent/configuration/infrastructure-agent-configuration-settings/).

#### Update to the latest version [#manual-update-version]

To update any of the components, you just need to modify the deployed yaml.

Updating any of the fields of the injected container will cause the pod to be re-created.

<Callout variant="important">
The agent cannot hot load the New Relic <InlinePopover type="licenseKey" />. After updating the secret, you need to rollout the deployments again.
</Callout>

#### Uninstall the Fargate integration [#manual-uninstall]

To remove the injected container and the related resources, you just have to remove the following:

* The sidecar from the workloads that should be no longer monitored.
* All the secrets containing the newrelic license.
* `ClusterRole` and `ClusterRoleBinding` objects.

Notice that removing the sidecar container will cause the pod to be re-created.

## Logging [#fargate-logging]

New Relic logging isn't available on Fargate nodes because of security constraints imposed by AWS, but here are some logging options:
* If you're using Fluentbit for logging, see [Kubernetes plugin for log forwarding](/docs/logs/forward-logs/kubernetes-plugin-log-forwarding/).
* If your log data is already being monitored by AWS FireLens, see [AWS FireLens plugin for log forwarding](/docs/logs/forward-logs/aws-firelens-plugin-log-forwarding/).
* If your log data is already being monitored by Amazon CloudWatch Logs, see [Stream logs using Kinesis Data Firehose](/docs/logs/forward-logs/stream-logs-using-kinesis-data-firehose/).
* See [AWS Lambda for sending CloudWatch logs](/docs/logs/forward-logs/aws-lambda-sending-cloudwatch-logs/).
* See [Three ways to forward logs from Amazon ECS to New Relic](https://newrelic.com/blog/how-to-relic/forward-logs-from-amazon-ecs-to-new-relic).

## Troubleshooting [#troubleshooting]

### DaemonSet replicas are being deployed into Fargate nodes

If you notice that any Infra `DaemonSet` replicas are being scheduled on Fargate nodes, it might be because the `nodeAffinity` rules are not configured properly.

Double-check that the solution was installed with the `global.fargate` option to `true`, either through the command line (`--set global.fargate=true`) or in the `values.yaml` file. If the installation method was not Helm, you’ll need to manually add `nodeAffinity` rules to exclude Fargate nodes.

### Event `FailedScheduling` due to untolerated taint

Remember to add in the `values.yaml` file the `tolerations` described in [Automatic injection installation](#auto-injection-install) if you get the following event while trying to create a pod:

```
LAST SEEN | TYPE | REASON | OBJECT | MESSAGE
:--|:--|:--|:--|:--
3m9s (x2 over 8m10s) | Warning | FailedScheduling | Pod/no-fargate-deploy-cbddd6ccf-8f9x4 | 0/2 nodes are available: 2 node(s) had untolerated taint {eks.amazonaws.com/compute-type: fargate}. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..
```

### Event `FailedScheduling` due to too many pods

Check if there's a Fargate profile with a selector that names the namespace where installation is occurring if you get the following event while trying to create a pod:

```
LAST SEEN | TYPE | REASON | OBJECT | MESSAGE
:--|:--|:--|:--|:--
61s | Warning | FailedScheduling | Pod/newrelic-bundle-newrelic-infra-operator-admission-create-d8ggt | 0/2 nodes are available: 2 Too many pods. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..
```

## View your EKS data [#view-data]

Here's an example of what a Fargate node looks like in the New Relic UI:

<img
title="Screenshot showing the Kubernetes explorer with a Fargate node"
alt="Screenshot showing the Kubernetes explorer with a Fargate node"
src={kubernetesFargateUi}
/>

To view your AWS data:

1. Go to <DoNotTranslate>**[one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > Infrastructure > Kubernetes**</DoNotTranslate> and do one of the following:
    * Select an integration name to view data.
    * Select the Explore data icon to view AWS data.
2. Filter your data using two Fargate tags:
    * `computeType=serverless`
    * `fargateProfile=[name of the Fargate profile to which the workload belongs]`

</TabsPageItem>

<TabsPageItem id="man-helm">

If you want to use Helm to install the integration, you have two options:

1. Our guided install experience, which will provide a Helm command with the required fields pre-populated. This option also allows for installing our integration as plain manifests rather than a Helm release.
2. Manual configuration via the `values.yaml` file. This tab will guide you through how to do that. 

[Helm](https://helm.sh/) is a package manager on top of Kubernetes. It facilitates installation, upgrades, or revision tracking, and it manages dependencies for the services that you install in Kubernetes. If you haven't already, create your free New Relic account below to start monitoring your data today.

    <ButtonLink
      role="button"
      to="https://onenr.io/0Y8wpoYJJQO"
      variant="primary"
    >
      Start the installer
    </ButtonLink>

## Compatibility and requirements [#compatibility]

Make sure [Helm](https://github.com/helm/helm#install) is installed on your machine. Version 3 of the Kubernetes Integration requires Helm version 3.

To install the Kubernetes integration using Helm, you will need your New Relic <InlinePopover type="licenseKey" /> and your Kubernetes cluster's name:

1. Find and copy your <InlinePopover type="licenseKey" />.
2. Choose a display name for your cluster. For example, you could use the output of:

    ```shell
    kubectl config current-context
    ```

<Callout variant="important">
Keep these values somewhere safe, as you will need them later during the installation process.
</Callout>

## Install the Kubernetes integration with Helm [#install-k8-helm]

New Relic has several Helm charts for the different components which offer different features for the platform:

* [`newrelic-infrastructure`](https://github.com/newrelic/nri-kubernetes/tree/main/charts/newrelic-infrastructure): Contains the main Kubernetes integration and the infrastructure agent. This is the core component for the New Relic Kubernetes experience, responsible for reporting most of the data that is surfaced in the Kubernetes Dashboard and the Kubernetes Cluster Explorer.
* [`newrelic-logging`](https://github.com/newrelic/helm-charts/tree/master/charts/newrelic-logging): Provides a DaemonSet with New Relic's [Fluent Bit](https://fluentbit.io/) output [plugin](https://github.com/newrelic/newrelic-fluent-bit-output) to easily forward your logs to [New Relic](/docs/logs/new-relic-logs/get-started/introduction-new-relic-logs).
* [`nri-kube-events`](https://github.com/newrelic/nri-kube-events/tree/main/charts/nri-kube-events): Collects and reports cluster events (such as `kubectl get events`) to New Relic.
* [`newrelic-prometheus-agent`](https://github.com/newrelic/newrelic-prometheus-configurator/tree/main/charts/newrelic-prometheus-agent): New Relic's Prometheus Configurator configures a [Prometheus in agent mode](/docs/infrastructure/prometheus-integrations/install-configure-prometheus-agent/install-prometheus-agent) and use our remote write endpoint to [reports metrics to New Relic](/docs/infrastructure/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/#remote-write).
* [`nri-metadata-injection`](https://github.com/newrelic/k8s-metadata-injection/tree/main/charts/nri-metadata-injection): Sets up a minimal `MutatingAdmissionWebhook` that injects a couple of environment variables in the containers. These contain metadata about the cluster and New Relic installation and will be later picked up by applications instrumented using APM, allowing to [correlate APM and infrastructure data](/docs/integrations/kubernetes-integration/link-your-applications/link-your-applications-kubernetes/).
* [`nri-statsd`](https://github.com/newrelic/helm-charts/tree/master/charts/nri-statsd): New Relic StatsD integration.

Although you can install these components separately, we strongly recommend you use the [`nri-bundle`](https://github.com/newrelic/helm-charts/tree/master/charts/nri-bundle) chart. New Relic provides this chart, which acts as a wrapper or a meta-package for the individual charts mentioned above. The use of this chart enables you these advantages: 

- It provides full control over which components are installed. Each component is installed as a separate [Helm dependency](https://helm.sh/docs/chart_template_guide/subcharts_and_globals/#overriding-values-from-a-parent-chart). You can configure them individually using the parameters mentioned [here](#configure).
- It ensures that their installed versions are compatible with each other.
- It ensures that their configuration values are consistent across the installed charts.

The `nri-bundle` chart is the one that is installed and configured by our [Kubernetes guided install](/docs/kubernetes-pixie/kubernetes-integration/installation/kubernetes-integration-install-configure#guided-install).

### Installing and configuring `nri-bundle` with Helm

1. Ensure you're using the appropriate context in the machine where you will run Helm and `kubectl`:

    You can check the available contexts with:
    
    ```shell
    kubectl config get-contexts
    ```
    
    And switch to the desired context using:
    
    ```shell
    kubectl config use-context _CONTEXT_NAME_
    ```

2. Add the New Relic Helm charts repo:

    ```shell
    helm repo add newrelic https://helm-charts.newrelic.com
    ```

3. Create a file named `values-newrelic.yaml`, which will be used to define your configuration:

    ```yaml
    global:
      licenseKey: _YOUR_NEW_RELIC_LICENSE_KEY_
      cluster: _K8S_CLUSTER_NAME_
    
    newrelic-prometheus-agent:
      # Automatically scrape prometheus metrics for annotated services in the cluster
      # Collecting prometheus metrics for large clusters might impact data usage significantly
      enabled: true
    nri-metadata-injection:
      # Deploy our webhook to link APM and Kubernetes entities
      enabled: true
    nri-kube-events:
      # Report Kubernetes events
      enabled: true
    newrelic-logging:
      # Report logs for containers running in the cluster
      enabled: true
    kube-state-metrics:
      # Deploy kube-state-metrics in the cluster.
      # Set this to true unless it is already deployed.
      enabled: true
    ```

4. Make sure everything is configured properly in the chart by running the following command. Notice that we're specifying `--dry-run` and `--debug`, so nothing will be installed in this step:

    ```shell
    helm upgrade --install newrelic-bundle newrelic/nri-bundle \
    --namespace newrelic --create-namespace \
    -f values-newrelic.yaml \
    --dry-run \
    --debug
    ```
    
    Please notice and adjust the following flags:
    
    * `global.licenseKey=YOUR_NEW_RELIC_LICENSE_KEY`: Must be set to a valid <InlinePopover type="licenseKey" /> for your account.
    * `global.cluster=K8S_CLUSTER_NAME`: It's used to identify the cluster in the New Relic UI, so should be a descriptive value not used by any other Kubernetes cluster configured in your New Relic account.
    * `kube-state-metrics.enabled=true`: Setting this to `true` will automatically install Kube State Metrics (KSM) for you, which is required for our integration to run. You can set this to false if KSM is already present in your cluster, even if it's on a different namespace.
    * `newrelic-prometheus-agent.enabled=true`: Will deploy our Prometheus Agent, which automatically collects data from Prometheus endpoints present in the cluster.
    * `nri-metadata-injection.enabled=true`: Will install our minimal webhook, which adds environment variables that, in turn, allows [linking applications instrumented with New Relic APM to Kubernetes](/docs/kubernetes-pixie/kubernetes-integration/link-your-applications/link-your-applications-kubernetes).

    Our Kubernetes charts have a comprehensive set of flags and tunables that can be edited to better fit your particular needs. Please, check the [Configure the integration](#configure) section below to see what can be changed.

5. Install the Kubernetes integration by running the command without `--debug` and `--dry-run`:

    ```shell
    helm upgrade --install newrelic-bundle newrelic/nri-bundle \
    --namespace newrelic --create-namespace \
    -f values-newrelic.yaml
    ```
    
    <Callout variant="important">
    Make sure you're using Kubernetes version 1.27.x or [a lower version that we support](/docs/kubernetes-pixie/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements).
    </Callout>

6. Check that pods are being deployed and reach a stable state:

    ```shell
    kubectl -n newrelic get pods -w
    ```

You should see:

* `newrelic-nrk8s-ksm` pod.
* `newrelic-nrk8s-kubelet` pod for each node in your cluster.
* `newrelic-nrk8s-control-plane` pod for each master node in your cluster, if any.
* `newrelic-kube-state-metrics` pod, if you included KSM with our installation.
* `newrelic-nri-kube-events` pod, if you enabled Kubernetes events reporting.
* `prometheus-agent` pod, if you enabled the [Prometheus agent](/docs/infrastructure/prometheus-integrations/install-configure-prometheus-agent/install-prometheus-agent) integration.
* `newrelic-newrelic-logging` pod for each node in your cluster, if you enabled the logging integration.

<InstallFeedback />

## Configure the integration [#configure]

Our `nri-bundle` chart. whose installation instructions can be found above, acts as a wrapper or a meta-package for a couple of other charts, which are the ones containing the components for our solution. By offering such a wrapper we can provide a controlled set of our components with versions that we know are compatible with each other, while keeping the component's charts relatively simple.

The [`nri-bundle`](https://github.com/newrelic/helm-charts/tree/master/charts/nri-bundle) chart wraps [multiple individual charts](#install-k8-helm) to gather different telemetry data and send it to New Relic. The bundle allows to selectively enable the desired child charts depending on your needs. To configure each individual component, you must use [Helm's dependency system](https://helm.sh/docs/chart_template_guide/subcharts_and_globals/#overriding-values-from-a-parent-chart), which in short means that the configuration for each child chart must be placed under a separate section (named after each child chart) in the [values-newrelic.yml file](https://helm.sh/docs/chart_template_guide/values_files/). For example, to configure the `newrelic-infrastructure` chart, you would add the following to the `values-newrelic.yaml`:

```yaml
# General settings that apply to all the child charts
global:
  licenseKey: _YOUR_NEW_RELIC_LICENSE_KEY_
  cluster: _K8S_CLUSTER_NAME_

# ... Other settings as shown above

# Specific configuration for the newrelic-infrastructure child chart
newrelic-infrastructure:
  verboseLog: true  # Enable debug logs
  privileged: false  # Install with minimal privileges
  # Other options from https://github.com/newrelic/helm-charts/tree/master/charts/newrelic-infrastructure-v3

# Specific configuration for the newrelic-logging child chart
newrelic-logging:
  fluentBit:
  retryLimit: 10
```

You can also pass child chart options through the command line by prefixing them with the child chart name and replace the nesting by dots:

```
helm upgrade --install newrelic-bundle newrelic/nri-bundle \
--namespace=newrelic \
--set global.licenseKey=_YOUR_NEW_RELIC_LICENSE_KEY_ \
--set global.cluster=_K8S_CLUSTER_NAME_ \
--set newrelic-infrastructure.privileged=false \
--set newrelic-infrastructure.verboseLog=true \
--set newrelic-logging.fluentBit.retryLimit=10
```

The full list of flags you can tweak (such as [scrape-interval](/docs/new-relic-solutions/observability-maturity/operational-efficiency/data-governance-optimize-ingest-guide#k8s-integration)) for each child chart can be found in their respective repositories:

* [`newrelic-infrastructure`](https://github.com/newrelic/nri-kubernetes/tree/main/charts/newrelic-infrastructure)
* Configure debug logs, privilege mode, control plane monitoring, etc.
* [`nri-kube-events`](https://github.com/newrelic/nri-kube-events/tree/main/charts/nri-kube-events)
* [`nri-metadata-injection`](https://github.com/newrelic/k8s-metadata-injection/tree/main/charts/nri-metadata-injection)
* Configure how the webhook for APM linkage is deployed.
* Configure which Prometheus endpoints are scraped.
* [`newrelic-logging`](https://github.com/newrelic/helm-charts/tree/master/charts/newrelic-logging)
* Configure which logs or log attributes are sent to New Relic.

<Callout variant="tip">
When specifying configuration options for the child charts, you must place them under a section named after the chart name in your `values-newrelic.yaml`.
</Callout>

<Callout variant="tip">
To pass child chart options through the command line, you need to prefix them with the child chart name and replace the nesting by dots.
</Callout>

</TabsPageItem>

<TabsPageItem id="gke-autopilot">

There are three ways to install the Kubernetes integration in your GKE Autopilot cluster:

* The New Relic CLI found in our guided install
* A Helm command with pre-populated required values
* A plain manifest with pre-populated required values

<CollapserGroup>
    <Collapser
        id="guided-gke"
        title="Guided install (recommended)"
    >

You can install in GKE autopilot using the same install flow as our [guided install](/docs/kubernetes-pixie/kubernetes-integration/installation/kubernetes-integration-install-configure/#guided-install) tab.


    </Collapser>
    <Collapser
        id="helm-gke"
        title="Manual Helm"
    >

Your complete Helm command should resemble the following:

    ```shell
    KSM_IMAGE_VERSION="v2.10.0" && \
    helm repo add newrelic https://helm-charts.newrelic.com && helm repo update && \
    kubectl create namespace newrelic ; helm upgrade --install newrelic-bundle newrelic/nri-bundle \
    --set global.licenseKey=<Your License Key> \
    --set global.cluster=<Your Cluster Name> \
    --namespace=newrelic \
    --set newrelic-infrastructure.privileged=false \
    --set newrelic-infrastructure.controlPlane.enabled=false \
    --set newrelic-infrastructure.kubelet.config.scheme=http \
    --set newrelic-infrastructure.kubelet.config.port=10255 \
    --set global.lowDataMode=true \
    --set kube-state-metrics.image.tag=${KSM_IMAGE_VERSION} \
    --set kube-state-metrics.enabled=true \
    --set kubeEvents.enabled=true \
    --set newrelic-prometheus-agent.enabled=true \
    --set newrelic-prometheus-agent.lowDataMode=true \
    --set newrelic-prometheus-agent.config.kubernetes.integrations_filter.enabled=false \
    --set newrelic-pixie.enabled=false
    --set logging.enabled=true 
    --set newrelic-logging.lowDataMode=true 
    --set newrelic-logging.fluentBit.linuxMountPath=/var/log  
    --set newrelic-logging.fluentBit.persistence.mode=persistentVolume 
    --set newrelic-logging.fluentBit.persistence.persistentVolume.storageClass=standard-rwx
    ```

[Fluent Bit](https://fluentbit.io/) can use a FileStore volume to prevent data loss or duplicated logs during Fluent Bit pod restarts or redeploys. Using FileStore incurs additional costs, as charged by Google. We recommend consulting your Google Cloud Admin or visit [FileStore docs](https://cloud.google.com/filestore/pricing) for details. Google FileStore API needs to be enabled in your GKE project. Check this [document](https://support.google.com/googleapi/answer/6158841) to know more about enabling APIs. 

If you don't want to use FileStore add the following:

* Set `"newrelic-logging.fluentBit.persistence.mode"` to `"none"`
* Remove `"newrelic-logging.fluentBit.persistence.persistentVolume.storageClass"`

Check our [Helm chart docs](https://github.com/newrelic/helm-charts/tree/master/charts/nri-bundle) for more information.

    </Collapser>
    <Collapser
        id="manifest-gke"
        title="Manifest"
    >

Update your manifest command with the following flags:

* Set `"newrelic-infrastructure.privileged"`, `"newrelic-infrastructure.controlPlane.enabled"` to `false`.
* Add `"newrelic-infrastructure.kubelet.config.scheme":"http"`
* Add `"newrelic-infrastructure.kubelet.config.port":"10255"` 
* Add  `"logging.enabled":"true"`
* Add  `"newrelic-logging.lowDataMode":"true"`
* Add  `"newrelic-logging.fluentBit.linuxMountPath":"/var/log"`
* Add  `"newrelic-logging.fluentBit.persistence.mode":"persistentVolume"`
* Add  `"newrelic-logging.fluentBit.persistence.persistentVolume.storageClass":"standard-rwx"`
* Remove Pixie payload from the command since Pixie isn't currently supported in GKE Autopilot. 

[Fluent Bit](https://fluentbit.io/) can use a FileStore volume to prevent data loss or duplicated logs during Fluent Bit pod restarts or redeploys. Using FileStore incurs additional costs, as charged by Google. We recommend consulting your Google Cloud Admin or visit [FileStore docs](https://cloud.google.com/filestore/pricing) for details. Google FileStore API needs to be enabled in your GKE project. Check this [document](https://support.google.com/googleapi/answer/6158841) to know more about enabling APIs. 

If you don't want to use FileStore add the following:

* Set `"newrelic-logging.fluentBit.persistence.mode"` to `"none"`
* Remove `"newrelic-logging.fluentBit.persistence.persistentVolume.storageClass"`

Here's an command that creates a Manifest file then applies it to the cluster:

```shell
KSM_IMAGE_VERSION="v2.10.0" && \
curl -X POST https://k8s-config-generator.service.newrelic.com/generate -H 'Content-Type: application/json' -d '{"global.cluster":"<Your Cluster Name>","global.namespace":"newrelic","newrelic-infrastructure.privileged":"false","newrelic-infrastructure.controlPlane.enabled":"false","newrelic-infrastructure.kubelet.config.scheme":"http","newrelic-infrastructure.kubelet.config.port":"10255","global.lowDataMode":"true","kube-state-metrics.image.tag":"'${KSM_IMAGE_VERSION}'","kube-state-metrics.enabled":"true","kubeEvents.enabled":"true","newrelic-prometheus-agent.enabled":"true","newrelic-prometheus-agent.lowDataMode":"true","newrelic-prometheus-agent.config.kubernetes.integrations_filter.enabled":"false","logging.enabled":"true", "newrelic-logging.lowDataMode":"true", "newrelic-logging.fluentBit.linuxMountPath":"/var/log", "newrelic-logging.fluentBit.persistence.mode":"persistentVolume", "newrelic-logging.fluentBit.persistence.persistentVolume.storageClass":"standard-rwx","global.licenseKey":"<Your License Key"}'  > newrelic.yaml && (kubectl create namespace newrelic ; kubectl apply -f newrelic.yaml)
```

    </Collapser>
</CollapserGroup>

</TabsPageItem>
</TabsPages>
</Tabs>

## Use your Kubernetes data

Learn more about:

* [Unprivileged and privileged modes](/docs/infrastructure/install-infrastructure-agent/linux-installation/linux-agent-running-modes)
* [Exploring your Kubernetes data in the UI](/docs/kubernetes-pixie/kubernetes-integration/understand-use-data/kubernetes-cluster-explorer)
* [Using your Kubernetes data](/docs/kubernetes-pixie/kubernetes-integration/understand-use-data/find-use-your-kubernetes-data) with queries, in charts, for alerts, etc.

<InstallFeedback />
