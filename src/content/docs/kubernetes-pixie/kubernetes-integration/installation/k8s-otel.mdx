---
title: 'OpenTelemetry for Kubernetes'
tags:
    - Kubernetes integration
    - OpenTelemetry
metaDescription: "Learn how to monitor your Kubernetes Cluster using OpenTelemetry"
freshnessValidatedDate: 2024-07-23
---

<Callout title="preview">
  We're still working on this feature, but we'd love for you to try it out!

  This feature is currently provided as part of a preview program pursuant to our [pre-release policies](/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy).
</Callout>

OpenTelemetry observability for Kubernetes provides complete, open-source setup paired with a top-notch Kubernetes UI that is already compatible with our proprietary Kubernetes instrumentation. Our Kubernetes UIs are designed to be provider agnostic, allowing you to select either OpenTelemetry or New Relic instrumentation based on your needs.

This document outlines the process for monitoring a Kubernetes cluster using OpenTelemetry. It involves the installation of the [`nr-k8s-otel-collector`](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector) Helm chart within the cluster and the deployment of the necessary Collectors to enable first-class observability.

By integrating Kubernetes components into the OpenTelemetry Collector, we can transmit metrics, events, and logs directly to New Relic. These telemetry signals automatically enhance our out-of-the-box experiences such as the [Kubernetes Navigator](/docs/kubernetes-pixie/kubernetes-integration/understand-use-data/kubernetes-cluster-explorer/#navigator-preview), [overview dashboard](/docs/kubernetes-pixie/kubernetes-integration/understand-use-data/kubernetes-cluster-explorer/#cluster-overview-dashboard), [Kubernetes events](/docs/kubernetes-pixie/kubernetes-integration/understand-use-data/kubernetes-cluster-explorer/#browse-your-kubernetes-events), or [Kubernetes APM summary page](/docs/apm/apm-ui-pages/monitoring/kubernetes-summary-page/).


## How it works? [#how-works]

The [`nr-k8s-otel-collector`](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector) Helm chart deploys OpenTelemetry Collectors configured to gather comprehensive telemetry data from your Kubernetes cluster.

<img
  title="K8s OpenTelemetry diagram"
  alt="K8s OpenTelemetry architecture diagram"
  src="/images/otel-k8s-arch.webp"
/>

* **Deamonset Collector**: Deployed on each worker node. It collects metrics from the underlying host, `cAdvisor`, and `Kubelet`, and gathers logs from containers.
* **Deployment Collector**: Deployed on the control plane node. It collects metrics from `kube-state-metrics` and Kubernetes cluster events. This collector is often a single instance managing cluster-wide data.


### OpenTelemetry components in New Relic Kubernetes collectors [#otel-components]

To help you understand how telemetry signals are gathered from the cluster and which components you can reuse in your custom configurations, here's a detailed explanation of the components used in our OpenTelemetry Collectors:


<CollapserGroup>

  <Collapser
    id="OtelComponents-receivers-sources"
    title="OpenTelemetry Components: receivers and sources"
  >

  The OpenTelemetry Collector uses various receivers to gather telemetry data from different sources within the Kubernetes cluster. These receivers are responsible for collecting metrics, logs, and events from the cluster components.

  <table>
    <thead>
      <tr>
        <th>Receiver Name</th>
        <th>Metrics Type</th>
        <th>Components / Sources</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>[OTLP](https://github.com/newrelic/helm-charts/blob/master/charts/nr-k8s-otel-collector/templates/deployment-configmap.yaml) for Deployment configuration</td>
        <td>Telemetry data (HTTP)</td>
        <td>Internal pod networks</td>
        <td>Gathers telemetry data transmitted over HTTP protocol toward defined endpoints.</td>
      </tr>
      <tr>
        <td>[Prometheus/ksm](https://github.com/newrelic/helm-charts/blob/master/charts/nr-k8s-otel-collector/templates/deployment-configmap.yaml) for Deployment configuration</td>
        <td>Cluster state metrics</td>
        <td>`kube-state-metrics` pods</td>
        <td>Scrapes metrics relevant to Kubernetes resource states, such as deployments, pods, and nodes.</td>
      </tr>
      <tr>
        <td>[Prometheus/controlplane](https://github.com/newrelic/helm-charts/blob/master/charts/nr-k8s-otel-collector/templates/deployment-configmap.yaml) for Deployment configuration</td>
        <td>Control plane metrics</td>
        <td>Kubernetes API server, scheduler, controller-manager</td>
        <td>Monitors control plane components for operational metrics, performance, and availability statistics.</td>
      </tr>
      <tr>
        <td>[K8s Events](https://github.com/newrelic/helm-charts/blob/master/charts/nr-k8s-otel-collector/templates/deployment-configmap.yaml) for Deployment configuration</td>
        <td>Infrastructure events</td>
        <td>Kubernetes Event API</td>
        <td>Captures events like pod creations, deletions, scaling activities, and failure events.</td>
      </tr>
      <tr>
        <td>[OTLP](https://github.com/newrelic/helm-charts/blob/master/charts/nr-k8s-otel-collector/templates/daemonset-configmap.yaml) for Deamonset configuration</td>
        <td>Telemetry data</td>
        <td>Internal pod networks</td>
        <td>Retrieves telemetry data from pods and containerized environments within the local network.</td>
      </tr>
      <tr>
        <td>[Prometheus](https://github.com/newrelic/helm-charts/blob/master/charts/nr-k8s-otel-collector/templates/daemonset-configmap.yaml) for Deamonset configuration</td>
        <td>Node-level metrics</td>
        <td>Host system metrics (e.g., node CPU, memory)</td>
        <td>Collects metrics from host and node-level components to analyze resource consumption.</td>
      </tr>
      <tr>
        <td>[K8s Events](https://github.com/newrelic/helm-charts/blob/master/charts/nr-k8s-otel-collector/templates/daemonset-configmap.yaml) for Deamonset configuration</td>
        <td>Infrastructure events</td>
        <td>Kubernetes Event API</td>
        <td>Similar to deployment, capturing Kubernetes events impacting nodes and local resources.</td>
      </tr>
    </tbody>
  </table>

  </Collapser>

  <Collapser
    id="OtelComponents-types-usage"
    title="OpenTelemetry Component types and usage"
  >

  The OpenTelemetry Collector employs various component types to process and transform the collected telemetry data. These components are essential for enriching, filtering, and formatting the data before it is sent to New Relic.


  <table>
    <thead>
      <tr>
        <th>Component type</th>
        <th>Component name</th>
        <th>Purpose</th>
        <th>Common usage</th>
        <th>Scope</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Processors</td>
        <td>`batch`</td>
        <td>Batches and optimizes telemetry data flow.</td>
        <td>Improves throughput and manages resource consumption.</td>
        <td>`Daemonset`, `Deployment`</td>
      </tr>
      <tr>
        <td></td>
        <td>`resource`</td>
        <td>Enriches telemetry data with resource attributes.</td>
        <td>Attaches metadata such as Kubernetes pod identifiers.</td>
        <td>`Daemonset`, `Deployment`</td>
      </tr>
      <tr>
        <td>Exporters</td>
        <td>`prometheus`</td>
        <td>Exports metrics for monitoring using Prometheus.</td>
        <td>Integrates with Prometheus for metrics visualization.</td>
        <td>`Daemonset`, `Deployment`</td>
      </tr>
      <tr>
        <td></td>
        <td>`otlp`</td>
        <td>Sends telemetry data using the OpenTelemetry Protocol.</td>
        <td>Ensures secure and performant data transport.</td>
        <td>`Daemonset`, `Deployment`</td>
      </tr>
      <tr>
        <td>Pipelines</td>
        <td>`metrics`</td>
        <td>Manages ingest, process, and export of metrics data.</td>
        <td>Coordinates data flow across components for metrics.</td>
        <td>`Daemonset, Deployment`</td>
      </tr>
      <tr>
        <td></td>
        <td>`traces`</td>
        <td>Orchestrates flow for trace data collection.</td>
        <td>Optimizes trace handling from ingestion to export.</td>
        <td>`Daemonset, Deployment`</td>
      </tr>
      <tr>
        <td>Connectors</td>
        <td>`k8s_attributes`</td>
        <td>Enriches telemetry data with Kubernetes metadata.</td>
        <td>Adds Kubernetes-related tags and information.</td>
        <td>`Daemonset`, `Deployment`</td>
      </tr>
    </tbody>
  </table>


  </Collapser>

  </CollapserGroup>


## Requirements [#requirements]

To send Kubernetes telemetry data to New Relic, you need an OpenTelemetry Collector. Our New Relic distribution of OpenTelemetry (NRDOT) is configured to automatically monitor your Kubernetes cluster. It deploys all necessary components through our [`nr-k8s-otel-collector`](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector) Helm chart.

If you opt for a different OpenTelemetry Collector, ensure it includes all the key components for comprehensive Kubernetes monitoring:

* [Attributes processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/attributesprocessor)
* [Filter processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/filterprocessor)
* [Filelog receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/filelogreceiver)
* [GroupByAttrs processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/groupbyattrsprocessor)
* [Hostmetrics receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/hostmetricsreceiver)
* [K8sAttributes processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/k8sattributesprocessor)
* [K8sevents receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/k8seventsreceiver)
* [Kubelet receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/kubeletstatsreceiver)
* [MetricsTransform processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/metricstransformprocessor)
* [Prometheus receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/prometheusreceiver)
* [ResourceDetection processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourcedetectionprocessor)
* [Resource processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourceprocessor)
* [Transform processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/transformprocessor)


### Supported Environments [#supported-envs]

Our OpenTelemetry monitoring for Kubernetes provides robust support across various deployment environments. Supported platforms include:

* **Cloud Vendors:**

  * Amazon EKS
  * Microsoft AKS
  * Google GKE
  * Red Hat OpenShift

* **On-Premise Clusters:** We offer support for on-premise Kubernetes clusters.
* **Kubernetes Versions:** Support aligns with the Kubernetes versions currently supported by each vendor, ensuring compatibility and effective monitoring solutions across these environments.

<Callout variant="tip">

  Windows nodes are not supported.

</Callout>


## Install your Kubernetes cluster with OpenTelemetry [#install]

You can monitor your Kubernetes cluster using one of the following methods:

* [Guided installation](#guided-install)
* Manual installation
  * [Using Helm](#manual-helm-install)
  * [Helmless installation](#manual-helmless-install)
  * [Instal using your own OpenTelemetry Collector](#use-own-otel-collector)



<CollapserGroup>

  <Collapser
    id="guided-install"
    title="Guided installation"
  >

  The guided installation is the recommended way to install OpenTelemetry for Kubernetes. It provides a streamlined process that automatically configures the necessary components and settings.

  **To install the Kubernetes for OpenTelemetry:**
  
  1. Log in to your New Relic account.
  2. Go to **left navigation pane > + Integration & Agents** and search for **Kubernetes (OpenTelemetry)**.
  3. `Question for Mikel: I am not seeing the installation steps in the UI, it is referring to the documentation. Is this expected? If yes, then we should remove the steps below and just refer to the documentation?`



  </Collapser>

  <Collapser
    id="manual-helm-install"
    title="Manual install: Helm"
  >

  The manual installation allows you to set up OpenTelemetry for Kubernetes with more control over the configuration. You can choose between Helm, Helmless, or your own OpenTelemetry Collector.

1. Download the [Helm chart values file](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector/values.yaml#L20-L24) adapt it to meet your specific requirements. For more information on the parameters, refer to the [parameters in `value.yaml` file](#).

   * Cluster name and <InlinePopover type="licenseKey"/> are mandatory.

   * Check the entire list of [configuration parameters](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector#values).

2. Install the [Helm chart](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector) together with the values file.

  ```shell
    helm repo add newrelic https://helm-charts.newrelic.com
    helm upgrade nr-k8s-otel-collector newrelic/nr-k8s-otel-collector -f your-custom-values.yaml -n newrelic --create-namespace --install
  ```

3. Ensure the pods have been successfully spun up.

  ```shell
    kubectl get pods -n newrelic --watch
  ```

4. Make sure New Relic is getting the data it needs, including metrics, events, and logs, by running the right queries. For more information, refer to [Introduction to the query builder](/docs/query-your-data/explore-query-data/query-builder/introduction-query-builder/).

  ```sql
    FROM Metric SELECT * WHERE k8s.cluster.name='<CLUSTER_NAME>'
    FROM InfrastructureEvent SELECT * WHERE k8s.cluster.name='<CLUSTER_NAME>'
    FROM Log SELECT * WHERE k8s.cluster.name='<CLUSTER_NAME>'
  ```

{/* 5. If you're using a GKE AutoPilot cluster, it's necessary to apply the following configuration in your `values.yaml` file to ensure compatibility and proper functionality of the OpenTelemetry Collectors.

  ```yaml
    gkeAutopilot: true
  ``` */}

  </Collapser>

  <Collapser
    id="manual-helmless-install"
    title="Manual install: Helmless"
  >

  The Helmless installation provides a way to set up OpenTelemetry for Kubernetes without using Helm. This method is suitable for users who prefer a more hands-on approach or have specific requirements that necessitate manual configuration.

1. Copy the contents of the `nr-k8s-otel-collector`'s [rendered examples directory](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector/examples/k8s/rendered) to your local workspace. This directory contains the Kubernetes manifest files.

2. Update the [`secret.yaml`](https://github.com/newrelic/helm-charts/blob/master/charts/nr-k8s-otel-collector/examples/k8s/rendered/secret.yaml) file in your local `rendered` directory. Replace `<Your Base64 encoded License key>` with your New Relic License Key, encoded in Base64.
  ```yaml
    data:
      licenseKey: <Your Base64 encoded License key>
  ```

    To `Base64` encode your license key:
      * **Linux/macOS:** Use `echo -n "YOUR_LICENSE_KEY" | base64`.
      * **Windows (PowerShell):** Use `[System.Convert]::ToBase64String([System.Text.Encoding]::UTF8.GetBytes("YOUR_LICENSE_KEY"))`.

3. Manually update your cluster name in both [`daemonset-configmap.yaml`](https://github.com/newrelic/helm-charts/blob/master/charts/nr-k8s-otel-collector/examples/k8s/rendered/daemonset-configmap.yaml) and [`deployment-configmap.yaml`](https://github.com/newrelic/helm-charts/blob/master/charts/nr-k8s-otel-collector/examples/k8s/rendered/deployment-configmap.yaml) files within your local `rendered` directory. Locate instances of `k8s.cluster.name` and replace `<cluster_name>` with the desired name for your cluster.

  ```yaml
    - key: k8s.cluster.name
      action: upsert
      value: <cluster_name>
  ```

4. After updating these required fields, create the `newrelic` namespace and deploy the manifests to your cluster using `kubectl`.

  ```bash
    kubectl create namespace newrelic
    kubectl apply -n newrelic -R -f rendered
  ```

<Callout variant="tip">
  
  When deploying without Helm, components deployed via the rendered manifests might include a prefix, such as `example-`. This prefix is typically used in Helm chart templates for dynamic naming based on the Helm release name. If you prefer cleaner naming conventions, you can adjust these prefixes directly in the manifest files before applying them.

</Callout>

  </Collapser>

    <Collapser
    id="use-own-otel-collector"
    title="Manual install using your own OpenTelemetry Collector"
  >

  If you prefer to use your own OpenTelemetry Collector, you can set it up to collect Kubernetes telemetry data. Ensure that your collector includes the necessary components for Kubernetes monitoring, as outlined in the [Requirements](#requirements) section.

  To configure your own OpenTelemetry Collector to send data to New Relic:

  1. If you're deploying your custom collector via our Helm chart, update the `image` settings in your `values.yaml` file to specify your desired OpenTelemetry Collector Contrib image:

    ```yaml
      nr-k8s-otel-collector:
        image:
          repository: otel/opentelemetry-collector-contrib # Example: Using the contrib distro
          tag: "latest" # Or a specific stable version like "0.98.0"
    ```

    If you are deploying your collector manually, ensure you have the `opentelemetry-collector-contrib` binary or container image available.

  2. Provide your custom OpenTelemetry Collector with a configuration file. This file muse define how the collector receives, processes, and exports telemetry data. Ensure your `config.yaml` includes:

    * **Receivers:** Necessary receivers to collect Kubernetes telemetry (for example, `kubeletstats`, `k8sevents`, `hostmetrics`, `prometheus` for `kube-state-metrics` and `cAdvisor`, `filelog` for container logs, and `otlp` for application traces and metrics). Refer to the [Prerequisites](#requirements) section for a list of essential components.
    * **Processors:** Crucial processors for enriching and transforming Kubernetes data for optimal display in New Relic (for example, `k8sattributes`, `resourcedetection`, `metricstransform`, and `batch`).
    * **New Relic OTLP Exporter:** An `otlphttp` exporter configured to send data to your New Relic account. This exporter requires your New Relic License Key as an API key header and the correct OTLP endpoint for your New Relic region.

      ```yaml
        # Example OTLP Exporter for New Relic
        exporters:
          otlphttp/newrelic:
            endpoint: "[https://otlp.nr-data.net:4317](https://otlp.nr-data.net:4317)" # For US accounts. Use "[https://otlp.eu00.nr-data.net:4317](https://otlp.eu00.nr-data.net:4317)" for EU.
            headers:
              api-key: "${env:NEW_RELIC_LICENSE_KEY}" # Recommended: Get license key from an environment variable
      ```

    * **Pipelines:** Define `service` pipelines (for `metrics`, `traces`, and `logs`) that connect your receivers through the necessary processors to the New Relic OTLP exporter.

      ```yaml
        # Example service pipelines
        service:
          pipelines:
            metrics:
              receivers: [kubeletstats, hostmetrics, prometheus, otlp]
              processors: [k8sattributes, resourcedetection, metricstransform, batch]
              exporters: [otlphttp/newrelic]
            traces:
              receivers: [otlp]
              processors: [k8sattributes, resourcedetection, batch]
              exporters: [otlphttp/newrelic]
            logs:
              receivers: [filelog, otlp]
              processors: [k8sattributes, resourcedetection, batch]
              exporters: [otlphttp/newrelic]
      ```

  </Collapser>

  </CollapserGroup>


## Advanced configuration [#advanced-config]

There are several advanced configuration options available for the `nr-k8s-otel-collector` Helm chart. These options allow you to customize the behavior of the OpenTelemetry Collector to better suit your needs. This section provides an overview of some of the key advanced configuration options you can use.

<CollapserGroup>

<Collapser
  id="enable-provider-compatibility"
  title="Enable GKE Autopilot or Red Hat OpenShift Compatibility"
>

  To ensure compatibility with specific Kubernetes environments, you can enable provider-specific configurations. This setting ensures compatibility and proper functionality of the OpenTelemetry Collectors by adapting to the specific constraints of these environments. You can enable this option in your [`values.yaml`]() file:

    ```yaml
      provider: "GKE_AUTOPILOT" # Or "OPEN_SHIFT" if applicable
    ```

</Collapser>

<Collapser
  id="enable-low-data-mode"
  title="Enable Low Data Mode"
>

The `LowDataMode` option is enabled by default to ingest only the metrics required by our Kubernetes UIs. This mode reduces the amount of data collected, focusing on essential metrics for Kubernetes monitoring. To fetch additional metrics, you can add new pipelines and configuring the appropriate receivers and processors in your [`values.yaml`]() file using the `extraConfig` section.

The following is an example of how to add the `cadvisor_version_info` metric to a new pipeline. You can reuse existing receivers or define your own. Processors are added to filter specific metrics and enrich them with Kubernetes attributes.

```yaml
  extraConfig:
    receivers:
    processors:
      filter/keep_cadvisor_version_info:
        metrics:
            metric:
              - name != "cadvisor_version_info" # Exclude all metrics except cadvisor_version_info
    exporters:
    connectors:
    pipelines:
      metrics/additional_metrics:
        receivers:
          - prometheus # This references the prometheus receiver defined above
        processors:
          - filter/keep_cadvisor_version_info
          - resource # Essential for basic resource attributes
          - k8sattributes/ksm # Essential for Kubernetes metadata enrichment
          - cumulativetodelta # Converts cumulative metrics to delta
          - batch # For efficient data sending
        exporters:
          - otlphttp/newrelic
```

For a comprehensive list of available receivers, processors, exporters, and pipelines that you can reuse in your configurations, refer to the [New Relic Helm Charts repository.](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector)

</Collapser>

<Collapser
  id="multiple-accounts"
  title="Send data to multiple New Relic accounts">


To send your Kubernetes telemetry data to multiple New Relic accounts simultaneously, inject your secondary license key(s) into the OpenTelemetry Collector container and configure additional OTLP exporters.

1. To inject your secondary license key(s):

  1. In the `env` section of your `values.yaml` file, add the following environment variable for each secondary license key you want to use:

    ```yaml
        daemonset:
          envs:
            - name: MY_SECONDARY_LICENSE_KEY_VAR # Choose a descriptive environment variable name
              valueFrom:
                secretKeyRef:
                  name: <Your Secret Name> # Name of your Kubernetes Secret
                  key: <Your Secret Key>    # Key within the Secret that holds the license key
        deployment:
          envs:
            - name: MY_SECONDARY_LICENSE_KEY_VAR
              valueFrom:
                secretKeyRef:
                  name: <Your Secret Name>
                  key: <Your Secret Key>
    ```

  2. In the `envForm` section of your `values.yaml` file, add the following environment variable for each secondary license key you want to use:

    ```yaml
        daemonset:
          envsFrom:
            - secretRef:
                name: <Your Secret Name>
        deployment:
          envsFrom:
            - secretRef:
                name: <Your Secret Name>
    ```

  3. To add an `otlphttp` exporter in the `extraConfig` section for each additional account, referencing the injected environment variable:

    ```yaml
        daemonset:
          configMap:
            extraConfig:
              exporters:
                otlphttp/secondAccount: # Unique name for this exporter
                  endpoint: "<Enter your New Relic otel endpoint>" # Adjust for your New Relic region
                  headers:
                    api-key: ${env:MY_SECONDARY_LICENSE_KEY_VAR} # Reference the env var
        deployment:
          configMap:
            extraConfig:
              exporters:
                otlphttp/secondAccount: # Unique name for this exporter
                  endpoint: "<Enter your New Relic otel endpoint>" # Adjust for your New Relic region
                  headers:
                    api-key: ${env:MY_SECONDARY_LICENSE_KEY_VAR} # Reference the env var
        # Important: Add this exporter to the relevant pipelines below
        pipelines:
          metrics:
            exporters:
              - otlphttp/newrelic # Original exporter
              - otlphttp/secondAccount # New exporter
          traces:
            exporters:
              - otlphttp/newrelic
              - otlphttp/secondAccount
          logs:
            exporters:
              - otlphttp/newrelic
              - otlphttp/secondAccount
    ```

    <Callout variant="tip">
      You must also add the `otlphttp/secondAccount` exporter to the relevant `pipelines` (metrics, traces, logs) within your `extraConfig` for both the `daemonset` and `deployment` collectors to ensure data is actually sent through this new exporter.
    </Callout>
  4. After updating your `values.yaml` file, apply the changes to your cluster:

    ```shell
      helm upgrade nr-k8s-otel-collector newrelic/nr-k8s-otel-collector -f your-custom-values.yaml -n newrelic
    ```

</Collapser>

<Collapser
  id="send-via-proxy"
  title="Send data via a proxy"
>

To send your Kubernetes telemetry data through a proxy, you can configure the OpenTelemetry Collector to use an HTTP proxy for outbound connections. This is particularly useful in environments where direct internet access is restricted or monitored.

1. (Option 1): Use the `proxy` section in your `values.yaml` file to the `nr-k8s-otel-collector` chart configuration.

  ```yaml
    proxy: '<Your-proxy-server-URL>' # Example: [http://squid-proxy.squid:3128](http://squid-proxy.squid:3128)
  ```

2. (Option 2): The OpenTelemetry Collector also respects standard proxy environment variables (`HTTP_PROXY`, `HTTPS_PROXY`, `NO_PROXY`). You can inject these directly into the collector pods, optionally pulling values from Kubernetes Secrets for enhanced security. Note that these settings must apply to both the `deployment` and `daemonset` collectors. For more information on OpenTelemetry proxy settings, refer to the [OpenTelemetry Collector documentation](https://opentelemetry.io/docs/collector/configuration/).

  * To inject proxy settings via environment variables in the `env` section of your `values.yaml` file:

    ```yaml
        daemonset:
          envs:
            - name: HTTPS_PROXY
              valueFrom:
                secretKeyRef:
                  name: <Your Secret Name> # Name of your Kubernetes Secret
                  key: <Your Secret Key>    # Key within the Secret that holds the proxy URL
        deployment:
          envs:
            - name: HTTPS_PROXY
              valueFrom:
                secretKeyRef:
                  name: <Your Secret Name>
                  key: <Your Secret Key>
      ```

    * To inject proxy settings via environment variables in the `envFrom` section of your `values.yaml` file:

    ```yaml
      nr-k8s-otel-collector:
        daemonset:
          envsFrom:
            - secretRef:
                name: <Your Secret Name>
        deployment:
          envsFrom:
            - secretRef:
                name: <Your Secret Name>
      ```
  

</Collapser>

<Collapser
  id="custom-helm-config"
  title="Add custom configurations in the Helm chart"
>

The `extraConfig` sections within the `values.yaml` file provide a powerful way to extend the functionality of both the `daemonset` and `deployment` collectors. Depending on your specific requirements, you can choose either collector to apply additional configurations, allowing you to tailor your monitoring experience.

These options offer flexibility for integrating specific settings not included by default. 

{/* For a comprehensive list of receivers, processors, exporters, and pipelines that you can reuse in your configurations, refer to [](#how-works). */}

For more details, always review the `values.yaml` file in the [Helm chart repository](https://github.com/newrelic/helm-charts/blob/master/charts/nr-k8s-otel-collector/values.yaml).


</Collapser>

<Collapser
  id="enable-prom-sd"
  title="Enable service discovery for Prometheus receiver"
  >

  To enable Prometheus service discovery within your Kubernetes cluster, use the `extraConfig` sections in your `deployment` collector's configuration. This allows the OpenTelemetry Collector to automatically discover and scrape metrics from services annotated with `prometheus.io/scrape`.

  Here's an example configuration snippet to set up service discovery using the standard `prometheus.io/scrape` annotation:

```yaml

  extraConfig:
    receivers:
      prometheus/discover:
        config:
          scrape_configs:
            - job_name: "auto-discovered-services"
              scrape_interval: 30s  # Set the scrape interval to 30 seconds
              kubernetes_sd_configs:
                - role: pod
              relabel_configs:
                - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
                  action: keep
                  regex: true
                - source_labels: [__meta_kubernetes_pod_label_app]
                  action: drop
                  regex: kube-state-metrics
                - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
                  action: replace
                  target_label: __address__
                  separator: ;
                  regex: (.+):(?:\d+);(.*)
                  replacement: $1:$2
                - action: replace
                  target_label: job_label
                  replacement: auto-discovery
    processors:
    exporters:
    connectors:
    pipelines:
      metrics/prom_auto_discover:
        receivers:
          - prometheus/discover
        processors:
          - resource/metrics
          - k8sattributes/ksm
          - cumulativetodelta
          - batch
        exporters:
          - otlphttp/newrelic

```

The `prom_auto_discover` pipeline employs several recommended processors to enhance your telemetry data's efficiency and relevance:

  * **resource:** Ensures your metrics data contains essential resource information, adding clarity to your data analysis.
  * **k8sattributes:** Incorporates Kubernetes-specific attributes into your metrics for detailed insights into your cluster's behavior and performance.
  * **cumulativetodelta:** Transforms cumulative metrics into delta metrics for improved tracking of changes over time.
  * **batch:** Processes and exports metrics in batches, optimizing performance during data collection.

These processors work together to refine your data for more precise monitoring and alerting. Customize the settings according to your specific use case to ensure seamless Prometheus service discovery within your Kubernetes environment.

</Collapser>

</CollapserGroup>


## Alerts [#alerts]

You can install essential alert policies by going through the guided installation flow in Integrations & Agents. This automatically sets up alert policy named **Kubernetes (OpenTelemetry) alert policy** in your New Relic account with multiple alert conditions designed for Kubernetes observability.

<img
  title="Kubernetes OpenTelemetry alert policy"
  alt="Kubernetes OpenTelemetry alert policy"
  src="/images/alert-k8s-policy.webp"
/>


## Uninstall your Kubernetes cluster with OpenTelemetry [#uninstall]

To stop monitoring a Kubernetes cluster with OpenTelemetry, run this command:

```shell
    helm uninstall nr-k8s-otel-collector -n newrelic
```

## Reduce data ingest [#reduce-data-ingest]

The `LowDataMode` option is enabled by default to ingest only the metrics required by our Kubernetes UIs.

If you need to cut down even more on data ingestion, increase the scrape interval in the [`nr-k8s-otel-collector` chart values](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector#values) for each deployed component.

## Metrics [#metrics]

* [Metrics - Full list](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector/docs/metrics-full.md)

* [Metrics - `LowDataMode` list](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector/docs/metrics-lowDataMode.md)

## Find and use data [#find]

Check out these documents to learn more on how to find data:

* [Explore your Kubernetes cluster](/docs/kubernetes-pixie/kubernetes-integration/understand-use-data/kubernetes-cluster-explorer/) to know the status of your cluster, from the control plane to nodes and pods.

* [Kubernetes APM summary page](/docs/apm/apm-ui-pages/monitoring/kubernetes-summary-page/) which offers insights into your Kubernetes integration alongside your monitored applications.

## Troubleshooting [#troubleshooting]

Check out the logs of the Collector pod that's experiencing issues. Run this command:

```shell
    kubectl logs <otel-pod-name> -n newrelic
```

You can also set the `verboseLog` parameter to `true` in the [`nr-k8s-otel-collector`](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector#values) Helm chart.

## Common errors [#common-erros]

Check out the [Common errors section](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector#common-errors) in our GitHub repository.

## Support [#support]

If you have issues with the OpenTelemetry observability for Kubernetes:

* Have a look at the [issues section on GitHub](https://github.com/newrelic/helm-charts/issues) for any similar problems or consider opening a new issue.
