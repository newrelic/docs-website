---
title: 'Kubernetes integration: compatibility and requirements'
tags:
  - Integrations
  - Kubernetes integration
  - Get started
metaDescription: Compatibility and requirements of the New Relic Kubernetes integration.
redirects:
  - /docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements
---

[New Relic's Kubernetes integration](/docs/integrations/kubernetes-integration/get-started/introduction-kubernetes-integration) can be [installed directly](/docs/integrations/kubernetes-integration/installation/kubernetes-installation-configuration#install) on a server or VM, or through several [cloud platforms](/docs/integrations/kubernetes-integration/installation/kubernetes-installation-configuration#cloud-platforms), such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration.

<Callout variant="important">
If you are using Openshift, you can also use `kubectl` most of the time but be careful that `kubectl` does not have commands like `oc login` or `oc adm`. You may need to use `oc` instead of `kubectl`.
</Callout>

## Compatibility [#compatibility]

Our Kubernetes is compatible and is continuously tested on the following versions:

<table>
  <thead>
    <tr>
      <th style={{ width: "300px" }}>
      </th>
      <th>
        Versions
      </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>
        Kubernetes cluster
      </td>
      <td>
        1.16 to 1.25
      </td>
    </tr>
  </tbody>
</table>

### Kubernetes Flavors

New Relic's Kubernetes integration is compatible with different flavors. We tested the integration with the following ones:

<table>
  <thead>
    <tr>
      <th style={{ width: "300px" }}>
        Flavor
      </th>
      <th>
        Notes
      </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>
        Minikube
      </td>
      <td>

      </td>
    </tr>
    <tr>
      <td>
        Kind
      </td>
      <td>

      </td>
    </tr>
    <tr>
      <td>
        K3s
      </td>
      <td>

      </td>
    </tr>
    <tr>
      <td>
        Kubeadm
      </td>
      <td>

      </td>
    </tr>
    <tr>
      <td>
        Amazon Elastic Kubernetes Service (EKS)
      </td>
      <td>
      </td>
    </tr>
        <tr>
      <td>
        Amazon Elastic Kubernetes Service Anywhere (EKS-Anywhere)
      </td>
      <td>
      </td>
    </tr>
    <tr>
      <td>
        Amazon Elastic Kubernetes Service on Fargate (EKS-Fargate)
      </td>
      <td>
        [Fargate installation docs](/docs/integrations/kubernetes-integration/installation/install-fargate-integration)
      </td>
    </tr>
    <tr>
      <td>
        Rancher Kubernetes Engine (RKE1)
      </td>
      <td>
        [Extra configuration](/docs/integrations/kubernetes-integration/installation/configure-control-plane-monitoring#rancher) is needed to instrument control plane compoenents
      </td>
    </tr>
    <tr>
      <td>
        Azure Kubernetes Service (AKS)
      </td>
      <td>

      </td>
    </tr>
    <tr>
      <td>
        Google Kubernetes Engine (GKE)
      </td>
      <td>

      </td>
    </tr>
    <tr>
      <td>
        OpenShift
      </td>
      <td>
        Tested with OpenShift 4.10 and lower. Note that 3.x versions are no longer supported.
      </td>
    </tr>
    <tr>
      <td>
        VMware Tanzu
      </td>
      <td>
        Compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10
      </td>
    </tr>
  </tbody>
</table>

Depending on the installation method, the [control plane monitoring](/docs/integrations/kubernetes-integration/installation/configure-control-plane-monitoring) is not available or may need extra configuration.

For example:

* Only API Server metrics are scrapable and available to instrument managed clusters (GKE, EKS, AKS) control plane because no endpoint exposes the needed metrics for etcd, Scheduler and Controller manager.
* To instrument Rancher control plane, since components `/metrics` are not always reachable by default and can't be autodiscovered, some [extra configuration](/docs/integrations/kubernetes-integration/installation/configure-control-plane-monitoring#rancher) is needed.

## Requirements [#reqs]

The New Relic Kubernetes integration has the following requirements:

* A New Relic account. Don't have one? [Sign up for free](https://newrelic.com/signup). No credit card required.
* Linux distribution [compatible with New Relic infrastructure agent](/docs/infrastructure/new-relic-infrastructure/getting-started/compatibility-requirements-new-relic-infrastructure#operating-systems).
* [`kube-state-metrics`](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics) running on the cluster.
* When using CRI-O as the container runtime, the [processes](/attribute-dictionary/?event=ProcessSample) inside containers are not reported. Performance data is collected at the [container](/docs/integrations/kubernetes-integration/understand-use-data/find-use-your-kubernetes-data#container-data) level.

<Callout variant="important">
  `kube-state-metrics` v2 or higher is supported from integration version 3.6.0 or higher.
  <br/>Install the Kubernetes integration up to version 3.5.0 if you are using `kube-state-metrics` 1.9.8 or lower.
  <br/>
  Check the `values.yaml` file if you're updating `kube-state-metrics` from v1.9.8 to v2 or higher because some variables may have changed:

  - [KSM v1.9.8](https://github.com/kubernetes/kube-state-metrics/blob/0f5e4969f24bfa5f7c884f66d9d6fdf6f1921c16/charts/kube-state-metrics/values.yaml)
  - [KSM v2 or higher](https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-state-metrics/values.yaml)
</Callout>

## Upgrading to the new version of KSM

### Changed label selectors
When upgrading to the new version of KSM, you might see an error like this:
```shell
   v1.LabelSelector{MatchLabels:map[string]string{"app.kubernetes.io/instance":"newrelic", "app.kubernetes.io/name":"kube-state-metrics"}, MatchExpressions:[]v1.LabelSelectorRequirement(nil)}: field is immutable
   ```
The label selector in the new version of the chart changed, but the name of the deployment did not. Therefore, in order to upgrade you need to remove the kube-state-metrics deployment, change the release name or re-create the release.

### Deprecated values
On the other hand, you can see the following log when upgrading:
```shell
   warning: skipped value for nri-bundle.kube-state-metrics.collectors: Not a table.
Error: UPGRADE FAILED: template: nri-bundle/charts/kube-state-metrics/templates/role.yaml:18:6: executing "nri-bundle/charts/kube-state-metrics/templates/role.yaml" at <has "certificatesigningrequests" $.Values.collectors>: error calling has: Cannot find has on type map
   ```
This is because of the KSM chart values changed and the map of collectors has been changed to a list of collectors.
The collector list is not needed anymore, and it has been removed from the new values.

If you still have those in your static Helm values, or if you are using the flag --reuse-values you should first remove the following values:
```
collectors:
    certificatesigningrequests: false
    ingresses: false
```

## Container information [#containers]

Our Kubernetes integration is [CRI](https://kubernetes.io/docs/concepts/architecture/cri)-agnostic. It's been specifically tested to be compatible with Containerd, CRI-O, and Docker.

## Install using Helm [#helm-install]

For instructions about how to install our integration using Helm, see [Manual install using Helm](/docs/integrations/kubernetes-integration/installation/install-kubernetes-integration-using-helm#h2-compatibility-and-requirements).
