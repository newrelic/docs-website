---
title: 'Kubernetes integration: compatibility and requirements'
tags:
  - Integrations
  - Kubernetes integration
  - Get started
metaDescription: Compatibility and requirements of the New Relic Kubernetes integration.
redirects:
  - /docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements
  - /docs/monitor-service-running-kubernetes
freshnessValidatedDate: never
---

The [Kubernetes integration](/docs/integrations/kubernetes-integration/get-started/introduction-kubernetes-integration) is compatible with many different platforms including GKE, EKS, AKS, OpenShift, and more. Each has a different compatibility with our integration. You can find more information in this page.

## Requirements [#reqs]

The New Relic Kubernetes integration requires a New Relic account. If you haven't already, create your free New Relic account below to start monitoring your data today.

You'll also need a Linux distribution [compatible with New Relic infrastructure agent](/docs/infrastructure/new-relic-infrastructure/getting-started/compatibility-requirements-new-relic-infrastructure#operating-systems).

<Callout variant="important">
  * `kube-state-metrics` v2 or higher is supported from integration version 3.6.0 or higher.

  * Install the Kubernetes integration up to version 3.5.0 if you're using `kube-state-metrics` 1.9.8 or lower.

  * Check the `values.yaml` file if you're updating `kube-state-metrics` from v1.9.8 to v2 or higher because some variables may have changed.
</Callout>

### Compatibility and requirements for Helm [#req-helm]

* Make sure you've [Helm](https://github.com/helm/helm?tab=readme-ov-file#install) is installed and that the minimum supported version is v3. Version 3 of the Kubernetes integration requires Helm version 3.

* Choose a display name for your cluster. For example, you could use this output:

  ```shell
  kubectl config current-context
  ```

### Compatibility and requirements for Manifest [#req-manifest]

If custom manifests have been used instead of Helm, you will need to first remove the old installation using `kubectl delete -f previous-manifest-file.yml`, and then proceed through the guided installer again. This will generate an updated set of manifests that can be deployed using `kubectl apply -f manifest-file.yml`.

### Compatibility and requirements for Windows [#req-windows]

<Callout title="preview">

  We're still working on this feature, but we'd love for you to try it out!

  This feature is currently provided as part of a preview program pursuant to our [pre-release policies](/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy). Check the [Kubernetes integration installation guide](/install/kubernetes) for the latest instructions.
</Callout>

To monitor Windows nodes with the New Relic Kubernetes integration, your environment must meet the following requirements.

**Node requirements:**

* For Linux node: Your Kubernetes cluster must include at least one Linux node. The core components of the integration are deployed on a Linux node to enable monitoring across the entire cluster.
* For Windows node: We support monitoring nodes running Windows Server LTSC versions 2019 and 2022.

  <Callout variant="important">
    Your cloud provider may support different Windows versions depending on the Kubernetes version your cluster is running. Always confirm that your node's operating system version is supported by your cloud provider.
  </Callout>

Due to limitations in cloud provider offerings and Kubernetes itself, several key installation scenarios are not supported for Windows nodes, including but not limited to:
* Windows nodes running in Red Hat OpenShift clusters.
* Amazon EKS Fargate clusters, as Fargate supports only Linux nodes.
* Google GKE Autopilot clusters, as Autopilot supports only Linux nodes.

## Container runtime [#containers]

Our Kubernetes integration is [CRI](https://kubernetes.io/docs/concepts/architecture/cri)-agnostic. It's been specifically tested to be compatible with Containerd. Note that Dockershim has been removed from the Kubernetes project as of release 1.24. Read the [Dockershim Removal FAQ](https://kubernetes.io/blog/2022/02/17/dockershim-faq/) for further details.

## Compatibility [#compatibility]

<Callout variant="important">
  If you are using Openshift, you can also use `kubectl` most of the time but be careful that `kubectl` does not have commands like `oc login` or `oc adm`. You may need to use `oc` instead of `kubectl`.
</Callout>

Our integration is compatible and is continuously tested on the following Kubernetes versions:

<table>
  <thead>
    <tr>
      <th style={{ width: "300px" }}/>

      <th>
        Versions
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        Kubernetes cluster
      </td>

      <td>
        1.30 to 1.34
      </td>
    </tr>
  </tbody>
</table>

<Callout variant="important">
  Starting from Kubernetes version 1.26, `@autoscaling/v2` has replaced the `@autoscaling/v2beta2` API. For continued `HorizontalPodAutoscaling` metric reporting, you must install `kube-state-metrics` version 2.7+ on the Kubernetes version 1.26+ clusters, because only `kube-state-metrics` v2.7+ can support the `@autoscaling/v2` API.
</Callout>

### Kubernetes Flavors

Kubernetes integration is compatible with different flavors. We tested the integration with the following ones:

<table>
  <thead>
    <tr>
      <th style={{ width: "300px" }}>
        Flavor
      </th>

      <th>
        Notes
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        Minikube
      </td>

      <td/>
    </tr>

    <tr>
      <td>
        Kind
      </td>

      <td/>
    </tr>

    <tr>
      <td>
        K3s
      </td>

      <td/>
    </tr>

    <tr>
      <td>
        Kubeadm
      </td>

      <td/>
    </tr>

    <tr>
      <td>
        Amazon Elastic Kubernetes Service (EKS)
      </td>

      <td/>
    </tr>

    <tr>
      <td>
        Amazon Elastic Kubernetes Service Anywhere (EKS-Anywhere)
      </td>

      <td/>
    </tr>

    <tr>
      <td>
        Amazon Elastic Kubernetes Service on Fargate (EKS-Fargate)
      </td>

      <td>
        [Fargate installation docs](/docs/kubernetes-pixie/kubernetes-integration/installation/kubernetes-eks-fargate)
      </td>
    </tr>

    <tr>
      <td>
        Rancher Kubernetes Engine (RKE1)
      </td>

      <td>
        [Extra configuration](/docs/integrations/kubernetes-integration/installation/configure-control-plane-monitoring#rancher) is needed to instrument control plane components
      </td>
    </tr>

    <tr>
      <td>
        Azure Kubernetes Service (AKS)
      </td>

      <td/>
    </tr>

    <tr>
      <td>
        Google Kubernetes Engine (GKE)
      </td>

      <td>
        Compatible with [standard and autopilot modes](https://cloud.google.com/kubernetes-engine/docs/concepts/choose-cluster-mode).
      </td>
    </tr>

    <tr>
      <td>
        OpenShift
      </td>

      <td>
        Tested with version 4.19
      </td>
    </tr>

    <tr>
      <td>
        VMware Tanzu
      </td>

      <td>
        Compatible with VMware Tanzu (Pivotal Platform) version 2.5 to 2.11, and Ops Manager version 2.5 to 2.10
      </td>
    </tr>
  </tbody>
</table>

Depending on the installation method, the [control plane monitoring](/docs/integrations/kubernetes-integration/installation/configure-control-plane-monitoring) is not available or may need extra configuration.

For example:

* Only API Server metrics are scrapable and available to instrument managed clusters (GKE, EKS, AKS) control plane because no endpoint exposes the needed metrics for etcd, scheduler and controller manager.
* To instrument Rancher control plane, since components `/metrics` are not always reachable by default and can't be autodiscovered, some [extra configuration](/docs/integrations/kubernetes-integration/installation/configure-control-plane-monitoring#rancher) is needed.


## Resource requirements

When deploying the New Relic Kubernetes integration, it is important to allocate appropriate resources to ensure the monitoring components operate efficiently.

The following are the recommended minimum resource requests and limits for each of the components deployed by the [newrelic_infrastructure](https://github.com/newrelic/nri-kubernetes/blob/main/charts/newrelic-infrastructure/values.yaml) chart.

### [Kubelet component](/docs/kubernetes-pixie/kubernetes-integration/get-started/kubernetes-components/#nrk8s-kubelet)

The following containers are included in the Kubelet component pod deployed in each Linux node.

**Kubelet container**
- **CPU**:
  - **Request**: `100m`
- **Memory**:
  - **Request**: `150M`
  - **Limit**: `300M`

**Agent container**
- **CPU**:
  - **Request**: `100m`
- **Memory**:
  - **Request**: `150M`
  - **Limit**: `300M`

### [Kubelet component - Windows](/docs/kubernetes-pixie/kubernetes-integration/get-started/kubernetes-components/#nrk8s-kubelet-windows)

The following containers are included in the Kubelet component pod deployed in each Windows node, when Windows is enabled.

**Kubelet container**
- **CPU**:
- **Request**: `100m`
- **Memory**:
- **Request**: `150M`
- **Limit**: `300M`

**Agent container**
- **CPU**:
- **Request**: `100m`
- **Memory**:
- **Request**: `150M`
- **Limit**: `300M`

### [Kube State Metric component](https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/get-started/kubernetes-components/#nrk8s-ksm)

**KSM container**
- **CPU**:
  - **Request**: `100m`
- **Memory**:
  - **Request**: `150M`
  - **Limit**: `850M`

**Forwarder container**
- **CPU**:
  - **Request**: `100m`
- **Memory**:
  - **Request**: `150M`
  - **Limit**: `850M`

### [Control plane component](https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/get-started/kubernetes-components/#nrk8s-kubelet)

- **CPU**:
  - **Request**: `100m`
- **Memory**:
  - **Request**: `150M`
  - **Limit**: `300M`

**Agent container**
- **CPU**:
  - **Request**: `100m`
- **Memory**:
  - **Request**: `150M`
  - **Limit**: `300M`

The following are the recommended resources requests and limits required by other components deployed as part of the [nri-bundle](https://docs.newrelic.com/docs/kubernetes-pixie/kubernetes-integration/get-started/kubernetes-components/#components)

### [Metadata injection](https://github.com/newrelic/k8s-metadata-injection/tree/main/charts/nri-metadata-injection)

- **CPU**:
  - **Request**: `100m`
- **Memory**:
  - **Request**: `30M`
  - **Limit**: `80M`

### [Logging](https://github.com/newrelic/helm-charts/tree/master/charts/newrelic-logging)

The following containers are included in the New Relic logging pod deployed in each node.

- **CPU**: 
  - **Request**: `250m`
  - **Limit**: `500m`
- **Memory**: 
  - **Request**: `64M`
  - **Limit**: `128M`


### Considerations

- **Cluster Size**: These resource recommendations are for typical cluster sizes. Larger clusters with more nodes and pods may require increased resource allocations to handle the additional data volume.
  
- **Custom Configurations**: If you enable additional features or custom configurations, consider adjusting the resources accordingly.

- **Monitoring and Adjustment**: After deployment, monitor the resource usage of these pods and adjust the requests and limits based on actual usage to optimize performance and cost.

These resource specifications can be adjusted in the `values.yaml` file of the Helm chart used for deploying the New Relic Kubernetes integration. 
By ensuring these resource requirements are met, you can maintain efficient and effective monitoring of your Kubernetes cluster with New Relic.
