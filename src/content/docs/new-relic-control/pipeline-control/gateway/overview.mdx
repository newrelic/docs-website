---
title: Pipeline Control gateway
metaDescription: 'Pipeline Control gateway filters noisy telemetry data, reduces ingestion costs, and improves signal-to-noise ratio using OpenTelemetry-based processing in your infrastructure.'
freshnessValidatedDate: never
---

Pipeline Control gateway is an OpenTelemetry-based data processing pipeline that runs in your infrastructure. It processes telemetry data before sending it to New Relic, giving you control over data costs, signal quality, and data management.

## What problems does Pipeline Control gateway solve?

Organizations are overwhelmed by noisy and irrelevant telemetry data due to lack of visibility and granular control over how data is processed. This makes it difficult to find meaningful insights, manage data effectively, and leads to higher costs and less efficient observability.

### Filter out noisy data

**Problem:** Debug logs, test environment data, and health checks flood your system with irrelevant information, making it hard to find critical issues.

**Solution with gateway:**
- Filter out all DEBUG logs from production environments
- Drop all telemetry from test environments before it leaves your network
- Remove health check logs that generate millions of entries daily
- Filter by log level, environment, service name, or any attribute

**Result:** Improved signal-to-noise ratio makes it easier to identify critical anomalies and trends.

{/* **Source:** JIRA NR-445524 "filter out noise", NR-442990 "boost signal-to-noise ratio" */}

### Reduce data ingestion costs

**Problem:** Your observability bill is $80,000/month, with 70% coming from routine log ingestion. High-volume, low-value data drives costs without providing insights.

**Solution with gateway:**
- Sample 95% of INFO logs while keeping 100% of ERROR and WARN logs
- Drop user-specific metrics for non-paying users (80% of your user base)
- Filter out redundant or unnecessary telemetry at the source
- Manage data at a granular level based on business value

**Result:** Reduce data volume by 85%, cutting your monthly bill from $80,000 to $12,000 while retaining all critical data.

{/* **Source:** JIRA NR-442976 "reducing data ingestion costs and improving performance" */}

### Add context and enrich data

**Problem:** Your microservices use different logging frameworks. Service A logs `level=ERROR`, Service B logs `severity=error`, Service C logs `log_level=3`. You can't create unified dashboards or alerts.

**Solution with gateway:**
- Normalize attribute names: Transform all variants to `severity.text=ERROR`
- Add organizational metadata: `team`, `cost_center`, `region` to all telemetry
- Enrich with business context: Add `business_criticality=HIGH` for checkout endpoints
- Standardize environment names: `env`, `environment`, `deploy_env` → `deployment.environment`

**Result:** One query works across all services. Unified dashboards show accurate cross-service metrics without requiring application code changes.

{/* **Source:** JIRA NR-445524 "add important context as metadata", "filtering, adding/updating/deleting attributes" */}

{/* #### 4. Parse and Structure Unstructured Logs

**Problem:** Your NGINX access logs are unstructured strings: `172.16.0.1 - GET /api/checkout 500 1234ms`. You can't query by status code, duration, or endpoint.

**Solution with Gateway:**
- Parse log formats with regex to extract: `http.method`, `http.path`, `http.status`, `http.duration`, `client.ip`
- Categorize responses: Add `severity=ERROR` when status >= 500
- Structure log fields for querying and analysis
- Convert unstructured text to queryable attributes

**Result:** Rich, queryable structured data. Create alerts on "all 500 errors on checkout taking >1 second." */}

{/* **Source:** JIRA NR-445524 "structure my logs", "restructuring log fields"; NR-473599 "parsing and enrichment" */}

{/* #### 5. Implement Intelligent Sampling

**Problem:** Your distributed system generates 100 million trace spans per day. Storing everything costs $30,000/month, but you miss critical traces when you blindly sample at 5%.

**Solution with Gateway:**
- Keep 100% of traces with errors (unconditional)
- Keep 100% of traces over 5 seconds duration
- Keep 50% of traces hitting critical endpoints (`/payment`, `/checkout`)
- Sample 1% of everything else (routine traffic)

**Result:** Reduce spans from 100M to 8M per day (92% reduction). Cut costs to $2,400/month while preserving every slow trace and every error. */}

{/* **Source:** JIRA NR-445547 "Change sampling rate in probabilistic sampling processor"; NR-453104 "Rate Sampling Rules" */}

{/* #### 6. Compliance and Long-Term Storage

**Problem:** Your application logs contain user emails, IP addresses, and payment information. GDPR and PCI-DSS require you to avoid storing this data, but your developers log it for debugging.

**Solution with Gateway:**
- Detect and redact email addresses: `user@example.com` → `[REDACTED]`
- Mask credit card numbers: `4532-1234-5678-9010` → `4532-****-****-9010`
- Remove API keys and tokens before they reach storage
- Archive logs to S3 for long-term storage and compliance

**Result:** Achieve compliance without modifying application code. Sensitive data never leaves your network.

---

**Source:** JIRA NR-474210 "archive logs for long-term storage, compliance" */}

### Who should use this?

You'll find Pipeline Control gateway useful if you:
- Are overwhelmed by noisy and irrelevant telemetry data
- Need to reduce data ingestion costs
- Want to improve signal-to-noise ratio to find critical issues faster
- Need to add organizational context to telemetry data
- Want to structure unstructured log data for better querying
- Need to comply with data privacy regulations
- Want control over what data leaves your network

## Core concepts

### Telemetry types

Gateway processes four telemetry types independently:

- **Metrics**: Numerical measurements (CPU usage, request rate, memory consumption)
- **Events**: Discrete occurrences (deployments, user signups, errors)
- **Logs**: Text-based records of application activity
- **Traces**: Distributed request flows across microservices (individual spans)

Each type flows through its own pipeline with its own processors.

### Processors

Processors are the building blocks of your pipeline. Each processor type serves a specific purpose:

**Transform processors** modify data using OTTL (OpenTelemetry Transformation Language):
- Add, update, or delete attributes
- Parse strings with regex patterns
- Derive new fields from existing data
- Hash high-cardinality attributes
- Example functions: `set()`, `replace_pattern()`, `delete_key()`, `Hash()`

**Filter processors** drop data using OTTL boolean expressions:
- Drop entire records matching conditions
- Remove specific attributes from records
- Filter based on multiple combined conditions
- Example expressions: `attributes["http.url"] matches ".*health.*"`, `duration < 100000000`

**Sampling processors** reduce data volume intelligently:
  - Set global sampling percentage (for example, sample 10% of all data)
  - Define conditional rules (for example, keep 100% of errors, sample 5% of success)
  - Control sampling by attribute values or patterns

### YAML-based configuration

Gateway configuration uses YAML files with a declarative structure:

```yaml
  version: 2.0.0
  configuration:
    simplified/v1:
      steps:
        transform/Logs:
          description: "Add environment metadata"
          config:
            statements:
              - set(attributes["environment"], "production")
          output:
            - filter/Logs

        filter/Logs:
          description: "Drop debug logs"
          config:
            condition: 'severity.text == "DEBUG"'
          output:
            - probabilistic_sampler/Logs
```

Key characteristics:
- Version declaration: version: `2.0.0` specifies the configuration schema
- Step naming: Format is processortype/TelemetryType (for example, `transform/Logs`, `filter/Metrics`)
- Output chaining: Each step declares its output targets, creating the processing pipeline
- Field naming: Uses camelCase (for example, `globalSamplingPercentage`, not `global_sampling_percentage`)

### Pipeline flow

{/* Every telemetry type follows this flow through the gateway:

1. RECEIVER → Accepts data from agents (OTLP, New Relic proprietary formats)
2. PROCESSORS → Transform, filter, or sample data (you configure these)
3. EXPORTER → Sends processed data to New Relic */}

{/* Example for logs:
receivelogs → transform/Logs → nrprocessor/Logs → probabilistic_sampler/Logs → newrelicexporter/Logs */}

{/* ## Architecture and data flow */}

{/* ### End-to-end telemetry journey

[Your applications]
↓
[Instrumentation: APM Agents, OTLP SDKs, Fluent Bit, etc.]
↓
[Gateway cluster (your network)]
├─ Receiver: Accepts OTLP + New Relic formats
├─ Transform: Parse, enrich, modify
├─ Filter: Drop unwanted data
└─ Sample: Keep X% of data
↓
[New Relic cloud rules (optional)]
└─ Additional NRQL-based filtering
↓
[NRDB storage]
↓
[Query & visualize]

**Key points:**
- Gateway processes data **before it leaves your network**
- Dropped data never reaches New Relic (no ingestion cost, no egress cost)
- Cloud Rules can add additional filtering **after** data reaches New Relic
- Both layers work together but are configured independently

### Internal gateway pipeline structure */}
The gateway organizes data into four independent pipelines: metrics, events, logs, and traces. This isolation ensures that a high volume of logs, for example, does not interfere with the processing or delivery of critical performance traces.

Each pipeline consists of three functional stages:

1. Receivers (Ingress)
Receivers are the entry points for your data. The gateway automatically listens for incoming telemetry from:

- **OpenTelemetry (OTLP):** Standard data from OTel SDKs and collectors.

- **New Relic agents:** Proprietary telemetry agents.

2. Processors (Logic and transformation)
This is where your custom rules are applied. You define how data is handled using three primary processor types:

- **Sample:** Reduce volume through probabilistic or conditional sampling

- **Filter:** Drop specific records or attributes based on conditions.

- **Transform:** Use the OpenTelemetry Transformation Language (OTTL) to parse logs, rename attributes, or enrich data with metadata.

3. Exporters (Egress)
Once data has been processed, filtered, and sampled, the exporter securely transmits the remaining high-value telemetry to the New Relic cloud.

When defining your pipeline in YAML, you will map your processors to specific telemetry types. To keep your configuration organized, we use a standard naming pattern: `processor_type/telemetry_type`.

**Examples:**

- **transform/logs:** Applies transformation logic specifically to log data.

- **filter/metrics:** Applies drop rules specifically to metrics.

- **sampling/traces:** Manages the volume of distributed traces.

**Note:** 
- Unlike cloud rules (which are account-specific), gateway rules apply across your entire organization.
- Processors only affect the telemetry type specified in their name. A filter/Logs rule will never accidentally drop your metrics or traces.

## Configuration methods

### UI-based configuration

The Gateway UI provides a form-based interface for creating rules without writing YAML:

- **Transformation rules:** Add/modify attributes using guided OTTL statement builder
- **Drop rules:** Create NRQL-based filtering rules with condition builders
- **Sample rate rules:** Set global and conditional sampling percentages with sliders

The UI generates YAML configuration in the background and provides real-time preview. See [UI guide](/docs/new-relic-control/pipeline-control/gateway/ui-guide) for detailed instructions.

### YAML configuration

For advanced users or infrastructure-as-code workflows, edit YAML configuration files directly:

- Full control over processor ordering and pipeline structure
- Version control with Git
- Automated deployment via CI/CD
- Access to advanced OTTL functions not exposed in UI

See [YAML overview](/docs/new-relic-control/pipeline-control/gateway/yaml-overview) for configuration reference.

### Configuration deployment

Gateway uses a unified configuration model:

1. **You create** a YAML configuration file defining all processing steps
2. **You deploy** via the UI (upload YAML) to your cluster
3. **Gateway applies** the configuration to all gateway clusters in your organization
4. **All clusters** process data identically using the same rules

**Version management:**
- Each configuration change creates a new version
- View version history and roll back if needed
- "Needs deployment" badge shows pending changes

## Next steps

Choose your path:

- **Visual configuration**: [Use the Gateway UI](/docs/new-relic-control/pipeline-control/gateway/ui-guide)
- **YAML configuration**: [Learn YAML structure](/docs/new-relic-control/pipeline-control/gateway/yaml-overview)
- **See examples**: [Complete configuration examples](/docs/new-relic-control/pipeline-control/gateway/examples)

Or dive into specific processor types:
- [Transform processor](/docs/new-relic-control/pipeline-control/gateway/transform-processor)
- [Filter processor](/docs/new-relic-control/pipeline-control/gateway/filter-processor)
- [Sampling processor](/docs/new-relic-control/pipeline-control/gateway/sampling-processor)