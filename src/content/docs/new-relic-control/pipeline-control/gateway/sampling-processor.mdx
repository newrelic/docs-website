---
title: Sampling processor
metaDescription: 'Use the sampling processor to reduce telemetry data volume with intelligent probabilistic sampling strategies.'
freshnessValidatedDate: never
---

The sampling processor implements probabilistic sampling to reduce data volume while preserving signal. Use it to keep all errors and slow requests while aggressively sampling routine success cases, reducing costs without losing diagnostic value.

## When to use sampling processor

Use the sampling processor when you need to:

- **Keep 100% of errors while sampling success cases**: Preserve all diagnostic data, drop routine traffic
- **Sample high-volume services more aggressively**: Different sampling rates by service tier or importance
- **Preserve slow requests/traces while sampling fast ones**: Keep performance outliers for analysis
- **Apply different sampling rates per environment or service**: Production at 10%, staging at 50%, test at 100%
- **Reduce trace volume from distributed systems**: Tail-based sampling decisions for complete traces

## How sampling works

The sampling processor uses **probabilistic sampling** with conditional rules:

1. **Global sampling percentage**: Default rate applied to all data that doesn't match conditional rules
2. **Conditional sampling rules**: Override the global rate when specific conditions match
3. **Source of randomness**: Consistent field (like `trace_id`) ensures related data is sampled together

**Evaluation order**: Conditional rules are evaluated in the order defined. The first matching rule determines the sampling rate. If no rules match, the global sampling percentage applies.

## Configuration

Add a sampling processor to your pipeline:

```yaml
configuration:
  simplified/v1:
    steps:
      probabilistic_sampler/Logs:
        description: "Keep errors, sample success"
        config:
          globalSamplingPercentage: 10
          conditionalSamplingRules:
            - name: "preserve-errors"
              description: "Keep all error logs"
              samplingPercentage: 100
              sourceOfRandomness: "trace_id"
              condition: 'severity.text == "ERROR" or severity.text == "FATAL"'
        output: ["exportlogs"]
```

**Config fields**:
- `globalSamplingPercentage`: Default sampling rate (0-100) for data that doesn't match conditional rules
- `conditionalSamplingRules`: Array of conditional rules (evaluated in order)
  - `name`: Rule identifier
  - `description`: Human-readable description
  - `samplingPercentage`: Sampling rate for matched data (0-100)
  - `sourceOfRandomness`: Field to use for sampling decision (typically `trace_id`)
  - `condition`: OTTL expression to match telemetry

## Sampling strategies

### Keep valuable data, drop routine traffic

The most common pattern: preserve all diagnostic data (errors, slow requests), aggressively sample routine success cases.

```yaml
probabilistic_sampler/Logs:
  description: "Intelligent log sampling"
  config:
    globalSamplingPercentage: 5  # Sample 5% of everything else
    conditionalSamplingRules:
      - name: "preserve-errors"
        description: "Keep all errors and fatals"
        samplingPercentage: 100
        sourceOfRandomness: "trace_id"
        condition: 'severity.text == "ERROR" or severity.text == "FATAL"'

      - name: "preserve-warnings"
        description: "Keep most warnings"
        samplingPercentage: 50
        sourceOfRandomness: "trace_id"
        condition: 'severity.text == "WARN"'
  output: ["exportlogs"]
```

**Result**: 100% of errors + 50% of warnings + 5% of everything else

### Sample by service tier

Different sampling rates for different service importance:

```yaml
probabilistic_sampler/Traces:
  description: "Service tier sampling"
  config:
    globalSamplingPercentage: 10
    conditionalSamplingRules:
      - name: "critical-services"
        description: "Keep most traces from critical services"
        samplingPercentage: 80
        sourceOfRandomness: "trace_id"
        condition: 'attributes["service.name"] == "checkout" or attributes["service.name"] == "payment"'

      - name: "standard-services"
        description: "Medium sampling for standard services"
        samplingPercentage: 30
        sourceOfRandomness: "trace_id"
        condition: 'attributes["service.tier"] == "standard"'
  output: ["exporttraces"]
```

### Sample by environment

Higher sampling in test environments, lower in production:

```yaml
probabilistic_sampler/Logs:
  description: "Environment-based sampling"
  config:
    globalSamplingPercentage: 10  # Production default
    conditionalSamplingRules:
      - name: "test-environment"
        description: "Keep all test data"
        samplingPercentage: 100
        sourceOfRandomness: "trace_id"
        condition: 'attributes["environment"] == "test"'

      - name: "staging-environment"
        description: "Keep half of staging data"
        samplingPercentage: 50
        sourceOfRandomness: "trace_id"
        condition: 'attributes["environment"] == "staging"'
  output: ["exportlogs"]
```

### Preserve slow requests

Keep performance outliers for analysis:

```yaml
probabilistic_sampler/Traces:
  description: "Preserve slow spans"
  config:
    globalSamplingPercentage: 1  # Sample 1% of fast requests
    conditionalSamplingRules:
      - name: "slow-requests"
        description: "Keep all requests over 1 second"
        samplingPercentage: 100
        sourceOfRandomness: "trace_id"
        condition: 'duration > 1000000000'  # 1 second in nanoseconds

      - name: "medium-requests"
        description: "Keep half of 100ms-1s requests"
        samplingPercentage: 50
        sourceOfRandomness: "trace_id"
        condition: 'duration > 100000000 and duration <= 1000000000'
  output: ["exporttraces"]
```

**Note**: Duration is in nanoseconds (1 second = 1,000,000,000 ns).

## Complete examples

### Example 1: Intelligent trace sampling for distributed tracing

Keep all errors and slow traces, sample routine fast successful requests:

```yaml
probabilistic_sampler/Traces:
  description: "Intelligent distributed trace sampling"
  config:
    globalSamplingPercentage: 1  # Keep 1% of routine traffic
    conditionalSamplingRules:
      # Always keep errors
      - name: "preserve-errors"
        description: "Keep all error spans"
        samplingPercentage: 100
        sourceOfRandomness: "trace_id"
        condition: 'status.code == 2'  # STATUS_CODE_ERROR

      # Keep slow requests (>1s)
      - name: "preserve-slow"
        description: "Keep all slow spans"
        samplingPercentage: 100
        sourceOfRandomness: "trace_id"
        condition: 'duration > 1000000000'

      # Keep medium-speed requests (100ms-1s) at 20%
      - name: "sample-medium"
        description: "Sample medium-speed spans"
        samplingPercentage: 20
        sourceOfRandomness: "trace_id"
        condition: 'duration > 100000000'
  output: ["exporttraces"]
```

**Why this works**: Using `trace_id` as `sourceOfRandomness` ensures all spans in a trace are sampled consistently. If the root span is sampled, all child spans are too.

### Example 2: Log volume reduction

Dramatically reduce log volume while keeping diagnostic data:

```yaml
probabilistic_sampler/Logs:
  description: "Aggressive log sampling, preserve errors"
  config:
    globalSamplingPercentage: 2  # Keep 2% of routine logs
    conditionalSamplingRules:
      - name: "keep-errors-fatals"
        description: "Keep all errors and fatals"
        samplingPercentage: 100
        sourceOfRandomness: "trace_id"
        condition: 'severity.number >= 17'  # ERROR and above

      - name: "keep-some-warnings"
        description: "Keep 25% of warnings"
        samplingPercentage: 25
        sourceOfRandomness: "trace_id"
        condition: 'severity.number >= 13 and severity.number < 17'  # WARN
  output: ["exportlogs"]
```

**Result**: From 1TB/day of logs → ~50GB/day while keeping 100% of errors.

### Example 3: Sample by HTTP status code

Keep failures, sample successes:

```yaml
probabilistic_sampler/Traces:
  description: "Sample by HTTP response status"
  config:
    globalSamplingPercentage: 5  # 5% of successes
    conditionalSamplingRules:
      - name: "keep-server-errors"
        description: "Keep all 5xx errors"
        samplingPercentage: 100
        sourceOfRandomness: "trace_id"
        condition: 'attributes["http.status_code"] >= 500'

      - name: "keep-client-errors"
        description: "Keep all 4xx errors"
        samplingPercentage: 100
        sourceOfRandomness: "trace_id"
        condition: 'attributes["http.status_code"] >= 400 and attributes["http.status_code"] < 500'
  output: ["exporttraces"]
```

### Example 4: Multi-tier service sampling

Different rates for different importance levels:

```yaml
probabilistic_sampler/Logs:
  description: "Business criticality sampling"
  config:
    globalSamplingPercentage: 1
    conditionalSamplingRules:
      # Critical business services: keep 80%
      - name: "critical-services"
        description: "High sampling for critical services"
        samplingPercentage: 80
        sourceOfRandomness: "trace_id"
        condition: 'attributes["business_criticality"] == "critical"'

      # Important services: keep 40%
      - name: "important-services"
        description: "Medium sampling for important services"
        samplingPercentage: 40
        sourceOfRandomness: "trace_id"
        condition: 'attributes["business_criticality"] == "important"'

      # Standard services: keep 10%
      - name: "standard-services"
        description: "Low sampling for standard services"
        samplingPercentage: 10
        sourceOfRandomness: "trace_id"
        condition: 'attributes["business_criticality"] == "standard"'
  output: ["exportlogs"]
```

### Example 5: Time-based sampling (off-peak reduction)

Higher sampling during business hours (requires external attribute tagging):

```yaml
probabilistic_sampler/Logs:
  description: "Time-based sampling (requires time attribute)"
  config:
    globalSamplingPercentage: 5  # Off-peak default
    conditionalSamplingRules:
      - name: "business-hours"
        description: "Higher sampling during business hours"
        samplingPercentage: 50
        sourceOfRandomness: "trace_id"
        condition: 'attributes["is_business_hours"] == true'
  output: ["exportlogs"]
```

**Note**: Requires adding `is_business_hours` attribute via transform processor before sampling.

### Example 6: Sample by endpoint pattern

Keep all admin endpoints, sample public API aggressively:

```yaml
probabilistic_sampler/Traces:
  description: "Endpoint-based sampling"
  config:
    globalSamplingPercentage: 10
    conditionalSamplingRules:
      - name: "admin-endpoints"
        description: "Keep all admin traffic"
        samplingPercentage: 100
        sourceOfRandomness: "trace_id"
        condition: 'attributes["http.path"] matches "/admin/.*"'

      - name: "api-endpoints"
        description: "Sample public API"
        samplingPercentage: 5
        sourceOfRandomness: "trace_id"
        condition: 'attributes["http.path"] matches "/api/.*"'
  output: ["exporttraces"]
```

## Source of randomness

The `sourceOfRandomness` field determines which attribute is used to make consistent sampling decisions.

**Common values**:
- `trace_id`: For distributed traces (ensures all spans in a trace are sampled together)
- `span_id`: For individual span sampling (not recommended for distributed tracing)
- Custom attribute: Any attribute that provides randomness

**Why it matters**: Using `trace_id` ensures that when you sample a trace, you get ALL spans from that trace, not just random individual spans. This is critical for understanding distributed transactions.

## Performance considerations

- **Order rules by frequency**: Put the most frequently matched conditions first to reduce evaluation time
- **Source of randomness performance**: Using `trace_id` is very efficient as it's already available
- **Sampling happens after other processors**: Place sampling near the end of your pipeline to avoid wasting CPU on data that will be dropped

**Efficient pipeline ordering**:
```yaml
steps:
  receivelogs:
    output: ["filter/Logs"]  # Drop unwanted data first

  filter/Logs:
    config:
      logs:
        - 'attributes["environment"] == "test"'
    output: ["transform/Logs"]

  transform/Logs:
    config:
      log_statements:
        - set(attributes["enriched"], "true")
    output: ["probabilistic_sampler/Logs"]  # Sample after enrichment

  probabilistic_sampler/Logs:
    config:
      globalSamplingPercentage: 10
    output: ["exportlogs"]
```

## Cost impact examples

### Example: 1TB/day → 100GB/day

**Before sampling**:
- 1TB of logs per day
- 90% are INFO level routine operations
- 8% are WARN
- 2% are ERROR/FATAL

**With intelligent sampling**:
```yaml
globalSamplingPercentage: 2  # Sample 2% of INFO
conditionalSamplingRules:
  - name: "errors"
    samplingPercentage: 100  # Keep 100% of errors
    condition: 'severity.number >= 17'
  - name: "warnings"
    samplingPercentage: 25  # Keep 25% of warnings
    condition: 'severity.number >= 13 and severity.number < 17'
```

**After sampling**:
- INFO: 900GB × 2% = 18GB
- WARN: 80GB × 25% = 20GB
- ERROR/FATAL: 20GB × 100% = 20GB
- **Total: ~58GB/day (94% reduction)**
- **All errors preserved for troubleshooting**

## OpenTelemetry resources

- [Probabilistic sampler processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/probabilisticsamplerprocessor)
- [OTTL syntax for conditions](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/README.md)

## Next steps

- Learn about [Transform processor](/docs/new-relic-control/pipeline-control/gateway/transform-processor) for data enrichment before sampling
- See [Filter processor](/docs/new-relic-control/pipeline-control/gateway/filter-processor) for dropping unwanted data
- Review [YAML configuration reference](/docs/new-relic-control/pipeline-control/gateway/yaml-overview) for complete syntax