---
title: Pipeline Control Gateway v2.0 documentation
metaDescription: 'Learn how to configure and use Pipeline Control Gateway v2.0 with YAML-based configuration, OpenTelemetry processors, and advanced data transformation capabilities.'
freshnessValidatedDate: never
---

## Overview

### What You Can Do with Pipeline Control Gateway

Pipeline Control Gateway lets you process your telemetry data before it reaches New Relic. Think of it as a smart filter and transformer that sits between your applications and your observability backend.

**Common Use Cases:**

- **Reduce Ingestion Costs**: Drop noisy debug logs, filter out test traffic, or sample high-volume metrics to cut your data ingestion by 50-90%
- **Clean Your Data**: Remove sensitive information, standardize attribute names, or fix inconsistent formatting across services
- **Add Context**: Enrich your telemetry with team names, cost centers, deployment metadata, or environment tags
- **Shape Your Data**: Parse unstructured log messages into structured attributes, aggregate metrics, or normalize schemas

### Who Should Use This?

You'll find Pipeline Control Gateway useful if you:
- Manage observability costs and need to reduce data volume
- Work with logs or metrics from multiple sources with different formats
- Need to add organizational context to telemetry data
- Want to drop unnecessary data before it reaches storage
- Need to comply with data privacy regulations by redacting sensitive information

### Key Capabilities

**Transform Your Data**: Parse log messages with regex, rename attributes in bulk, compute derived metrics, and apply conditional logic—all without custom code.

**Intelligent Sampling**: Keep 100% of errors but only 5% of successful requests. Sample based on response time, environment, user type, or any attribute you choose.

**Visual or Code-Based Authoring**: Build pipelines through a form-based interface or write YAML configurations directly. Choose the approach that fits your workflow.

**Test Before Deploy**: Run sample data through your pipeline configurations before deploying to production. See exactly what your transformations will do to your data.

## Core Concepts

### How Pipelines Work

Think of a pipeline as an assembly line for your telemetry data. Your logs, metrics, or traces flow through a series of stations (called **steps**), and at each station, you can modify, filter, or sample the data.

**Example:** Your application sends 10,000 log entries per minute. You create a pipeline that:
1. First step: Parse the log message to extract HTTP status codes
2. Second step: Drop all debug-level logs in production
3. Third step: Keep 100% of error logs but only 10% of successful requests

Result: You go from 10,000 logs/min to ~1,200 logs/min, saving costs while keeping all the important data.

### Three Building Blocks

**1. Pipelines**

A pipeline processes one type of telemetry data: metrics, logs, traces, or events.

You might create:
- A "production-logs" pipeline for application logs
- A "metrics-cost-reduction" pipeline for high-volume metrics
- A "trace-sampling" pipeline for distributed traces

Each pipeline is independent. Changes to one don't affect others.

**2. Steps**

Steps are the actions you take on your data. Each step happens in order—what comes out of Step 1 flows into Step 2, and so on.

Three types of steps:
- **Transform**: Change your data (add tags, parse strings, rename fields, redact values)
- **Filter**: Drop data you don't need (remove test traffic, eliminate noise)
- **Sample**: Keep only a percentage of data (10% of all logs, 100% of errors)

**3. Rules**

Rules let you apply logic conditionally. They answer questions like:
- "Should I apply this transformation?" → Only if environment = "production"
- "Should I drop this log?" → Only if severity = "DEBUG"
- "What sampling rate should I use?" → 100% for errors, 5% for everything else

Rules use simple expressions: `environment == "production"` or `http_status >= "500"`

### A Concrete Example

Let's say you're processing NGINX access logs that look like this:
```
GET /api/users 200 45ms
POST /api/orders 500 1203ms
GET /health 200 2ms
```

Your pipeline might look like:

**Pipeline: "nginx-access-logs"**
- Type: Logs

**Step 1: Parse Log Format**
- Action: Transform
- What it does: Extract HTTP method, path, status code, and response time from the log message
- Result: Each log now has structured attributes you can query

**Step 2: Remove Health Checks**
- Action: Filter
- Rule: Drop if path = "/health"
- Result: Health check logs are removed (they're noisy and low value)

**Step 3: Smart Sampling**
- Action: Sample
- Rules:
  - Keep 100% if status code >= 500 (errors)
  - Keep 50% if status code >= 400 (client errors)
  - Keep 5% otherwise (successful requests)
- Result: You keep all the important error logs but drastically reduce routine success logs

### How Data Flows

```
Your Application
       ↓
   [Raw Logs]
       ↓
Pipeline Control Gateway
       ↓
   Step 1: Parse → Add structure
       ↓
   Step 2: Filter → Remove /health
       ↓
   Step 3: Sample → Keep errors, sample successes
       ↓
 [Processed Logs]
       ↓
  New Relic Storage
```

Each step sees the output of the previous step. If Step 2 drops a log, Step 3 never sees it.

### What You Control

You control:
- **Which data** gets processed (by creating pipelines for specific signal types)
- **What happens** to the data (by adding transform, filter, and sample steps)
- **When it happens** (by using rules to apply logic conditionally)

You don't need to worry about:
- How the processing happens under the hood
- Where the pipeline runs
- How to scale for high data volumes (handled automatically)

## Configuration Guide

### YAML Schema Structure

PCG v2.0 configurations use a simplified YAML schema organized around the Pipeline/Step/Rule model. The configuration is structured under the `gateway` object and uses the `simplified/v1` configuration source:

```yaml
gateway:
  gateway_version: "2.0.0"
  autoscaling:
    minReplicas: 6
    maxReplicas: 10
    targetCPUUtilizationPercentage: 60
  configuration:
    simplified/v1:
      troubleshooting:
        proxy: false
        requestTraceLogs: false
      steps:
        step-name:
          description: "Step description"
          config:
            type: transform  # or nrprocessor, probabilistic_sampler
            # Processor-specific configuration
          output: ["next-step-name"]
```

**Key Concepts:**

- **gateway.gateway_version**: Specifies the gateway version (e.g., "2.0.0")
- **gateway.autoscaling**: Optional autoscaling configuration for the gateway deployment
- **gateway.configuration.simplified/v1**: The simplified configuration source for v2.0
- **steps**: A map of step names to step configurations
- **step.description**: Human-readable description of what the step does
- **step.config**: Processor-specific configuration including the processor type
- **step.output**: Array of step names that receive the output of this step (defines the pipeline flow)

### Complete Configuration Example

```yaml
gateway:
  gateway_version: "2.0.0"
  autoscaling:
    minReplicas: 6
    maxReplicas: 10
    targetCPUUtilizationPercentage: 60
  configuration:
    simplified/v1:
      troubleshooting:
        proxy: false
        requestTraceLogs: false
      steps:
        # Step 1: Receive metrics
        receive-metrics:
          description: "Receive metrics from OTLP and New Relic proprietary sources"
          output: ["add-environment-tags"]

        # Step 2: Transform - Add environment tags
        add-environment-tags:
          description: "Add environment and region metadata to all metrics"
          config:
            type: transform
            metric_statements:
              - context: datapoint
                statements:
                  - set(attributes["environment"], "production")
                  - set(attributes["region"], "us-east-1")
          output: ["drop-test-metrics"]

        # Step 3: Filter - Drop test metrics
        drop-test-metrics:
          description: "Remove metrics with test prefix"
          config:
            type: nrprocessor
            queries:
              - query:
                  name: drop-test-data
                  value: "DELETE FROM Metric WHERE metric.name LIKE 'test_%'"
                  category: DROP_DATA
          output: ["sample-high-cardinality"]

        # Step 4: Sample high-cardinality data
        sample-high-cardinality:
          description: "Sample 10% of high-cardinality metrics"
          config:
            type: probabilistic_sampler
            global_sampling_percentage: 10.0
            attribute_source: datapoint
          output: ["export-to-newrelic"]

        # Step 5: Export to New Relic
        export-to-newrelic:
          description: "Export processed metrics to New Relic"
          output: []
```

### Configuration Schema Reference

#### Gateway Configuration

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `gateway.gateway_version` | string | Yes | Gateway version (e.g., "2.0.0") |
| `gateway.autoscaling` | object | No | Autoscaling configuration for gateway deployment |
| `gateway.autoscaling.minReplicas` | integer | No | Minimum number of gateway replicas |
| `gateway.autoscaling.maxReplicas` | integer | No | Maximum number of gateway replicas |
| `gateway.autoscaling.targetCPUUtilizationPercentage` | integer | No | Target CPU utilization for autoscaling (default: 60) |
| `gateway.configuration` | object | Yes | Configuration source container |
| `gateway.configuration.simplified/v1` | object | Yes | Simplified configuration for v2.0 |

#### Simplified Configuration

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `troubleshooting` | object | No | Troubleshooting and debugging settings |
| `troubleshooting.proxy` | boolean | No | Enable proxy mode for debugging (default: false) |
| `troubleshooting.requestTraceLogs` | boolean | No | Enable request trace logging (default: false) |
| `steps` | object | Yes | Map of step names to step configurations |

#### Step Configuration

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `description` | string | No | Human-readable description of the step |
| `config` | object | Yes | Processor configuration including type and parameters |
| `config.type` | enum | Yes | Processor type: `transform`, `nrprocessor`, or `probabilistic_sampler` |
| `output` | array[string] | Yes | Array of step names that receive output from this step (empty array for terminal steps) |

### Configuration Management Best Practices

**Version Control**: Store pipeline configurations in Git repositories to enable change tracking, code review, and rollback capabilities.

**Validation**: Use schema validation tools to verify configuration syntax before deployment. Invalid configurations will be rejected at deployment time.

**Modular Design**: Break complex processing logic into multiple steps rather than single monolithic transformations. This improves readability and debugging.

**Environment Parameterization**: Use environment-specific configuration files to manage differences between development, staging, and production pipelines.

**Testing Strategy**: Test pipeline configurations in non-production environments before promoting to production. Use representative sample data to validate transformation logic.

## Transform Processor Reference

### Overview

The transform processor enables powerful data manipulation using OTTL (OpenTelemetry Transformation Language). It supports modifying attribute values, parsing strings with regex, aggregating metrics, renaming fields, and normalizing schemas.

### Supported Use Cases

1. **Attribute Modification**: Add, update, or remove attributes based on conditions
2. **String Parsing**: Extract structured data from unstructured log messages using regex or grok patterns
3. **Metric Aggregation**: Compute derived metrics from existing telemetry data
4. **Attribute Renaming**: Standardize attribute names across different data sources
5. **Schema Normalization**: Transform data to conform to organizational standards

### Configuration Structure

The transform processor configuration is specified within a step's `config` object:

```yaml
steps:
  step-name:
    description: "Transform step description"
    config:
      type: transform
      error_mode: ignore  # or propagate

      # For metrics
      metric_statements:
        - context: resource | datapoint | metric
          statements:
            - <OTTL statement>
            - <OTTL statement>

      # For logs
      log_statements:
        - context: resource | log
          statements:
            - <OTTL statement>
            - <OTTL statement>

      # For traces
      trace_statements:
        - context: resource | span
          statements:
            - <OTTL statement>
            - <OTTL statement>
    output: ["next-step"]
```

### OTTL Context Reference

The `context` determines which telemetry data the transformation operates on:

- **resource**: Operates on resource-level attributes (applies to all signals from that resource)
- **datapoint**: Operates on individual metric data points
- **metric**: Operates on metric metadata
- **log**: Operates on individual log records
- **span**: Operates on individual trace spans

### Transform Processor Examples

#### Example 1: Add Custom Attributes

```yaml
steps:
  enrich-with-metadata:
    description: "Add team and deployment metadata to metrics"
    config:
      type: transform
      metric_statements:
        - context: datapoint
          statements:
            - set(attributes["team"], "platform-engineering")
            - set(attributes["cost_center"], "engineering-ops")
            - set(attributes["deployed_by"], "terraform")
    output: ["next-step"]
```

#### Example 2: Parse Structured Data from Log Messages

```yaml
steps:
  parse-nginx-logs:
    description: "Extract HTTP method, path, status, and response time from NGINX logs"
    config:
      type: transform
      log_statements:
        - context: log
          statements:
            - set(attributes["http_method"], ExtractPatterns(body, "^(\\w+)"))
            - set(attributes["http_path"], ExtractPatterns(body, "^\\w+\\s+([^\\s]+)"))
            - set(attributes["http_status"], ExtractPatterns(body, "\\s(\\d{3})\\s"))
            - set(attributes["response_time_ms"], ExtractPatterns(body, "\\s(\\d+)ms$"))
    output: ["next-step"]
```

#### Example 3: Conditional Attribute Transformation

```yaml
steps:
  normalize-environment-names:
    description: "Normalize environment attribute names across services"
    config:
      type: transform
      metric_statements:
        - context: datapoint
          statements:
            - set(attributes["environment"], "production") where attributes["env"] == "prod"
            - set(attributes["environment"], "staging") where attributes["env"] == "stage"
            - set(attributes["environment"], "development") where attributes["env"] == "dev"
            - delete_key(attributes, "env")
    output: ["next-step"]
```

#### Example 4: Compute Derived Metrics

```yaml
steps:
  calculate-error-rate:
    description: "Calculate error rate from HTTP request metrics"
    config:
      type: transform
      metric_statements:
        - context: datapoint
          statements:
            - set(metric.name, "http.error_rate") where metric.name == "http.requests"
            - set(value, value / 100.0) where attributes["status_code"] >= "400"
    output: ["next-step"]
```

#### Example 5: Rename Attributes for Schema Standardization

```yaml
steps:
  standardize-attribute-names:
    description: "Standardize attribute names to OpenTelemetry semantic conventions"
    config:
      type: transform
      log_statements:
        - context: log
          statements:
            - set(attributes["service.name"], attributes["app_name"])
            - set(attributes["service.version"], attributes["app_version"])
            - set(attributes["deployment.environment"], attributes["environment"])
            - delete_key(attributes, "app_name")
            - delete_key(attributes, "app_version")
    output: ["next-step"]
```

#### Example 6: Truncate High-Cardinality Attributes

```yaml
steps:
  reduce-cardinality:
    description: "Reduce cardinality by truncating and normalizing high-cardinality fields"
    config:
      type: transform
      metric_statements:
        - context: datapoint
          statements:
            - truncate_all(attributes, 1000) where IsMatch(attributes["user_id"], ".*")
            - replace_pattern(attributes["url"], "/users/\\d+/", "/users/{id}/")
    output: ["next-step"]
```

### OTTL Function Reference

The transform processor supports all OTTL editor functions. Commonly used functions include:

| Function | Description | Example |
|----------|-------------|---------|
| `set(target, value)` | Assigns a value to a target field | `set(attributes["env"], "prod")` |
| `delete_key(map, key)` | Removes a key from a map | `delete_key(attributes, "temp_field")` |
| `replace_pattern(target, pattern, replacement)` | Regex-based string replacement | `replace_pattern(body, "\\d{4}", "XXXX")` |
| `ExtractPatterns(source, pattern)` | Extracts first regex match | `ExtractPatterns(body, "^ERROR:\\s+(.+)$")` |
| `truncate_all(map, limit)` | Truncates all string values in a map | `truncate_all(attributes, 500)` |
| `IsMatch(value, pattern)` | Tests if value matches regex | `IsMatch(attributes["host"], "prod-.*")` |
| `Concat(values, delimiter)` | Concatenates string values | `Concat([attributes["app"], attributes["env"]], "-")` |

For the complete OTTL function reference, see: [OpenTelemetry OTTL Editor Functions](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/pkg/ottl/ottlfuncs)

### Error Handling

The `error_mode` configuration controls how the transform processor handles errors:

- **ignore** (default): Logs errors but continues processing subsequent statements and telemetry items
- **propagate**: Stops processing and returns errors to the pipeline, potentially blocking data flow

Use `propagate` mode in development/staging environments to catch configuration errors. Use `ignore` mode in production to ensure processing resilience.

### Performance Considerations

- **Statement Ordering**: Place filter conditions early in statement lists to avoid unnecessary processing
- **Regex Complexity**: Complex regex patterns can impact throughput; test with representative data volumes
- **Context Selection**: Use the most specific context possible (e.g., `datapoint` instead of `resource` when appropriate)
- **Conditional Execution**: Use `where` clauses to limit when transformations execute

## Filter Processor Reference

### Overview

The filter processor (implemented as `nrprocessor`) uses NRQL-like queries to conditionally drop telemetry data or specific attributes based on attribute values, metric names, log content, or other signal properties.

<Callout variant="tip">
  While the OpenTelemetry ecosystem uses OTTL-based filtering, Pipeline Control Gateway implements filtering through `nrprocessor` with familiar NRQL query syntax for compatibility with existing Pipeline Control rules.
</Callout>

### Configuration Structure

```yaml
steps:
  filter-step-name:
    description: "Filter step description"
    config:
      type: nrprocessor
      queries:
        - query:
            name: "rule-name"
            description: "Optional rule description"
            value: "DELETE FROM <DataType> WHERE <condition>"
            category: DROP_DATA  # or DROP_ATTRIBUTE
    output: ["next-step"]
```

**Query Categories:**
- **DROP_DATA**: Removes entire telemetry records that match the WHERE condition
- **DROP_ATTRIBUTE**: Removes specific attributes from matching records while preserving the record

**Supported Data Types:**
- `Metric` - for metric data
- `Log` - for log data
- `Span` - for distributed trace spans
- `LogExtendedRecord` - for extended log records

### Filter Processor Examples

#### Example 1: Drop Test Environment Data

```yaml
steps:
  filter-test-data:
    description: "Drop metrics from test and local environments"
    config:
      type: nrprocessor
      queries:
        - query:
            name: drop-test-environments
            value: "DELETE FROM Metric WHERE environment IN ('test', 'local')"
            category: DROP_DATA
    output: ["next-step"]
```

#### Example 2: Drop High-Cardinality Debug Logs

```yaml
steps:
  filter-debug-logs:
    description: "Drop debug and trace level logs to reduce volume"
    config:
      type: nrprocessor
      queries:
        - query:
            name: drop-debug-logs
            value: "DELETE FROM Log WHERE level = 'DEBUG'"
            category: DROP_DATA
        - query:
            name: drop-trace-logs
            value: "DELETE FROM Log WHERE message LIKE 'TRACE:%'"
            category: DROP_DATA
    output: ["next-step"]
```

#### Example 3: Drop Specific Metric Names

```yaml
steps:
  filter-internal-metrics:
    description: "Drop internal and test metrics"
    config:
      type: nrprocessor
      queries:
        - query:
            name: drop-internal-metrics
            value: "DELETE FROM Metric WHERE metric.name RLIKE '^internal\\..*'"
            category: DROP_DATA
        - query:
            name: drop-test-metrics
            value: "DELETE FROM Metric WHERE metric.name RLIKE '.*\\.test\\..*'"
            category: DROP_DATA
    output: ["next-step"]
```

#### Example 4: Drop Based on Combined Conditions

```yaml
steps:
  filter-low-priority-spans:
    description: "Drop successful fast spans and low-priority internal spans"
    config:
      type: nrprocessor
      queries:
        - query:
            name: drop-fast-success-spans
            value: "DELETE FROM Span WHERE http.status_code = '200' AND duration < 100"
            category: DROP_DATA
        - query:
            name: drop-low-priority-internal
            value: "DELETE FROM Span WHERE span.kind = 'internal' AND priority = 'low'"
            category: DROP_DATA
    output: ["next-step"]
```

#### Example 5: Drop Noisy Error Patterns

```yaml
steps:
  filter-known-errors:
    description: "Drop known benign error messages and empty logs"
    config:
      type: nrprocessor
      queries:
        - query:
            name: drop-connection-reset
            value: "DELETE FROM Log WHERE message LIKE '%Connection reset by peer%'"
            category: DROP_DATA
        - query:
            name: drop-dns-failures
            value: "DELETE FROM Log WHERE message LIKE '%Temporary DNS resolution failure%'"
            category: DROP_DATA
        - query:
            name: drop-empty-logs
            value: "DELETE FROM Log WHERE message = ''"
            category: DROP_DATA
    output: ["next-step"]
```

### NRQL Query Operators

Filter rules use NRQL query syntax with WHERE clause conditions:

| Operator | Description | Example |
|----------|-------------|---------|
| `=` | Equality comparison | `environment = 'test'` |
| `!=` | Inequality comparison | `status_code != 200` |
| `<`, `>`, `<=`, `>=` | Numeric comparison | `duration > 5000` |
| `AND`, `OR` | Logical operators | `level = 'ERROR' AND environment = 'production'` |
| `LIKE` | Pattern matching with wildcards | `message LIKE '%error%'` |
| `RLIKE` | Regex pattern matching | `metric.name RLIKE '^http\\..*'` |
| `IN` | Membership test | `environment IN ('test', 'dev', 'local')` |

For more NRQL functions supported in filter queries, see the [NRQL Functions Reference](/docs/new-relic-control/pipeline-control/nrql-functions) in the current Pipeline Control documentation.

## Conditional Sampling Processor Reference

### Overview

The probabilistic sampler processor implements rate-based sampling with optional conditional sampling rules. This enables intelligent data reduction strategies that preserve high-value telemetry while reducing storage and processing costs.

### Configuration Structure

```yaml
steps:
  sampling-step-name:
    description: "Sampling step description"
    config:
      type: probabilistic_sampler
      global_sampling_percentage: <float>  # 0.0 to 100.0 (default: 100.0)
      hash_seed: <int>                     # Optional: for deterministic sampling
      attribute_source: <enum>             # Optional: trace_id (default), record, datapoint

      # Optional: Conditional sampling rules
      conditional_sampling_rules:
        - name: "rule-name"
          description: "Rule description"
          sampling_percentage: <float>
          condition: "<matching condition>"
          source_of_randomness: <enum>  # Optional: trace_id, record
    output: ["next-step"]
```

### Sampling Strategies

**Uniform Sampling**: All telemetry has equal probability of being sampled:

```yaml
steps:
  uniform-sample-logs:
    description: "Sample 10% of all logs uniformly"
    config:
      type: probabilistic_sampler
      global_sampling_percentage: 10.0  # Keep 10% of logs
    output: ["next-step"]
```

**Conditional Sampling**: Different sampling rates for different data categories:

```yaml
steps:
  priority-based-sampling:
    description: "Sample based on priority attribute"
    config:
      type: probabilistic_sampler
      global_sampling_percentage: 5.0
      conditional_sampling_rules:
        - name: high-priority
          sampling_percentage: 100.0
          condition: "priority = 'high'"
        - name: medium-priority
          sampling_percentage: 25.0
          condition: "priority = 'medium'"
    output: ["next-step"]
```

### Sampling Processor Examples

#### Example 1: Basic Rate-Based Sampling

```yaml
steps:
  sample-high-volume-metrics:
    description: "Keep 1% of high-volume metrics"
    config:
      type: probabilistic_sampler
      global_sampling_percentage: 1.0  # Keep 1% of metrics
      attribute_source: record
    output: ["next-step"]
```

#### Example 2: Preserve Errors, Sample Successes

```yaml
steps:
  error-aware-sampling:
    description: "Preserve all errors, sample 10% of successes"
    config:
      type: probabilistic_sampler
      global_sampling_percentage: 10.0
      conditional_sampling_rules:
        - name: preserve-errors
          sampling_percentage: 100.0
          condition: "attributes[\"http.status_code\"] >= 400"
    output: ["next-step"]
```

#### Example 3: Environment-Based Sampling

```yaml
steps:
  environment-sampling:
    description: "Apply different sampling rates per environment"
    config:
      type: probabilistic_sampler
      global_sampling_percentage: 5.0
      conditional_sampling_rules:
        - name: production-sampling
          sampling_percentage: 50.0
          condition: "attributes[\"environment\"] == \"production\""
        - name: staging-sampling
          sampling_percentage: 25.0
          condition: "attributes[\"environment\"] == \"staging\""
    output: ["next-step"]
```

#### Example 4: Duration-Based Trace Sampling

```yaml
steps:
  duration-based-trace-sampling:
    description: "Sample based on trace duration"
    config:
      type: probabilistic_sampler
      global_sampling_percentage: 2.0
      attribute_source: trace_id
      conditional_sampling_rules:
        - name: slow-traces
          sampling_percentage: 100.0
          condition: "duration > 1000"
        - name: moderate-traces
          sampling_percentage: 50.0
          condition: "duration > 500"
    output: ["next-step"]
```

### Deterministic Sampling

Use `hash_seed` to ensure consistent sampling decisions across distributed systems:

```yaml
steps:
  consistent-sampling:
    description: "Deterministic sampling for distributed traces"
    config:
      type: probabilistic_sampler
      global_sampling_percentage: 10.0
      hash_seed: 42
      attribute_source: trace_id
    output: ["next-step"]
```

This ensures that all spans in a distributed trace receive the same sampling decision based on the trace ID hash.

### Sampling Performance Considerations

- **Attribute Source**: Use `trace_id` for distributed traces to maintain trace completeness
- **Conditional Rules**: Complex conditions in `conditional_sampling_rules` can impact throughput; keep rules simple when possible
- **Hash Seed Consistency**: Ensure all pipeline instances use the same hash seed for deterministic sampling

## Gateway UI Configuration

### Overview

The Pipeline Control Gateway UI provides a form-based interface for creating and managing gateway rules without writing YAML directly. The UI organizes processing logic into three rule categories—Transformations, Drop rules, and Sample rate—each configurable through guided modal dialogs with real-time code preview and validation.

### Key Features

**Rule-Based Configuration**: Create transformation, filtering, and sampling rules through dedicated modal dialogs with structured form inputs.

**Inline Code Preview**: View the generated YAML configuration in real-time as you configure rules through the UI forms.

**Guided Rule Creation**: Configure rules through step-by-step forms with condition builders, statement editors, and inline validation.

**Gateway Visualization**: View your data flow through a Sources → Gateway → Destinations diagram showing how telemetry flows through your configured rules.

**Conditional Logic Builder**: Define conditions to match specific data patterns before applying transformations, drops, or sampling rates.

### UI Organization

The Gateway UI is organized into three main sections:

#### 1. Sample Rate Rules

Configure probabilistic sampling with conditional rules to control what percentage of telemetry data reaches New Relic. Supports:
- Global sampling percentage (0-100%)
- Conditional sampling scenarios based on attribute matching
- Sample rate preview showing estimated data reduction

**Common Use Cases:**
- "Sample 10% of all logs but keep 100% of errors"
- "Sample 1% of high-cardinality metrics"
- "Keep all traces with duration > 5000ms, sample 5% otherwise"

#### 2. Drop Rules

Create NRQL-based rules to conditionally drop entire telemetry records or specific attributes. Supports:
- Pattern matching with LIKE and RLIKE operators
- Attribute-based filtering (environment, metric name, log level)
- Combined conditions with AND/OR logic

**Common Use Cases:**
- "Drop test environment data"
- "Drop debug-level logs in production"
- "Remove health check spans"
- "Drop metrics with test prefix"

#### 3. Transformations

Define OTTL-based transformation rules to modify, enrich, or parse telemetry data. Supports:
- Attribute addition and modification
- String parsing with regex patterns
- Conditional transformations with matching rules
- Multiple OTTL statements per transformation

**Common Use Cases:**
- "Add environment and region tags to all metrics"
- "Parse NGINX log format into structured attributes"
- "Normalize service names across environments"
- "Extract HTTP status codes from log messages"

### Creating Rules in the UI

#### Creating a Transformation Rule

1. **Navigate to Gateway View**: Access the Gateway UI from "Data Management > Pipeline Control Gateway"

2. **Open Transformations Section**: Click on the "Transformations" collapsible section to view existing transformation rules

3. **Click "Create transformation rule"**: Opens the transformation rule creation modal

4. **Configure Basic Information**:
   - **Rule name**: Enter a descriptive name (e.g., "parse-nginx-logs")
   - **Description**: Explain what the transformation does (optional but recommended)

5. **Define Conditions to Match Your Data** (optional):
   - Add conditions to target specific telemetry records
   - Example: `EvaluateConditions("EVAL(LIMIT=")` or attribute-based matching
   - Leave empty to apply transformation to all records

6. **Define OTTL Statements**:
   - Add one or more OTTL transformation statements
   - Example: `set(attributes["parsed_date"], ParseDateTime(body, "%Y-%m-%d"))`
   - Each statement executes in sequence
   - Use the "+ Add a statement" button to add multiple transformations

7. **Preview Generated Code**: View the YAML configuration in the right panel showing how your rule translates to gateway configuration

8. **Validate and Create**: Click "Create rule" to add the transformation to your gateway

**Example Transformation Rule:**
```
Rule name: enrich-with-metadata
Description: Add team and deployment metadata to metrics

Conditions: (empty - applies to all)

OTTL Statements:
- set(attributes["team"], "platform-engineering")
- set(attributes["cost_center"], "engineering-ops")
- set(attributes["deployed_by"], "terraform")
```

#### Creating a Drop Rule

1. **Navigate to Gateway View**: Access the Gateway UI

2. **Open Drop Rules Section**: Click on "Drop rules" to view existing filtering rules

3. **Click "Create drop rule"**: Opens the drop rule creation modal

4. **Configure Basic Information**:
   - **Rule name**: Enter a descriptive name (e.g., "drop-test-metrics")
   - **Description**: Explain what data gets dropped

5. **Define NRQL Query**:
   - Write a DELETE statement with WHERE clause
   - Example: `DELETE FROM Metric WHERE environment = 'test'`
   - Use LIKE for pattern matching: `DELETE FROM Log WHERE message LIKE '%health check%'`
   - Use RLIKE for regex: `DELETE FROM Metric WHERE metric.name RLIKE '^internal\..*'`

6. **Select Category**:
   - **DROP_DATA**: Removes entire telemetry records
   - **DROP_ATTRIBUTE**: Removes specific attributes while preserving records

7. **Preview and Create**: Review generated configuration and click "Create rule"

**Example Drop Rule:**
```
Rule name: drop-debug-logs
Description: Remove debug-level logs from production environment

NRQL Query: DELETE FROM Log WHERE level = 'DEBUG' AND environment = 'production'
Category: DROP_DATA
```

#### Creating a Sample Rate Rule

1. **Navigate to Gateway View**: Access the Gateway UI

2. **Open Sample Rate Section**: Click on "Sample rate" to view sampling configuration

3. **Click "Edit sample rate"**: Opens the sample rate configuration modal

4. **Configure Global Sampling**:
   - Set the default sampling percentage (0-100%)
   - Use the slider or enter a numeric value
   - This applies to all data that doesn't match conditional rules

5. **Add Conditional Scenarios** (optional):
   - Define conditions for different sampling rates
   - Example: "Keep 100% of errors" → `attributes["http.status_code"] >= 400` → 100%
   - Example: "Sample 25% of staging" → `attributes["environment"] == "staging"` → 25%
   - Add multiple scenarios with different conditions and percentages

6. **Preview Impact**: The UI shows estimated data reduction based on your sampling configuration

7. **Save Configuration**: Click "Save changes" to apply sampling rules

**Example Sample Rate Configuration:**
```
Global sampling percentage: 10%

Conditional scenarios:
1. Preserve errors
   Condition: attributes["http.status_code"] >= 400
   Sample rate: 100%

2. Preserve slow requests
   Condition: duration > 5000
   Sample rate: 100%

3. Sample staging moderately
   Condition: attributes["environment"] == "staging"
   Sample rate: 25%
```

### UI Components and Layout

#### Gateway Map View

The main interface displays:

**Sources Section**: Shows configured data sources sending telemetry to the gateway
- OTLP sources
- New Relic proprietary sources
- Custom instrumentation endpoints

**Gateway Section**: Central processing area with three rule categories
- **Sample rate**: Displays configured sampling rules and estimated reduction
- **Drop rules**: Lists all active drop rules with rule counts (e.g., "24 more rules")
- **Transformations**: Shows configured transformation rules organized by type

**Destinations Section**: Shows where processed data flows
- New Relic (primary destination)
- Cloud destinations (if configured)

**Flow Visualization**: A simple diagram showing Sources → Gateway → Destinations data flow

#### Rule Creation Modals

All rule creation dialogs follow a consistent pattern:

**Left Panel - Configuration Form**:
- Rule name and description fields
- Condition builder for matching data
- Statement editor (OTTL for transformations, NRQL for drops)
- Action buttons (Create rule, Cancel, Preview)

**Right Panel - Code Preview**:
- Real-time YAML generation showing resulting configuration
- Syntax-highlighted code display
- Copy to clipboard functionality
- Shows exactly how your form inputs translate to gateway config

#### Navigation and Organization

**Collapsible Sections**: Each rule category (Sample rate, Drop rules, Transformations) can be expanded or collapsed to focus on specific configuration areas

**Rule Counts**: Visual indicators show how many rules exist in each category (e.g., "Sample rules: 3", "Drop rules: 24 more")

**Signal Type Tabs**: Switch between different telemetry types:
- Summary (all signal types)
- Metrics
- Events & Logs
- Traces

### Workflow Examples

#### Example 1: Add Environment Tags to All Metrics

1. Navigate to Gateway UI
2. Expand "Transformations" section
3. Click "Create transformation rule"
4. Enter rule name: "add-environment-tags"
5. Add OTTL statements:
   - `set(attributes["environment"], "production")`
   - `set(attributes["region"], "us-east-1")`
6. View generated YAML in preview panel
7. Click "Create rule"
8. Rule appears in Transformations list

#### Example 2: Drop Test Traffic and Debug Logs

1. Navigate to Gateway UI
2. Expand "Drop rules" section
3. Click "Create drop rule"
4. Enter rule name: "drop-test-and-debug"
5. Enter NRQL: `DELETE FROM Log WHERE environment IN ('test', 'local') OR level = 'DEBUG'`
6. Select category: DROP_DATA
7. Click "Create rule"
8. Rule appears in Drop rules list

#### Example 3: Implement Intelligent Sampling

1. Navigate to Gateway UI
2. Click on "Sample rate" section
3. Click "Edit sample rate"
4. Set global sampling percentage: 5%
5. Add conditional scenario:
   - Name: "preserve-errors"
   - Condition: `attributes["error"] == "true"`
   - Sample rate: 100%
6. Add second scenario:
   - Name: "preserve-slow-traces"
   - Condition: `duration > 3000`
   - Sample rate: 100%
7. Review estimated impact
8. Click "Save changes"

### Configuration Management in the UI

**Rule Modification**: Click on any existing rule to open its configuration dialog in edit mode. Modify settings and save changes.

**Rule Deletion**: Use the delete icon next to rules to remove them from the gateway configuration.

**Rule Ordering**: Rules within each category execute in the order displayed. Use drag handles (if available) to reorder rules.

**Bulk Operations**: Select multiple rules to enable, disable, or delete them together.

**Export to YAML**: Download the complete gateway configuration as a YAML file for version control or manual editing.

**Import from YAML**: Upload existing YAML configurations to populate the UI with pre-configured rules.

### Advanced UI Features

**Condition Builder**: Some dialogs include a visual condition builder for constructing complex matching logic without writing expressions manually.

**Statement Templates**: Pre-built OTTL statement templates for common transformations like:
- Add custom attribute
- Parse log message
- Rename attribute
- Remove sensitive data
- Compute derived metric

**Validation Feedback**: Real-time validation shows errors in OTTL statements or NRQL queries before rule creation:
- Syntax errors highlighted in red
- Suggestions for corrections
- Links to documentation for OTTL functions or NRQL operators

**Rule Impact Preview**: For sampling rules, view estimated data reduction percentages and projected cost savings.

### UI Tips and Best Practices

**Start Simple**: Create one rule at a time and verify it works before adding complexity. Use the Summary view to see all rules across signal types.

**Use Descriptive Names**: Name rules clearly to explain their purpose (e.g., "parse-nginx-access-logs" not "rule1"). Future team members will thank you.

**Document Rule Intent**: Always fill in the description field explaining why a rule exists and what business requirement it addresses.

**Test Conditions Carefully**: Use specific conditions to avoid accidentally dropping or transforming unintended data. Start with narrow conditions and expand as needed.

**Monitor Rule Impact**: After deploying rules, check New Relic dashboards to verify data volume changes match expectations.

**Export for Version Control**: Regularly export your gateway configuration to YAML and commit to Git for backup and change tracking.

**Organize by Purpose**: Group related rules using naming conventions (e.g., prefix with "prod-", "staging-", or by team name).

**Review Rule Counts**: If you see "24 more rules" in a category, consider consolidating or simplifying. Too many rules can make maintenance difficult.

### Limitations and Considerations

**No Visual Pipeline Canvas**: Unlike some data pipeline tools, PCG does not provide a drag-and-drop visual canvas. Configuration is form-based and rule-oriented.

**Rule Execution Order**: Rules within a category execute sequentially. Later rules see data modified by earlier rules. Plan your rule order carefully.

**No Real-Time Testing**: The UI does not provide live "test with sample data" functionality. Test rule changes in a non-production environment first.

**Limited Undo**: Deleted rules cannot be recovered unless you have a YAML backup. Always export configurations before making major changes.

## Reference Links and Additional Resources

### OpenTelemetry Documentation

- [OpenTelemetry Transformation Language (OTTL)](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/pkg/ottl)
- [OTTL Editor Functions](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/pkg/ottl/ottlfuncs)
- [Transform Processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/transformprocessor)
- [Filter Processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/filterprocessor)
- [Probabilistic Sampler](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/probabilisticsamplerprocessor)
- [OpenTelemetry Semantic Conventions](https://opentelemetry.io/docs/specs/semconv/)

## Appendix: Complete Examples

### Example 1: Production-Ready Metrics Pipeline

```yaml
gateway:
  gateway_version: "2.0.0"
  configuration:
    simplified/v1:
      steps:
        # Step 1: Add standard environment tags
        add-environment-metadata:
          description: "Add production environment metadata to metrics"
          config:
            type: transform
            metric_statements:
              - context: datapoint
                statements:
                  - set(attributes["environment"], "production")
                  - set(attributes["region"], resource.attributes["cloud.region"])
                  - set(attributes["team"], "platform-engineering")
          output: ["normalize-metric-names"]

        # Step 2: Normalize metric names
        normalize-metric-names:
          description: "Normalize metric naming conventions"
          config:
            type: transform
            metric_statements:
              - context: metric
                statements:
                  - replace_pattern(metric.name, "^http_", "http.")
                  - replace_pattern(metric.name, "_duration$", ".duration")
          output: ["filter-test-metrics"]

        # Step 3: Drop internal test metrics
        filter-test-metrics:
          description: "Drop test and debug metrics"
          config:
            type: nrprocessor
            queries:
              - query:
                  name: drop-test-metrics
                  value: "DELETE FROM Metric WHERE metricName LIKE 'test.%' OR metricName LIKE 'internal.debug.%' OR environment = 'local'"
                  category: DROP_DATA
          output: ["conditional-sampling"]

        # Step 4: Intelligent sampling
        conditional-sampling:
          description: "Sample metrics with priority-based rules"
          config:
            type: probabilistic_sampler
            global_sampling_percentage: 10.0
            attribute_source: datapoint
            conditional_sampling_rules:
              - name: preserve-errors
                sampling_percentage: 100.0
                condition: "IsMatch(metric.name, \"error\")"
              - name: high-priority
                sampling_percentage: 50.0
                condition: "attributes[\"priority\"] == \"high\""
          output: ["export"]
```

### Example 2: Log Parsing and Enrichment Pipeline

```yaml
gateway:
  gateway_version: "2.0.0"
  configuration:
    simplified/v1:
      steps:
        # Step 1: Parse NGINX access log format
        parse-nginx-format:
          description: "Extract fields from NGINX access logs"
          config:
            type: transform
            log_statements:
              - context: log
                statements:
                  - set(attributes["http.method"], ExtractPatterns(body, "^(\\w+)"))
                  - set(attributes["http.target"], ExtractPatterns(body, "^\\w+\\s+([^\\s]+)"))
                  - set(attributes["http.status_code"], ExtractPatterns(body, "\\s(\\d{3})\\s"))
                  - set(attributes["http.response_size"], ExtractPatterns(body, "\\s(\\d+)\\s"))
                  - set(attributes["http.duration_ms"], ExtractPatterns(body, "(\\d+)ms$"))
          output: ["categorize-responses"]

        # Step 2: Categorize by status code
        categorize-responses:
          description: "Categorize responses by HTTP status code"
          config:
            type: transform
            log_statements:
              - context: log
                statements:
                  - set(attributes["response.category"], "success") where attributes["http.status_code"] >= "200" and attributes["http.status_code"] < "300"
                  - set(attributes["response.category"], "redirect") where attributes["http.status_code"] >= "300" and attributes["http.status_code"] < "400"
                  - set(attributes["response.category"], "client_error") where attributes["http.status_code"] >= "400" and attributes["http.status_code"] < "500"
                  - set(attributes["response.category"], "server_error") where attributes["http.status_code"] >= "500"
          output: ["filter-debug-logs"]

        # Step 3: Drop debug logs in production
        filter-debug-logs:
          description: "Drop debug and empty logs in production"
          config:
            type: nrprocessor
            queries:
              - query:
                  name: drop-debug-logs
                  value: "DELETE FROM Log WHERE (severity_text = 'DEBUG' AND environment = 'production') OR message = ''"
                  category: DROP_DATA
          output: ["sample-success-logs"]

        # Step 4: Sample high-volume success logs
        sample-success-logs:
          description: "Preserve errors, sample successes"
          config:
            type: probabilistic_sampler
            global_sampling_percentage: 5.0
            conditional_sampling_rules:
              - name: preserve-server-errors
                sampling_percentage: 100.0
                condition: "attributes[\"response.category\"] == \"server_error\""
              - name: sample-client-errors
                sampling_percentage: 50.0
                condition: "attributes[\"response.category\"] == \"client_error\""
          output: ["export"]
```

### Example 3: Distributed Trace Processing Pipeline

```yaml
gateway:
  gateway_version: "2.0.0"
  configuration:
    simplified/v1:
      steps:
        # Step 1: Enrich with deployment metadata
        add-deployment-context:
          description: "Add deployment version and environment to spans"
          config:
            type: transform
            trace_statements:
              - context: span
                statements:
                  - set(attributes["deployment.version"], resource.attributes["service.version"])
                  - set(attributes["deployment.environment"], resource.attributes["deployment.environment"])
          output: ["normalize-service-names"]

        # Step 2: Normalize service names
        normalize-service-names:
          description: "Remove environment suffixes from service names"
          config:
            type: transform
            trace_statements:
              - context: span
                statements:
                  - replace_pattern(attributes["service.name"], "-prod$", "")
                  - replace_pattern(attributes["service.name"], "-staging$", "")
          output: ["filter-health-checks"]

        # Step 3: Drop health check spans
        filter-health-checks:
          description: "Drop health check and monitoring spans"
          config:
            type: nrprocessor
            queries:
              - query:
                  name: drop-health-checks
                  value: "DELETE FROM Span WHERE name LIKE '%/health%' OR name LIKE '%/ping%' OR name LIKE '%/ready%'"
                  category: DROP_DATA
          output: ["intelligent-trace-sampling"]

        # Step 4: Sample with error and latency preservation
        intelligent-trace-sampling:
          description: "Sample traces while preserving errors and slow traces"
          config:
            type: probabilistic_sampler
            global_sampling_percentage: 2.0
            attribute_source: trace_id
            hash_seed: 42
            conditional_sampling_rules:
              - name: preserve-errors
                sampling_percentage: 100.0
                condition: "attributes[\"error\"] == \"true\""
              - name: preserve-very-slow
                sampling_percentage: 100.0
                condition: "duration > 5000"
              - name: sample-slow
                sampling_percentage: 25.0
                condition: "duration > 1000"
          output: ["export"]
```
