---
title: "Managing configurations"
metaDescription: "Overview of the Agent Control configuration"
freshnessValidatedDate: never
---
<Callout title="preview">
  We're still working on this feature, but we'd love for you to try it out!

  This feature is currently provided as part of a preview program pursuant to our [pre-release policies](/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy).
</Callout>

<Callout variant="important">
Agent Control currently only supports **Kubernetes**. The configurations and examples provided in this document are specific to this environment.
</Callout>

Agent Control provides two methods for managing agent configurations:

- **Local configuration:** A comprehensive `values.yaml` file used during the initial Helm installation.

- **Remote configuration:** A centralized, YAML-based configuration you create in New Relic Control that is remotely deployed to your entire fleet.

Remote configuration is the recommended method for day-to-day management. It ensures consistent agent behavior across your environment, simplifies change management, and enables you to scale without manually updating local YAML files on each host.

<Callout variant="tip">
The `values-newrelic.yaml` file, which traditionally defined New Relic agent settings, now also includes configuration for Agent Control. The parameters you define in this file determine how both Agent Control and its managed agents operate. This file is referred to as the local configuration.
</Callout>

## Understanding the two layers of configuration
Agent Control's configuration is structured in two layers:

1. **Agent Control's core configuration:** These are the top-level settings that control how Agent Control operates, such as its connection to New Relic, its identity, and fleet management details.

2. **Managed agents' configurations:** These are the individual `chart_values` for each subagent (e.g., Infrastructure Agent, Fluent Bit) that Agent Control deploys and manages.

When a local and remote configuration are both present, Agent Control applies the following logic:

1. Remote configuration takes precedence. Any settings defined in a remote configuration from New Relic Control will override the corresponding settings in the local `values.yaml` file.
2. To intentionally override remote settings with your local configuration, you can deploy an empty remote configuration via New Relic Control. This change will apply to all clusters in the selected fleet.

## Kubernetes configuration

These instructions and examples apply to Agent Control running on a Kubernetes cluster.


### Local `values.yaml` configuration for Kubernetes

The local configuration file for Kubernetes, used during installation, contains all the settings for Agent Control and its managed agents.

This example shows the two layers of configuration within a single file.

<CollapserGroup>
  <Collapser
    id="agent-control-config"
    title="Agent Control configuration"
  >

    ```yaml
    # Layer 1: Agent Control's Core Configuration
    global:
      cluster: "YOUR_CLUSTER_NAME"
      licenseKey: "YOUR_LICENSE_KEY"
      userKey: "YOUR_USER_KEY"

    # Values related to the agent control's Helm chart release.
    # `https://github.com/newrelic/helm-charts/blob/master/charts/agent-control/values.yaml`
    agent-control-deployment:
      identityClientId: ""
      identityClientSecret: ""
      config:
        fleet_control:
          # Optional: Specify a fleet_id (entity guid) to automatically connect to an existing fleet.
          fleet_id: ""
          auth:
            # New Relic organization ID where agent will connect to.
            organizationId: "YOUR_ORGANIZATION_ID"

        # Layer 2: Managed Agents' Configurations
        # List of managed agents that will be deployed. The key represents the name of the agent and the value holds the configuration.
        subAgents:
          infrastructure:
            type: newrelic/com.newrelic.infrastructure:0.1.0
            content:
              # Ref: `https://github.com/newrelic/helm-charts/tree/master/charts/nri-bundle`
              # Recommended: check and define an explicit chart version (latest stable)
              chart_version: "*"
              chart_values:
                newrelic-infrastructure:
                enableProcessMetrics: true
          logs:
            type: newrelic/io.fluentbit:0.1.0
            content:
              # Ref: `https://github.com/newrelic/helm-charts/tree/master/charts/newrelic-logging`
              # Recommended: check and define an explicit chart version (latest stable)
              chart_version: "*"
              chart_values:
                newrelic-logging:
                  sendMetrics: true
          agent-operator:
            type: com.newrelic.k8s_agent_operator:0.1.0
            content:
              chart_version: "*"
    ```

  </Collapser>

</CollapserGroup>

The sample demonstrates how to configure Agent Control along with two managed agents: the Kubernetes infrastructure agent and Fluent Bit for log forwarding. For example, if you don't want to send health metrics for your Fluent Bit log collector, simply set `sendMetrics: false` in the YAML file before running the install command.

### Remote configuration for Kubernetes

Remote configuration ensures consistent agent behavior across your environment, simplifies change management, and enables you to scale observability without manually managing local YAML files.

To deploy configurations centrally across clusters, define this same YAML content in the **Configurations** section of [Fleet Control](/docs/new-relic-control/fleet-control/overview). You can then apply the configuration to an entire fleet of clusters as part of a remote deployment. This is referred to as the **remote configuration** file. 

<Callout variant="warning">
When you define a configuration in the New Relic Control UI, the YAML structure is different. You only provide the YAML that corresponds to the `content` block for a single agent.
</Callout>


## Sample configurations: Agent Control on Kubernetes
The following examples show how to configure Agent Control to manage different sets of agents. These configurations can be used either during initial installation or as part of a remote configuration in Fleet Control.

To explore all available configuration settings, refer to [`values-newrelic.yaml`](https://github.com/newrelic/helm-charts/blob/master/charts/agent-control/values.yaml).

The following examples show how to configure Agent Control with a set of subagents using the local `values.yaml` file.

### Agent Control with New Relic infrastructure and Fluent Bit
This example deploys Agent Control with infrastructure monitoring and Fluent Bit for log collection.

<CollapserGroup>
  <Collapser
        id="agent-control-config"
        title="Local config for infrastructure and Fluent Bit"
      >
    ```yaml
    global:
      cluster: "YOUR_CLUSTER_NAME"
      licenseKey: "YOUR_LICENSE_KEY"
      userKey: "YOUR_USER_KEY"

    # See `https://github.com/newrelic/helm-charts/blob/master/charts/agent-control/values.yaml`
    agent-control-deployment:
      identityClientId: ""
      identityClientSecret: ""
      config:
        fleet_control:
          # Optional
          # fleet_id: YOUR_FLEET_ENTITY_GUID
          auth:
            organizationId: "YOUR_ORGANIZATION_ID"
        subAgents:
          infrastructure:
            type: newrelic/com.newrelic.infrastructure:0.1.0
            content:
              # Ref: `https://github.com/newrelic/helm-charts/tree/master/charts/nri-bundle`
              # Recommended: check and define an explicit chart version (latest stable)
              chart_version: "*"

              #chart_values:
              #  newrelic-infrastructure:
              #    enableProcessMetrics: true
          logs:
            type: newrelic/io.fluentbit:0.1.0
            content:
              # Ref: `https://github.com/newrelic/helm-charts/tree/master/charts/newrelic-logging`
              # Recommended: check and define an explicit chart version (latest stable)
              chart_version: "*"

              #chart_values:
              #  newrelic-logging:
              #    sendMetrics: true
          agent-operator:
            type: com.newrelic.k8s_agent_operator:0.1.0
            chart_version: "*"
    ```
  </Collapser>

</CollapserGroup>


### Agent Control with OpenTelemetry and custom collector settings
This example deploys Agent Control with the New Relic distribution of OpenTelemetry (NRDOT) collector and disables the `filelog` receiver in the managed [`nr-k8s-otel-collector` Helm chart](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector#values).

<Callout variant="important">
Security Best Practice: Do not store sensitive values like your license key directly in the configuration. We recommend using a Kubernetes secret. Agent Control can then securely pull these values from the secret at runtime.
</Callout>

<CollapserGroup>
  <Collapser
    id="otel-config"
    title="OpenTelemetry configuration"
  >

    ```yaml
    global:
      cluster: "YOUR_CLUSTER_NAME"
      licenseKey: "YOUR_LICENSE_KEY"
    # Values related to the agent control's Helm chart release.
    # `https://github.com/newrelic/helm-charts/blob/master/charts/agent-control/values.yaml`
    agent-control-deployment:
      identityClientId: ""
      identityClientSecret: ""
      config:
        fleet_control:
          # Optional: Specify a fleet_id (entity guid) to automatically connect to an existing fleet.
          fleet_id: ""
          auth:
            # New Relic organization ID where agent will connect to.
            organizationId: "YOUR_ORGANIZATION_ID"

        # List of managed agents that will be deployed. The key represents the name of the agent and the value holds the configuration.
        subAgents:
          infrastructure:
            type: newrelic/com.newrelic.infrastructure:0.1.0
            content:
              # Ref: `https://github.com/newrelic/helm-charts/tree/master/charts/nri-bundle%60
              # Recommended: check and define an explicit chart version (latest stable)
              chart_version: "*"
          agent-operator:
            type: newrelic/com.newrelic.k8s_agent_operator:0.1.0
            content:
              chart_version: "*"
          fluentbit:
            type: newrelic/io.fluentbit:0.1.0
            content:
              # Ref: `https://github.com/newrelic/helm-charts/tree/master/charts/newrelic-logging`
              # Recommended: check and define an explicit chart version (latest stable)
              chart_version: "*"
              chart_values:
                global:
                  lowDataMode: true
          prometheus:
            type: newrelic/com.newrelic.prometheus:0.1.0
            content:
              chart_version: "*"
              chart_values:
                global:
                  lowDataMode: true
                newrelic-prometheus-agent:
                  config:
                    kubernetes:
                      integrations_filter:
                        enabled: false
    ```

  </Collapser>

</CollapserGroup>

## Sample configurations: Remote Agent Configurations on Kubernetes
The following examples show how to configure individual agents remotely from the **New Relic Control** UI.

### Remote configuration: New Relic infrastructure
This example shows how to remotely configure the New Relic infrastructure agent for Kubernetes using Fleet Control. It enables process metrics collection by setting `enableProcessMetrics: true`.

<CollapserGroup>
  <Collapser
    id="infra-remote-config"
    title="Infrastructure remote configuration"
  >

    ```yaml
    chart_version: "*"
    chart_values:
      newrelic-infrastructure:
        enableProcessMetrics: true
    ```

  </Collapser>

</CollapserGroup>

### Remote configuration: Fluent Bit
This example configured Fluent Bit remotely via Fleet Control. It enables health metric reporting from the log collector by setting `sendMetrics: true`.

<CollapserGroup>
  <Collapser
    id="fluentbit-remote-config"
    title="Fluent Bit configuration"
  >

    ```yaml
    chart_version: "*"
    chart_values:
      newrelic-logging:
        sendMetrics: true
    ```

  </Collapser>

</CollapserGroup>

### Remote configuration: Prometheus
This example configures the Prometheus agent remotely using Fleet Control. It enables `low-data mode` to reduce telemetry volume and disable default integrations.

<CollapserGroup>
  <Collapser
    id="prometheus-config"
    title="Prometheus configuration"
  >

    ```yaml
    chart_version: "*"
    chart_values:
      newrelic-prometheus-agent:
        lowDataMode: true
    ```

  </Collapser>

</CollapserGroup>

### Remote configuration: OpenTelemetry
This example configures the New Relic OpenTelemetry collector and enable `lowDataMode` as a valid option.

<Callout variant="important">
Security Best Practice: Do not store sensitive values like your license key directly in the configuration. We recommend using a Kubernetes secret. Agent Control can then securely pull these values from the secret at runtime.
</Callout>

<CollapserGroup>
<Collapser id="otel-config" title="OpenTelemetry remote configuration">

    <Callout variant="important">
      Create a Kubernetes secret to securely store the New Relic license key and use it in the `chart_values` in replacement of the `licenseKey` value:

          ```yaml
          customSecretName: "your-secret-name"
          customSecretLicenseKey: "your-secret-key"
          ```
    </Callout>

      We recommend using Fleet Control to define and deploy OpenTelemetry configuration across your fleets.
      To configure OpenTelemetry remotely, create a configuration in Fleet Control with the structure shown below. You can adjust values such as `lowDataMode` or `receivers.filelog.enabled`, and include any other relevant Helm chart settings based on your needs.

          ```yaml
          chart_version: "*"
          chart_values:
            newrelic-prometheus-agent:
              lowDataMode: true
          ```

  </Collapser>

</CollapserGroup>

## Proxy configuration for Kubernetes

Agent Control supports proxy configuration to route traffic through corporate proxies. Proxy settings can be set through environment variables or directly in the configuration file.

### Proxy precedence

Agent Control will use proxy settings in the following order of precedence:

1. `proxy` configuration field in the Agent Control configuration
2. `HTTP_PROXY` environment variable
3. `HTTPS_PROXY` environment variable

<Callout variant="important">
  Proxy configuration is currently not compatible with fetching the certificate for signature validation. If you need to setup a proxy, you have these options:

  - Add a firewall exception to `https://newrelic.com` so requests to that endpoint can skip the proxy (recommended)
  - Use a local certificate through `fleet_control.signature_validation.certificate_pem_file_path` (certificate rotation must be handled manually)
  - Disable signature validation by setting `fleet_control.signature_validation.enabled: false` (highly discouraged for security reasons)
</Callout>


### Proxy configuration with self-signed certificates

For proxy setups using HTTPS authentication with self-signed certificates, you need to provide the CA certificate bundle and configure proxy authentication:

<CollapserGroup>
  <Collapser
    id="k8s-proxy-config"
    title="Kubernetes proxy configuration example"
  >

    ```yaml
    global:
      cluster: "YOUR_CLUSTER_NAME"
      licenseKey: "YOUR_LICENSE_KEY"

    agent-control-deployment:
      config:
        agentControl:
          content:
            proxy:
              url: https://proxy-service:8080
        subAgents: {}

      # Mount CA certificate bundle to Agent Control
      extraVolumeMounts:
        - mountPath: /etc/ssl/certs/
          name: ca-certs
      extraVolumes:
        - name: ca-certs
          secret:
            secretName: ca-certs

    # Configure Flux components to use proxy
    flux2:
      sourceController:
        extraEnv:
          # Configure Flux source-controller to proxy all requests
          - name: HTTPS_PROXY
            value: https://proxy-service:8080
          # Except for in-cluster requests
          - name: "NO_PROXY"
            value: ".cluster.local.,.cluster.local,cluster.local,.svc,127.0.0.0/8,10.0.0.0/8"
        volumeMounts:
          # Mount CA certificate bundle to source-controller trust root store. The bundle should contain the
          # proxy CA cert.
          - mountPath: /etc/ssl/certs/
            name: ca-certs
        volumes:
            - name: ca-certs
              secret:
                secretName: ca-certs


    ```

  </Collapser>
</CollapserGroup>

### Proxy configuration for managed agents

<Callout variant="caution">
  Configuring a proxy in Agent Control does **not** automatically configure the same proxy settings for the agents it manages. Each agent has its own proxy configuration that must be set separately according to that agent's specific configuration format and requirements.
</Callout>

When using a proxy, you must also configure proxy settings for each managed agent individually. Refer to each agent's specific documentation for proxy configuration options.
