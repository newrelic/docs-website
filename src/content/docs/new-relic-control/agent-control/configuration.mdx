---
title: "Configuration"
metaDescription: "Overview of the Agent Control configuration"
freshnessValidatedDate: never
---
<Callout title="preview">
  We're still working on this feature, but we'd love for you to try it out!

  This feature is currently provided as part of a preview program pursuant to our [pre-release policies](/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy).
</Callout>

The [`values-newrelic.yaml`](https://github.com/newrelic/helm-charts/blob/master/charts/agent-control/values.yaml) file, which traditionally defined New Relic agent settings, now also includes configuration for Agent Control. The parameters you define in this file determine how both Agent Control and its managed agents operate. This file is referred to as the **local configuration**.

Here's an example configuration:

<CollapserGroup>
  <Collapser
    id="agent-control-config"
    title="Agent Control configuration"
  >

    ```yaml
    global:
      cluster: "YOUR_CLUSTER_NAME"
      licenseKey: "YOUR_LICENSE_KEY"
      userKey: "YOUR_USER_KEY"

    # Values related to the agent control's Helm chart release.
    # `https://github.com/newrelic/helm-charts/blob/master/charts/agent-control/values.yaml`
    agent-control-deployment:
      identityClientId: ""
      identityClientSecret: ""
      config:
        fleet_control:
          # Optional: Specify a fleet_id (entity guid) to automatically connect to an existing fleet.
          fleet_id: ""
          auth:
            # New Relic organization ID where agent will connect to.
            organizationId: "YOUR_ORGANIZATION_ID"

        # List of managed agents that will be deployed. The key represents the name of the agent and the value holds the configuration.
        subAgents:
          infrastructure:
            type: newrelic/com.newrelic.infrastructure:0.1.0
            content:
              # Ref: `https://github.com/newrelic/helm-charts/tree/master/charts/nri-bundle`
              # Recommended: check and define an explicit chart version (latest stable)
              chart_version: "*"
              chart_values:
                newrelic-infrastructure:
                enableProcessMetrics: true
          logs:
            type: newrelic/io.fluentbit:0.1.0
            content:
              # Ref: `https://github.com/newrelic/helm-charts/tree/master/charts/newrelic-logging`
              # Recommended: check and define an explicit chart version (latest stable)
              chart_version: "*"
              chart_values:
                newrelic-logging:
                  sendMetrics: true
          agent-operator:
            type: com.newrelic.k8s_agent_operator:0.1.0
            content:
              chart_version: "*"
    ```

  </Collapser>

</CollapserGroup>

The sample demonstrates how to configure Agent Control along with two managed agents: the Kubernetes infrastructure agent and Fluent Bit for log forwarding. For example, if you don't want to send health metrics for your Fluent Bit log collector, simply set `sendMetrics: false` in the YAML file before running the install command.

To deploy configurations centrally across clusters, define this same YAML content in the **Configurations** section of [Fleet Control](/docs/new-relic-control/fleet-control/overview). You can then apply the configuration to an entire fleet of clusters as part of a remote deployment. This is referred to as the **remote configuration** file.

Remote configuration ensures consistent agent behavior across your environment, simplifies change management, and enables you to scale observability without manually managing local YAML files.

Agent Control uses Kubernetes `ConfigMaps` to store and apply configuration settings. If both local and remote configurations are present, **remote configuration** takes precedence by default. To intentionally override remote settings and fall back to local configuration, you can deploy an **empty remote configuration** via Fleet Control. Keep in mind that this change will apply to **all clusters** in the selected fleet.

To explore all available configuration settings, refer to [`values-newrelic.yaml`](https://github.com/newrelic/helm-charts/blob/master/charts/agent-control/values.yaml).

## Sample configurations
The following examples show how to configure Agent Control to manage different sets of agents. These configurations can be used either during initial installation or as part of a remote configuration in Fleet Control.

### New Relic infrastructure and Fluent Bit
This example deploys Agent Control with infrastructure monitoring and Fluent Bit for log collection.

<CollapserGroup>
  <Collapser
        id="agent-control-config"
        title="Local config for infrastructure and Fluent Bit"
      >
    ```yaml
    global:
      cluster: "YOUR_CLUSTER_NAME"
      licenseKey: "YOUR_LICENSE_KEY"
      userKey: "YOUR_USER_KEY"

    # See `https://github.com/newrelic/helm-charts/blob/master/charts/agent-control/values.yaml`
    agent-control-deployment:
      identityClientId: ""
      identityClientSecret: ""
      config:
        fleet_control:
          # Optional
          # fleet_id: YOUR_FLEET_ENTITY_GUID
          auth:
            organizationId: "YOUR_ORGANIZATION_ID"
        subAgents:
          infrastructure:
            type: newrelic/com.newrelic.infrastructure:0.1.0
            content:
              # Ref: `https://github.com/newrelic/helm-charts/tree/master/charts/nri-bundle`
              # Recommended: check and define an explicit chart version (latest stable)
              chart_version: "*"

              #chart_values:
              #  newrelic-infrastructure:
              #    enableProcessMetrics: true
          logs:
            type: newrelic/io.fluentbit:0.1.0
            content:
              # Ref: `https://github.com/newrelic/helm-charts/tree/master/charts/newrelic-logging`
              # Recommended: check and define an explicit chart version (latest stable)
              chart_version: "*"

              #chart_values:
              #  newrelic-logging:
              #    sendMetrics: true
          agent-operator:
            type: com.newrelic.k8s_agent_operator:0.1.0
            chart_version: "*"
    ```
  </Collapser>

</CollapserGroup>



### OpenTelemetry with custom collector settings
This example deploys Agent Control with the New Relic distribution of OpenTelemetry (NRDOT) collector and disables the `filelog` receiver in the managed [`nr-k8s-otel-collector` Helm chart](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector#values).

<CollapserGroup>
  <Collapser
    id="otel-config"
    title="OpenTelemetry configuration"
  >

    ```yaml
    global:
      cluster: "YOUR_CLUSTER_NAME"
      licenseKey: "YOUR_LICENSE_KEY"
    # Values related to the agent control's Helm chart release.
    # `https://github.com/newrelic/helm-charts/blob/master/charts/agent-control/values.yaml`
    agent-control-deployment:
      identityClientId: ""
      identityClientSecret: ""
      config:
        fleet_control:
          # Optional: Specify a fleet_id (entity guid) to automatically connect to an existing fleet.
          fleet_id: ""
          auth:
            # New Relic organization ID where agent will connect to.
            organizationId: "YOUR_ORGANIZATION_ID"

        # List of managed agents that will be deployed. The key represents the name of the agent and the value holds the configuration.
        subAgents:
          infrastructure:
            type: newrelic/com.newrelic.infrastructure:0.1.0
            content:
              # Ref: `https://github.com/newrelic/helm-charts/tree/master/charts/nri-bundle%60
              # Recommended: check and define an explicit chart version (latest stable)
              chart_version: "*"
          agent-operator:
            type: newrelic/com.newrelic.k8s_agent_operator:0.1.0
            content:
              chart_version: "*"
          fluentbit:
            type: newrelic/io.fluentbit:0.1.0
            content:
              # Ref: `https://github.com/newrelic/helm-charts/tree/master/charts/newrelic-logging`
              # Recommended: check and define an explicit chart version (latest stable)
              chart_version: "*"
              chart_values:
                global:
                  lowDataMode: true
          prometheus:
            type: newrelic/com.newrelic.prometheus:0.1.0
            content:
              chart_version: "*"
              chart_values:
                global:
                  lowDataMode: true
                newrelic-prometheus-agent:
                  config:
                    kubernetes:
                      integrations_filter:
                        enabled: false
    ```

  </Collapser>

</CollapserGroup>

### Remote configuration: New Relic infrastructure
This example shows how to remotely configure the New Relic infrastructure agent for Kubernetes using Fleet Control. It enables process metrics collection by setting `enableProcessMetrics: true`.

<CollapserGroup>
  <Collapser
    id="infra-remote-config"
    title="Infrastructure remote configuration"
  >

    ```yaml
    chart_version: "*"
    chart_values:
      newrelic-infrastructure:
        enableProcessMetrics: true
    ```

  </Collapser>

</CollapserGroup>

### Remote configuration: Fluent Bit
This example configured Fluent Bit remotely via Fleet Control. It enables health metric reporting from the log collector by setting `sendMetrics: true`.

<CollapserGroup>
  <Collapser
    id="fluentbit-remote-config"
    title="Fluent Bit configuration"
  >

    ```yaml
    chart_version: "*"
    chart_values:
      newrelic-logging:
        sendMetrics: true
    ```

  </Collapser>

</CollapserGroup>

### Remote configuration: Prometheus
This example configures the Prometheus agent remotely using Fleet Control. It enables `low-data mode` to reduce telemetry volume and disable default integrations.

<CollapserGroup>
  <Collapser
    id="prometheus-config"
    title="Prometheus configuration"
  >

    ```yaml
    chart_version: "*"
    chart_values:
      newrelic-prometheus-agent:
        lowDataMode: true
    ```

  </Collapser>

</CollapserGroup>

### Remote configuration: OpenTelemetry


<CollapserGroup>
  <Collapser
    id="prometheus-config"
    title="Prometheus configuration"
  >

    <Callout variant="important">
      Create a Kubernetes secret to securely store the New Relic license key and use it in the `chart_values` in replacement of the `licenseKey` value:

          ```yaml
          customSecretName: "your-secret-name"
          customSecretLicenseKey: "your-secret-key"
          ```
    </Callout>

      We recommend using Fleet Control to define and deploy OpenTelemetry configuration across your fleets.
      To configure OpenTelemetry remotely, create a configuration in Fleet Control with the structure shown below. You can adjust values such as `lowDataMode` or `receivers.filelog.enabled`, and include any other relevant Helm chart settings based on your needs.

          ```yaml
          chart_version: "*"
          chart_values:
            newrelic-prometheus-agent:
              lowDataMode: true
          ```

  </Collapser>

</CollapserGroup>

## Proxy configuration

Agent Control supports proxy configuration to route traffic through corporate proxies or network intermediaries. The proxy configuration can be set through environment variables or directly in the configuration file.

### Proxy precedence

Agent Control will use proxy settings in the following order of precedence:

1. `proxy` configuration field in the Agent Control configuration
2. `HTTP_PROXY` environment variable
3. `HTTPS_PROXY` environment variable

<Callout variant="important">
  Proxy configuration is currently not compatible with fetching the certificate for signature validation. If you need to setup a proxy, you have these options:

  - Add a firewall exception to `https://newrelic.com` so requests to that endpoint can skip the proxy (recommended)
  - Use a local certificate through `fleet_control.signature_validation.certificate_pem_file_path` (certificate rotation must be handled manually)
  - Disable signature validation by setting `fleet_control.signature_validation.enabled: false` (highly discouraged for security reasons)
</Callout>


### Proxy configuration with self-signed certificates

For proxy setups using HTTPS authentication with self-signed certificates, you need to provide the CA certificate bundle and configure proxy authentication:

<CollapserGroup>
  <Collapser
    id="k8s-proxy-config"
    title="Kubernetes proxy configuration example"
  >

    ```yaml
    global:
      cluster: "YOUR_CLUSTER_NAME"
      licenseKey: "YOUR_LICENSE_KEY"

    agent-control-deployment:
      config:
        agentControl:
          content:
            proxy:
              url: https://proxy-service:8080
        subAgents: {}

      # Mount CA certificate bundle to Agent Control
      extraVolumeMounts:
        - mountPath: /etc/ssl/certs/
          name: ca-certs
      extraVolumes:
        - name: ca-certs
          secret:
            secretName: ca-certs

    # Configure Flux components to use proxy
    agent-control-cd:
      flux2:
        sourceController:
          extraEnv:
            # Configure Flux source-controller to proxy all requests
            - name: HTTPS_PROXY
              value: https://proxy-service:8080
            # Except for in-cluster requests
            - name: "NO_PROXY"
              value: ".cluster.local.,.cluster.local,cluster.local,.svc,127.0.0.0/8,10.0.0.0/8"
          volumeMounts:
            # Mount CA certificate bundle to source-controller trust root store. The bundle should contain the
            # proxy CA cert.
            - mountPath: /etc/ssl/certs/
              name: ca-certs
          volumes:
            - name: ca-certs
              secret:
                secretName: ca-certs


    ```

  </Collapser>
</CollapserGroup>

### Proxy configuration for managed agents

<Callout variant="caution">
  Configuring a proxy in Agent Control does **not** automatically configure the same proxy settings for the agents it manages. Each agent has its own proxy configuration that must be set separately according to that agent's specific configuration format and requirements.
</Callout>

When using a proxy, you must also configure proxy settings for each managed agent individually. Refer to each agent's specific documentation for proxy configuration options.

## Secrets management

Agent Control provides a robust mechanism for managing sensitive data, such as passwords and API keys, by retrieving them from dedicated secret providers. This ensures that sensitive information is not hard-coded directly into configuration files. The system currently supports the following secret providers:

- HashiCorp Vault: referred to as `nr-vault` in configurations.
- Kubernetes Secrets: referred to as `nr-kubesec` in configurations.

### Defining Secrets in Configuration
To utilize secrets, define them within your Agent-Control configuration YAML file by following these steps:

1. **Define the `secrets_providers` section:** Configure your secret providers centrally in this section. Ensure each entry corresponds to a supported provider.
2. **Configure secret sources:** For each provider, specify one or more sources. A source includes the necessary configuration details (e.g., URL, token) for Agent control to connect to and retrieve a group of secrets.
3. **Use Placeholders in Agent Configurations:** Instead of the actual sensitive data, you will use a placeholder string within your agent's configuration. Agent Control automatically replaces these placeholders with the retrieved secrets during the rendering process.

<Callout variant="important">
    If Agent Control fails to retrieve a secret for any reason, the configuration rendering will fail, and the agent will not be executed. This is a critical security feature to prevent agents from running with incomplete or incorrect configurations.
</Callout>

The following agent-control configuration example demonstrates how to configure for retrieving secrets from two Vault sources within the secrets_providers section:

```yaml
secrets_providers:
  vault:
    sources:
      local-instance:
        url: http://localhost:8200/v1/
        token: root
        engine: kv2
      remote:
        url: http://my-remote-server:8200/v1/
        token: root
        engine: kv1

fleet_control:
  ...

agents:
  ...
```

#### Using Secrets in an Agent Configuration

Once the sources are defined, you can reference them using a specific placeholder syntax. In an agent configuration, you can add a vault placeholder with the correct path and Agent Control will retrieve the secret and use it to render the final configuration file that the agent will use.

Example agent configuration using Vault placeholders:

```yaml
config_agent: |+
  enable_process_metrics: true
  custom_attributes:
    username: "${nr-vault:local-instance:secret:my_secret:username}"
    organization: "${nr-vault:remote:my_mount:my_path:organization}"
```

In this example:

The placeholder `${nr-vault:local-instance:secret:my_secret:username}` instructs Agent Control to retrieve the value associated with the key username from the secret at the path `secret/my_secret` using the local-instance Vault source.
The placeholder `${nr-vault:remote:my_mount:my_path:organization}` similarly retrieves the value for the organization key from the remote source.

After successful retrieval, Agent Control will render these secrets from the specified source and path, storing the result in a K8s secret or private config file for use by the corresponding agent.

### Vault secrets

Vault sources need to be set up with the following settings:

<table>
    <thead>
    <tr>
        <th style={{ width: "250px" }}>
            YAML key
        </th>
        <th>
            Description
        </th>
    </tr>
    </thead>

    <tbody>
    <tr>
        <td>
            `url`
        </td>
        <td>
            URL to request data from
        </td>
    </tr>

    <tr>
        <td>
            `token`
        </td>
        <td>
            Used to authenticate to the endpoint.
        </td>
    </tr>

    <tr>
        <td>
            `engine`
        </td>
        <td>
            Specify **`kv1`** or **`kv2`**.
        </td>
    </tr>
    </tbody>
</table>

In the configuration file, each secret stored in Vault can be accessed by setting a placeholder with:
- **source_name**: The name of the Vault source defined in `secrets_providers`.
- **mount**: The name of the secrets engine mount.
- **path**: The specific path to the secret.
- **specific key**: The specific key within the secret to be retrieved.

Example of full placeholder format:
```
"${nr-vault:source_name:my_mount:my_path:my_value}"
```

### Kubernetes secrets
If the agent-control pod has permissions, such as through a Service Account and Role-Based Access Control (RBAC), to access the required secrets and namespaces, Agent control can directly access secrets from the Kubernetes API without needing a separate sources configuration.

In the agent configuration file, each secret value can be retrieved by using a placeholder that specifies:

- **namespace**: The Kubernetes namespace where the secret is located.
- **name**: The name of the Kubernetes secret object.
- **specific key**: The specific key within the secret from which to retrieve the value.

For example, use the placeholder format:
```
"${nr-kubesec:my_namespace:my_secret:my_value}"
```
