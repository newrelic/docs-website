---
title: "Configuration"
metaDescription: "Overview of the Agent Control configuration"
freshnessValidatedDate: never
---
<Callout title="preview">
  We're still working on this feature, but we'd love for you to try it out!

  This feature is currently provided as part of a preview program pursuant to our [pre-release policies](/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy).
</Callout>

The [`values-newrelic.yaml`](https://github.com/newrelic/helm-charts/blob/master/charts/agent-control/values.yaml) file, which traditionally defined New Relic agent settings, now also includes configuration for Agent Control. The parameters you define in this file determine how both Agent Control and its managed agents operate. This file is referred to as the **local configuration**.

Here's an example configuration:

<CollapserGroup>
  <Collapser
    id="agent-control-config"
    title="Agent Control configuration"
  >

    ```yaml
    global:
      cluster: "YOUR_CLUSTER_NAME"
      licenseKey: "YOUR_LICENSE_KEY"
      userKey: "YOUR_USER_KEY"

    # Values related to the agent control's Helm chart release.
    # `https://github.com/newrelic/helm-charts/blob/master/charts/agent-control/values.yaml`
    agent-control-deployment:
      identityClientId: ""
      identityClientSecret: ""
      config:
        fleet_control:
          # Optional: Specify a fleet_id (entity guid) to automatically connect to an existing fleet.
          fleet_id: ""
          auth:
            # New Relic organization ID where agent will connect to.
            organizationId: "YOUR_ORGANIZATION_ID"

        # List of managed agents that will be deployed. The key represents the name of the agent and the value holds the configuration.
        subAgents:
          infrastructure:
            type: newrelic/com.newrelic.infrastructure:0.1.0
            content:
              # Ref: `https://github.com/newrelic/helm-charts/tree/master/charts/nri-bundle`
              # Recommended: check and define an explicit chart version (latest stable)
              chart_version: "*"
              chart_values:
                newrelic-infrastructure:
                enableProcessMetrics: true
          logs:
            type: newrelic/io.fluentbit:0.1.0
            content:
              # Ref: `https://github.com/newrelic/helm-charts/tree/master/charts/newrelic-logging`
              # Recommended: check and define an explicit chart version (latest stable)
              chart_version: "*"
              chart_values:
                newrelic-logging:
                  sendMetrics: true
          agent-operator:
            type: com.newrelic.k8s_agent_operator:0.1.0
            content:
              chart_version: "*"
    ```

  </Collapser>

</CollapserGroup>

The sample demonstrates how to configure Agent Control along with two managed agents: the Kubernetes infrastructure agent and Fluent Bit for log forwarding. For example, if you don't want to send health metrics for your Fluent Bit log collector, simply set `sendMetrics: false` in the YAML file before running the install command.

To deploy configurations centrally across clusters, define this same YAML content in the **Configurations** section of [Fleet Control](/docs/new-relic-control/fleet-control/overview). You can then apply the configuration to an entire fleet of clusters as part of a remote deployment. This is referred to as the **remote configuration** file.

Remote configuration ensures consistent agent behavior across your environment, simplifies change management, and enables you to scale observability without manually managing local YAML files.

Agent Control uses Kubernetes `ConfigMaps` to store and apply configuration settings. If both local and remote configurations are present, **remote configuration** takes precedence by default. To intentionally override remote settings and fall back to local configuration, you can deploy an **empty remote configuration** via Fleet Control. Keep in mind that this change will apply to **all clusters** in the selected fleet.

To explore all available configuration settings, refer to [`values-newrelic.yaml`](https://github.com/newrelic/helm-charts/blob/master/charts/agent-control/values.yaml).

## Sample configurations
The following examples show how to configure Agent Control to manage different sets of agents. These configurations can be used either during initial installation or as part of a remote configuration in Fleet Control.

### New Relic infrastructure and Fluent Bit
This example deploys Agent Control with infrastructure monitoring and Fluent Bit for log collection.

<CollapserGroup>
  <Collapser
        id="agent-control-config"
        title="Local config for infrastructure and Fluent Bit"
      >
    ```yaml
    global:
      cluster: "YOUR_CLUSTER_NAME"
      licenseKey: "YOUR_LICENSE_KEY"
      userKey: "YOUR_USER_KEY"

    # See `https://github.com/newrelic/helm-charts/blob/master/charts/agent-control/values.yaml`
    agent-control-deployment:
      identityClientId: ""
      identityClientSecret: ""
      config:
        fleet_control:
          # Optional
          # fleet_id: YOUR_FLEET_ENTITY_GUID
          auth:
            organizationId: "YOUR_ORGANIZATION_ID"
        subAgents:
          infrastructure:
            type: newrelic/com.newrelic.infrastructure:0.1.0
            content:
              # Ref: `https://github.com/newrelic/helm-charts/tree/master/charts/nri-bundle`
              # Recommended: check and define an explicit chart version (latest stable)
              chart_version: "*"

              #chart_values:
              #  newrelic-infrastructure:
              #    enableProcessMetrics: true
          logs:
            type: newrelic/io.fluentbit:0.1.0
            content:
              # Ref: `https://github.com/newrelic/helm-charts/tree/master/charts/newrelic-logging`
              # Recommended: check and define an explicit chart version (latest stable)
              chart_version: "*"

              #chart_values:
              #  newrelic-logging:
              #    sendMetrics: true
          agent-operator:
            type: com.newrelic.k8s_agent_operator:0.1.0
            chart_version: "*"
    ```
  </Collapser>

</CollapserGroup>



### OpenTelemetry with custom collector settings
This example deploys Agent Control with the New Relic distribution of OpenTelemetry (NRDOT) collector and disables the `filelog` receiver in the managed [`nr-k8s-otel-collector` Helm chart](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector#values).

<CollapserGroup>
  <Collapser
    id="otel-config"
    title="OpenTelemetry configuration"
  >

    ```yaml
    global:
      cluster: "YOUR_CLUSTER_NAME"
      licenseKey: "YOUR_LICENSE_KEY"
    # Values related to the agent control's Helm chart release.
    # `https://github.com/newrelic/helm-charts/blob/master/charts/agent-control/values.yaml`
    agent-control-deployment:
      identityClientId: ""
      identityClientSecret: ""
      config:
        fleet_control:
          # Optional: Specify a fleet_id (entity guid) to automatically connect to an existing fleet.
          fleet_id: ""
          auth:
            # New Relic organization ID where agent will connect to.
            organizationId: "YOUR_ORGANIZATION_ID"

        # List of managed agents that will be deployed. The key represents the name of the agent and the value holds the configuration.
        subAgents:
          infrastructure:
            type: newrelic/com.newrelic.infrastructure:0.1.0
            content:
              # Ref: `https://github.com/newrelic/helm-charts/tree/master/charts/nri-bundle%60
              # Recommended: check and define an explicit chart version (latest stable)
              chart_version: "*"
          agent-operator:
            type: newrelic/com.newrelic.k8s_agent_operator:0.1.0
            content:
              chart_version: "*"
          fluentbit:
            type: newrelic/io.fluentbit:0.1.0
            content:
              # Ref: `https://github.com/newrelic/helm-charts/tree/master/charts/newrelic-logging`
              # Recommended: check and define an explicit chart version (latest stable)
              chart_version: "*"
              chart_values:
                global:
                  lowDataMode: true
          prometheus:
            type: newrelic/com.newrelic.prometheus:0.1.0
            content:
              chart_version: "*"
              chart_values:
                global:
                  lowDataMode: true
                newrelic-prometheus-agent:
                  config:
                    kubernetes:
                      integrations_filter:
                        enabled: false
    ```

  </Collapser>

</CollapserGroup>

### Remote configuration: New Relic infrastructure
This example shows how to remotely configure the New Relic infrastructure agent for Kubernetes using Fleet Control. It enables process metrics collection by setting `enableProcessMetrics: true`.

<CollapserGroup>
  <Collapser
    id="infra-remote-config"
    title="Infrastructure remote configuration"
  >

    ```yaml
    chart_version: "*"
    chart_values:
      newrelic-infrastructure:
        enableProcessMetrics: true
    ```

  </Collapser>

</CollapserGroup>

### Remote configuration: Fluent Bit
This example configured Fluent Bit remotely via Fleet Control. It enables health metric reporting from the log collector by setting `sendMetrics: true`.

<CollapserGroup>
  <Collapser
    id="fluentbit-remote-config"
    title="Fluent Bit configuration"
  >

    ```yaml
    chart_version: "*"
    chart_values:
      newrelic-logging:
        sendMetrics: true
    ```

  </Collapser>

</CollapserGroup>

### Remote configuration: Prometheus
This example configures the Prometheus agent remotely using Fleet Control. It enables `low-data mode` to reduce telemetry volume and disable default integrations.

<CollapserGroup>
  <Collapser
    id="prometheus-config"
    title="Prometheus configuration"
  >

    ```yaml
    chart_version: "*"
    chart_values:
      newrelic-prometheus-agent:
        lowDataMode: true
    ```

  </Collapser>

</CollapserGroup>

### Remote configuration: OpenTelemetry


<CollapserGroup>
  <Collapser
    id="prometheus-config"
    title="Prometheus configuration"
  >

    <Callout variant="important">
      Create a Kubernetes secret to securely store the New Relic license key and use it in the `chart_values` in replacement of the `licenseKey` value:

          ```yaml
          customSecretName: "your-secret-name"
          customSecretLicenseKey: "your-secret-key"
          ```
    </Callout>

      We recommend using Fleet Control to define and deploy OpenTelemetry configuration across your fleets.
      To configure OpenTelemetry remotely, create a configuration in Fleet Control with the structure shown below. You can adjust values such as `lowDataMode` or `receivers.filelog.enabled`, and include any other relevant Helm chart settings based on your needs.

          ```yaml
          chart_version: "*"
          chart_values:
            newrelic-prometheus-agent:
              lowDataMode: true
          ```

  </Collapser>

</CollapserGroup>

## Proxy configuration

Agent Control supports proxy configuration to route traffic through corporate proxies or network intermediaries. The proxy configuration can be set through environment variables or directly in the configuration file.

### Proxy precedence

Agent Control will use proxy settings in the following order of precedence:

1. `proxy` configuration field in the Agent Control configuration
2. `HTTP_PROXY` environment variable
3. `HTTPS_PROXY` environment variable

<Callout variant="important">
  Proxy configuration is currently not compatible with fetching the certificate for signature validation. If you need to setup a proxy, you have these options:

  - Add a firewall exception to `https://newrelic.com` so requests to that endpoint can skip the proxy (recommended)
  - Use a local certificate through `fleet_control.signature_validation.certificate_pem_file_path` (certificate rotation must be handled manually)
  - Disable signature validation by setting `fleet_control.signature_validation.enabled: false` (highly discouraged for security reasons)
</Callout>


### Proxy configuration with self-signed certificates

For proxy setups using HTTPS authentication with self-signed certificates, you need to provide the CA certificate bundle and configure proxy authentication:

<CollapserGroup>
  <Collapser
    id="k8s-proxy-config"
    title="Kubernetes proxy configuration example"
  >

    ```yaml
    global:
      cluster: "YOUR_CLUSTER_NAME"
      licenseKey: "YOUR_LICENSE_KEY"

    agent-control-deployment:
      config:
        agentControl:
          content:
            proxy:
              url: https://proxy-service:8080
        subAgents: {}

      # Mount CA certificate bundle to Agent Control
      extraVolumeMounts:
        - mountPath: /etc/ssl/certs/
          name: ca-certs
      extraVolumes:
        - name: ca-certs
          secret:
            secretName: ca-certs

    # Configure Flux components to use proxy
    flux2:
      sourceController:
        extraEnv:
          # Configure Flux source-controller to proxy all requests
          - name: HTTPS_PROXY
            value: https://proxy-service:8080
          # Except for in-cluster requests
          - name: "NO_PROXY"
            value: ".cluster.local.,.cluster.local,cluster.local,.svc,127.0.0.0/8,10.0.0.0/8"
        volumeMounts:
          # Mount CA certificate bundle to source-controller trust root store. The bundle should contain the
          # proxy CA cert.
          - mountPath: /etc/ssl/certs/
            name: ca-certs
        volumes:
            - name: ca-certs
              secret:
                secretName: ca-certs


    ```

  </Collapser>
</CollapserGroup>

### Proxy configuration for managed agents

<Callout variant="caution">
  Configuring a proxy in Agent Control does **not** automatically configure the same proxy settings for the agents it manages. Each agent has its own proxy configuration that must be set separately according to that agent's specific configuration format and requirements.
</Callout>

When using a proxy, you must also configure proxy settings for each managed agent individually. Refer to each agent's specific documentation for proxy configuration options.
