---
title: Confluent cloud integration
tags:
  - Integrations
  - Confluent cloud integrations
  - Apache Kafka

metaDescription: " New Relic's Confluent cloud integration for Kafka: what data it reports, and how to enable it."
redirects:
  - /docs/infrastructure/other-infrastructure-integrations/confluent-cloud-integration
freshnessValidatedDate: never
---

New Relic offers an integration for collecting your [Confluent Cloud managed streaming for Apache Kafka](https://www.confluent.io/confluent-cloud/) data. This document explains how to activate this integration and describes the data that can be reported.

## Prerequisites

* A New Relic account
* An active Confluent Cloud account
* A Confluent Cloud API key and secret
* `MetricsViewer` access on the Confluent Cloud account

## Activate integration [#activate]

To enable this integration, go to <DNT>**Integrations & Agents**</DNT>, select <DNT>**Confluent Cloud -> API Polling**</DNT> and follow the instructions.

<Callout variant="important">
If you have IP Filtering set up, add the following IP addresses to your filter. 
* `162.247.240.0/22`
* `152.38.128.0/19`

For more information about  New Relic IP ranges for cloud integration, refer [this document](/docs/new-relic-solutions/get-started/networks/#webhooks). 
For instructions to perform this task, refer [this document](https://docs.confluent.io/cloud/current/security/access-control/ip-filtering/manage-ip-filters.html).
</Callout>




## Configuration and polling [#polling]


Default polling information for the Confluent Cloud Kafka integration:

* New Relic polling interval: 5 minutes
* Confluent Cloud data interval: 1 minute

You can change the polling frequency only during the initial configuration. 

## View and use data [#find-data]


You can [query and explore your data](/docs/using-new-relic/data/understand-data/query-new-relic-data) using the following [event type](/docs/data-apis/understand-data/new-relic-data-types/#metrics-in-service-levels):

<table>
  <thead>
    <tr>
      <th>
        Entity
      </th>

      <th>
        Data type
      </th>

      <th>
        Provider
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        Cluster
      </td>

      <td>
        `Metric`
      </td>

      <td>
        `Confluent`
      </td>
    </tr>
    <tr>
      <td>
        Connector
      </td>

      <td>
        `Metric`
      </td>

      <td>
        `Confluent`
      </td>
    </tr>
    <tr>
      <td>
        ksql
      </td>

      <td>
        `Metric`
      </td>

      <td>
        `Confluent`
      </td>
    </tr>
  </tbody>
</table>

For more on how to use your data, see [Understand and use integration data](/docs/infrastructure/integrations/find-use-infrastructure-integration-data).

## Metric data [#metrics]

This integration records Confluent cloud Kafka data for cluster, connector, and ksql.

### Cluster data
<table>
  <thead>
    <tr>
      <th style={{ width: "275px" }}>
        Metric
      </th>

      <th style={{ width: "150px" }}>
        Unit
      </th>

      <th>
        Description
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        `cluster_load_percent`
      </td>

      <td>
        Percent
      </td>

      <td>
        A measure of the utilization of the cluster. The value is between 0.0 and 1.0.
        Only dedicated tier clusters has this metric data.

      </td>
    </tr>

    <tr>
      <td>
        `hot_partition_ingress`
      </td>

      <td>
        Percent
      </td>

      <td>
        An indicator of the presence of a hot partition caused by ingress throughput. The value is 1.0 when a hot partition is detected, and empty when there is no hot partition detected.
      </td>
    </tr>

    <tr>
      <td>
        `hot_partition_egress`
      </td>

      <td>
        Percent
      </td>

      <td>
        An indicator of the presence of a hot partition caused by egress throughput. The value is 1.0 when a hot partition is detected, and empty when there is no hot partition detected.
      </td>
    </tr>

    <tr>
      <td>
        `request_bytes`
      </td>

      <td>
        Bytes
      </td>

      <td>
        The delta count of total request bytes from the specified request types sent over the network. Each sample is the number of bytes sent since the previous data point. The count is sampled every 60 seconds.
        </td>
    </tr>

    <tr>
      <td>
        `response_bytes`
      </td>

      <td>
        Bytes
      </td>

      <td>
        The delta count of total response bytes from the specified response types sent over the network. Each sample is the number of bytes sent since the previous data point. The count is sampled every 60 seconds.
      </td>
    </tr>
  
    <tr>
      <td>
        `received_bytes`
      </td>

      <td>
        Bytes
      </td>

      <td>
        The delta count of bytes of the customer's data received from the network. Each sample is the number of bytes received since the previous data sample. The count is sampled every 60 seconds.
      </td>
    </tr>
  
    <tr>
      <td>
        `sent_bytes`
      </td>

      <td>
        Bytes
      </td>

      <td>
        The delta count of bytes of the customer's data sent over the network. Each sample is the number of bytes sent since the previous data point. The count is sampled every 60 seconds.
      </td>
    </tr>

      <tr>
      <td>
        `received_records`
      </td>

      <td>
        Count
      </td>

      <td>
        The delta count of records received. Each sample is the number of records received since the previous data sample. The count is sampled every 60 seconds.
       </td>
    </tr>

        <tr>
      <td>
        `sent_records`
      </td>

      <td>
        Count
      </td>

      <td>
       The delta count of records sent. Each sample is the number of records sent since the previous data point. The count is sampled every 60 seconds.
      </td>
    </tr>

        <tr>
      <td>
        `partition_count`
      </td>

      <td>
        Count
      </td>

      <td>
        The number of partitions.
      </td>
    </tr>

        <tr>
      <td>
        `consumer_lag_offsets`
      </td>

      <td>
       Milliseconds
      </td>

      <td>
        The lag between a group member's committed offset and the partition's high watermark.
      </td>
    </tr>

  <tr>
      <td>
        `successful_authentication_count`
      </td>

      <td>
        Count
      </td>

      <td>
        The delta count of successful authentications. Each sample is the number of successful authentications since the previous data point. The count sampled every 60 seconds.
      </td>
    </tr>

  <tr>
      <td>
        `active_connection_count`
      </td>

      <td>
        Count
      </td>

      <td>
        The count of active authenticated connections.
      </td>
    </tr>

  

  </tbody>
</table>

### Connector data
<table>
    <thead>
    <tr>
      <th style={{ width: "275px" }}>
        Metric
      </th>

      <th style={{ width: "150px" }}>
        Unit
      </th>

      <th>
        Description
      </th>
    </tr>
  </thead>
  <tbody>
        <tr>
      <td>
        `sent_records`
      </td>

      <td>
        Count
      </td>

      <td>
        The delta count of total number of records sent from the transformations and written to Kafka for the source connector. Each sample is the number of records sent since the previous data point. The count is sampled every 60 seconds.

      </td>
    </tr>
        <tr>
      <td>
        `connector_status`
      </td>

      <td>
        Bit
      </td>

      <td>
        The status of a connector within the system. Its value is always set to 1, signifying the connector's presence. The connector's current operational state is identified through the metric.status tag.

      </td>
    </tr>
    <tr>
      <td>
        `connector_task_status`
      </td>

      <td>
        Bit
      </td>

      <td>
        The status of a connector's task within the system. Its value is always set to 1, signifying the connector task's presence. The connector's current operational state is identified through the metric.status tag.

      </td>
    </tr>
    <tr>
      <td>
        `connector_task_batch_size_avg`
      </td>

      <td>
        Count
      </td>

      <td>
        The average batch size (measured by record count) per minute. For a source connector, it indicates the average batch size sent to Kafka. For a sink connector, it indicates the average batch size read by the sink task.

      </td>
    </tr>
     <tr>
      <td>
        `connector_task_batch_size_max`
      </td>

      <td>
        Count
      </td>

      <td>
        The maximum batch size (measured by record count) per minute. For a source connector, it indicates the max batch size sent to Kafka. For a sink connector, it indicates the max batch size read by the sink task.

      </td>
    </tr>  
      <tr>
      <td>
        `received_records`
      </td>

      <td>
        Count
      </td>

      <td>
        The delta count of total number of records received by the sink connector. Each sample is the number of records received since the previous data point. The count is sampled every 60 seconds.

      </td>
    </tr> 
      <tr>
      <td>
        `sent_bytes`
      </td>

      <td>
        Bytes
      </td>

      <td>
        The delta count of total number of records received by the sink connector. Each sample is the number of records received since the previous data point. The count is sampled every 60 seconds.

      </td>
    </tr> 
       <tr>
      <td>
        `received_bytes`
      </td>

      <td>
        Bytes
      </td>

      <td>
        The delta count of total bytes received by the sink connector. Each sample is the number of bytes received since the previous data point. The count is sampled every 60 seconds.

      </td>
    </tr> 
    <tr>
      <td>
        `dead_letter_queue_records`
      </td>

      <td>
        Count
      </td>

      <td>
        The delta count of dead letter queue records written to Kafka for the sink connector. The count is sampled every 60 seconds.

      </td>
    </tr> 
    
  </tbody>
</table>

### ksql data
<table>
    <thead>
    <tr>
      <th style={{ width: "275px" }}>
        Metric
      </th>

      <th style={{ width: "150px" }}>
        Unit
      </th>

      <th>
        Description
      </th>
    </tr>
  </thead>
  <tbody>
        <tr>
      <td>
        `streaming_unit_count`
      </td>

      <td>
        Count
      </td>

      <td>
        The count of Confluent Streaming Units (CSUs) for this KSQL instance. The count is sampled every 60 seconds. The implicit time aggregation for this metric is MAX.

      </td>
    </tr>
        <tr>
      <td>
        `query_saturation`
      </td>

      <td>
        Percent
      </td>

      <td>
        The maximum saturation for a given ksqlDB query across all nodes. Returns a value between 0 and 1, a value close to 1 indicates that ksqlDB query processing is bottlenecked on available resources.

      </td>
    </tr>
    <tr>
      <td>
        `task_stored_bytes`
      </td>

      <td>
        Bytes
      </td>

      <td>
        The size of a given task's state stores in bytes.

      </td>
    </tr>
    <tr>
      <td>
        `storage_utilization`
      </td>

      <td>
        Percent
      </td>

      <td>
        The total storage utilization for a given ksqlDB application.

      </td>
    </tr>
     <tr>
      <td>
        `consumed_total_bytes`
      </td>

      <td>
        Bytes
      </td>

      <td>
        The delta count of bytes consumed from Kafka by continuous queries over the requested period.

      </td>
    </tr>  
      <tr>
      <td>
        `produced_total_bytes`
      </td>

      <td>
        Bytes
      </td>

      <td>
        The delta count of bytes produced to Kafka by continuous queries over the requested period.

      </td>
    </tr> 
      <tr>
      <td>
        `offsets_processed_total`
      </td>

      <td>
        Count
      </td>

      <td>
        The delta count of offsets processed by a given query or task or topic, or offset.

      </td>
    </tr> 
       <tr>
      <td>
        `committed_offset_lag`
      </td>

      <td>
        Milliseconds
      </td>

      <td>
        The current lag between the committed offset and end offset for a given query or task or topic, or offset.

      </td>
    </tr> 
      <tr>
      <td>
        `processing_errors_total`
      </td>

      <td>
        Count
      </td>

      <td>
        Delta count of the number of record processing errors of a query over the requested period.

      </td>
    </tr> 
       <tr>
      <td>
        `query_restarts`
      </td>

      <td>
        Count
      </td>

      <td>
        Delta count of the number of failures that cause a query to restart over the requested period.

      </td>
    </tr> 
    
  </tbody>
</table>

## What's next

<DocTiles>
  
  
  <DocTile title="Data and UI" path="/docs/message-queues-streaming/ui-data/understand-ui" >Learn how to use New Relic to monitor your Kafka clusters</DocTile>
</DocTiles>

