---
title: Parsing log data
tags:
  - Logs
  - Log management
  - UI and data
translate:
  - jp
  - kr
metaDescription: How New Relic uses parsing and how to send customized log data.
redirects:
  - /docs/logs/new-relic-logs/ui-data/customizing-log-data
  - /docs/logs/new-relic-logs/ui-data/customize-logs-using-parsing
  - /docs/logs/log-management/ui-data/parsing
freshnessValidatedDate: never
---

Log <DNT>parsing</DNT> is the process of translating unstructured log data into attributes (key:value pairs) based on rules you define. You can use these attributes in your NRQL queries to facet or filter logs in useful ways.

New Relic offers two types of parsing:

* **Query-time parsing:** Creates temporary attributes used only during query execution. Parsing happens when you run a query, after data is already stored. Use query-time parsing when you're doing exploratory analysis or one-time investigations, need to test parsing patterns before creating permanent rules, want to extract attributes from logs already stored in NRDB, or don't need the parsed attributes for dashboards or alerts. Learn more about [query-time parsing](/docs/logs/ui-data/query-time-parsing).

* **Ingest-time parsing:** Creates permanent attributes stored with your log data in NRDB. Parsing happens during log ingestion, before data is written to storage. Use ingest-time parsing when you need attributes available for all queries and dashboards, want to parse logs automatically as they're ingested, are setting up long-term log analysis and monitoring, or want to avoid query-time processing overhead. These are the ingest-time parsing options available in New Relic:

  * **Built-in parsing rules:** Pre-configured patterns for common log formats (Apache, NGINX, CloudFront, MongoDB, etc.). Simply add a `logtype` attribute when forwarding logs. See the [full list of built-in rules](/docs/logs/ui-data/built-log-parsing-rules).

  * **Custom parsing rules:** Create your own parsing logic for custom or proprietary log formats or write Grok/Regex patterns directly. The rest of this document focuses on custom parsing rules. See [Create custom parsing rules](#custom-parsing) below for details.

You can also create, query, and manage your log parsing rules by using NerdGraph, our GraphQL API. A helpful tool for this is our [Nerdgraph API explorer](https://api.newrelic.com/graphiql). For more information, see our [NerdGraph tutorial for parsing](/docs/apis/nerdgraph/examples/nerdgraph-log-parsing-rules-tutorial/).

<DNT>
{/*
Here's a 5-minute video about log parsing:

<Video
  id="xPWM46yw3bQ"
  type="youtube"
/>
*/}
</DNT>

## How log parsing works [#how-it-works]

Here's an overview of how New Relic implements parsing of logs:

<table>
  <thead>
    <tr>
      <th style={{ width: "100px" }}>
        Log parsing
      </th>

      <th>
        How it works
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        What
      </td>

      <td>
        * Parsing is applied to a specific selected field.
        * Each parsing rule is created by using a NRQL `WHERE` clause that determines which logs the rule will attempt to parse.
        * No-Code Parsing automatically detects data formats and patterns in your log samples.
      </td>
    </tr>

    <tr>
      <td>
        When
      </td>

      <td>
        * Parsing will only be applied once to each log message. If multiple parsing rules match the log, only the first that succeeds will be applied.
        * Parsing rules are unordered. If more than one parsing rules matches a log, one is chosen at random. Be sure to build your parsing rules so that they do not match the same logs.
        * Parsing takes place during log ingestion, before data is written to NRDB. Once data has been written to storage, it can no longer be parsed.
        * Parsing occurs in the pipeline <DNT>**before**</DNT> data enrichments take place. Be careful when defining the matching criteria for a parsing rule. If the criteria is based on an attribute that doesn't exist until after parsing or enrichment take place, that data won't be present in the logs when matching occurs. As a result, no parsing will happen.
      </td>
    </tr>

    <tr>
      <td>
        How
      </td>

      <td>
        * New Relic automatically highlights detected patterns in your log samples and displays them in the <DNT>**Patterns we detected**</DNT> section.
        * You can accept auto-detected patterns, manually select additional text to parse, or switch to traditional Grok/Regex syntax for advanced use cases.
        * The <DNT>**Preview output**</DNT> panel shows results against up to 10 sample logs, displaying which logs match your rule and what fields will be extracted.
        * Once patterns are detected, a rule is automatically drafted using standard Grok patterns that you can view and modify in the <DNT>**Rule code**</DNT> section.
      </td>
    </tr>
  </tbody>
</table>

## Parsing example [#parsing-defined]

A good example is a default NGINX access log containing unstructured text. It's useful for searching but not much else. Here's an example of a typical line:

```
127.180.71.3 - - [10/May/1997:08:05:32 +0000] "GET /downloads/product_1 HTTP/1.1" 304 0 "-" "Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)"
```

In an unparsed format, you would need to do a full text search to answer most questions. After parsing, the log is organized into attributes, like `response code` and `request URL`:

```json
{
  "remote_addr":"93.180.71.3",
  "time":"1586514731",
  "method":"GET",
  "path":"/downloads/product_1",
  "version":"HTTP/1.1",
  "response":"304",
  "bytesSent": 0,
  "user_agent": "Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)"
}
```

Parsing makes it easier to create [custom queries](/docs/using-new-relic/data/understand-data/query-new-relic-data) that facet on those values. This helps you understand the distribution of response codes per request URL and quickly find problematic pages.

## Create custom parsing rules [#custom-parsing]

You can create custom parsing rules using either the parsing UI with automatic pattern detection or by writing Grok/Regex patterns directly.

<Callout variant="important">
  Once a parsing rule is active, data parsed by the rule is permanently changed. This can't be reverted.
</Callout>

### Create a rule with automatic pattern detection [#automatic-detection]

The parsing UI provides a way to create parsing rules where New Relic automatically detects common patterns for you:

<CollapserGroup>
  <Collapser
    id="from-log-entry"
    title="Create a rule from a log entry"
  >
    1. Go to <DNT>**[one.newrelic.com](https://one.newrelic.com) > Logs**</DNT> (or <DNT>**APM & Services**</DNT> **>** Select an entity **>** <DNT>**Logs**</DNT>).
    2. Set a filter condition in the logs table to narrow down the logs you want to parse.
    3. Click a log to open <DNT>**Log details**</DNT>.
    4. Click on the field you want to parse and select <DNT>**Create ingest time parsing rule**</DNT>.
        <img
        title="Log parsing rules"
        alt="Screenshot of log filtering in UI"
        src="/images/logs_filtering.webp"
        />    
    5. In the <DNT>**Create a parsing rule**</DNT> page, provide a name for the rule and review the <DNT>**Patterns we detected**</DNT> section, which shows patterns that New Relic automatically identified. 
    6. Click and drag to highlight text in <DNT>**message data**</DNT> to interact with the patterns in the following ways:
        * <DNT>**Auto detect patterns**</DNT>: New Relic automatically detects patterns in your log samples and suggests them as fields to extract. Detected patterns are highlighted for easy identification.
        * <DNT>**Select text to parse**</DNT>: Manually select any text to create a new pattern and extract it as a field. This allows you to customize your parsing rules beyond the auto-detected patterns.
        * <DNT>**Remove selected patterns**</DNT>: If you find that a detected pattern is not relevant or is extracting unwanted data, you can click and drag on the pattern and click <DNT>**Remove selected patterns**</DNT> to exclude it from your parsing rule.
    7. Click the required pattern and configure it accordingly.
        <img
        title="Log parsing rules"
        alt="Screenshot of creating a parsing rule in UI"
        src="/images/create_a_parsing_rule.webp"
        />
    8. Review the <DNT>**Preview output**</DNT> panel. Check that sample logs show a green checkmark, indicating they match your rule.
        * To change your sample, under the <DNT>**Preview output**</DNT> panel, expand the required sample and click <DNT>**Use as sample**</DNT>. You'll see the <DNT>**Selected sample**</DNT> checkmark next to the sample you selected.
    9. Click <DNT>**Save rule**</DNT> to activate immediately, or <DNT>**Save as draft**</DNT> to activate the rule later.
  </Collapser>

  <Collapser
    id="from-parsing-view"
    title="Create a rule from the Parsing view"
  >
    1. Go to <DNT>**[one.newrelic.com](https://one.newrelic.com) > Logs > Parsing**</DNT> and click <DNT>**Create parsing rule**</DNT>.
    2. Enter a rule name and define your log filter using a NRQL `WHERE` clause or paste a sample log directly.
        * If you're using a log filter, click <DNT>**Run your query**</DNT>, select the field you want to parse, and click <DNT>**Next**</DNT>.
        * If you're pasting a sample log, paste the sample log, select the field you want to parse, and click <DNT>**Next**</DNT>.
          <img
          title="Log parsing rules"
          alt="Screenshot of creating a parsing rule in UI"
          src="/images/Create_a_parsing_rule_.webp"
          />
    3. Review the <DNT>**Patterns we detected**</DNT> section showing patterns that New Relic automatically identified. Click and drag to highlight text in <DNT>**message data**</DNT> to interact with the patterns in the following ways:
        * <DNT>**Auto detect patterns**</DNT>: New Relic automatically detects patterns in your log samples and suggests them as fields to extract. Detected patterns are highlighted for easy identification.
        * <DNT>**Select text to parse**</DNT>: Manually select any text to create a new pattern and extract it as a field. This allows you to customize your parsing rules beyond the auto-detected patterns.
        * <DNT>**Remove selected patterns**</DNT>: If you find that a detected pattern is not relevant or is extracting unwanted data, you can click and drag on the pattern and click <DNT>**Remove selected patterns**</DNT> to exclude it from your parsing rule.
    4. Click the required pattern and configure it accordingly.
        <img
        title="Log parsing rules"
        alt="Screenshot of creating a parsing rule in UI"
        src="/images/_create_a_parsing_rule_.webp"
        />
    5. Review the <DNT>**Preview output**</DNT> panel. Check that sample logs show a green checkmark, indicating they match your rule.
        * To change your sample, under the <DNT>**Preview output**</DNT> panel, expand the required sample and click <DNT>**Use as sample**</DNT>. You'll see the <DNT>**Selected sample**</DNT> checkmark next to the sample you selected.
    6. Click <DNT>**Save rule**</DNT> to activate immediately, or <DNT>**Save as draft**</DNT> to activate later.
  </Collapser>
</CollapserGroup>

<Callout variant="tip">
  When naming attributes, use lowercase with underscores (for example, `user_id`, `request_time`). Avoid special characters except underscores, and don't start names with numbers.  
</Callout>

### Write your own custom rules [#grok]

Many logs are formatted or structured in a unique way. In order to parse them, custom logic must be built and applied. Advanced users can click on <DNT>**Write your own rule**</DNT> on the <DNT>**Create a parsing rule**</DNT> page to switch to the code editor and modify patterns directly.

<img
title="Log parsing rules"
alt="Screenshot of switching to the new editor in UI"
src="/images/switch_to_new_editor.webp"
/>

Parsing patterns are specified using Grok, an industry standard for parsing log messages. Any incoming log with a `logtype` field will be checked against our [built-in parsing rules](#built-in-rules), and if possible, the associated Grok pattern is applied to the log.

Grok is a superset of regular expressions that adds built-in named patterns to be used in place of literal complex regular expressions. For instance, instead of having to remember that an integer can be matched with the regular expression `(?:[+-]?(?:[0-9]+))`, you can just write `%{INT}` to use the Grok pattern `INT`, which represents the same regular expression.

Grok patterns have the syntax:

```
%{PATTERN_NAME[:OPTIONAL_EXTRACTED_ATTRIBUTE_NAME[:OPTIONAL_TYPE[:OPTIONAL_PARAMETER]]]}
```

Where:

* `PATTERN_NAME` is one of the supported Grok patterns. The pattern name is just a user-friendly name representing a regular expression. They are exactly equal to the corresponding regular expression.
* `OPTIONAL_EXTRACTED_ATTRIBUTE_NAME`, if provided, is the name of the attribute that will be added to your log message with the value matched by the pattern name. It's equivalent to using a named capture group using regular expressions. If this is not provided, then the parsing rule will just match a region of your string, but not extract an attribute with its value.
* `OPTIONAL_TYPE` specifies the type of attribute value to extract. If omitted, values are extracted as strings. For instance, to extract the value `123` from `"File Size: 123"` as a number into attribute `file_size`, use `value: %{INT:file_size:int}`.
* `OPTIONAL_PARAMETER` specifies an optional parameter for certain types. Currently only the `datetime` type takes a parameter, see below for details.

You can also use a mix of regular expressions and Grok pattern names in your matching string.

Click this link for a list of supported [Grok patterns](https://github.com/thekrakken/java-grok/tree/master/src/main/resources/patterns), and here for a list of supported [Grok types](#grok-types).

Note that variable names must be explicitly set and be lowercase like `%{URI:uri}`. Expressions such as `%{URI}` or `%{URI:URI}` would not work.

<CollapserGroup>
  <Collapser
    id="grok-example"
    title="Grok example: Getting useful data out of your logs"
  >
    A log record could look something like this:

    ```json
    {
      "message": "54.3.120.2 2048 0"
    }
    ```

    This information is accurate, but it's not exactly intuitive what it means. Grok patterns help you extract and understand the telemetry data you want. For example, a log record like this is much easier to use:

    ```json
    {
      "host_ip": "43.3.120.2",
      "bytes_received": 2048,
      "bytes_sent": 0
    }
    ```

    To do this, create a Grok pattern that extracts these three fields; for example:

    ```
    %{IP:host_ip} %{INT:bytes_received} %{INT:bytes_sent}
    ```

    After processing, your log record will include the fields `host_ip`, `bytes_received`, and `bytes_sent`. Now you can use these fields in New Relic to filter, facet, and perform statistical operations on your log data. For more details about how to parse logs with Grok patterns in New Relic, see [our blog post](https://newrelic.com/blog/how-to-relic/how-to-use-grok-log-parsing).
  </Collapser>

  <Collapser
    id="grok-ui"
    title="Create a rule by writing Grok patterns"
  >
    1. Go to <DNT>**[one.newrelic.com](https://one.newrelic.com) > Logs > Parsing**</DNT> and click <DNT>**Create parsing rule**</DNT>.
    2. Enter a rule name and define your log filter using a NRQL `WHERE` clause or paste a sample log directly.
        * If you're using a log filter, click <DNT>**Run your query**</DNT>, select the field you want to parse, and click <DNT>**Next**</DNT>.
        * If you're pasting a sample log, paste the sample log, select the field you want to parse, and click <DNT>**Next**</DNT>.
    3. Click <DNT>**Write your own rule**</DNT> to switch from automatic pattern detection to the code editor.
    4. In the <DNT>**Grok pattern**</DNT> field, write your custom Grok pattern. For example, to parse an Inventory Service error message:

       ```
       Inventory error: %{DATA:error_message} for product %{INT:product_id}
       ```

       Where:
       * `Inventory error:` matches the literal text in your log
       * `%{DATA:error_message}` extracts the error message as a string attribute
       * `%{INT:product_id}` extracts the product ID as an integer attribute

    5. Review the <DNT>**Preview output**</DNT> panel to verify your pattern matches the sample logs correctly. Matching logs show a green checkmark.
    6. Click <DNT>**Save rule**</DNT> to activate immediately, or <DNT>**Save as draft**</DNT> to activate later.

Once active, your logs will be enriched with the extracted fields (like `error_message` and `product_id`), which you can use in queries, dashboards, and alerts.

    <Callout variant="tip">
      To switch to the legacy editor, click <DNT>**Switch to original editor**</DNT> in the top right corner of the <DNT>**Create a parsing rule**</DNT> page.
    </Callout>
  </Collapser>

  <Collapser
    id="grok-types"
    title="Supported Grok Types"
  >
    The `OPTIONAL_TYPE` field specifies the type of attribute value to extract. If omitted, values are extracted as strings.

    Supported types are:

    <table>
      <thead>
        <tr>
          <th>
            Type specified in Grok
          </th>

          <th>
            Type stored in the New Relic database
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `boolean`
          </td>

          <td>
            `boolean`
          </td>
        </tr>

        <tr>
          <td>
            `byte`
            `short`
            `int`
            `integer`
          </td>

          <td>
            `integer`
          </td>
        </tr>

        <tr>
          <td>
            `long`
          </td>

          <td>
            `long`
          </td>
        </tr>

        <tr>
          <td>
            `float`
          </td>

          <td>
            `float`
          </td>
        </tr>

        <tr>
          <td>
            `double`
          </td>

          <td>
            `double`
          </td>
        </tr>

        <tr>
          <td>
            `string` (default)
            `text`
          </td>

          <td>
            `string`
          </td>
        </tr>

        <tr>
          <td>
            `date`
            `datetime`
          </td>

          <td>
            Time as a `long`

            By default it is interpreted as ISO 8601. If `OPTIONAL_PARAMETER` is present, it specifies
            the [date and time pattern string](https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/text/SimpleDateFormat.html)to use to interpret the `datetime`.

            Note that this is only available during parsing. We have an additional, [separate timestamp interpretation step](/docs/logs/ui-data/timestamp-support) that occurs for all logs later in the ingestion pipeline.
          </td>
        </tr>

        <tr>
          <td>
            `json`
          </td>

          <td>
            JSON structured data. See [Parsing JSON mixed with plain text](#parsing-json) for more information.
          </td>
        </tr>

        <tr>
          <td>
            `csv`
          </td>

          <td>
            CSV data. See [Parsing CSV](#parsing-csv) for more information.
          </td>
        </tr>

        <tr>
          <td>
            `geo`
          </td>

          <td>
            Geographic location from IP addresses. See [Geolocating IP addresses (GeoIP)](#geo) for more information.
          </td>
        </tr>

        <tr>
          <td>
            `key value pairs`
          </td>

          <td>
            Key Value Pair . See [Parsing Key Value Pairs](#parsing-key-value-pairs) for more information.
          </td>
        </tr>
      </tbody>
    </table>
  </Collapser>

  <Collapser
    id="grok-multiline"
    title="Grok multiline parsing"
  >
    If you have multiline logs, be aware that the `GREEDYDATA` Grok pattern does not match newlines (it is equivalent to `.*`).

    So instead of using `%{GREEDYDATA:some_attribute}` directly, you will need to add the multiline flag in front of it: `(?s)%{GREEDYDATA:some_attribute}`
  </Collapser>

  <Collapser
    id="parsing-json"
    title="Parsing JSON mixed with plain text"
  >
    The New Relic logs pipeline parses your log JSON messages by default, but sometimes you have JSON log messages that are mixed with plain text. In this situation, you may want to be able to parse them and then be able to filter using the JSON attributes.
    If that is the case, you can use the `json` [grok type](#grok-syntax), which will parse the JSON captured by the grok pattern. This format relies on 3 main parts: the grok syntax, the prefex you would like to assign to the parsed json attributes, and the `json` [grok type](#grok-syntax). Using the `json` [grok type](#grok-syntax), you can extract and parse JSON from logs that are not properly formatted; for example, if your logs are prefixed with a date/time string:

    ```json
    2015-05-13T23:39:43.945958Z {"event": "TestRequest", "status": 200, "response": {"headers": {"X-Custom": "foo"}}, "request": {"headers": {"X-Custom": "bar"}}}
    ```

    In order to extract and parse the JSON data from this log format, create the following Grok expression:

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json}
    ```

    The resulting log is:

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    my_attribute_prefix.event: "TestRequest"
    my_attribute_prefix.status: 200
    my_attribute_prefix.response.headers.X-Custom: "foo"
    my_attribute_prefix.request.headers.X-Custom: "bar"
    ```

    You can define the list of attributes to extract or drop with the options `keepAttributes` or `dropAttributes`.
    For example, with the following Grok expression:

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json({"keepAttributes": ["my_attribute_prefix.event", "my_attribute_prefix.response.headers.X-Custom"]})}
    ```

    The resulting log is:

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    my_attribute_prefix.event: "TestRequest"
    my_attribute_prefix.request.headers.X-Custom: "bar"
    ```

    If you want to omit the `my_attribute_prefix` prefix, you can include the `"noPrefix": true` in the configuration.

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json({"noPrefix": true})}
    ```

    If you want to omit the `my_attribute_prefix` prefix and only keep the `status` attribute, you can include `"noPrefix": true` and `"keepAttributes: ["status"]` in the configuration.

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json({"noPrefix": true, "keepAttributes": ["status"]})}
    ```

    If your JSON has been escaped, you can use the `isEscaped` option to be able to parse it.
    If your JSON has been escaped and then quoted, you need to match the quotes as well, as shown below.
    For example, with the following Grok expression:

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} "%{GREEDYDATA:my_attribute_prefix:json({"isEscaped": true})}"
    ```

    Would be able to parse the escaped message:

    ```
    2015-05-13T23:39:43.945958Z "{\"event\": \"TestRequest\", \"status\": 200, \"response\": {\"headers\": {\"X-Custom\": \"foo\"}}, \"request\": {\"headers\": {\"X-Custom\": \"bar\"}}}"
    ```

    The resulting log is:

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    my_attribute_prefix.event: "TestRequest"
    my_attribute_prefix.status: 200
    my_attribute_prefix.response.headers.X-Custom: "foo"
    my_attribute_prefix.request.headers.X-Custom: "bar"
    ```

    To configure the `json` [Grok type](#grok-syntax), use `:json(_CONFIG_)`:

    * `json({"dropOriginal": true})`: Drop the JSON snippet that was used in parsing. When set to `true` (default value), the parsing rule will drop the original JSON snippet. Note the JSON attributes will remain in the message field.
    * `json({"dropOriginal": false})`: This will show the JSON payload that was extracted. When set to `false`, the full JSON-only payload will be displayed under an attribute named in `my_attribute_prefix` above. Note the JSON attributes will remain in the message field here as well giving the user 3 different views of the JSON data. If storage of all three versions is a concern it's recommended to use the default of `true` here.
    * `json({"depth": 62})`: Levels of depth you want to parse the JSON value (defaulted to 62).
    * `json({"keepAttributes": ["attr1", "attr2", ..., "attrN"]})`: Specifies which attributes will be extracted from the JSON. The provided list cannot be empty. If this configuration option is not set, all attributes are extracted.
    * `json({"dropAttributes": ["attr1", "attr2", ..., "attrN"]})`: Specifies which attributes to be dropped from the JSON. If this configuration option is not set, no attributes are dropped.
    * `json({"noPrefix": true})`: Set this option to `true` to remove the prefix from the attributes extracted from the JSON.
    * `json({"isEscaped": true})`: Set this option to `true` to parse JSON that has been escaped (which you typically see when JSON is stringified, for example `{\"key\": \"value\"}`)
  </Collapser>

  <Collapser
    id="parsing-csv"
    title="Parsing CSV"
  >
    If your system sends comma-separated values (CSV) logs and you need to parse them in New Relic, you can use the `csv` [Grok type](#grok-syntax), which parses the CSV captured by the Grok pattern.
    This format relies on 3 main parts: the Grok syntax, the prefix you would like to assign to the parsed CSV attributes, and the `csv` [Grok type](#grok-syntax). Using the `csv` [Grok type](#grok-syntax), you can extract and parse CSV from logs.

    Given the following CSV log line as an example:

    ```
    "2015-05-13T23:39:43.945958Z,202,POST,/shopcart/checkout,142,10"
    ```

    And a parsing rule with the following shape:

    ```
    %{GREEDYDATA:log:csv({"columns": ["timestamp", "status", "method", "url", "time", "bytes"]})}
    ```

    Will parse your log as follows:

    ```
    log.timestamp: "2015-05-13T23:39:43.945958Z"
    log.status: "202"
    log.method: "POST"
    log.url: "/shopcart/checkout"
    log.time: "142"
    log.bytes: "10"
    ```

    If you need to omit the "log" prefix, you can include the `"noPrefix": true` in the configuration.

    ```
    %{GREEDYDATA:log:csv({"columns": ["timestamp", "status", "method", "url", "time", "bytes"], "noPrefix": true})}
    ```

    #### Columns configuration:

    * It's mandatory to indicate the columns in the CSV Grok type configuration (which should be a valid JSON).
    * You can ignore any column by setting "\_" (underscore) as the column name to drop it from the resulting object.

    #### Optional configuration options:

    While the "columns" configuration is mandatory, it's possible to change the parsing of the CSV with the following settings.

    * <DNT>**dropOriginal**</DNT>: (Defaults to `true`) Drop the CSV snippet used in parsing. When set to `true` (default value), the parsing rule drops the original field.
    * <DNT>**noPrefix**</DNT>: (Defaults to `false`) Doesn't include the Grok field name as prefix on the resulting object.
    * <DNT>**separator**</DNT>: (Default to `,`) Defines the character/string that split each column.
      * Another common scenario is tab-separated values (TSV), for that you should indicate `\t` as separator, ex. `%{GREEDYDATA:log:csv({"columns": ["timestamp", "status", "method", "url", "time", "bytes"], "separator": "\t"})`
    * <DNT>**quoteChar**</DNT>: (Default to `"`) Defines the character that optionally surrounds a column content.
  </Collapser>

  <Collapser
    id="geo"
    title="Geolocating IP addresses (GeoIP)"
  >
    If your system sends logs containing IPv4 addresses, New Relic can locate them geographically and enrich log events with the specified attributes. You can use the `geo` [Grok type](#grok-syntax), which finds the position of an IP address captured by the Grok pattern. This format can be configured to return one or more fields related to the address, such as the city, country, and latitude/longitude of the IP.

    Given the following log line as an example:

    ```
    2015-05-13T23:39:43.945958Z 146.190.212.184
    ```

    And a parsing rule with the following shape:

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:ip:geo({"lookup":["city","region","countryCode", "latitude","longitude"]})}
    ```

    We'll parse your log as follows:

    ```
    ip: 146.190.212.184
    ip.city: North Bergen
    ip.countryCode: US
    ip.countryName: United States
    ip.latitude: 40.793
    ip.longitude: -74.0247
    ip.postalCode: 07047
    ip.region: NJ
    ip.regionName: New Jersey
    containerTimestamp:2015-05-13T23:39:43.945958Z
    ISO8601_TIMEZONE:Z
    ```

    #### Lookup configuration:

    It's mandatory to specify the desired `lookup` fields returned by the `geo` action. At least one item is required from the following options.

    * <DNT>**city**</DNT>: Name of city
    * <DNT>**countryCode**</DNT>: Abbreviation of country
    * <DNT>**countryName**</DNT>: Name of country
    * <DNT>**latitude**</DNT>: Latitude
    * <DNT>**longitude**</DNT>: Longitude
    * <DNT>**postalCode**</DNT>: Postal code, zip code, or similar
    * <DNT>**region**</DNT>: Abbreviation of state, province, or territory
    * <DNT>**regionName**</DNT>: Name of state, province, or territory
  </Collapser>

  <Collapser
    id="parsing-key-value-pairs"
    title="Parsing Key Value Pairs"
  >
    The New Relic logs pipeline parses your log messages by default, but sometimes you have log messages that are formatted as key-value pairs. In this situation, you may want to be able to parse them and then be able to filter using the key-value attributes.

    If that is the case, you can use the `keyvalue` [grok type](#grok-syntax), which will parse the key-value pairs captured by the grok pattern. This format relies on 3 main parts: the grok syntax, the prefix you would like to assign to the parsed key-value attributes, and the `keyvalue` [grok type](#grok-syntax). Using the `keyvalue` [grok type](#grok-syntax), you can extract and parse key-value pairs from logs that are not properly formatted; for example, if your logs are prefixed with a date/time string:

    ```json
      2015-05-13T23:39:43.945958Z key1=value1,key2=value2,key3=value3
    ```

    In order to extract and parse the key-value data from this log format, create the following Grok expression::

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:keyvalue()}
    ```

    The resulting log is:

    ```
      containerTimestamp: "2015-05-13T23:39:43.945958Z"
      my_attribute_prefix.key1: "value1"
      my_attribute_prefix.key2: "value2"
      my_attribute_prefix.key3: "value3"
    ```

    You can define the custom delimiter and separator also to extract the required key value pairs.

    ```json
    2015-05-13T23:39:43.945958Z event:TestRequest request:bar
    ```

    For example, with the following Grok expression:

    ```
      %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:keyvalue({"delimiter": " ", "keyValueSeparator": ":"})}
    ```

    The resulting log is:

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    my_attribute_prefix.event: "TestRequest"
    my_attribute_prefix.request: "bar"
    ```

    If you want to omit the `my_attribute_prefix` prefix, you can include the `"noPrefix": true` in the configuration.

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:keyValue({"noPrefix": true})}
    ```

    The resulting log is:

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    event: "TestRequest"
    request: "bar"
    ```


    If you want to set your custom quote character prefix, you can include the "quoteChar": in the configuration.

    ```json
    2015-05-13T23:39:43.945958Z nbn_demo='INFO',message='This message contains information with spaces ,sessionId='abc123'
    ```

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:keyValue({"quoteChar": "'"})}
    ```

    The resulting log is:

    ```
    "my_attribute_prefix.message": "'This message contains information with spaces",
    "my_attribute_prefix.nbn_demo": "INFO",
    "my_attribute_prefix.sessionId": "abc123"
    ```

    #### Grok Pattern Parameters

     You can customize the parsing behavior with the following options to suit your log formats:

    * **delimiter**
      * **Description:** String separating each key-value pair.
      * **Default Value:** `,` (comma)
      * **Override:** Set the field `delimiter` to change this behavior.

    * **keyValueSeparator**
      * **Description:** String used to assign values to keys.
      * **Default Value:** `=`
      * **Override:** Set the field `keyValueSeparator` for custom separator usage.

    * **quoteChar**
      * **Description:** Character used to enclose values with spaces or special characters.
      * **Default Value:** `"` (double quote)
      * **Override:** Define a custom character using `quoteChar`.

    * **dropOriginal**
      * **Description:** Drops the original log message after parsing. Useful for reducing log storage.
      * **Default Value:** `true`
      * **Override:** Set `dropOriginal` to `false` to retain the original log message.

    * **noPrefix**
      * **Description:** When `true`, excludes Grok field name as a prefix in the resulting object.
      * **Default Value:** `false`
      * **Override:** Enable by setting `noPrefix` to `true`.

    * **escapeChar**
      * **Description:** Define a custom escape character to handle special log characters.
      * **Default Value:** "\" (backslash)
      * **Override:** Customize with `escapeChar`.

    * **trimValues**
      * **Description:** Allows trimming of values that contain whitespace.
      * **Default Value:** `false`
      * **Override:** Set `trimValues` to `true` to activate trimming.

    * **trimKeys**
      * **Description:** Allows trimming of keys that contain whitespace.
      * **Default Value:** `true`
      * **Override:** Set `trimKeys` to `true` to activate trimming.
  </Collapser>
</CollapserGroup>

## Limits

Parsing is computationally expensive, which introduces risk. Parsing is done for custom rules defined in an account and for matching patterns to a log. A large number of patterns or poorly defined custom rules will consume a huge amount of memory and CPU resources while also taking a very long time to complete.

In order to prevent problems, we apply two parsing limits: per-message-per-rule and per-account.

<table>
  <thead>
    <tr>
      <th style={{ width: "200px" }}>
        Limit
      </th>

      <th>
        Description
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        Per-message-per-rule
      </td>

      <td>
        The per-message-per-rule limit prevents the time spent parsing any single message from being greater than 100 ms. If that limit is reached, the system will cease attempting to parse the log message with that rule.

        The ingestion pipeline will attempt to run any other applicable on that message, and the message will still be passed through the ingestion pipeline and stored in NRDB. The log message will be in its original, unparsed format.
      </td>
    </tr>

    <tr>
      <td>
        Per-account
      </td>

      <td>
        The per-account limit exists to prevent accounts from using more than their fair share of resources. The limit considers the total time spent processing <DNT>**all**</DNT> log messages for an account per-minute.
      </td>
    </tr>
  </tbody>
</table>

<Callout variant="tip">
  To easily check if your rate limits have been reached, go to your [system
  <DNT>**Limits**</DNT> page](/docs/telemetry-data-platform/ingest-manage-data/manage-data/view-system-limits#limits-ui) in the New Relic UI.
</Callout>

## Add the `logtype` attribute [#logattr]

When aggregating logs, it's important to provide metadata that makes it easy to organize, search, and parse those logs. One simple way of doing this is to add the attribute `logtype` to the log messages when they're shipped. [Built-in parsing rules](/docs/logs/ui-data/built-log-parsing-rules) are applied by default to certain `logtype` values.

<Callout variant="tip">
  The fields `logType`, `logtype`, and `LOGTYPE` are all supported for built-in rules. For ease of searching, we recommend that you align on a single syntax in your organization.
</Callout>

Here are some examples of how to add `logtype` to logs sent by some of our [supported shipping methods](/docs/logs/enable-new-relic-logs).

<CollapserGroup>
  <Collapser
    className="freq-link"
    id="infrastructure-log-forwarder-example"
    title="New Relic infrastructure agent example"
  >
    Add `logtype` as an [`attribute`](/docs/logs/forward-logs/forward-your-logs-using-infrastructure-agent#attributes). You must set the logtype for each named source.

    ```yml
    logs:
      - name: file-simple
        file: /path/to/file
        attributes:
          logtype: fileRaw
      - name: nginx-example
        file: /var/log/nginx.log
        attributes:
          logtype: nginx
    ```
  </Collapser>

  <Collapser
    className="freq-link"
    id="fluentd-example"
    title="Fluentd example"
  >
    Add a filter block to the `.conf` file, which uses a `record_transformer` to add a new field. In this example we use a `logtype` of `nginx` to trigger the build-in NGINX parsing rule. Check out other [Fluentd examples](https://github.com/newrelic/fluentd-examples).

    ```apacheconf
    <filter containers>
      @type record_transformer
      enable_ruby true
      <record>
        #Add logtype to trigger a built-in parsing rule for nginx access logs
        logtype nginx
        #Set timestamp from the value contained in the field "time"
        timestamp record["time"]
        #Add hostname and tag fields to all records
        hostname "#{Socket.gethostname}"
        tag ${tag}
      </record>
    </filter>
    ```
  </Collapser>

  <Collapser
    className="freq-link"
    id="fluentbit-example"
    title="Fluent Bit example"
  >
    Add a filter block to the `.conf` file that uses a `record_modifier` to add a new field. In this example we use a `logtype` of `nginx` to trigger the build-in NGINX parsing rule. Check out other [Fluent Bit examples](https://github.com/newrelic/fluentbit-examples).

    ```ini
    [FILTER]
        Name   record_modifier
        Match  *
        Record logtype nginx
        Record hostname ${HOSTNAME}
        Record service_name Sample-App-Name
    ```
  </Collapser>

  <Collapser
    className="freq-link"
    id="logstash-example"
    title="Logstash example"
  >
    Add a filter block to the Logstash configuration which uses an `add_field` mutate filter to add a new field. In this example we use a `logtype` of `nginx` to trigger the build-in NGINX parsing rule. Check out other [Logstash examples](https://github.com/newrelic/logstash-examples).

    ```ini
    filter {
      mutate {
        add_field => {
          "logtype" => "nginx"
          "service_name" => "myservicename"
          "hostname" => "%{host}"
        }
      }
    }
    ```
  </Collapser>

  <Collapser
    className="freq-link"
    id="api-example"
    title="Logs API example"
  >
    You can add attributes to the JSON request sent to New Relic. In this example we add a `logtype` attribute of value `nginx` to trigger the built-in NGINX parsing rule. Learn more about using the [Logs API](/docs/logs/new-relic-logs/log-api/introduction-log-api).

    ```
    POST /log/v1 HTTP/1.1
    Host: log-api.newrelic.com
    Content-Type: application/json
    X-License-Key: YOUR_LICENSE_KEY
    Accept: */*
    Content-Length: 133
    {
      "timestamp": TIMESTAMP_IN_UNIX_EPOCH,
      "message": "User 'xyz' logged in",
      "logtype": "nginx",
      "service": "login-service",
      "hostname": "login.example.com"
    }
    ```
  </Collapser>
</CollapserGroup>

## Manage parsing rules [#manage-rules]

After creating parsing rules, you can manage them from <DNT>**Logs > Parsing**</DNT>. Draft rules are saved but not yet activated. You can activate them when you're ready to apply them to incoming logs.

To edit a parsing rule:

1. In your parsing rules list, click the rule name or click <DNT>**... > Edit**</DNT> and make the required changes.
2. Click <DNT>**Save rule**</DNT> (or <DNT>**Save as draft**</DNT> if you want to keep it disabled).

Changes apply to logs ingested after the update. To enable, disable, or delete a parsing rule:

1. Find the rule in your parsing rules list and click <DNT>**...**</DNT> menu.
2. Choose an action:
   * <DNT>**Enable:**</DNT> Activates the draft rule (applies to newly ingested logs immediately)
   * <DNT>**Disable:**</DNT> Temporarily pauses the active rule
   * <DNT>**Delete:**</DNT> Removes the rule completely

## Troubleshooting [#troubleshooting]

If parsing isn't working the way you intended, it may be due to:

* <DNT>**Logic:**</DNT> The parsing rule matching logic doesn't match the logs you want.
* <DNT>**Timing:**</DNT> If your parsing matching rule targets a value that doesn't exist yet, it will fail. This can occur if the value is added later in the pipeline as part of the enrichment process.
* <DNT>**Limits:**</DNT> There is a fixed amount of time available every minute to process logs via parsing, patterns, drop filters, etc. If the maximum amount of time has been spent, parsing will be skipped for additional log event records.

To resolve these problems, create or adjust your [custom parsing rules](#custom-parsing).

## Related documentation [#related-docs]

<DocTiles>
  <DocTile title="Built-in log parsing rules" path="/docs/logs/ui-data/built-log-parsing-rules">Explore pre-built patterns from New Relic.</DocTile>
  <DocTile title="Query log data" path="/docs/logs/log-management/ui-data/query-logs">Use parsed attributes in NRQL queries.</DocTile>
</DocTiles>