---
title: Parsing log data
tags:
  - Logs
  - Log management
  - UI and data
translate:
  - jp
  - kr
metaDescription: How New Relic uses parsing and how to send customized log data.
redirects:
  - /docs/logs/new-relic-logs/ui-data/customizing-log-data
  - /docs/logs/new-relic-logs/ui-data/customize-logs-using-parsing
  - /docs/logs/log-management/ui-data/parsing
freshnessValidatedDate: never
---

Log <DNT>parsing</DNT> is the process of translating unstructured log data into attributes (key:value pairs) based on rules you define. You can use these attributes in your NRQL queries to facet or filter logs in useful ways.

New Relic parses log data automatically according to certain parsing rules. In this doc, you'll learn how logs parsing works, and how to create your own custom parsing rules.

You can also create, query, and manage your log parsing rules by using NerdGraph, our GraphQL API. A helpful tool for this is our [Nerdgraph API explorer](https://api.newrelic.com/graphiql). For more information, see our [NerdGraph tutorial for parsing](/docs/apis/nerdgraph/examples/nerdgraph-log-parsing-rules-tutorial/).

Here's a 5-minute video about log parsing:

<Video
  id="xPWM46yw3bQ"
  type="youtube"
/>

## Parsing example [#parsing-defined]

A good example is a default NGINX access log containing unstructured text. It's useful for searching but not much else. Here's an example of a typical line:

```
127.180.71.3 - - [10/May/1997:08:05:32 +0000] "GET /downloads/product_1 HTTP/1.1" 304 0 "-" "Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)"
```

In an unparsed format, you would need to do a full text search to answer most questions. After parsing, the log is organized into attributes, like `response code` and `request URL`:

```json
{
  "remote_addr":"93.180.71.3",
  "time":"1586514731",
  "method":"GET",
  "path":"/downloads/product_1",
  "version":"HTTP/1.1",
  "response":"304",
  "bytesSent": 0,
  "user_agent": "Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)"
}
```

Parsing makes it easier to create [custom queries](/docs/using-new-relic/data/understand-data/query-new-relic-data) that facet on those values. This helps you understand the distribution of response codes per request URL and quickly find problematic pages.

## How log parsing works [#how-it-works]

Here's an overview of how New Relic implements parsing of logs:

<table>
  <thead>
    <tr>
      <th style={{ width: "100px" }}>
        Log parsing
      </th>

      <th>
        How it works
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        What
      </td>

      <td>
        * Parsing is applied to a specific selected field.  By default, the `message` field is used. However, any field/attribute can be chosen, even one that doesn't currently exist in your data.
        * Each parsing rule is created by using a NRQL `WHERE` clause that determines which logs the rule will attempt to parse.
        * To simplify the matching process, we recommend adding a [`logtype`](#logtype) attribute to your logs. However, you are not limited to using `logtype`; one or more attributes can be used as matching criteria in the NRQL `WHERE` clause.
      </td>
    </tr>

    <tr>
      <td>
        When
      </td>

      <td>
        * Parsing will only be applied once to each log message. If multiple parsing rules match the log, only the first that succeeds will be applied.
        * Parsing rules are unordered. If more than one parsing rules matches a log, one is chosen at random. Be sure to build your parsing rules so that they do not match the same logs.
        * Parsing takes place during log ingestion, before data is written to NRDB. Once data has been written to storage, it can no longer be parsed.
        * Parsing occurs in the pipeline <DNT>**before**</DNT> data enrichments take place. Be careful when defining the matching criteria for a parsing rule. If the criteria is based on an attribute that doesn't exist until after parsing or enrichment take place, that data won't be present in the logs when matching occurs. As a result, no parsing will happen.
      </td>
    </tr>

    <tr>
      <td>
        How
      </td>

      <td>
        * Rules can be written in [Grok](#grok), regex, or a mixture of the two. Grok is a collection of patterns that abstract away complicated regular expressions.
      </td>
    </tr>
  </tbody>
</table>

## Parse attributes using Grok [#grok]

Parsing patterns are specified using Grok, an industry standard for parsing log messages. Any incoming log with a `logtype` field will be checked against our [built-in parsing rules](#built-in-rules), and if possible, the associated Grok pattern is applied to the log.

Grok is a superset of regular expressions that adds built-in named patterns to be used in place of literal complex regular expressions. For instance, instead of having to remember that an integer can be matched with the regular expression `(?:[+-]?(?:[0-9]+))`, you can just write `%{INT}` to use the Grok pattern `INT`, which represents the same regular expression.

Grok patterns have the syntax:

```
%{PATTERN_NAME[:OPTIONAL_EXTRACTED_ATTRIBUTE_NAME[:OPTIONAL_TYPE[:OPTIONAL_PARAMETER]]]}
```

Where:

* `PATTERN_NAME` is one of the supported Grok patterns. The pattern name is just a user-friendly name representing a regular expression. They are exactly equal to the corresponding regular expression.
* `OPTIONAL_EXTRACTED_ATTRIBUTE_NAME`, if provided, is the name of the attribute that will be added to your log message with the value matched by the pattern name. It's equivalent to using a named capture group using regular expressions. If this is not provided, then the parsing rule will just match a region of your string, but not extract an attribute with its value.
* `OPTIONAL_TYPE` specifies the type of attribute value to extract. If omitted, values are extracted as strings. For instance, to extract the value `123` from `"File Size: 123"` as a number into attribute `file_size`, use `value: %{INT:file_size:int}`.
* `OPTIONAL_PARAMETER` specifies an optional parameter for certain types. Currently only the `datetime` type takes a parameter, see below for details.

You can also use a mix of regular expressions and Grok pattern names in your matching string.

Click this link for a list of supported [Grok patterns](https://github.com/thekrakken/java-grok/tree/master/src/main/resources/patterns), and here for a list of supported [Grok types](#grok-types).

Note that variable names must be explicitly set and be lowercase like `%{URI:uri}`. Expressions such as `%{URI}` or `%{URI:URI}` would not work.

<CollapserGroup>
  <Collapser
    id="grok-example"
    title="Grok example: Getting useful data out of your logs"
  >
    A log record could look something like this:

    ```json
    {
      "message": "54.3.120.2 2048 0"
    }
    ```

    This information is accurate, but it's not exactly intuitive what it means. Grok patterns help you extract and understand the telemetry data you want. For example, a log record like this is much easier to use:

    ```json
    {
      "host_ip": "43.3.120.2",
      "bytes_received": 2048,
      "bytes_sent": 0
    }
    ```

    To do this, create a Grok pattern that extracts these three fields; for example:

    ```
    %{IP:host_ip} %{INT:bytes_received} %{INT:bytes_sent}
    ```

    After processing, your log record will include the fields `host_ip`, `bytes_received`, and `bytes_sent`. Now you can use these fields in New Relic to filter, facet, and perform statistical operations on your log data. For more details about how to parse logs with Grok patterns in New Relic, see [our blog post](https://newrelic.com/blog/how-to-relic/how-to-use-grok-log-parsing).
  </Collapser>

  <Collapser
    id="grok-ui"
    title="UI example: Creating a Grok parsing rule"
  >
    If you have the correct permissions, you can create parsing rules in our UI to create, test, and enable Grok parsing. For example, to get a specific type of error message for one of your microservices called Inventory Services, you would create a Grok parsing rule that looks for a specific error message and product. To do this:

    1. Give the rule a name; for example, `Inventory Services error parsing`.
    2. Select an existing field to parse (default = `message`), or enter a new field name.
    3. Identify the NRQL `WHERE` clause that acts as a pre-filter for the incoming logs; for example, `entity.name='Inventory Service'`. This pre-filter narrows down the number of logs that need to be processed by your rule, removing unnecessary processing.
    4. Select a matching log if one exists, or click on the Paste log tab to paste in a sample log.
    5. Add the Grok parsing rule; for example:

       ```
       Inventory error: %{DATA:error_message} for product %{INT:product_id}
       ```

       Where:

    * `Inventory error`: Your parsing rule's name
    * `error_message`: The error message you want to select
    * `product_id`: The product ID for Inventory Service

    6. Enable and save the parsing rule.

       Soon you will see that your Inventory Service logs are enriched with two new fields: `error_message` and `product_id`. From here, you can query on these fields, create charts and dashboards, set alerts, etc.

       For complete details, see [our docs for adding custom parsing rules in the UI](#custom-parsing).
  </Collapser>

  <Collapser
    id="grok-types"
    title="Supported Grok Types"
  >
    The `OPTIONAL_TYPE` field specifies the type of attribute value to extract. If omitted, values are extracted as strings.

    Supported types are:

    <table>
      <thead>
        <tr>
          <th>
            Type specified in Grok
          </th>

          <th>
            Type stored in the New Relic database
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `boolean`
          </td>

          <td>
            `boolean`
          </td>
        </tr>

        <tr>
          <td>
            `byte`
            `short`
            `int`
            `integer`
          </td>

          <td>
            `integer`
          </td>
        </tr>

        <tr>
          <td>
            `long`
          </td>

          <td>
            `long`
          </td>
        </tr>

        <tr>
          <td>
            `float`
          </td>

          <td>
            `float`
          </td>
        </tr>

        <tr>
          <td>
            `double`
          </td>

          <td>
            `double`
          </td>
        </tr>

        <tr>
          <td>
            `string` (default)
            `text`
          </td>

          <td>
            `string`
          </td>
        </tr>

        <tr>
          <td>
            `date`
            `datetime`
          </td>

          <td>
            Time as a `long`

            By default it is interpreted as ISO 8601. If `OPTIONAL_PARAMETER` is present, it specifies
            the [date and time pattern string](https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/text/SimpleDateFormat.html)to use to interpret the `datetime`.

            Note that this is only available during parsing. We have an additional, [separate timestamp interpretation step](/docs/logs/ui-data/timestamp-support) that occurs for all logs later in the ingestion pipeline.
          </td>
        </tr>

        <tr>
          <td>
            `json`
          </td>

          <td>
            JSON structured data. See [Parsing JSON mixed with plain text](#parsing-json) for more information.
          </td>
        </tr>

        <tr>
          <td>
            `csv`
          </td>

          <td>
            CSV data. See [Parsing CSV](#parsing-csv) for more information.
          </td>
        </tr>

        <tr>
          <td>
            `geo`
          </td>

          <td>
            Geographic location from IP addresses. See [Geolocating IP addresses (GeoIP)](#geo) for more information.
          </td>
        </tr>
      </tbody>
    </table>
  </Collapser>

  <Collapser
    id="grok-multiline"
    title="Grok multiline parsing"
  >
    If you have multiline logs, be aware that the `GREEDYDATA` Grok pattern does not match newlines (it is equivalent to `.*`).

    So instead of using `%{GREEDYDATA:some_attribute}` directly, you will need to add the multiline flag in front of it: `(?s)%{GREEDYDATA:some_attribute}`
  </Collapser>

  <Collapser
    id="parsing-json"
    title="Parsing JSON mixed with plain text"
  >
    The New Relic logs pipeline parses your log JSON messages by default, but sometimes you have JSON log messages that are mixed with plain text. In this situation, you may want to be able to parse them and then be able to filter using the JSON attributes.
    If that is the case, you can use the `json` [grok type](#grok-syntax), which will parse the JSON captured by the grok pattern. This format relies on 3 main parts: the grok syntax, the prefex you would like to assign to the parsed json attributes, and the `json` [grok type](#grok-syntax). Using the `json` [grok type](#grok-syntax), you can extract and parse JSON from logs that are not properly formatted; for example, if your logs are prefixed with a date/time string:

    ```json
    2015-05-13T23:39:43.945958Z {"event": "TestRequest", "status": 200, "response": {"headers": {"X-Custom": "foo"}}, "request": {"headers": {"X-Custom": "bar"}}}
    ```

    In order to extract and parse the JSON data from this log format, create the following Grok expression:

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json}
    ```

    The resulting log is:

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    my_attribute_prefix.event: "TestRequest"
    my_attribute_prefix.status: 200
    my_attribute_prefix.response.headers.X-Custom: "foo"
    my_attribute_prefix.request.headers.X-Custom: "bar"
    ```

    You can define the list of attributes to extract or drop with the options `keepAttributes` or `dropAttributes`.
    For example, with the following Grok expression:

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json({"keepAttributes": ["my_attribute_prefix.event", "my_attribute_prefix.response.headers.X-Custom"]})}
    ```

    The resulting log is:

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    my_attribute_prefix.event: "TestRequest"
    my_attribute_prefix.request.headers.X-Custom: "bar"
    ```

    If you want to omit the `my_attribute_prefix` prefix, you can include the `"noPrefix": true` in the configuration.

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json({"noPrefix": true})}
    ```

    If you want to omit the `my_attribute_prefix` prefix and only keep the `status` attribute, you can include `"noPrefix": true` and `"keepAttributes: ["status"]` in the configuration.

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json({"noPrefix": true, "keepAttributes": ["status"]})}
    ```

    If your JSON has been escaped, you can use the `isEscaped` option to be able to parse it.
    If your JSON has been escaped and then quoted, you need to match the quotes as well, as shown below.
    For example, with the following Grok expression:

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} "%{GREEDYDATA:my_attribute_prefix:json({"isEscaped": true})}"
    ```

    Would be able to parse the escaped message:

    ```
    2015-05-13T23:39:43.945958Z "{\"event\": \"TestRequest\", \"status\": 200, \"response\": {\"headers\": {\"X-Custom\": \"foo\"}}, \"request\": {\"headers\": {\"X-Custom\": \"bar\"}}}"
    ```

    The resulting log is:

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    my_attribute_prefix.event: "TestRequest"
    my_attribute_prefix.status: 200
    my_attribute_prefix.response.headers.X-Custom: "foo"
    my_attribute_prefix.request.headers.X-Custom: "bar"
    ```

    To configure the `json` [Grok type](#grok-syntax), use `:json(_CONFIG_)`:

    * `json({"dropOriginal": true})`: Drop the JSON snippet that was used in parsing. When set to `true` (default value), the parsing rule will drop the original JSON snippet. Note the JSON attributes will remain in the message field.
    * `json({"dropOriginal": false})`: This will show the JSON payload that was extracted. When set to `false`, the full JSON-only payload will be displayed under an attribute named in `my_attribute_prefix` above. Note the JSON attributes will remain in the message field here as well giving the user 3 different views of the JSON data. If storage of all three versions is a concern it's recommended to use the default of `true` here.
    * `json({"depth": 62})`: Levels of depth you want to parse the JSON value (defaulted to 62).
    * `json({"keepAttributes": ["attr1", "attr2", ..., "attrN"]})`: Specifies which attributes will be extracted from the JSON. The provided list cannot be empty. If this configuration option is not set, all attributes are extracted.
    * `json({"dropAttributes": ["attr1", "attr2", ..., "attrN"]})`: Specifies which attributes to be dropped from the JSON. If this configuration option is not set, no attributes are dropped.
    * `json({"noPrefix": true})`: Set this option to `true` to remove the prefix from the attributes extracted from the JSON.
    * `json({"isEscaped": true})`: Set this option to `true` to parse JSON that has been escaped (which you typically see when JSON is stringified, for example `{\"key\": \"value\"}`)
  </Collapser>

  <Collapser
    id="parsing-csv"
    title="Parsing CSV"
  >
    If your system sends comma-separated values (CSV) logs and you need to parse them in New Relic, you can use the `csv` [Grok type](#grok-syntax), which parses the CSV captured by the Grok pattern.
    This format relies on 3 main parts: the Grok syntax, the prefix you would like to assign to the parsed CSV attributes, and the `csv` [Grok type](#grok-syntax). Using the `csv` [Grok type](#grok-syntax), you can extract and parse CSV from logs.

    Given the following CSV log line as an example:

    ```
    "2015-05-13T23:39:43.945958Z,202,POST,/shopcart/checkout,142,10"
    ```

    And a parsing rule with the following shape:

    ```
    %{GREEDYDATA:log:csv({"columns": ["timestamp", "status", "method", "url", "time", "bytes"]})}
    ```

    Will parse your log as follows:

    ```
    log.timestamp: "2015-05-13T23:39:43.945958Z"
    log.status: "202"
    log.method: "POST"
    log.url: "/shopcart/checkout"
    log.time: "142"
    log.bytes: "10"
    ```

    If you need to omit the "log" prefix, you can include the `"noPrefix": true` in the configuration.

    ```
    %{GREEDYDATA:log:csv({"columns": ["timestamp", "status", "method", "url", "time", "bytes"], "noPrefix": true})}
    ```

    ### Columns configuration:

    * It's mandatory to indicate the columns in the CSV Grok type configuration (which should be a valid JSON).
    * You can ignore any column by setting "\_" (underscore) as the column name to drop it from the resulting object.

    ### Optional configuration options:

    While the "columns" configuration is mandatory, it's possible to change the parsing of the CSV with the following settings.

    * <DNT>**dropOriginal**</DNT>: (Defaults to `true`) Drop the CSV snippet used in parsing. When set to `true` (default value), the parsing rule drops the original field.
    * <DNT>**noPrefix**</DNT>: (Defaults to `false`) Doesn't include the Grok field name as prefix on the resulting object.
    * <DNT>**separator**</DNT>: (Default to `,`) Defines the character/string that split each column.
      * Another common scenario is tab-separated values (TSV), for that you should indicate `\t` as separator, ex. `%{GREEDYDATA:log:csv({"columns": ["timestamp", "status", "method", "url", "time", "bytes"], "separator": "\t"})`
    * <DNT>**quoteChar**</DNT>: (Default to `"`) Defines the character that optionally surrounds a column content.
  </Collapser>

  <Collapser
    id="geo"
    title="Geolocating IP addresses (GeoIP)"
  >
    If your system sends logs containing IPv4 addresses, New Relic can locate them geographically and enrich log events with the specified attributes. You can use the `geo` [Grok type](#grok-syntax), which finds the position of an IP address captured by the Grok pattern. This format can be configured to return one or more fields related to the address, such as the city, country, and latitude/longitude of the IP.

    Given the following log line as an example:

    ```
    2015-05-13T23:39:43.945958Z 146.190.212.184
    ```

    And a parsing rule with the following shape:

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:ip:geo({"lookup":["city","region","countryCode", "latitude","longitude"]})}
    ```

    We'll parse your log as follows:

    ```
    ip: 146.190.212.184
    ip.city: North Bergen
    ip.countryCode: US
    ip.countryName: United States
    ip.latitude: 40.793
    ip.longitude: -74.0247
    ip.postalCode: 07047
    ip.region: NJ
    ip.regionName: New Jersey
    containerTimestamp:2015-05-13T23:39:43.945958Z
    ISO8601_TIMEZONE:Z
    ```

    ### Lookup configuration:

    It's mandatory to specify the desired `lookup` fields returned by the `geo` action. At least one item is required from the following options.

    * <DNT>**city**</DNT>: Name of city
    * <DNT>**countryCode**</DNT>: Abbreviation of country
    * <DNT>**countryName**</DNT>: Name of country
    * <DNT>**latitude**</DNT>: Latitude
    * <DNT>**longitude**</DNT>: Longitude
    * <DNT>**postalCode**</DNT>: Postal code, zip code, or similar
    * <DNT>**region**</DNT>: Abbreviation of state, province, or territory
    * <DNT>**regionName**</DNT>: Name of state, province, or territory
  </Collapser>
</CollapserGroup>

## Organizing by logtype [#type]

New Relic's log ingestion pipeline can parse data by matching a log event to a rule that describes how the log should be parsed. There are two ways log events can be parsed:

* Use a [built-in rule](#built-in-rules).
* Define a [custom rule](#custom-parsing).

Rules are a combination of matching logic and parsing logic. Matching is done by defining a query match on an attribute of the logs. Rules aren't applied retroactively. Logs collected before a rule is created aren't parsed by that rule.

The simplest way to organize your logs and how they're parsed is to include the `logtype` field in your log event. This tells New Relic what built-in rule to apply to the logs.

<Callout variant="important">
  Once a parsing rule is active, data parsed by the rule is permanently changed.
  This can't be reverted.
</Callout>

## Limits

Parsing is computationally expensive, which introduces risk. Parsing is done for custom rules defined in an account and for matching patterns to a log. A large number of patterns or poorly defined custom rules will consume a huge amount of memory and CPU resources while also taking a very long time to complete.

In order to prevent problems, we apply two parsing limits: per-message-per-rule and per-account.

<table>
  <thead>
    <tr>
      <th style={{ width: "200px" }}>
        Limit
      </th>

      <th>
        Description
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        Per-message-per-rule
      </td>

      <td>
        The per-message-per-rule limit prevents the time spent parsing any single message from being greater than 100 ms. If that limit is reached, the system will cease attempting to parse the log message with that rule.

        The ingestion pipeline will attempt to run any other applicable on that message, and the message will still be passed through the ingestion pipeline and stored in NRDB. The log message will be in its original, unparsed format.
      </td>
    </tr>

    <tr>
      <td>
        Per-account
      </td>

      <td>
        The per-account limit exists to prevent accounts from using more than their fair share of resources. The limit considers the total time spent processing <DNT>**all**</DNT> log messages for an account per-minute.
      </td>
    </tr>
  </tbody>
</table>

<Callout variant="tip">
  To easily check if your rate limits have been reached, go to your [system
  <DNT>**Limits**</DNT> page](/docs/telemetry-data-platform/ingest-manage-data/manage-data/view-system-limits#limits-ui) in the New Relic UI.
</Callout>

## Built-in parsing rules [#built-in-rules]

Common log formats have well-established parsing rules already created for them. To get the benefit of built-in parsing rules, add the `logtype` attribute when forwarding logs. Set the value to something listed in the following table, and the rules for that type of log will be applied automatically.

### List of built-in rules [#rulesets]

The following `logtype` attribute values map to a predefined parsing rule. For example, to query the Application Load Balancer:

* From the New Relic UI, use the format `logtype:"alb"`.
* From [NerdGraph](/docs/apis/nerdgraph/examples/nerdgraph-log-parsing-rules-tutorial/), use the format `logtype = 'alb'`.

To learn what fields are parsed for each rule, see our documentation about [built-in parsing rules](/docs/logs/ui-data/built-log-parsing-rules).

<table>
  <thead>
    <tr>
      <th style={{ width: "200px" }}>
        `logtype`
      </th>

      <th>
        Log source
      </th>

      <th>
        Example matching query
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        [`apache`](/docs/logs/ui-data/built-log-parsing-rules#apache)
      </td>

      <td>
        Apache access logs
      </td>

      <td>
        `logtype:"apache"`
      </td>
    </tr>

    <tr>
      <td>
        [`apache_error`](/docs/logs/ui-data/built-log-parsing-rules#apache_error)
      </td>

      <td>
        Apache error logs
      </td>

      <td>
        `logtype:"apache_error"`
      </td>
    </tr>

    <tr>
      <td>
        [`alb`](/docs/logs/ui-data/built-log-parsing-rules#application-load-balancer)
      </td>

      <td>
        Application load balancer logs
      </td>

      <td>
        `logtype:"alb"`
      </td>
    </tr>

    <tr>
      <td>
        [`cassandra`](/docs/logs/ui-data/built-log-parsing-rules#cassandra)
      </td>

      <td>
        Cassandra logs
      </td>

      <td>
        `logtype:"cassandra"`
      </td>
    </tr>

    <tr>
      <td>
        [`cloudfront-web`](/docs/logs/ui-data/built-log-parsing-rules#cloudfront)
      </td>

      <td>
        CloudFront (standard web logs)
      </td>

      <td>
        `logtype:"cloudfront-web"`
      </td>
    </tr>

    <tr>
      <td>
        [`cloudfront-rtl`](/docs/logs/ui-data/built-log-parsing-rules#cloudfront-rtl)
      </td>

      <td>
        CloudFront (real-time web logs)
      </td>

      <td>
        `logtype:"cloudfront-rtl"`
      </td>
    </tr>

    <tr>
      <td>
        [`elb`](/docs/logs/ui-data/built-log-parsing-rules#elastic-load-balancer)
      </td>

      <td>
        Elastic Load Balancer logs
      </td>

      <td>
        `logtype:"elb"`
      </td>
    </tr>

    <tr>
      <td>
        [`haproxy_http`](/docs/logs/ui-data/built-log-parsing-rules#haproxy)
      </td>

      <td>
        HAProxy logs
      </td>

      <td>
        `logtype:"haproxy_http"`
      </td>
    </tr>

    <tr>
      <td>
        [`ktranslate-health`](/docs/logs/ui-data/built-log-parsing-rules#ktranslate-health)
      </td>

      <td>
        KTranslate container health logs
      </td>

      <td>
        `logtype:"ktranslate-health"`
      </td>
    </tr>

    <tr>
      <td>
        [`linux_cron`](/docs/logs/ui-data/built-log-parsing-rules/#linux_cron)
      </td>

      <td>
        Linux cron
      </td>

      <td>
        `logtype:"linux_cron"`
      </td>
    </tr>

    <tr>
      <td>
        [`linux_messages`](/docs/logs/ui-data/built-log-parsing-rules/#linux_messages)
      </td>

      <td>
        Linux messages
      </td>

      <td>
        `logtype:"linux_messages"`
      </td>
    </tr>

    <tr>
      <td>
        [`iis_w3c`](/docs/logs/ui-data/built-log-parsing-rules/#iis)
      </td>

      <td>
        Microsoft IIS server logs - W3C format
      </td>

      <td>
        `logtype:"iis_w3c"`
      </td>
    </tr>

    <tr>
      <td>
        [`mongodb`](/docs/logs/ui-data/built-log-parsing-rules#mongodb)
      </td>

      <td>
        MongoDB logs
      </td>

      <td>
        `logtype:"mongodb"`
      </td>
    </tr>

    <tr>
      <td>
        [`monit`](/docs/logs/ui-data/built-log-parsing-rules#monit)
      </td>

      <td>
        Monit logs
      </td>

      <td>
        `logtype:"monit"`
      </td>
    </tr>

    <tr>
      <td>
        [`mysql-error`](/docs/logs/ui-data/built-log-parsing-rules#mysql-error)
      </td>

      <td>
        MySQL error logs
      </td>

      <td>
        `logtype:"mysql-error"`
      </td>
    </tr>

    <tr>
      <td>
        [`nginx`](/docs/logs/ui-data/built-log-parsing-rules#nginx)
      </td>

      <td>
        NGINX access logs
      </td>

      <td>
        `logtype:"nginx"`
      </td>
    </tr>

    <tr>
      <td>
        [`nginx-error`](/docs/logs/ui-data/built-log-parsing-rules#nginx-error)
      </td>

      <td>
        NGINX error logs
      </td>

      <td>
        `logtype:"nginx-error"`
      </td>
    </tr>

    <tr>
      <td>
        [`postgresql`](/docs/logs/ui-data/built-log-parsing-rules#postgresql)
      </td>

      <td>
        Postgresql logs
      </td>

      <td>
        `logtype:"postgresql"`
      </td>
    </tr>

    <tr>
      <td>
        [`rabbitmq`](/docs/logs/ui-data/built-log-parsing-rules#rabbitmq)
      </td>

      <td>
        Rabbitmq logs
      </td>

      <td>
        `logtype:"rabbitmq"`
      </td>
    </tr>

    <tr>
      <td>
        [`redis`](/docs/logs/ui-data/built-log-parsing-rules#redis)
      </td>

      <td>
        Redis logs
      </td>

      <td>
        `logtype:"redis"`
      </td>
    </tr>

    <tr>
      <td>
        [`route-53`](/docs/logs/ui-data/built-log-parsing-rules#route53)
      </td>

      <td>
        Route 53 logs
      </td>

      <td>
        `logtype:"route-53"`
      </td>
    </tr>

    <tr>
      <td>
        [`syslog-rfc5424`](/docs/logs/ui-data/built-log-parsing-rules/#syslog-rfc5424)
      </td>

      <td>
        Syslogs with RFC5424 format
      </td>

      <td>
        `logtype:"syslog-rfc5424"`
      </td>
    </tr>
  </tbody>
</table>

### Add the `logtype` attribute [#logattr]

When aggregating logs, it's important to provide metadata that makes it easy to organize, search, and parse those logs. One simple way of doing this is to add the attribute `logtype` to the log messages when they're shipped. [Built-in parsing rules](#built-in-rules) are applied by default to certain `logtype` values.

<Callout variant="tip">
  The fields `logType`, `logtype`, and `LOGTYPE` are all supported for built-in rules. For ease of searching, we recommend that you align on a single syntax in your organization.
</Callout>

Here are some examples of how to add `logtype` to logs sent by some of our [supported shipping methods](/docs/logs/enable-new-relic-logs).

<CollapserGroup>
  <Collapser
    className="freq-link"
    id="infrastructure-log-forwarder-example"
    title="New Relic infrastructure agent example"
  >
    Add `logtype` as an [`attribute`](/docs/logs/forward-logs/forward-your-logs-using-infrastructure-agent#attributes). You must set the logtype for each named source.

    ```yml
    logs:
      - name: file-simple
        file: /path/to/file
        attributes:
          logtype: fileRaw
      - name: nginx-example
        file: /var/log/nginx.log
        attributes:
          logtype: nginx
    ```
  </Collapser>

  <Collapser
    className="freq-link"
    id="fluentd-example"
    title="Fluentd example"
  >
    Add a filter block to the `.conf` file, which uses a `record_transformer` to add a new field. In this example we use a `logtype` of `nginx` to trigger the build-in NGINX parsing rule. Check out other [Fluentd examples](https://github.com/newrelic/fluentd-examples).

    ```apacheconf
    <filter containers>
      @type record_transformer
      enable_ruby true
      <record>
        #Add logtype to trigger a built-in parsing rule for nginx access logs
        logtype nginx
        #Set timestamp from the value contained in the field "time"
        timestamp record["time"]
        #Add hostname and tag fields to all records
        hostname "#{Socket.gethostname}"
        tag ${tag}
      </record>
    </filter>
    ```
  </Collapser>

  <Collapser
    className="freq-link"
    id="fluentbit-example"
    title="Fluent Bit example"
  >
    Add a filter block to the `.conf` file that uses a `record_modifier` to add a new field. In this example we use a `logtype` of `nginx` to trigger the build-in NGINX parsing rule. Check out other [Fluent Bit examples](https://github.com/newrelic/fluentbit-examples).

    ```ini
    [FILTER]
        Name   record_modifier
        Match  *
        Record logtype nginx
        Record hostname ${HOSTNAME}
        Record service_name Sample-App-Name
    ```
  </Collapser>

  <Collapser
    className="freq-link"
    id="logstash-example"
    title="Logstash example"
  >
    Add a filter block to the Logstash configuration which uses an `add_field` mutate filter to add a new field. In this example we use a `logtype` of `nginx` to trigger the build-in NGINX parsing rule. Check out other [Logstash examples](https://github.com/newrelic/logstash-examples).

    ```ini
    filter {
      mutate {
        add_field => {
          "logtype" => "nginx"
          "service_name" => "myservicename"
          "hostname" => "%{host}"
        }
      }
    }
    ```
  </Collapser>

  <Collapser
    className="freq-link"
    id="api-example"
    title="Logs API example"
  >
    You can add attributes to the JSON request sent to New Relic. In this example we add a `logtype` attribute of value `nginx` to trigger the built-in NGINX parsing rule. Learn more about using the [Logs API](/docs/logs/new-relic-logs/log-api/introduction-log-api).

    ```
    POST /log/v1 HTTP/1.1
    Host: log-api.newrelic.com
    Content-Type: application/json
    X-License-Key: YOUR_LICENSE_KEY
    Accept: */*
    Content-Length: 133
    {
      "timestamp": TIMESTAMP_IN_UNIX_EPOCH,
      "message": "User 'xyz' logged in",
      "logtype": "nginx",
      "service": "login-service",
      "hostname": "login.example.com"
    }
    ```
  </Collapser>
</CollapserGroup>

## Create and view custom parsing rules [#custom-parsing]

Many logs are formatted or structured in a unique way. In order to parse them, custom logic must be built and applied.

<img
  title="Log parsing rules"
  alt="Screenshot of log parsing in UI"
  src="/images/logs_screenshot-full_parsing-ui.webp"
/>

<figcaption>
  From the left nav in the logs UI, select <DNT>**Parsing**</DNT>, then create your own custom parsing rule with a valid NRQL `WHERE` clause and Grok pattern.
</figcaption>

To create and manage your own, custom parsing rules:

1. Go to <DNT>**[one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > Logs**</DNT>.
2. From <DNT>**Manage data**</DNT> on the left nav of the logs UI, click <DNT>**Parsing**</DNT>, then click <DNT>**Create parsing rule**</DNT>.
3. Enter a name for the new parsing rule.
4. Select an existing field to parse (default = `message`), or enter a new field name.
5. Enter a valid NRQL `WHERE` clause to match the logs you want to parse.
6. Select a matching log if one exists, or click on the <DNT>**Paste log**</DNT> tab to paste in a sample log. Note that if you copy text from the logs UI or the query builder to paste into the parsing UI, ensure that it's the <DNT>_Unformatted_</DNT> version.
7. Enter the parsing rule and validate it's working by viewing the results in the <DNT>**Output**</DNT> section. To learn about Grok and custom parsing rules, read [our blog post about how to parse logs with Grok patterns](https://blog.newrelic.com/product-news/how-to-use-grok-log-parsing).
8. Enable and save the custom parsing rule.

To view existing parsing rules:

1. Go to <DNT>**[one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > Logs**</DNT>.
2. From <DNT>**Manage data**</DNT> on the left nav of the logs UI, click <DNT>**Parsing**</DNT>.

## Troubleshooting [#troubleshooting]

If parsing isn't working the way you intended, it may be due to:

* <DNT>**Logic:**</DNT> The parsing rule matching logic doesn't match the logs you want.
* <DNT>**Timing:**</DNT> If your parsing matching rule targets a value that doesn't exist yet, it will fail. This can occur if the value is added later in the pipeline as part of the enrichment process.
* <DNT>**Limits:**</DNT> There is a fixed amount of time available every minute to process logs via parsing, patterns, drop filters, etc. If the maximum amount of time has been spent, parsing will be skipped for additional log event records.

To resolve these problems, create or adjust your [custom parsing rules](#custom-parsing).
