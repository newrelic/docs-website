---
title: AI monitoring APIs
tags:
  - Agents
  - Java agent
  - API guides
metaDescription: 'For information about customizing New Relic''s Java agent for AI monitoring.'
freshnessValidatedDate: never
redirects:
    - /docs/apm/agents/java-agent/api-guides/java-ai-monitoring-apis
    - /docs/apm/agents/java-agent/api-guides/java-ai-monitoring
---

When you've instrumented your app with AI monitoring, you can access some APIs for collecting token count and user feedback. To use the AI monitoring APIs, check that your Java agent is updated to version 8.12.0 or higher.

## Record token count [#token-count]

If you've disabled the agent with `ai_monitoring.record_content.enabled=false`, you can use the [`setLlmTokenCountCallback(LlmTokenCountCallback llmTokenCountCallback)`](https://newrelic.github.io/java-agent-api/javadoc/com/newrelic/api/agent/AiMonitoring.html#setLlmTokenCountCallback) API to calculate `token_count` attributes. This calculates token counts for `LlmEmbedding` and `LlmChatCompletionMessage` events without recording message contents. 

1. Implement the `LlmTokenCountCallback` so it overrides the `calculateLlmTokenCount(String model, String content)` method. This calculates a token count based on a given LLM model name and the LLM message content or prompt: 

    ```java
        class MyTokenCountCallback implements LlmTokenCountCallback {
            @Override
            public int calculateLlmTokenCount(String model, String content) {
                // Implement custom token calculating logic here based on the LLM model and content.
                // Return an integer representing the calculated token count.
                return 0;
            }
        }
    ```
1.  Create an instance of the `LlmTokenCountCallback` implementation to register the callback, then pass it to the `setLlmTokenCountCallback` API. For example: 

    ```java
        LlmTokenCountCallback myTokenCountCallback = new MyTokenCountCallback();
            // The callback needs to be registered at some point before invoking the LLM model
        NewRelic.getAgent().getAiMonitoring.setLlmTokenCountCallback(myTokenCountCallback);
    ```

To use the callback, implement `LlmTokenCountCallback` so that:

* It returns an integer that represents the number of tokens for a particular prompt, completion message, or embedding. 
* If the values are less than or equal to 0, `LlmTokenCountCallbacks` will not be attached to an event.

Keep in mind that this API should be called only once. Calling this API multiple times will replace each previous callback. 

## Record user feedback [#user-feedback]

AI monitoring can correlate trace IDs between a generated message from your LLM models and an end user's feedback. The [`recordLlmFeedbackEvent`](https://newrelic.github.io/java-agent-api/javadoc/com/newrelic/api/agent/AiMonitoring.html#recordLlmFeedbackEvent) API constructs an argument with a map of the `LlmFeedbackEventAttributes.Builder` class. 

Recording user feedback takes two steps:

1. Use the [`TraceMetadata.getTraceId()`](https://newrelic.github.io/java-agent-api/javadoc/com/newrelic/api/agent/TraceMetadata.html#getTraceId) API to acquire the trace ID for the currently executing transaction:

    ```java
    String traceId = NewRelic.getAgent().getTraceMetadata().getTraceId();
    ```

1. Add the [`recordLlmFeedbackEvent(Map<String, Object> llmFeedbackEventAttributes)`](https://newrelic.github.io/java-agent-api/javadoc/com/newrelic/api/agent/AiMonitoring.html#recordLlmFeedbackEvent) to correlate the trace ID with a feedback event. Here is an example of how you might record an LLM feedback event:

    ```java
    String traceId = ... // acquired directly from New Relic API or retrieved from some propagation mechanism

    Map<String, String> metadata = new HashMap<>();
    metadata.put("interestingKey", "interestingVal");

    LlmFeedbackEventAttributes.Builder llmFeedbackEvenAttrBuilder = new LlmFeedbackEventAttributes.Builder(traceId, ratingString);

    Map<String, Object> llmFeedbackEventAttributes = llmFeedbackEvenAttrBuilder
            .category("General")
            .message("Ok")
            .metadata(metadata)
            .build();

    NewRelic.getAgent().getAiMonitoring().recordLlmFeedbackEvent(llmFeedbackEventAttributes);
    ```

The `LlmFeedbackEventAttributes.Builder` class takes the following parameters:

<table>
    <thead>
        <tr>
            <th style="width:250px">
                Parameter
            </th>
            <th>
                Definition
            </th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>
                `trace_id` (Required)
            </td>
            <td>
                ID of the trace where the chat completion(s) related to the feedback occurred
            </td>
        </tr>
        <tr>
            <td>
                `rating` (Required)
            </td>
            <td>
                An end user rating. For example, values could read `good`, `bad`, or an integer rating.
            </td>
        </tr>
        <tr>
            <td>
                `category` (Optional)
            </td>
            <td>
                Category of the feedback as provided by the end user. For example, values could read `informative` or `inaccurate`.
            </td>
        </tr>
        <tr>
            <td>
                `message` (Optional)
            </td>
            <td>
                Freeform text feedback from an end user
            </td>
        </tr>
        <tr>
            <td>
                `metadata` (Optional)
            </td>
            <td>
                Set of key-value pairs to store any other desired data to submit with the feedback event
            </td>
        </tr>
    </tbody>
</table>

If the user feedback records on a different thread or different service from where the LLM prompt or response occurred, you need to:

* Acquire the trace ID from the originating thread or service
* Propagate that ID to where the user feedback event will be recorded

## Add custom LLM attributes [#custom-attributes]

You can adjust your agent to collect custom LLM attributes:

* Any custom attributes added with the [`NewRelic.addCustomParameter(...)`](https://newrelic.github.io/java-agent-api/javadoc/com/newrelic/api/agent/NewRelic.html#addCustomParameter(java.lang.String,boolean)) API can be prefixed with `llm.` This automatically copies those attributes to `LlmEvent`s
* If you're adding custom attributes to `LlmEvent`s with the `addCustomParameters` API , make sure the API call occurs before invoking the Bedrock SDK.
* One optional custom attribute with special meaning is `llm.conversation_id`. You can use this to group LLM messages into specific conversations in APM.