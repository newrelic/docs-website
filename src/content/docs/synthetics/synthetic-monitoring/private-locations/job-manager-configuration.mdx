---
title: Synthetics job manager configuration
tags:
  - Synthetics
  - Synthetic monitoring
  - Private locations
metaDescription: Customize your New Relic synthetics job manager.
redirects:
---

Learn how to configure your [synthetics job manager](/docs/synthetics/synthetic-monitoring/private-locations/install-job-manager) by using [environment variables](#environment-variables) in your configuration.

<Callout variant="important">
  [Custom modules](/docs/synthetics/synthetic-monitors/private-locations/containerized-private-minion-cpm-configuration/#custom-modules), [permanent data storage](/docs/synthetics/synthetic-monitors/private-locations/containerized-private-minion-cpm-configuration/#preserve-data-volume), and [user defined environment variables](/docs/synthetics/synthetic-monitors/private-locations/containerized-private-minion-cpm-configuration/#vars-scripted-monitors) are not supported for the synthetics job manager at this time.
</Callout>

As a note, New Relic is not liable for any modifications you make to the synthetics job manager files.

## Environment variables [#environment-variables]

Environmental variables allow you to fine-tune the synthetics job manager configuration to meet your specific environmental and functional needs.

<CollapserGroup>
  <Collapser
    id="docker-env-config"
    title="Docker environment configuration"
  >
    The variables are provided at startup using the `-e, --env` argument.

    The following table shows all the environment variables that synthetics job manager supports. `PRIVATE_LOCATION_KEY` is required, and all other variables are optional.

    <table>
      <thead>
        <tr>
          <th>
            Name
          </th>

          <th>
            Description
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `PRIVATE_LOCATION_KEY`
          </td>

          <td>
            **REQUIRED.** Private location key, as found on the Private Location entity list.
          </td>
        </tr>

        <tr>
          <td>
            `DOCKER_API_VERSION`
          </td>

          <td>
            Format: `"vX.Y"` API version to be used with the given Docker service.

            Default: `v1.35.`
          </td>
        </tr>

        <tr>
          <td>
            `DOCKER_HOST`
          </td>

          <td>
            Points the synthetics job manager to a given `DOCKER_HOST`. If absent, the default value is `/var/run/docker.sock.`
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_ENDPOINT`
          </td>

          <td>
            For US-based accounts, the endpoint is: `https://synthetics-horde.nr-data.net.`

            For [EU-based](/docs/using-new-relic/welcome-new-relic/get-started/introduction-eu-region-data-center#partner-hierarchy) accounts, the endpoint is: `https://synthetics-horde.eu01.nr-data.net/`

            Ensure your synthetics job manager can connect to the appropriate endpoint in order to serve your monitor.
          </td>
        </tr>

        <tr>
          <td>
            `DOCKER_REGISTRY`
          </td>

          <td>
            The Docker Registry domain where the runtime images are hosted. Use this to override `docker.io` as the default.
          </td>
        </tr>

        <tr>
          <td>
            `DOCKER_REPOSITORY`
          </td>

          <td>
            The Docker repository / organization where the runtime images are hosted. Use this to override `newrelic` as the default.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_HOST`
          </td>

          <td>
            Proxy server host used for Horde communication. Format: `"localhost"`.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_PORT`
          </td>

          <td>
            Proxy server port used for Horde communication. Format: `8888`.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_USERNAME`
          </td>

          <td>
            Proxy server username used for Horde communication. Format: `"username"`.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_PW`
          </td>

          <td>
            Proxy server password used for Horde communication. Format: `"password"`.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_ACCEPT_SELF_SIGNED_CERT`
          </td>

          <td>
            Accept self signed proxy certificates for the proxy server connection used for Horde communication? Acceptable values: `true`
          </td>
        </tr>

        <tr>
          <td>
            `CHECK_TIMEOUT`
          </td>

          <td>
            The maximum amount of seconds that your monitor checks are allowed to run. This value must be an integer between 0 seconds (excluded) and 900 seconds (included) (for example, from 1 second to 15 minutes).

            Default: 180 seconds
          </td>
        </tr>

        <tr>
          <td>
            `LOG_LEVEL`
          </td>

          <td>
            When contacting New Relic Support, they may ask you to increase this to `"DEBUG"` or `"TRACE"`.

            Default: `INFO.`
          </td>
        </tr>

      </tbody>
    </table>
  </Collapser>

  <Collapser
    id="kubernetes-env-config"
    title="Kubernetes environment configuration"
  >
    The variables are provided at startup using the `--set` argument.

    The following list shows all the environment variables that synthetics job manager supports. `synthetics.privateLocationKey` is required, and all other variables are optional.

    A number of additional advanced settings are available and fully documented in [our Helm chart README](https://github.com/newrelic/helm-charts/blob/master/charts/synthetics-job-manager/README.md)

    <table>
      <thead>
        <tr>
          <th>
            Name
          </th>

          <th>
            Description
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `synthetics.privateLocationKey`
          </td>

          <td>
            **REQUIRED if `synthetics.privateLocationKeySecretName` is not set**. [Private location key](/docs/synthetics/synthetic-monitoring/private-locations/install-job-manager/#private-location-key) of the private location, as found on the private location web page.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.privateLocationKeySecretName`
          </td>

          <td>
            **REQUIRED if `synthetics.privateLocationKey` is not set**. Name of the Kubernetes secret that contains the key `privateLocationKey`, which contains the authentication key associated with your synthetics private location.
          </td>
        </tr>

        <tr>
          <td>
            `replicaCount`
          </td>

          <td>
            Number of replicas to maintain with your installation

            Default: `1.`
          </td>
        </tr>

        <tr>
          <td>
            `imagePullSecrets`
          </td>

          <td>
            The name of the secret object used to pull an image from a specified container registry.
          </td>
        </tr>
        
        <tr>
          <td>
            `fullnameOverride`
          </td>

          <td>
            Name override used for your Deployment, replacing the default.
          </td>
        </tr>
        
        <tr>
          <td>
            `appVersionOverride`
          </td>

          <td>
            Release version of synthetics-job-manager to use instead of the version specified in [chart.yml](https://github.com/newrelic/helm-charts/blob/master/charts/synthetics-job-manager/Chart.yaml).
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.logLevel`
          </td>

          <td>
            If contacting New Relic Support, they may ask you to increase this to `"DEBUG"` or `"TRACE"`.

            Default: `INFO.`
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.hordeApiEndpoint`
          </td>

          <td>
            For US-based accounts, the endpoint is: `https://synthetics-horde.nr-data.net.`

            For [EU-based](/docs/using-new-relic/welcome-new-relic/get-started/introduction-eu-region-data-center#partner-hierarchy) accounts, the endpoint is: `https://synthetics-horde.eu01.nr-data.net/`

            Ensure your synthetics job manager can connect to the appropriate endpoint in order to serve your monitor.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.minionDockerRunnerRegistryEndpoint`
          </td>

          <td>
            The Docker Registry and Organization where the Minion Runner image is hosted. Use this to override `quay.io/newrelic` as the default (for example, `docker.io/newrelic`)
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.apiProxyHost`
          </td>

          <td>
            Proxy server used for Horde communication. Format: `"host"`.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.apiProxyPort`
          </td>

          <td>
            Proxy server port used for Horde communication. Format: `port`.
          </td>
        </tr>


        <tr>
          <td>
            `synthetics.hordeApiProxySelfSignedCert`
          </td>

          <td>
            Accept self signed certificates when using a proxy server for Horde communication. Acceptable values: `true`.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.hordeApiProxyUsername`
          </td>

          <td>
            Proxy server username for Horde communication. Format: `"username"`
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.hordeApiProxyPw`
          </td>

          <td>
            Proxy server password for Horde communication. Format: `"password"`.
          </td>
        </tr>

        <tr>
          <td>
            `global.checkTimeout`
          </td>

          <td>
            The maximum amount of seconds that your monitor checks are allowed to run. This value must be an integer between 0 seconds (excluded) and 900 seconds (included) (for example, from 1 second to 15 minutes).

            Default: 180 seconds
          </td>
        </tr>

        <tr>
          <td>
            `image.repository`
          </td>

          <td>
            The container to pull.

            Default: `docker.io/newrelic/synthetics-job-runner`
          </td>
        </tr>

        <tr>
          <td>
            `image.pullPolicy`
          </td>

          <td>
            The pull policy.

            Default: `IfNotPresent`
          </td>
        </tr>

        <tr>
          <td>
            `podSecurityContext`
          </td>

          <td>
            Set a custom security context for the synthetics-job-manager pod.
          </td>
        </tr>
      
        <tr>
          <td>
            `ping-runtime.enabled`
          </td>

          <td>
            Whether or not the persistent ping runtime should be deployed. This can be disabled if you do not use ping monitors.

            Default: `true`
          </td>
        </tr>

        <tr>
          <td>
            `ping-runtime.replicaCount`
          </td>

          <td>
            The number of ping runtime containers to deploy. Increase the replicaCount to scale the deployment based on your ping monitoring needs.

            Default: `1`
          </td>
        </tr>

        <tr>
          <td>
            `ping-runtime.image.repository`
          </td>

          <td>
            The container image to pull for the ping runtime.

            Default: `docker.io/newrelic/synthetics-ping-runtime`
          </td>
        </tr>
        
        <tr>
          <td>
            `ping-runtime.image.pullPolicy`
          </td>

          <td>
            The pull policy for the ping-runtime container.

            Default: `IfNotPresent`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.enabled`
          </td>

          <td>
            Whether or not the Node.js API runtime should be deployed. This can be disabled if you do not use scripted API monitors.

            Default: `true`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.parallelism`
          </td>

          <td>
            The number of Node.js API runtime `CronJobs` to deploy. The maximum number of concurrent Node.js API jobs that will execute at any time. [Additional details](#kubernetes-sizing).

            Default: `1`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.completions`
          </td>

          <td>
            The number of Node.js API runtime `CronJobs` to complete per minute. Increase this setting along with parallelism to improve throughput. This should be increased any time parallelism is increased and completions should always be at least greater than or equal to parallelism. . Increase this setting if you notice periods of time with no API runtime jobs running. [Additional details](#kubernetes-sizing).

            Default: `6`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.image.repository`
          </td>

          <td>
            The container image to pull for the Node.js API runtime.

            Default: `docker.io/newrelic/synthetics-node-api-runtime`
          </td>
        </tr>
        
        <tr>
          <td>
            `node-api-runtime.image.pullPolicy`
          </td>

          <td>
            The pull policy for the Node.js API runtime container.

            Default: `IfNotPresent`
          </td>
        </tr>


        <tr>
          <td>
            `node-browser-runtime.enabled`
          </td>

          <td>
            Whether or not the Node.js browser runtime should be deployed. This can be disabled if you do not use simple or scripted browser monitors.

            Default: `true`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.parallelism`
          </td>

          <td>
            The number of Chrome browser runtime `CronJobs` to deploy. The maximum number of concurrent Chrome browser jobs that will execute at any time. [Additional details](#kubernetes-sizing).

            Default: `1`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.completions`
          </td>

          <td>
            The number of Chrome browser runtime `CronJobs` to complete per minute. Increase this setting along with parallelism to improve throughput. This should be increased any time parallelism is increased and completions should always be at least greater than or equal to parallelism. Increase this setting if you notice periods of time with no browser runtime jobs running. [Additional details](#kubernetes-sizing).

            Default: `6`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.image.repository`
          </td>

          <td>
            The container image to pull for the Node.js browser runtime.

            Default: `docker.io/newrelic/synthetics-node-browser-runtime`
          </td>
        </tr>
        
        <tr>
          <td>
            `node-browser-runtime.image.pullPolicy`
          </td>

          <td>
            The pull policy for the Node.js browser runtime container.

            Default: `IfNotPresent`
          </td>
        </tr>

      </tbody>
    </table>
  </Collapser>
</CollapserGroup>

## Sizing considerations for Kubernetes and Docker [#kubernetes-sizing]

<Callout variant="tip">
  Docker specific sizing considerations will be available soon.
</Callout>

If you're working in larger environments, you may need to customize the job manager configuration to meet minimum requirements to execute synthetic monitors efficiently. Many factors can impact sizing requirements for a synthetics job manager deployment, including:

* If all runtimes are required based on expected usage
* The number of jobs per minute by monitor type (ping, simple or scripted browser, and scripted API)
* Job duration, including jobs that time out at around 3 minutes
* The number of job failures. For job failures, automatic retries are scheduled when a monitor starts to fail to provide built-in 3/3 retry logic. These additional jobs add to the throughput requirements of the synthetic job manager.

In addition to the sizing configuration settings listed below, additional synthetics job managers can be deployed with the same private location key to load balance jobs across multiple environments.

<CollapserGroup>
  <Collapser
    id="kubernetes"
    title="Kubernetes"
  >
Each runtime used by the K8s synthetic job manager can be sized independently by setting values in the [helm chart](https://github.com/newrelic/helm-charts/tree/master/charts/synthetics-job-manager). 

Additional ping runtimes can be started to help execute ping monitor load by increasing the `ping-runtime.replicaCount` setting from the default value of `1`. 

The Node.js API and Node.js Browser runtimes are sized independently using a combination of the `parallelism` and `completions` settings. Ideal configurations for these settings will vary based on customer requirements. 

The `parallelism` setting controls how many jobs of that type (Node.js API or Node.js Browser) can run concurrently in your K8s cluster. The `parallelism` setting is the equivalent of the `synthetics.heavyWorkers` configuration in the containerized private minion (CPM). Ensure that your K8s cluster has enough resources available to run this number of concurrent runtimes based on their [resource request and limit values](/docs/synthetics/synthetic-monitoring/private-locations/install-job-manager/#kubernetes-requirements). 

The `completions` setting controls how many jobs of this type must complete before waiting on the every 1 minute `CronJob` schedule to start additional jobs of this runtime type. The `completions` setting should be set to an estimate of how many jobs can be completed in one minute, taking the `parallelism` setting into account. When `completions` is greater than 1, completed jobs will display in `kubectl get pods` but resources are released as soon as those jobs are marked complete or failed.

Formulae to help calculate a baseline for `completions` and `parallelism` for each runtime:

```m
completions = 60 / avg job duration
parallelism = jobs per minute / completions
```

Different runtimes will likely have different job durations and rates. The following queries can be used to obtain average duration and rate for a private location.

```sql
# non-ping average job duration by runtime type
FROM SyntheticCheck SELECT average(duration) AS 'avg job duration' WHERE type != 'SIMPLE' AND location = 'YOUR_PRIVATE_LOCATION' FACET type SINCE 1 hour ago

# non-ping jobs per minute by runtime type
FROM SyntheticCheck SELECT rate(uniqueCount(id), 1 minute) AS 'jobs per minute' WHERE type != 'SIMPLE' AND location = 'YOUR_PRIVATE_LOCATION' FACET type SINCE 1 hour ago
```

<Callout variant="tip">
  The above queries are based on current results. If your private location does not have any results or the job manager is not performing at its best, query results may not be accurate. In that case, try a few different values for `completions` and `parallelism` until you see jobs filling at least 1 minute with completions (enough completions) and the queue is not growing (enough parallelism).
</Callout>

<table>
    <thead>
      <tr>
        <th style={{ width: "300px" }}>
          Example
        </th>

        <th>
          Description
        </th>
      </tr>
    </thead>

    <tbody>
      <tr>
        <td>
          `parallelism=1` 

          `completions=1`
        </td>

        <td>
          The runtime will execute 1 job per minute. After 1 job completes, the `CronJob` configuration will start a new job at the next minute. **Throughput will be extremely limited with this configuration.**
        </td>
      </tr>

      <tr>
        <td>
          `parallelism=1` 

          `completions=6`
        </td>

        <td>
          The runtime will execute 1 job at a time. After the job completes, a new job will start immediately. After the `completions` setting number of jobs completes, the `CronJob` configuration will start a new job and reset the completions counter. **Throughput will be limited, but slightly better.** A single long running job will block the processing of any other jobs of this type.
        </td>
      </tr>

      <tr>
        <td>
          `parallelism=3` 

          `completions=24`
        </td>

        <td>
          The runtime will execute 3 jobs at once. After any of these jobs complete, a new job will start immediately. After the `completions` setting number of jobs completes, the `CronJob` configuration will start a new job and reset the completions counter. **Throughput is much better with this or similar configurations.** A single long running job will have limited impact to the processing of other jobs of this type.
        </td>
      </tr>
    </tbody>
  </table>

If jobs take longer to complete, fewer completions are needed to fill 1 minute but more parallel pods are needed. Similarly, if more jobs need to be processed per minute, more parallel pods are needed.

You can use the `completions` value to obtain a good starting point for the value of `parallelism`. The `parallelism` setting directly affects how many jobs per minute can be run. Too small a value and the queue may grow. Too large a value and the node may become resource constrained.
    
If your `parallelism` settings is working well to keep the queue at zero, it's not a bad idea to set a higher value for `completions` than what was initially calculated from `60 / avg job duration`. This can help to accommodate cases where jobs complete faster than expected and avoid the scenario where the `completions` value is reached before 1 minute has elapsed. The most efficient `completions` setting will fill the entire minute with running and completing jobs. If not enough completions, job processing for that runtime will sit idle for some number of seconds until the next `CronJob` can spin up another set of `completions`.
  </Collapser>  
</CollapserGroup>
