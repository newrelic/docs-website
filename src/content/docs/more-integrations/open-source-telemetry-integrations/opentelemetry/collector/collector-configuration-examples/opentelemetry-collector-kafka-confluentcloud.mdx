---
title: Collector for Confluent Cloud & Kafka monitoring
tags:
  - Integrations
  - Open source telemetry integrations
  - OpenTelemetry
  - Kafka
  - Confluent Cloud
metaDescription: You can collect Kafka metrics from Confluent using the OpenTelemetry Collector.
---

You can collect metrics about your Confluent Cloud-managed Kafka deployment with the OpenTelemetry Collector. The collector is a component of OpenTelemetry that collects, processes, and exports telemetry data to New Relic (or any observability backend).

Complete the steps below to collect Kafka metrics from Confluent using an OpenTelemetry collector running in docker.

<Steps>
    <Step>
        ## Make sure you're set up

      Before you start, you need to have the <InlinePopover type="licenseKey" /> for the account you want to report data to. You should also verify that:

      * You have a docker daemon running
      * You have [Docker Compose](https://github.com/newrelic/newrelic-opentelemetry-examples/tree/main/other-examples/collector/confluentcloud) installed
      * You have a [Confluent Cloud account](https://www.confluent.io/get-started/)  

    </Step>

    <Step>
        ## Download or clone the example repos

        Download [New Relic's OpenTelemetry Examples repo](https://github.com/newrelic/newrelic-opentelemetry-examples)as this setup uses its example collector configuration. Once installed, open the [Confluent Cloud example](https://github.com/newrelic/newrelic-opentelemetry-examples/tree/main/other-examples/collector/confluentcloud) directory. For more information, you can check the `README` there as well.

    </Step>
    <Step>
        ##  Add the authentication files 

        This example setup uses TLS to authenticate the request to Confluent Cloud. There are multiple methods to authenticate, so you should follow your company best practices and authentication methods. 
        
        * TLS/SSL requires you to create keys and certificates, create your own Certificate Authority (CA), and sign the certificate. 
        * Doing this should leave you with three files which need to be added to this directory. 
        * Those files are referenced in this example as the follwing files: `key.pem`, `cert.pem`, `ca.pem`.

        <Callout variant="important">
          For more information about TLS authentication with Confluent Cloud, check the [documentation on authenticating with TLS](https://docs.confluent.io/platform/current/kafka/authentication_ssl.html) as well as the [security tutorial](https://docs.confluent.io/platform/current/security/security_tutorial.html).

          For dev/test Confluent environments, you can simplify this by using plain text authentication.

        </Callout>

    </Step>
    <Step>
      ##  Set environment variables and run the collector
      Export the following variables or add them in a `.env` file, then run the `docker compose up` command.

```bash
# Open the confluent cloud example directory
cd newrelic-opentelemetry-examples/other-examples/collector/confluentcloud

# Set environment variables.
export NEW_RELIC_API_KEY=<YOUR_API_KEY>
export NEW_RELIC_OTLP_ENDPOINT=https://otlp.nr-data.net
export CLUSTER_ID=<YOUR_CLUSTER_ID>
export CLUSTER_API_KEY=<YOUR_CLUSTER_API_KEY>
export CLUSTER_API_SECRET=<YOUR_CLUSTER_API_SECRET>
export CLUSTER_BOOTSTRAP_SERVER=<YOUR_CLUSTER_BOOTSTRAP_SERVER>

# run the collector in docker
docker compose up
```

      ### Local Variable information

    <table>
      <thead>
        <tr>
          <th style={{ width: "200px"}}>
            Variable
          </th>
          <th>
            Description
          </th>
          <th>
            Docs
          </th>
        </tr>
        </thead>
        <tbody>
          <tr>
            <td>
              `NEW_RELIC_API_KEY`
            </td>
            <td>
              New Relic Ingest API Key
            </td>
            <td>
              [API Key docs](https://docs.newrelic.com/docs/apis/intro-apis/new-relic-api-keys/)
            </td>
          </tr>
          <tr>
            <td>
              `NEW_RELIC_OTLP_ENDPOINT`
            </td>
            <td>
              New Relic OTLP endpoint is https://otlp.nr-data.net:4318
            </td>
            <td>
              [OTLP endpoint config docs](/docs/more-integrations/open-source-telemetry-integrations/opentelemetry/get-started/opentelemetry-set-up-your-app/#review-settings)
            </td>
          </tr>
          <tr>
            <td>
              `CLUSTER_ID`
            </td>
            <td>
              ID of cluster from Confluent Cloud
            </td>
            <td>
              Available in your Confluent cluster settings
            </td>
          </tr>
          <tr>
            <td>
              `CONFLUENT_API_ID`
            </td>
            <td>
              Cloud API key
            </td>
            <td>
              [Cloud API key docs](https://docs.confluent.io/cloud/current/monitoring/metrics-api.html#metrics-quick-start)
            </td>
          </tr>
          <tr>
            <td>
              `CONFLUENT_API_SECRET`
            </td>
            <td>
              Cloud API secret
            </td>
            <td>
              [Cloud API key docs](https://docs.confluent.io/cloud/current/monitoring/metrics-api.html#metrics-quick-start)
            </td>
          </tr>
          <tr>
            <td>
              `CLUSTER_BOOTSTRAP_SERVER`
            </td>
            <td>
              Bootstrap Server for cluster
            </td>
            <td>
              Available in your cluster settings
            </td>
          </tr>
        </tbody>
    </table>
    </Step>
    <Step>

       ## View your data in New Relic

      You can view your Confluent Cloud data in a few different ways.
        
      * Navigate to the [New Relic marketplace](https://one.newrelic.com/marketplace) and search for `Confluent`. The available dashboards can be installed right onto your account!
      * Navigate to the metrics explorer and filter for `confluent_kafka`. This data can be added to any custom alert or dashboard.
        

    </Step>
</Steps>

### Confluent Cloud metrics [#confluent-metrics]

This integration covers all the *Exportable* metrics within the [Confluent Cloud Metrics API](https://api.telemetry.confluent.cloud/docs/descriptors/datasets/cloud). We have a partial list of the *Exportable* metrics below:

<table>
  <thead>
    <tr>
      <th style={{ width: "200px" }}>
        Name
      </th>
      <th>
        Description
      </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>
        confluent_kafka_server_received_bytes
      </td>
      <td>
        The delta count of bytes of the customer's data received from the network. Each sample is the number of bytes received since the previous data sample. The count is sampled every 60 seconds.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_sent_bytes
      </td>
      <td>
        The delta count of bytes of the customer's data sent over the network. Each sample is the number of bytes sent since the previous data point. The count is sampled every 60 seconds.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_received_records
      </td>
      <td>
        The delta count of records received. Each sample is the number of records received since the previous data sample. The count is sampled every 60 seconds.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_sent_records
      </td>
      <td>
        The delta count of records sent. Each sample is the number of records sent since the previous data point. The count is sampled every 60 seconds.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_retained_bytes
      </td>
      <td>
        The current count of bytes retained by the cluster. The count is sampled every 60 seconds.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_active_connection_count
      </td>
      <td>
        The count of active authenticated connections.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_request_count
      </td>
      <td>
        The delta count of requests received over the network. Each sample is the number of requests received since the previous data point. The count sampled every 60 seconds.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_partition_count
      </td>
      <td>
        The number of partitions
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_successful_authentication_count
      </td>
      <td>
        The delta count of successful authentications. Each sample is the number of successful authentications since the previous data point. The count sampled every 60 seconds.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_consumer_lag_offsets
      </td>
      <td>
        The lag between a group member's committed offset and the partition's high watermark. 
      </td>
    </tr>
  </tbody>
</table>


