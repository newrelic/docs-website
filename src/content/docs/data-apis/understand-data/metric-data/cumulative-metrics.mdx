---
title: "Cumulative metric reporting details (OTel and Prometheus)"
tags:
  - Integrations
  - Open source telemetry integrations
  - OpenTelemetry
metaDescription: Support for cumulative metrics at New Relic
---

import moreintegrationsOtlpFacetQuery from 'images/more-integrations_screenshot-crop_otlp-facet-query.png'

If you report cumulative metrics from [our OpenTelemetry integration](/docs/more-integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-introduction) or our [Prometheus remote write integration](/docs/infrastructure/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration), it wlil help you to understand how New Relic handles that data (for example, how we convert that to delta measurements). Understanding this can help you understand your New Relic views, help you query your data, and help you understand data-reporting issues. 

## Cumulative and delta metrics explained [#explained]

When collecting metric data from an application, it's important to consider *how* that data was measured when deciding how to use and interpret it at query time. The [metric **type**](/docs/data-apis/understand-data/metric-data/metric-data-type/#metric-types) is one important factor, with certain New Relic aggregation functions working with some types and not others. But another important factor is the metric's **temporality**. 

The two temporalities are **delta** and **cumulative**. Delta indicates that measurements are reset between reporting intervals. Cumulative indicates there is not reset and the measurements are accumulated. Prometheus is a common example of a cumulative metrics collector ([Prometheus docs on data types](https://prometheus.io/docs/concepts/metric_types)), and OpenTelemetry also defines ways to collect cumulative metrics ([OpenTelemetry docs on temporality](https://opentelemetry.io/docs/reference/specification/metrics/data-model/#temporality)). 

New Relic supports sending Prometheus and OpenTelemetry cumulative data and will perform delta conversion at ingest to make interacting with that data via [NRQL](/docs/query-your-data/nrql-new-relic-query-language/get-started/introduction-nrql-new-relics-query-language) easier and more aligned with other metrics in our platform. 

## Prometheus remote write support [#prom-support]

For more information, refer to our [Prometheus Remote Write](/docs/infrastructure/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic) documentation.

## OpenTelemetry support [#otel-support]

[OpenTelemetry defines support](https://opentelemetry.io/docs/reference/specification/metrics/data-model/#metric-points) for collecting and sending cumulative sums and cumulative histograms. New Relic will statefully reorder, buffer data, and emit deltas between sequential metric measurements in event time. If data for a [timeseries](https://opentelemetry.io/docs/reference/specification/metrics/data-model/#timeseries-model) suddenly decreases in value, we treat this as a reset and will emit that new measurement as its own delta value (in other words, as if it were preceded by a `0` measurement). 

[OpenTelemetry also defines](https://opentelemetry.io/docs/reference/specification/metrics/data-model/#resets-and-gaps) situations where a decrease in value is unexpected, and we do our best to detect these cases and notify you via [New Relic integration errors](/docs/data-apis/manage-data/nrintegrationerror) (see [troubleshooting](#troubleshooting) below).

### Cumulative sums [#cumulative-sums]

To query your cumulative Sum as a delta, you query it like it were any other delta metric sent to New Relic. For example, if you wanted the change of your metric overtime you can query that like so:

```sql
FROM Metric SELECT rate(sum(my.cumulative.sum), 1 minute) TIMESERIES
```

Even though the data was recorded by your application and sent as a sequence of monotonically increasing values, calling sum() on it will treat it as though it were a sequence of delta values; no need to compute any derivative()!

When converting a sum to a delta, New Relic will also emit the cumulative value alongside the delta to maintain the ability for you to query the latest cumulative value. To chart the cumulative value, run a query like this:

```sql
FROM Metric SELECT latest(my.cumulative.sum[cumulative]) TIMESERIES
```

Note that data points are plotted at their associated `timestamps` which are the start of an interval. However, cumulative values are associated with the `endTimestamp` of the data point, so you may need to consider the width of a data point when interpreting cumulative queries.


### Cumulative histograms [#cumulative-histograms]

OpenTelemetry defines support for both [explicit bound histograms](https://opentelemetry.io/docs/reference/specification/metrics/data-model/#histogram) as well as [exponential histograms](https://opentelemetry.io/docs/reference/specification/metrics/data-model/#exponentialhistogram), and New Relic can accept either. Querying cumulative histograms is the same as querying any other histogram, because we convert it to a delta for you:

```sql
FROM Metric SELECT histogram(my.cumulative.histogram)
```

Percentiles work as well:

```sql
FROM Metric SELECT percentile(my.cumulative.histogram, 95, 99) TIMESERIES
```

Unlike cumulative sums, however, we do not store the original cumulative representation of the histogram, so only the delta representation is queryable.

## Troubleshooting and known limitations [#troubleshooting]

Here are some details about how some different data-reporting states and issues are handled. 

### Reordering data [#reorder-data]

We understand that many things can cause data points to arrive at New Relic out of order. As such, we will buffer data points and reorder them if we detect an unexpected gap in the reporting timeseries. Buffering is bounded and eventually we will consider a data point "too late for resequencing". In this case, a delta is computed across the detected gap and processing of the timeseries continues.

### Stale data [#stale-data]

As delta conversion is a stateful operation, we must be cognizant of timeseries that may stop reporting and eventually drop its state. If a timeseries has not reported any new data points for **5 minutes**, we will flush the state we have, including computing deltas across any buffered gaps. This means that if a data point arrives at a later point in time, it will be treated as if it were the beginning of that timeseries, effectively losing the delta between the last data point before the flush and the first data point after the flush. This means that metric reporting intervals should be less than **5 minutes** to get the benefit of delta conversion.

### Translation errors [#translation-errors]

Delta conversion involves the assumption that two data points sequential in event time will have monotonically increasing cumulative values. The only time this assumption is expected to break is when the process being monitored is restarted. If monotonicity is broken for any other reason we will still treat this as a reset, but will attempt to notify you by generating a [New Relic integration error](/docs/data-apis/manage-data/nrintegrationerror) event for your account. This is possible to do for OpenTelemetry data **but not Prometheus**, because OpenTelemetry includes more information that can be used to detect such situations. The most common cause for an unexpected break in monotonicity is when the client side application hits cardinality limits and drops data to relieve memory pressure. In certain cases, this acts as an unexpected reset and can result in an unexpected decrease in values sent to New Relic. It is recommended that you look for instances of this in your OTLP logs:

```
Instrument % has exceeded the maximum allowed accumulations (2000)
```

OpenTelemetry provides a way to reduce cardinality client side using [**Views**](https://opentelemetry.io/docs/reference/specification/metrics/sdk/#view) and is the recommended path to fix these issues. Another option is to explore exporting your [OTLP metrics using Delta temporality](https://opentelemetry.io/docs/reference/specification/metrics/sdk_exporters/otlp/#additional-configuration) which can help save on memory.

### Cardinality limits [#card-limits]

During translation, we also loosely enforce metric cardinality limits that are based on your metric entitlements as a system protection. Rather than enforcing the limit [per day, as we do with rollups](/docs/data-apis/ingest-apis/metric-api/metric-api-limits-restricted-attributes/#violation-unique-timeseries), this limit is enforced as the number of concurrent timeseries being tracked. Once there are too many concurrent [unique metric timeseries](/docs/data-apis/ingest-apis/metric-api/NRQL-high-cardinality-metrics/#what-why), we will drop new incoming timeseries until an old one ages out (see Stale Data).
