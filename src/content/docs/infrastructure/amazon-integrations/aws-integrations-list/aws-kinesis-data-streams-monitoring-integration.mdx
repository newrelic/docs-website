---
title: Amazon Kinesis Data Streams monitoring integration
tags:
  - Integrations
  - Amazon integrations
  - AWS integrations list
metaDescription: 'New Relic''s integration with Amazon AWS Kinesis Data Streams: How to activate it and what data it reports.'
redirects:
  - /docs/integrations/amazon-integrations/aws-integrations-list/aws-kinesis-data-streams-monitoring-integration
  - /docs/aws-kinesis-integration
  - /docs/infrastructure/infrastructure-integrations/amazon-integrations/aws-kinesis-integration
  - /docs/infrastructure/amazon-integrations/amazon-integrations/aws-kinesis-monitoring-integration
  - /docs/infrastructure/amazon-integrations/amazon-integrations/aws-kinesis-streams-monitoring-integration
  - /docs/integrations/amazon-integrations/aws-integrations-list/aws-kinesis-streams-monitoring-integration
---

[New Relic](/docs/infrastructure/integrations-getting-started/getting-started/introduction-infrastructure-integrations) includes an integration for monitoring Amazon Kinesis Data Streams. This document explains how to activate the integration and describes the data that can be reported.

## Features

[Amazon Kinesis Data Streams](http://docs.aws.amazon.com/streams/latest/dev/introduction.html) is a platform for streaming data on AWS, making it easy to load and analyze data in real time. It also gives you the ability to build custom streaming data applications for specialized needs.

New Relic's Kinesis Data Streams integration gathers metric and configuration data on all of the streams associated with your account.

Data collected includes bytes put to and retrieved from the stream, records put and retrieved, time taken by operations, and other [metrics](#metrics). You can view your Kinesis Data Streams data in pre-built dashboards and create custom queries and charts in New Relic.

You also have an option for [enabling shard data collection](#polling). AWS data records are sequentially processed in shards, which are in turn grouped in streams. The number of shards in a stream determines the total data read and write capacity of the service.

## Activate integration [#activate]

To enable this integration follow standard procedures to [Connect AWS services to New Relic](/docs/infrastructure/infrastructure-integrations/getting-started/connect-aws-integrations-infrastructure).

## Polling and configuration [#polling]

You can change the polling frequency and filter data using [configuration options](/docs/integrations/new-relic-integrations/getting-started/configure-polling-frequency-data-collection-cloud-integrations).

Default [polling](/docs/infrastructure/amazon-integrations/aws-integrations-list/aws-polling-intervals-infrastructure-integrations) information for the Amazon Kinesis Data Streams integration:

* New Relic polling interval: 15 minutes
* Amazon CloudWatch data interval: 1 minute

Shard data: To collect shard data, enable **Collect shards data** in the integration's configuration options in the UI.

<Callout variant="important">
  If you enable collecting shard data, this will increase the number of API calls to your AWS CloudWatch service. There will be seven calls per shard per polling interval.
</Callout>

## Find and use data [#find-data]

To find your integration data, go to [**one.newrelic.com**](http://one.newrelic.com) **> Infrastructure > AWS** and select one of the Kinesis Data Streams integration links.

You can [query and explore your data](/docs/using-new-relic/data/understand-data/query-new-relic-data) using the `QueueSample` [event type](/docs/insights/use-insights-ui/getting-started/introduction-new-relic-insights#event-type). The `provider` value includes:

* Stream metrics: `KinesisStream`
* Shard metrics: `KinesisStreamShard`

For more on how to use your data, see [Understand and use integration data](/docs/infrastructure/integrations/find-use-infrastructure-integration-data).

## Metric data [#metrics]

This integration collects the following metrics. For additional information, see Amazon's documentation for [monitoring Amazon Kinesis Data Streams](http://docs.aws.amazon.com/streams/latest/dev/monitoring-with-cloudwatch.html).

This integration collects the following metrics:

<table>
  <thead>
    <tr>
      <th style={{ width: "285" }}>
        Name
      </th>

      <th>
        Description
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        `getRecordsBytes`
      </td>

      <td>
        The number of bytes retrieved from the Kinesis stream, measured over the specified time period. Minimum, Maximum, and Average statistics represent the bytes in a single `GetRecords` operation for the stream in the specified time period.
      </td>
    </tr>

    <tr>
      <td>
        `getRecordsIteratorAgeMilliseconds`
      </td>

      <td>
        The age of the last record in all `GetRecords` calls made against an Kinesis stream, measured over the specified time period, in milliseconds. Age is the difference between the current time and when the last record of the `GetRecords` call was written to the stream. The Minimum and Maximum statistics can be used to track the progress of Kinesis consumer applications. A value of zero indicates that the records being read are completely caught up with the stream.
      </td>
    </tr>

    <tr>
      <td>
        `getRecordsLatency`
      </td>

      <td>
        The time taken in milliseconds per `GetRecords` operation, measured over the specified time period.
      </td>
    </tr>

    <tr>
      <td>
        `getRecordsRecords`
      </td>

      <td>
        The number of records retrieved from the shard, measured over the specified time period. Minimum, Maximum, and Average statistics represent the records in a single `GetRecords` operation for the stream in the specified time period.
      </td>
    </tr>

    <tr>
      <td>
        `incomingBytes`
      </td>

      <td>
        The number of bytes successfully put to the Kinesis stream over the specified time period. This metric includes bytes from `PutRecord` and `PutRecords` operations. Minimum, Maximum, and Average statistics represent the bytes in a single put operation for the stream in the specified time period.
      </td>
    </tr>

    <tr>
      <td>
        `incomingRecords`
      </td>

      <td>
        The number of records successfully put to the Kinesis stream over the specified time period. This metric includes record counts from `PutRecord` and `PutRecords` operations. Minimum, Maximum, and Average statistics represent the records in a single put operation for the stream in the specified time period.
      </td>
    </tr>

    <tr>
      <td>
        `putRecordBytes`
      </td>

      <td>
        The number of bytes put to the Kinesis stream using the `PutRecord` operation over the specified time period.
      </td>
    </tr>

    <tr>
      <td>
        `putRecordLatency`
      </td>

      <td>
        The time taken in milliseconds per `PutRecord` operation, measured over the specified time period.
      </td>
    </tr>

    <tr>
      <td>
        `putRecordSuccess`
      </td>

      <td>
        The number of successful `PutRecord` operations per Kinesis stream, measured over the specified time period. Average reflects the percentage of successful writes to a stream.
      </td>
    </tr>

    <tr>
      <td>
        `putRecordsBytes`
      </td>

      <td>
        The number of bytes put to the Kinesis stream using the `PutRecords` operation over the specified time period.
      </td>
    </tr>

    <tr>
      <td>
        `putRecordsLatency`
      </td>

      <td>
        The time taken in milliseconds per `PutRecords` operation, measured over the specified time period.
      </td>
    </tr>

    <tr>
      <td>
        `putRecordsRecords`
      </td>

      <td>
        The number of successful records in a `PutRecords` operation per Kinesis stream, measured over the specified time period.
      </td>
    </tr>

    <tr>
      <td>
        `putRecordsSuccess`
      </td>

      <td>
        The number of `PutRecords` operations where at least one record succeeded, per Kinesis stream, measured over the specified time period.
      </td>
    </tr>

    <tr>
      <td>
        `readProvisionedThroughputExceeded`
      </td>

      <td>
        The number of GetRecords calls throttled for the stream over the specified time period. The most commonly used statistic for this metric is Average.
      </td>
    </tr>

    <tr>
      <td>
        `writeProvisionedThroughputExceeded`
      </td>

      <td>
        The number of records rejected due to throttling for the stream over the specified time period. This metric includes throttling from `PutRecord` and `PutRecords` operations. The most commonly used statistic for this metric is Average.

        When the Minimum statistic has a non-zero value, records were being throttled for the stream during the specified time period.

        When the Maximum statistic has a value of 0 (zero), no records were being throttled for the stream during the specified time period.
      </td>
    </tr>
  </tbody>
</table>

## Inventory data [#inventory]

<Callout variant="important" title="EOL NOTICE">
After March 2022, we're discontinuing support for several capabilities, including inventory data for cloud integrations. For more details, including how you can easily prepare for this transition, see our [Explorers Hub post](https://discuss.newrelic.com).
</Callout>

<Callout variant="tip">
  Tags (indicated with an **\***) are only fetched when [tags collection](/docs/integrations/new-relic-integrations/getting-started/configure-polling-frequency-data-collection-cloud-integrations#collect_tags) is on.
</Callout>

### aws/kinesis/stream

<dd>
  <table>
    <thead>
      <tr>
        <th style={{ width: "400px" }}>
          Name
        </th>

        <th>
          Description
        </th>
      </tr>
    </thead>

    <tbody>
      <tr>
        <td>
          `awsRegion`
        </td>

        <td>
          The AWS region that the stream was provisioned in.
        </td>
      </tr>

      <tr>
        <td>
          `streamName`
        </td>

        <td>
          The name of the stream.
        </td>
      </tr>

      <tr>
        <td>
          `status`
        </td>

        <td>
          The current status of the stream being described (CREATING, DELETING, ACTIVE, UPDATING).
        </td>
      </tr>

      <tr>
        <td>
          `retentionPeriodHours`
        </td>

        <td>
          The current retention period, in hours.
        </td>
      </tr>

      <tr>
        <td>
          `openShardsCount`
        </td>

        <td>
          Number of opened shards.
        </td>
      </tr>

      <tr>
        <td>
          `closedShardsCount`
        </td>

        <td>
          Number of closed shards.
        </td>
      </tr>

      <tr>
        <td>
          `tags`\*
        </td>

        <td>
          Stream tags.
        </td>
      </tr>
    </tbody>
  </table>
</dd>

### aws/kinesis/shard

<dd>
  <table>
    <thead>
      <tr>
        <th style={{ width: "400px" }}>
          Name
        </th>

        <th>
          Description
        </th>
      </tr>
    </thead>

    <tbody>
      <tr>
        <td>
          `awsRegion`
        </td>

        <td>
          The AWS region that the shard was provisioned in.
        </td>
      </tr>

      <tr>
        <td>
          `streamName`
        </td>

        <td>
          The name of the stream.
        </td>
      </tr>

      <tr>
        <td>
          `shardId`
        </td>

        <td>
          The unique identifier of the shard within the stream.
        </td>
      </tr>

      <tr>
        <td>
          `parentShardId`
        </td>

        <td>
          The shard ID of the shard's parent.
        </td>
      </tr>

      <tr>
        <td>
          `adjacentParentShardId`
        </td>

        <td>
          The shard ID of the shard adjacent to the shard's parent.
        </td>
      </tr>

      <tr>
        <td>
          `startingHashKey`
        </td>

        <td>
          The starting hash key of the hash key range.
        </td>
      </tr>

      <tr>
        <td>
          `endingHashKey`
        </td>

        <td>
          The ending hash key of the hash key range.
        </td>
      </tr>

      <tr>
        <td>
          `startingSequenceNumber`
        </td>

        <td>
          The starting sequence number for the range.
        </td>
      </tr>

      <tr>
        <td>
          `endingSequenceNumber`
        </td>

        <td>
          The ending sequence number for the range. Shards that are in the OPEN state have an ending sequence number of null.
        </td>
      </tr>

      <tr>
        <td>
          `tags`\*
        </td>

        <td>
          Shard tags.
        </td>
      </tr>
    </tbody>
  </table>
</dd>
