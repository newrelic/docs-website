---
title: Elasticsearch OpenTelemetry monitoring integration
tags:
  - OpenTelemetry
  - Elasticsearch
  - Integrations
metaDescription: "Install and configure the OpenTelemetry Collector to monitor Elasticsearch clusters and send data to New Relic."
redirects:
  - /install/elasticsearch-otel
---

Our Elasticsearch OpenTelemetry (OTel) integration leverages the OpenTelemetry Collector to ingest performance metrics and logs from your [Elasticsearch](https://www.elastic.co/) cluster. We provide standardized visibility at the cluster and node level, allowing you to monitor health, search performance, and resource usage.



To install the integration, complete the following steps:

1. [Compatibility and requirements](#prerequisites).
2. [Configure the Opentelemetry Collector](#config).
3. [Set Environment variables for the collector](#start)
4. [Find and use data](#find-and-use).


## Step 1: Compatibility and requirements [#prerequisites]

* **Elasticsearch Version:** 7.16 or higher.
* **Privileges:** If security is enabled, you must have either the monitor or manage cluster privilege. Refer to elasticsearch [security privileges](https://www.elastic.co/docs/reference/elasticsearch/security-privileges) doc for more information.
* **Network:** Outbound HTTPS connectivity (port 443) to [New Relic's OTLP ingest endpoint](http://localhost:8001/docs/opentelemetry/best-practices/opentelemetry-otlp/#configure-endpoint-port-protocol:~:text=HTTP-,Endpoint,-Supported%20ports).
* **Account:** A [New Relic account](https://newrelic.com/signup) with a <InlinePopover type="licenseKey"/>
* **Collector:** [OpenTelemetry Collector Contrib](https://github.com/open-telemetry/opentelemetry-collector-contrib/releases/latest) installed and running on host. Please ensure the OpenTelemetry Collector is installed via an official package (.deb or .rpm). A standalone binary installation will not create the required systemd service unit necessary for these commands to function.


## Step 2: Configure the Opentelemetry Collector [#config]

Create or update your OpenTelemetry Collector configuration file (typically located at /etc/otelcol-contrib/config.yaml) with the below configurations:


<CollapserGroup>

   <Collapser id="basic-config" title="Basic Metrics Configuration">
    This is the standard configuration to scrape metrics from an unsecured Elasticsearch node:
    > **Note:** Replace the `endpoint` value with your Elasticsearch cluster endpoint and update `elasticsearch.cluster.name` in the processor block with a unique name to uniquely identify your cluster in New Relic.
    
    ```yaml
    # =================================================================================================
# OpenTelemetry Collector Configuration for Elasticsearch and Host
# This configuration collects metrics and logs for a complete observability solution.
# =================================================================================================
# -------------------------------------------------------------------------------------------------
# Receivers
# Receivers define how data gets into the Collector. This config uses four receivers:
# - elasticsearch: to scrape metrics from the Elasticsearch API
# - hostmetrics: to collect system-level metrics from the host itself
# - filelog: to tail Elasticsearch log files
# - otlp: to accept data from other OpenTelemetry-instrumented services
# -------------------------------------------------------------------------------------------------
receivers:
  # The dedicated receiver for Elasticsearch.
  # It automatically discovers the cluster name.
  elasticsearch:
    endpoint: "https://localhost:9200"
    collection_interval: 15s
    metrics:
      elasticsearch.breaker.tripped:
        enabled: true
      elasticsearch.cluster.data_nodes:
        enabled: true
      elasticsearch.cluster.health:
        enabled: true
      elasticsearch.cluster.in_flight_fetch:
        enabled: true
      elasticsearch.cluster.nodes:
        enabled: true
      elasticsearch.cluster.pending_tasks:
        enabled: true
      elasticsearch.cluster.shards:
        enabled: true
      elasticsearch.cluster.state_update.time:
        enabled: true
      elasticsearch.index.documents:
        enabled: true
      elasticsearch.index.operations.merge.current:
        enabled: true
      elasticsearch.index.operations.time:
        enabled: true
      elasticsearch.indexing_pressure.memory.total.primary_rejections:
        enabled: true
      elasticsearch.node.cache.count:
        enabled: true
      elasticsearch.node.cache.evictions:
        enabled: true
      elasticsearch.node.cache.memory.usage:
        enabled: true
      elasticsearch.node.cluster.io:
        enabled: true
      elasticsearch.node.documents:
        enabled: true
      elasticsearch.node.disk.io.read:
        enabled: true
      elasticsearch.node.disk.io.write:
        enabled: true
      elasticsearch.node.fs.disk.available:
        enabled: true
      elasticsearch.node.fs.disk.total:
        enabled: true
      elasticsearch.node.http.connections:
        enabled: true
      elasticsearch.node.ingest.documents.current:
        enabled: true
      elasticsearch.node.ingest.operations.failed:
        enabled: true
      elasticsearch.node.open_files:
        enabled: true
      elasticsearch.node.operations.completed:
        enabled: true
      elasticsearch.node.operations.current:
        enabled: true
      elasticsearch.node.operations.get.completed:
        enabled: true
      elasticsearch.node.operations.get.time:
        enabled: true
      elasticsearch.node.operations.time:
        enabled: true
      elasticsearch.node.shards.reserved.size:
        enabled: true
      elasticsearch.node.thread_pool.tasks.finished:
        enabled: true
      elasticsearch.os.cpu.load_avg.1m:
        enabled: true
      elasticsearch.os.cpu.load_avg.5m:
        enabled: true
      elasticsearch.os.cpu.load_avg.15m:
        enabled: true
      elasticsearch.os.memory:
        enabled: true
      jvm.gc.collections.count:
        enabled: true
      jvm.gc.collections.elapsed:
        enabled: true
      jvm.memory.heap.max:
        enabled: true
      jvm.memory.heap.used:
        enabled: true
      jvm.memory.heap.utilization:
        enabled: true
      jvm.threads.count:
        enabled: true
  hostmetrics:
    collection_interval: 60s # Recommended for cost savings and stability
    scrapers:
      cpu:
        metrics:
          # CPU Utilization and Time are the core metrics
          system.cpu.utilization: {enabled: true}
          system.cpu.time: {enabled: true}
      
      load:
        metrics:
          # Load Averages (used for system health dashboards)
          system.cpu.load_average.1m: {enabled: true}
          system.cpu.load_average.5m: {enabled: true}
          system.cpu.load_average.15m: {enabled: true}
      
      memory:
        metrics:
          # Memory Usage and Utilization
          system.memory.usage: {enabled: true}
          system.memory.utilization: {enabled: true}
      
      disk:
        metrics:
          # Disk I/O operations (throughput)
          system.disk.io: {enabled: true}
          system.disk.operations: {enabled: true}
      
      filesystem:
        metrics:
          # Filesystem usage (disk space capacity)
          system.filesystem.usage: {enabled: true}
          system.filesystem.utilization: {enabled: true}
          
      network:
        # Since this was already working, keeping it simple is best.
        # But for completeness:
        metrics:
          system.network.io: {enabled: true}
          system.network.packets: {enabled: true}
      process:
        metrics:
          process.cpu.utilization:
            enabled: true
        
# -------------------------------------------------------------------------------------------------
# Processors
# -------------------------------------------------------------------------------------------------
processors:
  cumulativetodelta: {}
  resource/cluster_name_override:
    attributes:
      # Use the actual cluster name defined in your Elasticsearch config
      - key: elasticsearch.cluster.name
        value: "<elasticsearch-cluster-name>" # <-- REPLACE THIS WITH A UNIQUE CLUSTER NAME TO UNIQUELY IDENTIFY YOUR CLUSTER IN NEW RELIC
        action: upsert
  resourcedetection:
    detectors: [ system ]
    system:
      resource_attributes:
        host.name:
          enabled: true
        host.id:
          enabled: true
        os.type:
          enabled: true 
  # This processor batches data for more efficient sending.
  batch:
    timeout: 10s
    send_batch_size: 1024
  # 1. CARDINALITY REDUCTION: Drops volatile or redundant attributes
  attributes/cardinality_reduction:
    actions:
      # Filter out VOLATILE PROCESS IDS (High churn)
      - key: process.pid
        action: delete
      - key: process.parent_pid
        action: delete
      # Filter out REDUNDANT/STATIC METADATA (Already known at the Resource level)
      - key: elasticsearch.node.version
        action: delete
      - key: os.type
        action: delete
  transform/metadata_nullify:
    # We use 'metric_statements' to run OTTL logic on the metric signal
    metric_statements:
      - context: metric  # <-- Targets the high-level Metric structure itself
        statements:
          # Sets the 'description' field to an empty string ("")
          - set(description, "")
          # Sets the 'unit' field to an empty string ("")
          - set(unit, "")      
exporters:
  # This exporter sends all data to New Relic via OTLP/HTTP.
  otlphttp:
    endpoint: ${env:NEWRELIC_OTLP_ENDPOINT}
    headers:
      api-key: ${env:NEWRELIC_LICENSE_KEY}   
# -------------------------------------------------------------------------------------------------
# Service
# The service block defines the pipelines. Note the new 'logs' pipeline.
# -------------------------------------------------------------------------------------------------
service:
  pipelines:
    metrics/elasticsearch:
      receivers: [elasticsearch]
      processors: [resourcedetection, resource/cluster_name_override, attributes/cardinality_reduction, cumulativetodelta, transform/metadata_nullify, batch]
      exporters: [otlphttp]
    metrics/host:
      receivers: [hostmetrics]
      processors: [resourcedetection,batch]
      exporters: [otlphttp]
    ```
  </Collapser>

  <Collapser id="secure-config" title="Authentication & SSL Configuration">
    If your cluster requires credentials or SSL, add the `username`, `password`, and `tls` blocks:
    

    ```yaml
    receivers:
      elasticsearch:
        endpoint: "https://localhost:9200"
        username: "elastic"
        password: "your_password"
        tls:
          ca_file: "/etc/elasticsearch/certs/http_ca.crt"
          insecure_skip_verify: false
        collection_interval: 15s
    ```
  </Collapser>

  <Collapser id="logging-config" title="Enabling Logs (filelog receiver)">
    To send Elasticsearch logs to New Relic, add the `filelog` receiver. Ensure the `otelcol-contrib` user has read access to the log files.

    ```yaml
    receivers:
      filelog:
        include:
          - /var/log/elasticsearch/elasticsearch.log
          - /var/log/elasticsearch/*.log
        start_at: beginning

    service:
      pipelines:
        logs:
          receivers: [filelog]
          processors: [resource/cluster_name_override]
          exporters: [otlphttp]
    ```
    **Tip:** Grant permission by adding the user to the group: 
    `sudo usermod -a -G elasticsearch otelcol-contrib`
  </Collapser>

  <Collapser id="custom-attributes" title="Adding Custom Metadata">
    Use the `resource/static_override` processor to add custom tags (e.g., environment, team) to your data:

    ```yaml
    processors:
      resource/static_override:
        attributes:
          - key: env
            value: "production"
            action: upsert
    service:
      pipelines:
        metrics/elasticsearch:
          receivers: [elasticsearch]
          processors: [resourcedetection, resource/cluster_name_override, resource/static_override, attributes/cardinality_reduction, cumulativetodelta, transform/metadata_nullify, batch]
          exporters: [otlphttp]
        metrics/host:
          receivers: [hostmetrics]
          processors: [resourcedetection, resource/static_override, batch]
          exporters: [otlphttp]        

    ```
  </Collapser>
</CollapserGroup>

## Step 3: Set Environment variables for the collector [#start]

Inject your New Relic <InlinePopover type="licenseKey"/> and OTLP endpoint into the collector service so the exporter can authenticate.

1. Create a systemd override directory:
   ```bash
   sudo mkdir -p /etc/systemd/system/otelcol-contrib.service.d
   ```
2. Write `environment.conf` with your OTLP endpoint. Replace `YOUR_LICENSE_KEY` with the New Relic license key and `YOUR_OTLP_ENDPOINT` with the appropriate endpoint for your region. Refer to the OTLP endpoint configuration [documentation](https://docs.newrelic.com/docs/opentelemetry/best-practices/opentelemetry-otlp/#configure-endpoint-port-protocol) to select the right endpoint.

   ```bash
   cat <<EOF | sudo tee /etc/systemd/system/otelcol-contrib.service.d/environment.conf
   [Service]
   Environment="NEWRELIC_OTLP_ENDPOINT=YOUR_OTLP_ENDPOINT"
   Environment="NEWRELIC_LICENSE_KEY=YOUR_LICENSE_KEY"
   EOF
   ```
3. Reload systemd and restart the collector:
   ```bash
   sudo systemctl daemon-reload
   sudo systemctl restart otelcol-contrib.service
   ```

## Step 4: Find and use data [#find-and-use]

1. Go to **[one.newrelic.com](https://one.newrelic.com) > Integrations & Agents** and search **Elasticsearch(Opentelemetry)**.
2. Under **Dashboards**, click **Elasticsearch OTEL Monitoring**.
3. In the popup window, select your account.
4. Click View dashboard, and see your Elasticsearch data in New Relic.

The Elasticsearch metrics are attached to the `Metric` [event type](/docs/using-new-relic/data/understand-data/new-relic-data-types#events-new-relic). You can [query this data](/docs/using-new-relic/data/understand-data/query-new-relic-data) for troubleshooting purposes or to create custom charts and dashboards.




## Troubleshoot the Elasticsearch OpenTelemetry integration [#troubleshooting]

If you have completed the installation but don't see data in New Relic, locate the symptom below to find the matching fix.

<CollapserGroup>

  <Collapser id="troubleshoot-collector-stopped" title="Collector service stopped or failed">
    <p><strong>How to check</strong></p>
    ```bash
    sudo systemctl status otelcol-contrib
    ```
    <p><strong>Resolution</strong></p>
    <ul>
      <li>If the service is inactive, start it: <InlineCode>sudo systemctl start otelcol-contrib</InlineCode></li>
      <li>If the service failed, fix configuration errors and restart: <InlineCode>sudo systemctl restart otelcol-contrib</InlineCode></li>
    </ul>
  </Collapser>

  <Collapser id="troubleshoot-collector-logs-errors" title="Collector logs report scraping or export errors">
    <p><strong>How to check</strong></p>
    ```bash
    sudo journalctl -u otelcol-contrib.service -f
    ```
    <p><strong>Resolution</strong></p>
    <p>Review the log output and resolve the root cause (for example, connection problems, authentication failures, or permission issues).</p>
  </Collapser>

  <Collapser id="troubleshoot-connection-refused" title="Connection refused when calling Elasticsearch">
    <p><strong>Error sample</strong>: <InlineCode>dial tcp [::1]:9200: connect: connection refused</InlineCode></p>
    <p><strong>Resolution</strong></p>
    <ul>
      <li>Ensure the <InlineCode>endpoint</InlineCode> in <InlineCode>config.yaml</InlineCode> matches the Elasticsearch host and port.</li>
      <li>Confirm Elasticsearch is running and reachable from the collector host.</li>
    </ul>
  </Collapser>

  <Collapser id="troubleshoot-403" title="403 Forbidden when exporting to New Relic">
    <p><strong>Error sample</strong>: <InlineCode>permanent error: 403 Forbidden</InlineCode></p>
    <p><strong>Resolution</strong></p>
    <ul>
      <li>Verify <InlineCode>NEWRELIC_LICENSE_KEY</InlineCode> in <InlineCode>/etc/systemd/system/otelcol-contrib.service.d/environment.conf</InlineCode>.</li>
      <li>Reload systemd and restart the collector:
        ```bash
        sudo systemctl daemon-reload
        sudo systemctl restart otelcol-contrib
        ```
      </li>
    </ul>
  </Collapser>

  <Collapser id="troubleshoot-permission-denied" title="Permission denied when collecting logs">
    <p><strong>Error sample</strong>: <InlineCode>permission denied</InlineCode> or <InlineCode>cannot open file</InlineCode></p>
    <p><strong>Resolution</strong></p>
    <ul>
      <li>Add the collector user to the Elasticsearch group:
        ```bash
        sudo usermod -a -G elasticsearch otelcol-contrib
        ```
      </li>
      <li>Restart the collector: <InlineCode>sudo systemctl restart otelcol-contrib</InlineCode></li>
    </ul>
  </Collapser>

  <Collapser id="troubleshoot-api-reachability" title="Cannot reach the Elasticsearch API from the collector">
    <p><strong>How to check</strong></p>
    ```bash
    # Unsecured cluster
    curl -I http://localhost:9200

    # With authentication
    curl -u username:password -k https://localhost:9200
    ```
    <p><strong>Resolution</strong></p>
    <p>Verify the cluster is healthy, credentials are valid, and firewall or security settings permit access.</p>
  </Collapser>

  <Collapser id="troubleshoot-missing-entity" title="Elasticsearch entity missing in New Relic UI">
    <p><strong>Resolution</strong></p>
    <ul>
      <li>Ensure the <InlineCode>resourcedetection</InlineCode> processor is included in every metrics pipeline.</li>
      <li>Verify <InlineCode>elasticsearch.cluster.name</InlineCode> is set via the <InlineCode>resource/cluster_name_override</InlineCode> processor.</li>
    </ul>
  </Collapser>

  <Collapser id="troubleshoot-logs-missing" title="Metrics present but logs missing">
    <p><strong>Resolution</strong></p>
    <ul>
      <li>Confirm <InlineCode>filelog</InlineCode> receiver paths are correct and absolute.</li>
      <li>Check that the logs pipeline includes both the <InlineCode>filelog</InlineCode> receiver and the <InlineCode>otlphttp</InlineCode> exporter.</li>
    </ul>
  </Collapser>

</CollapserGroup>

