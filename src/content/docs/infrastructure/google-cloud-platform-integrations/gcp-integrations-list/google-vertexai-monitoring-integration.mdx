---
title: Google VertexAI monitoring integration
tags:
  - Integrations
  - Google Cloud Platform integrations
  - GCP integrations list
metaDescription: 'New Relic Google VertexAI integration: the data it reports and how to enable it.'
redirects:
  - /docs/integrations/google-cloud-platform-integrations/gcp-integrations-list/google-vertexai-monitoring-integration
freshnessValidatedDate: never
---

[New Relic's integrations](/docs/infrastructure/integrations-getting-started/getting-started/introduction-infrastructure-integrations) include an integration for reporting your GCP Run data to our products. Here, we explain how to activate the integration and what data it collects.

## Activate integration [#activate]

To enable the integration, follow the standard procedures to [connect your GCP service to New Relic](/docs/connect-google-cloud-platform-services-infrastructure).

## Configuration and polling [#polling]

You can change the polling frequency and filter data using [configuration options](/docs/integrations/new-relic-integrations/getting-started/configure-polling-frequency-data-collection-cloud-integrations).

Default [polling](/docs/integrations/google-cloud-platform-integrations/getting-started/polling-intervals-gcp-integrations) information for the GCP Run integration:

* New Relic polling interval: 5 minutes

## Find and use data [#find-data]

To find your integration data, go to <DNT>**[one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > Infrastructure > GCP**</DNT> and select an integration.

Data is attached to the following [event types](/docs/data-apis/understand-data/new-relic-data-types/#event-data):

<table>
  <thead>
    <tr>
      <th>
        Entity
      </th>

      <th>
        Event Type
      </th>

      <th>
        Provider
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        Endpoint
      </td>

      <td>
        `GcpVertexAiEndpointSample`
      </td>

      <td>
        `GcpVertexAiEndpoint`
      </td>
    </tr>

    <tr>
      <td>
        Feature store
      </td>

      <td>
        `GcpVertexAiFeaturestoreSample`
      </td>

      <td>
        `GcpVertexAiFeaturestore`
      </td>
    </tr>

    <tr>
      <td>
        Feature Online Store
      </td>

      <td>
        `GcpVertexAiFeatureOnlineStoreSample`
      </td>

      <td>
        `GcpVertexAiFeatureOnlineStore`
      </td>
    </tr>

    <tr>
      <td>
        Location
      </td>

      <td>
        `GcpVertexAiLocationSample`
      </td>

      <td>
        `GcpVertexAiLocation`
      </td>
    </tr>

    <tr>
      <td>
        Index
      </td>

      <td>
        `GcpVertexAiIndexSample`
      </td>

      <td>
        `GcpVertexAiIndex`
      </td>
    </tr>

    <tr>
      <td>
        PipelineJob
      </td>

      <td>
        `GcpVertexAiPipelineJobSample`
      </td>

      <td>
        `GcpVertexAiPipelineJob`
      </td>
    </tr>
  </tbody>
</table>

For more on how to use your data, see [Understand and use integration data](/docs/infrastructure/integrations/find-use-infrastructure-integration-data).

## Metric data [#metrics]

This integration collects GCP data for VertexAI.

### VertexAI Endpoint data

<table>
  <thead>
    <tr>
      <th>
        Metric
      </th>

      <th>
        Unit
      </th>

      <th>
        Description
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        `prediction.online.accelerator.duty_cycle`
      </td>

      <td>
        Percent
      </td>

      <td>
        Average fraction of time over the past sample period during which the accelerator(s) were actively processing.
      </td>
    </tr>

    <tr>
      <td>
        `prediction.online.accelerator.memory.bytes_used`
      </td>

      <td>
        Bytes
      </td>

      <td>
        Amount of accelerator memory allocated by the deployed model replica.
      </td>
    </tr>

    <tr>
      <td>
        `prediction.online.error_count`
      </td>

      <td>
        Count
      </td>

      <td>
        Number of online prediction errors.
      </td>
    </tr>

    <tr>
      <td>
        `prediction.online.memory.bytes_used`
      </td>

      <td>
        Bytes
      </td>

      <td>
        Amount of memory allocated by the deployed model replica and currently in use.
      </td>
    </tr>

    <tr>
      <td>
        `prediction.online.network.received_bytes_count`
      </td>

      <td>
        Bytes
      </td>

      <td>
        Number of bytes received over the network by the deployed model replica.
      </td>
    </tr>

    <tr>
      <td>
        `prediction.online.network.sent_bytes_count`
      </td>

      <td>
        Bytes
      </td>

      <td>
        Number of bytes sent over the network by the deployed model replica.
      </td>
    </tr>

    <tr>
      <td>
        `prediction.online.prediction_count`
      </td>

      <td>
        Count
      </td>

      <td>
        Number of online predictions.
      </td>
    </tr>

    <tr>
      <td>
        `prediction.online.prediction_latencies`
      </td>

      <td>
        Milliseconds
      </td>

      <td>
        Online prediction latency of the deployed model.
      </td>
    </tr>

    <tr>
      <td>
        `prediction.online.private.prediction_latencies`
      </td>

      <td>
        Milliseconds
      </td>

      <td>
        Online prediction latency of the private deployed model.
      </td>
    </tr>

    <tr>
      <td>
        `prediction.online.replicas`
      </td>

      <td>
        Count
      </td>

      <td>
        Number of active replicas used by the deployed model.
      </td>
    </tr>

    <tr>
      <td>
        `prediction.online.response_count`
      </td>

      <td>
        Count
      </td>

      <td>
        Number of different online prediction response codes.
      </td>
    </tr>

    <tr>
      <td>
        `prediction.online.target_replicas`
      </td>

      <td>
        Count
      </td>

      <td>
        Target number of active replicas needed for the deployed model.
      </td>
    </tr>
  </tbody>
</table>

### VertexAI Featurestore data

<table>
  <thead>
    <tr>
      <th>
        Metric
      </th>

      <th>
        Unit
      </th>

      <th>
        Description
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        `featurestore.cpu_load`
      </td>

      <td>
        Percent
      </td>

      <td>
        The average CPU load for a node in the Featurestore online storage.
      </td>
    </tr>

    <tr>
      <td>
        `featurestore.cpu_load_hottest_node`
      </td>

      <td>
        Percent
      </td>

      <td>
        The CPU load for the hottest node in the Featurestore online storage.
      </td>
    </tr>

    <tr>
      <td>
        `featurestore.node_count`
      </td>

      <td>
        Count
      </td>

      <td>
        The number of nodes for the Featurestore online storage.
      </td>
    </tr>

    <tr>
      <td>
        `featurestore.online_entities_updated`
      </td>

      <td>
        Count
      </td>

      <td>
        Number of entities updated on the Featurestore online storage.
      </td>
    </tr>

    <tr>
      <td>
        `featurestore.online_serving.latencies`
      </td>

      <td>
        Milliseconds
      </td>

      <td>
        Online serving latencies by EntityType.
      </td>
    </tr>

    <tr>
      <td>
        `featurestore.online_serving.request_bytes_count`
      </td>

      <td>
        Bytes
      </td>

      <td>
        Request size by EntityType.
      </td>
    </tr>

    <tr>
      <td>
        `featurestore.online_serving.request_count`
      </td>

      <td>
        Count
      </td>

      <td>
        Featurestore online serving count by EntityType.
      </td>
    </tr>

    <tr>
      <td>
        `featurestore.online_serving.response_size`
      </td>

      <td>
        Bytes
      </td>

      <td>
        Response size by EntityType.
      </td>
    </tr>

    <tr>
      <td>
        `featurestore.storage.billable_processed_bytes`
      </td>

      <td>
        Bytes
      </td>

      <td>
        Number of bytes billed for offline data processed.
      </td>
    </tr>

    <tr>
      <td>
        `featurestore.storage.stored_bytes`
      </td>

      <td>
        Bytes
      </td>

      <td>
        Bytes stored in Featurestore.
      </td>
    </tr>

    <tr>
      <td>
        `featurestore.streaming_write.offline_processed_count`
      </td>

      <td>
        Count
      </td>

      <td>
        Number of streaming write requests processed for offline storage.
      </td>
    </tr>

    <tr>
      <td>
        `featurestore.streaming_write.offline_write_delays`
      </td>

      <td>
        Seconds
      </td>

      <td>
        Time (in second) since the write API is called until it is written to offline storage.
      </td>
    </tr>
  </tbody>
</table>

### VertexAI FeatureOnlineStore data

<table>
  <thead>
    <tr>
      <th>
        Metric
      </th>

      <th>
        Unit
      </th>

      <th>
        Description
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        `featureonlinestore.online_serving.request_count`
      </td>

      <td>
        Count
      </td>

      <td>
        Number of serving count by FeatureView.
      </td>
    </tr>

    <tr>
      <td>
        `featureonlinestore.online_serving.serving_bytes_count`
      </td>

      <td>
        Bytes
      </td>

      <td>
        Serving response size by FeatureView.
      </td>
    </tr>

    <tr>
      <td>
        `featureonlinestore.online_serving.serving_latencies`
      </td>

      <td>
        Milliseconds
      </td>

      <td>
        Online serving latencies by FeatureView.
      </td>
    </tr>

    <tr>
      <td>
        `featureonlinestore.running_sync`
      </td>

      <td>
        Milliseconds
      </td>

      <td>
        Number of running syncs at given point of time.
      </td>
    </tr>

    <tr>
      <td>
        `featureonlinestore.serving_data_ages`
      </td>

      <td>
        Seconds
      </td>

      <td>
        Measure of the serving data age in seconds.
      </td>
    </tr>

    <tr>
      <td>
        `featureonlinestore.serving_data_by_sync_time`
      </td>

      <td>
        Count
      </td>

      <td>
        Breakdown of data in Feature Online Store by synced timestamp.
      </td>
    </tr>

    <tr>
      <td>
        `featureonlinestore.storage.bigtable_cpu_load`
      </td>

      <td>
        Percent
      </td>

      <td>
        The average CPU load of nodes in the Feature Online Store.
      </td>
    </tr>

    <tr>
      <td>
        `featureonlinestore.storage.bigtable_cpu_load_hottest_node`
      </td>

      <td>
        Percent
      </td>

      <td>
        The CPU load of the hottest node in the Feature Online Store.
      </td>
    </tr>

    <tr>
      <td>
        `featureonlinestore.storage.bigtable_nodes`
      </td>

      <td>
        Count
      </td>

      <td>
        The number of nodes for the Feature Online Store(Bigtable).
      </td>
    </tr>

    <tr>
      <td>
        `featureonlinestore.storage.stored_bytes`
      </td>

      <td>
        Count
      </td>

      <td>
        Bytes stored in the Feature Online Store.
      </td>
    </tr>
  </tbody>
</table>

### VertexAI Location data

<table>
  <thead>
    <th>
      Metric
    </th>

    <th>
      Unit
    </th>

    <th>
      Description
    </th>
  </thead>

  <tbody>
    <tr>
      <td>
        `online_prediction_requests_per_base_model`
      </td>

      <td>
        Count
      </td>

      <td>
        Number of requests per base model.
      </td>
    </tr>

    <tr>
      <td>
        `quota.online_prediction_requests_per_base_model.exceeded`
      </td>

      <td>
        Count
      </td>

      <td>
        Number of attempts to exceed the limit on quota metric.
      </td>
    </tr>

    <tr>
      <td>
        `quota.online_prediction_requests_per_base_model.limit`
      </td>

      <td>
        Count
      </td>

      <td>
        Current limit on quota metric.
      </td>
    </tr>

    <tr>
      <td>
        `quota.online_prediction_requests_per_base_model.usage`
      </td>

      <td>
        Count
      </td>

      <td>
        Current usage on quota metric.
      </td>
    </tr>

    <tr>
      <td>
        `executing_vertexai_pipeline_jobs`
      </td>

      <td>
        Count
      </td>

      <td>
        Number of pipeline jobs being executed.
      </td>
    </tr>

    <tr>
      <td>
        `executing_vertexai_pipeline_tasks`
      </td>

      <td>
        Count
      </td>

      <td>
        Number of pipeline tasks being executed.
      </td>
    </tr>
  </tbody>
</table>

### VertexAI Index data

<table>
  <thead>
    <th>
      Metric
    </th>

    <th>
      Unit
    </th>

    <th>
      Description
    </th>
  </thead>

  <tbody>
    <tr>
      <td>
        `matching_engine.stream_update.datapoint_count`
      </td>

      <td>
        Count
      </td>

      <td>
        Number of successfully upserted or removed datapoints.
      </td>
    </tr>

    <tr>
      <td>
        `matching_engine.stream_update.latencies`
      </td>

      <td>
        Milliseconds
      </td>

      <td>
        The latencies between the user receives a UpsertDatapointsResponse or RemoveDatapointsResponse and that update takes effect.
      </td>
    </tr>

    <tr>
      <td>
        `matching_engine.stream_update.request_count`
      </td>

      <td>
        Count
      </td>

      <td>
        Number of stream update requests.
      </td>
    </tr>
  </tbody>
</table>

### VertexAI Pipeline Job data

<table>
  <thead>
    <th>
      Metric
    </th>

    <th>
      Unit
    </th>

    <th>
      Description
    </th>
  </thead>

  <tbody>
    <tr>
      <td>
        `pipelinejob.duration`
      </td>

      <td>
        Seconds
      </td>

      <td>
        Runtime seconds of the pipeline job being executed (from creation to end).
      </td>
    </tr>

    <tr>
      <td>
        `pipelinejob/task_completed_count`
      </td>

      <td>
        Count
      </td>

      <td>
        Total number of completed Pipeline Tasks.
      </td>
    </tr>
  </tbody>
</table>
