---
title: "Install and configure New Relic eBPF integration"
metaDescription: "Learn how to install and configure the New Relic eBPF agent for your Linux host and Kubernetes cluster."
tags:
    - New Relic integrations with eBPF
    - New Relic eBPF agent
    - eBPF integration
    - eAPM
    - Kubernetes cluster
    - New Relic eBPF agent for Kubernetes
freshnessValidatedDate: never
---

<Callout title="Preview">
We're still working on this feature, but we'd love for you to try it out!

This feature is currently provided as part of a preview pursuant to our [pre-release policies](/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy/). It is not available to customers subject to HIPAA or FedRAMP regulations.
</Callout>

You can install the New Relic eBPF agent on your Kubernetes cluster to monitor your entire system health. The eBPF agent provides deep visibility into application performance without requiring code changes or deploying language-specific agents.

## Compatibility and requirements [#requirements]

* Ensure that all [Kubernetes integration compatibility and requirements](/docs/kubernetes-pixie/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/) are met.


## Install the eBPF agent [#install]

To install the eBPF agent:

1. Log in to your New Relic account.
2. Go to **left navigation pane > + Integration & Agents > eBPF Agent**.
3. On the Select an account screen, select the account you want to install the eBPF agent on, and click **Continue**.
4. On the Select an installation method page, select **Kubernetes**, and click **Continue**.
5. On the Enter your user key screen, select one of the following options, then click **Continue**:

    * **Use an existing key**: If you already have a user key, provide the user key. For more information, refer to [User keys](/docs/apis/intro-apis/new-relic-api-keys/#user-key).
    * **Create a new key**: If you don't have a user key, click **Create a new key** to create one.

6. On the Configure the Kubernetes integration screen:
    
    1. Enter the deployment name for the Kubernetes.
    2. (Optional) Enter the namespace for the integration. The default namespace is `newrelic`.
    3. Click **Continue**.

7. On the Install the Kubernetes integration screen:

    1. Copy and paste the displayed command to install the eBPF agent on your Kubernetes cluster using Helm.
    2. *(Optional)* To download the `values.yaml` configuration file, click **Download**. For more on the configuration parameters, refer to [K8s configuration parameters](#config-params).
    3. *(Optional)* Update the `values.yaml` file as needed and save it.
    4. *(Optional)* To apply the configuration changes, run the following command:
    
        ```bash
            helm repo update ; helm upgrade --install nr-ebpf-agent newrelic/nr-ebpf-agent -n newrelic --values values.yaml
        ```
    5. To verify the installation, run the following command:

        ```bash
            kubectl get pods -n newrelic
        ```


## Find and use data [#data]

Once your host is instrumented and configured to export data to New Relic, you can configure eBPF Dashboard to view your data in the New Relic UI:

1. Go to **[one.newrelic.com](https://one.newrelic.com) > Dashboards**.
2. In the **Dashboards**, click **+ Create a dashboard**.
3. In the **Create a dashboard** window, click **Browse pre-built dashboards**.
4. In the search bar, type **eBPF** and select the **eBPF**.
5. *(Optional)* In the displayed window, click **Edit** to change the account.
6. Click **Setup eBPF Agent** to setup the data source or click **Skip this step** if the eBPF agent is already setup.
7. Click **View dashboard** to view the data collected by the eBPF agent.

<Callout variant="tip">

    You can also find your entity in the New Relic platform by navigating to <DNT>**APM & Services >  Services - OpenTelemetry**</DNT>. For more information on New Relic OpenTelemetry UI, see [OpenTelemetry APM UI](/docs/opentelemetry/get-started/apm-monitoring/opentelemetry-apm-ui).
    
</Callout>

{/* 
Once your app is instrumented and configured to export data to New Relic, you should be able to find your data in the New Relic UI:

    * Find your entity at <DNT>**All entities > Services - OpenTelemetry**</DNT>. The entity name is set to the value of the app's `service.name` resource attribute. For more information on how New Relic service entities are derived from OpenTelemetry resource attributes, see [Services](/docs/opentelemetry/best-practices/opentelemetry-best-practices-resources/#services).
    * Use [NRQL](/docs/nrql/get-started/introduction-nrql-new-relics-query-language/) to query directly for [traces](https://one.newrelic.com/launcher/nr1-core.explorer?overlay=eyJuZXJkbGV0SWQiOiJkYXRhLWV4cGxvcmF0aW9uLnF1ZXJ5LWJ1aWxkZXIiLCJpbml0aWFsQWN0aXZlSW50ZXJmYWNlIjoibnJxbEVkaXRvciIsImluaXRpYWxOcnFsVmFsdWUiOiIiLCJpbml0aWFsUXVlcmllcyI6W3sibnJxbCI6IkZST00gU3BhbiBTRUxFQ1QgY291bnQoKikgd2hlcmUgbmV3cmVsaWMuc291cmNlPSclb3RscCUnIFRJTUVTRVJJRVMifV0sImluaXRpYWxDaGFydFNldHRpbmdzIjp7ImNoYXJ0VHlwZSI6IkNIQVJUX0xJTkUiLCJsaW1pdCI6NzU0MiwibGlua2VkRW50aXR5R3VpZCI6bnVsbCwibGlua2VkRGFzaGJvYXJkSWQiOm51bGwsInlTY2FsZSI6eyJzdGF0aWMiOmZhbHNlLCJkb21haW4iOltudWxsLG51bGxdfSwieVplcm8iOnRydWV9fQo=), [metrics](https://one.newrelic.com/launcher/nr1-core.explorer?overlay=eyJuZXJkbGV0SWQiOiJkYXRhLWV4cGxvcmF0aW9uLnF1ZXJ5LWJ1aWxkZXIiLCJpbml0aWFsQWN0aXZlSW50ZXJmYWNlIjoibnJxbEVkaXRvciIsImluaXRpYWxOcnFsVmFsdWUiOiIiLCJpbml0aWFsUXVlcmllcyI6W3sibnJxbCI6IkZST00gTWV0cmljIFNFTEVDVCBjb3VudCgqKSB3aGVyZSBuZXdyZWxpYy5zb3VyY2UgTElLRSAnJW90bHAlJyBUSU1FU0VSSUVTIn1dLCJpbml0aWFsQ2hhcnRTZXR0aW5ncyI6eyJjaGFydFR5cGUiOiJDSEFSVF9MSU5FIiwibGltaXQiOjc1NDIsImxpbmtlZEVudGl0eUd1aWQiOm51bGwsImxpbmtlZERhc2hib2FyZElkIjpudWxsLCJ5U2NhbGUiOnsic3RhdGljIjpmYWxzZSwiZG9tYWluIjpbbnVsbCxudWxsXX0sInlaZXJvIjp0cnVlfX0K), and [logs](https://one.newrelic.com/launcher/nr1-core.explorer?overlay=eyJuZXJkbGV0SWQiOiJkYXRhLWV4cGxvcmF0aW9uLnF1ZXJ5LWJ1aWxkZXIiLCJpbml0aWFsQWN0aXZlSW50ZXJmYWNlIjoibnJxbEVkaXRvciIsImluaXRpYWxOcnFsVmFsdWUiOiIiLCJpbml0aWFsUXVlcmllcyI6W3sibnJxbCI6IkZST00gTG9nIFNFTEVDVCBjb3VudCgqKSB3aGVyZSBuZXdyZWxpYy5zb3VyY2U9JyVvdGxwJScgVElNRVNFUklFUyJ9XSwiaW5pdGlhbENoYXJ0U2V0dGluZ3MiOnsiY2hhcnRUeXBlIjoiQ0hBUlRfTElORSIsImxpbWl0Ijo3NTQyLCJsaW5rZWRFbnRpdHlHdWlkIjpudWxsLCJsaW5rZWREYXNoYm9hcmRJZCI6bnVsbCwieVNjYWxlIjp7InN0YXRpYyI6ZmFsc2UsImRvbWFpbiI6W251bGwsbnVsbF19LCJ5WmVybyI6dHJ1ZX19Cg==).
    * See [OpenTelemetry APM UI](/docs/opentelemetry/get-started/apm-monitoring/opentelemetry-apm-ui) for more information.

If you can't find your entity and don't see your data with NRQL, see [OTLP troubleshooting](/docs/opentelemetry/best-practices/opentelemetry-otlp-troubleshooting).
You can find the data collected by the eBPF agent in the New Relic Opentelementry UI.

 */}


## Configuration parameters [#config-params]

The [`values.yaml`](https://github.com/newrelic/helm-charts/blob/master/charts/nr-ebpf-agent/values.yaml) file contains the following configuration sections:

<CollapserGroup>

    <Collapser
        id="general-configuration"
        title="General configuration"
    >

These parameters control the core identity and data destination for the eBPF agent.

<table>
    <thead>
        <tr>
            <th>Parameter</th>
            <th>Description</th>
            <th>Data Type</th>
            <th>Example</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>`cluster`</td>
            <td>Specifies the name of your Kubernetes cluster. This field is mandatory.</td>
            <td>String</td>
            <td>`"production-cluster"`</td>
        </tr>
        <tr>
            <td>`licenseKey`</td>
            <td>Specifies your New Relic license key. Required if `customSecretName` is not used.</td>
            <td>String</td>
            <td>`"8356...FFFFNRAL"`</td>
        </tr>
        <tr>
            <td>`nrStaging`</td>
            <td>If `true`, sends data to New Relic's staging environment.</td>
            <td>Boolean</td>
            <td>`true`</td>
        </tr>
        <tr>
            <td>`customSecretName`</td>
            <td>Specifies the name of a Kubernetes secret that contains your license key. Use this to avoid providing the key directly.</td>
            <td>String</td>
            <td>`"newrelic-license-secret"`</td>
        </tr>
        <tr>
            <td>`customSecretLicenseKey`</td>
            <td>Specifies the key within the secret where the license key value is stored. Used with `customSecretName`.</td>
            <td>String</td>
            <td>`"license"`</td>
        </tr>
        <tr>
            <td>`region`</td>
            <td>Specifies your New Relic account region (`US` or `EU`). Required when using `customSecretName`.</td>
            <td>String</td>
            <td>`"US"`</td>
        </tr>
        <tr>
            <td>`proxy`</td>
            <td>Specifies the URL of a proxy server, including the port, to route all outgoing agent data through.</td>
            <td>String</td>
            <td>`"http://user:pass@host:port"`</td>
        </tr>
        <tr>
            <td>`logLevel`</td>
            <td>Defines the logging verbosity level for the agent. Valid options: `OFF`, `FATAL`, `ERROR`, `WARNING`, `INFO`, `DEBUG`.</td>
            <td>String</td>
            <td>`"INFO"`</td>
        </tr>
        <tr>
            <td>`logFilePath`</td>
            <td>Specifies a file path inside the agent container for log output. If the path is invalid, logs are directed to stdout.</td>
            <td>String</td>
            <td>`"/var/log/nr-ebpf-agent.log"`</td>
        </tr>
    </tbody>
</table>

</Collapser>

    <Collapser
        id="data-filtering"
        title="Data filtering configuration"
    >


These parameters control which data is collected and sent to New Relic, helping you manage data ingestion.

<table>
    <thead>
        <tr>
            <th>Parameter</th>
            <th>Description</th>
            <th>Data Type</th>
            <th>Example</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>`dropDataIpServiceNames`</td>
            <td>If `true`, prevents the agent from reporting telemetry for services identified only by an IP address.</td>
            <td>Boolean</td>
            <td>`true`</td>
        </tr>
        <tr>
            <td>`dropDataNewRelic`</td>
            <td>If `true`, drops all telemetry originating from the `newrelic` namespace to prevent self-monitoring.</td>
            <td>Boolean</td>
            <td>`true`</td>
        </tr>
        <tr>
            <td>`dropAPMEnabledPods`</td>
            <td>If `true`, drops telemetry from pods that are already monitored by a New Relic APM agent to avoid data duplication.</td>
            <td>Boolean</td>
            <td>`true`</td>
        </tr>
        <tr>
            <td>`dropDataForNamespaces`</td>
            <td>Specifies a list of Kubernetes namespaces from which all telemetry will be dropped.</td>
            <td>Strings</td>
            <td>`["kube-system", "monitoring"]`</td>
        </tr>
        <tr>
            <td>`dropDataServiceNameRegex`</td>
            <td>Defines a Go-style regular expression. Data from services with names matching this pattern will be dropped.</td>
            <td>String</td>
            <td>`"kube-dns\|otel-collector"`</td>
        </tr>
        <tr>
            <td>`allowServiceNameRegex`</td>
            <td>Defines a Go-style regular expression that acts as an allowlist for `dropDataServiceNameRegex`. Matching services are kept, even if they also match the drop pattern.</td>
            <td>String</td>
            <td>`"allowed-otel-collector"`</td>
        </tr>
        <tr>
            <td>`dropDataForEntity`</td>
            <td>Specifies a list of application names (from the `NEW_RELIC_APP_NAME` environment variable) to be excluded from monitoring.</td>
            <td>Strings</td>
            <td>`["my-test-app", "temp-service"]`</td>
        </tr>
        <tr>
            <td>`tableStoreDataLimitMB`</td>
            <td>Defines the memory limit in Megabytes (MiB) for the agent's internal data store. This is the primary control for RAM usage.</td>
            <td>String</td>
            <td>`"500"`</td>
        </tr>
    </tbody>
</table>

</Collapser>

    <Collapser
        id="protocol-tracing"
        title="Protocol tracing configuration"
    >


This section allows you to enable monitoring for specific network protocols and configure how trace data (spans) is collected. You can enable or disable monitoring for protocols like HTTP, MySQL, and others, and set parameters for span collection based on latency or error rates. The following protocols are supported:

* HTTP
* MySQL
* PostgreSQL
* MongoDB
* Apache Cassandra
* Redis
* Kafka
* DNS


<table>
    <thead>
        <tr>
            <th>Parameter</th>
            <th>Description</th>
            <th>Data Type</th>
            <th>Example</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>`protocols.<protocol-name>.enabled`</td>
            <td>If `true`, enables monitoring for the specified protocol for example, `http`, `mysql`, and any others.</td>
            <td>Boolean</td>
            <td>`true`</td>
        </tr>
        <tr>
            <td>`protocols.<protocol-name>.spans.enabled`</td>
            <td>If `true`, exports trace spans for the enabled protocol.</td>
            <td>Boolean</td>
            <td>`true`</td>
        </tr>
        <tr>
            <td>`protocols.<protocol-name>.spans.samplingLatency`</td>
            <td>Defines the latency-based sampling threshold for exporting spans. Valid options: `p1`, `p10`, `p50`, `p90`, `p99`.</td>
            <td>String</td>
            <td>`"p90"`</td>
        </tr>
        <tr>
            <td>`protocols.http.spans.samplingErrorRate`</td>
            <td>For HTTP only. Exports spans from any route where the error rate exceeds the specified percentage (1-100).</td>
            <td>String</td>
            <td>`"5"`</td>
        </tr>
    </tbody>
</table>

</Collapser>

    <Collapser
        id="daemonset-configs"
        title="DaemonSet configurations"
    >


These sections control the deployment settings for the solution's main components. An asterisk `(*)` denotes the component name.

<table>
    <thead>
        <tr>
            <th>Parameter</th>
            <th>Description</th>
            <th>Data Type</th>
            <th>Example</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>`*.image.repository`</td>
            <td>Specifies the container image repository for the component.</td>
            <td>String</td>
            <td>`"docker.io/newrelic/newrelic-ebpf-agent"`</td>
        </tr>
        <tr>
            <td>`*.image.pullPolicy`</td>
            <td>Defines the pull policy for the container image.</td>
            <td>String</td>
            <td>`"IfNotPresent"`</td>
        </tr>
        <tr>
            <td>`*.image.tag`</td>
            <td>Specifies the version tag of the container image to deploy.</td>
            <td>String</td>
            <td>`"agent-0.2.4"`</td>
        </tr>
        <tr>
            <td>`*.resources.limits.memory`</td>
            <td>Defines the maximum memory the container can use.</td>
            <td>String</td>
            <td>`"2Gi"`</td>
        </tr>
        <tr>
            <td>`*.resources.limits.cpu`</td>
            <td>Defines the maximum CPU the container can use.</td>
            <td>String</td>
            <td>`"1"`</td>
        </tr>
        <tr>
            <td>`*.resources.requests.memory`</td>
            <td>Defines the minimum memory requested for the container at startup.</td>
            <td>String</td>
            <td>`"250Mi"`</td>
        </tr>
        <tr>
            <td>`*.resources.requests.cpu`</td>
            <td>Defines the minimum CPU requested for the container at startup.</td>
            <td>String</td>
            <td>`"100m"`</td>
        </tr>
        <tr>
            <td>`*.tolerations`</td>
            <td>Defines pod tolerations to allow scheduling on nodes with specific taints.</td>
            <td>Objects</td>
            <td>`[{"key": "special", "operator": "Exists"}]`</td>
        </tr>
        <tr>
            <td>`*.affinity`</td>
            <td>Defines pod affinity and anti-affinity rules for scheduling.</td>
            <td>Object</td>
            <td>`{}`</td>
        </tr>
        <tr>
            <td>`*.podAnnotations`</td>
            <td>Specifies custom annotations to add to the component's pod.</td>
            <td>Object</td>
            <td>`{"iam.amazonaws.com/role": "my-role"}`</td>
        </tr>
    </tbody>
</table>


</Collapser>

    <Collapser
        id="global-pod-scheduling"
        title="Global pod and scheduling configuration"
    >

These parameters apply to all pods deployed by the Helm chart, unless overridden by a component-specific setting.

<table>
    <thead>
        <tr>
            <th>Parameter</th>
            <th>Description</th>
            <th>Data Type</th>
            <th>Example</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>`podLabels`</td>
            <td>Specifies additional labels to apply to all pods deployed by the chart.</td>
            <td>Object</td>
            <td>`{"team": "observability"}`</td>
        </tr>
        <tr>
            <td>`priorityClassName`</td>
            <td>Specifies the `PriorityClass` for all pods.</td>
            <td>String</td>
            <td>`"high-priority"`</td>
        </tr>
        <tr>
            <td>`nodeSelector`</td>
            <td>Constrains pods to only run on nodes with matching labels.</td>
            <td>Object</td>
            <td>`{"disktype": "ssd"}`</td>
        </tr>
    </tbody>
</table>

</Collapser>

    <Collapser
        id="tls-configuration"
        title="TLS Configuration"
    >

This section configures secure communication between the eBPF agent and client components.

<table>
    <thead>
        <tr>
            <th>Parameter</th>
            <th>Description</th>
            <th>Data Type</th>
            <th>Example</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>`tls.enabled`</td>
            <td>If `true`, enables TLS for internal communication between components.</td>
            <td>Boolean</td>
            <td>`true`</td>
        </tr>
        <tr>
            <td>`tls.autoGenerateCert.enabled`</td>
            <td>If `true`, directs Helm to automatically generate a self-signed certificate for TLS.</td>
            <td>Boolean</td>
            <td>`true`</td>
        </tr>
        <tr>
            <td>`tls.autoGenerateCert.recreate`</td>
            <td>If `true`, a new certificate is generated on every `helm upgrade`.</td>
            <td>Boolean</td>
            <td>`false`</td>
        </tr>
        <tr>
            <td>`tls.autoGenerateCert.certPeriodDays`</td>
            <td>Defines the validity period in days for the auto-generated certificate.</td>
            <td>Integer</td>
            <td>`730`</td>
        </tr>
        <tr>
            <td>`tls.certFile`</td>
            <td>Specifies the path to your custom PEM-encoded certificate file. `autoGenerateCert.enabled` must be `false`.</td>
            <td>String</td>
            <td>`"my-certs/tls.crt"`</td>
        </tr>
        <tr>
            <td>`tls.keyFile`</td>
            <td>Specifies the path to your custom PEM-encoded private key file.</td>
            <td>String</td>
            <td>`"my-certs/tls.key"`</td>
        </tr>
        <tr>
            <td>`tls.caFile`</td>
            <td>Specifies the path to your custom Certificate Authority (CA) certificate file.</td>
            <td>String</td>
            <td>`"my-certs/ca.crt"`</td>
        </tr>
    </tbody>
</table>

</Collapser>

</CollapserGroup>