---
title: "Create an alert from a chart"
metaDescription: "How to create an alert from a chart with New Relic"
---

import alertsCreate from 'images/alerts_screenshot-crop_create-query.webp'

import alertsSignal from 'images/alerts_screenshot-crop_signal-select.webp'

import alertsTune from 'images/alerts_screenshot-crop_signal-tune.webp'

import alertsThresholds from 'images/alerts_screenshot-crop_set-thresholds.webp'

import alertsDetails from 'images/alerts_screenshot-crop_condition-details.webp'

Starting an alert from a chart is one of the easiest ways to set up a New Relic alert condition. Instead of creating a NRQL query from scratch, charts let you use pre-existing NRQL conditions to begin the process. In this example, we'll set up an alert condition using an error response chart.

<img
    title="Create chart alert"
    alt="A screenshot displaying where to create an alert condition from a chart in New Relic"
    src={alertsCreate}
/>

<Callout variant="tip">
If you need more information on any of the options or terms covered in the steps below, check out our [alerts docs](/docs/alerts-applied-intelligence/applied-intelligence/incident-intelligence/basic-alerting-concepts/).
    </Callout>

<Steps>
    <Step>
## Select a chart and define your signal

<img
    title="Defining a signal"
    alt="A screenshot detailing how to select a query for an alert condition"
    src={alertsSignal}
/>

1. Go to any dashboard. Then, find a chart you want to create an alert from, click the **...** menu, and select **Create alert condition**.

2. Select the signal, or stream of data, you want to use to create the alert condition. To keep things simple, select the **Build a query** option for your first condition.

3. Modify the NRQL query to match what you want you to monitor, if necessary. For now, we recommend leaving the query as is, but once you have a better handle on alerts and NRQL queries, you can change the query to monitor nearly any data threshold.

4. Select **Next** once you've finished to save your progress and move on to the next step.
    </Step>
    <Step>
## Adjust the signal behavior

<img
    title="Tuning your signal"
    alt="A screenshot displaying how to set options for a signal in an alert condition"
    src={alertsTune}
/>

1. **Set the data aggregation options**:
    * **Window duration**: Use the **Window duration** setting to determine how frequently we aggregate your data, such as groupings of every 5 minutes or every hour. This feature's setting depends on what kind of data you're monitoring. If you're unsure of what it should be, you can leave it at the default setting.
    * **Sliding window aggregation**: Choose whether or not you want to use [sliding window aggregation](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/#sliding-window-aggregation). This feature creates smoother charts when dealing with erratic data by creating overlapping aggregation windows. This feature is disabled by default, and we recommend keeping it disabled for your first alert
    * **Streaming method**: The options most commonly used are **Event flow** and **Event timer**. **Event flow** is best suited for frequent and consistently reporting data, while **Event timer** is best suited for data that reports inconsisntely, infrequently, or ingests in batches. If you want to learn more about the individual settings to see which method should be used in each scenario, see [our docs](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/streaming-alerts-key-terms-concepts/#aggregation-methods)

2. **Set the timing option**: This determines how long we wait for events in each window before we evaluate them. A longer delay will mean fewer false alerts, but it will also mean longer potential downtime before an incident is open. This capability is also shown differently depending on the option you select: it's called **Delay** in the **Event flow** and **Cadence** options and **Timer** in **Event timer**. For your first alert, we recommend keeping the default time setting.

3. **Select the gap filling strategy**: Gap-filling is a reactive feature that fills blank periods in data reporting with user-defined synthetic value. If you believe the data reporting will be sporadic, you can avoid false alerts by filling in empty aggregation windows with this data. For your first alert, select **None** under **Gap-filling strategy**.

4. **Select the evaluation delay**: If used, this option enables a period of time for New Relic to wait before evaluating a new signal against your set threshold. It's useful for preventing false positive incidents when a data stream begins to report. For your first alert, you can leave it to the disabled toggle unless you know that this option applies to your data.

5. Select **Next** once you've finished to save your progress and move on to the next step.
    </Step>
    <Step>
## Set the static thresholds

<img
    title="Set static thresholds"
    alt="A screenshot displaying how to set static thresholds for an alert condition"
    src={alertsThresholds}
/>

1. Start by selecting whether the threshold will have a status of **Critical** or **Warning** in the **Security level** section.

2. Set the limit that, when met, will open an incident. These values will change depending on what you want the condition to do. For example, if you wanted to create a condition for an error message, you might set the threshold to **equal to 1 at least once in 5 minutes**, but if you wanted to create a condition for longer periods of latency issues, you might set the threshold to **above or equal to 50 for at least 15 minutes**.

3. If you want to add another threshold, select either **Add threshold** or **Add lost signal threshold**, depending on your needs. Lost signal thresholds are important for monitoring entities that may stop reporting or queries that may [return null values](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/#query-order).

4. Select **Next** once you've finished to save your progress and move on to the next step.
    </Step>
    <Step>
## Add alert condition details

<img
    title="Alert condition details"
    alt="A screenshot displaying how to create the details for an alert"
    src={alertsDetails}
/>

1. Add a name for your alert in the **Name** field. This will help you identify this condition, as well as find the condition in the future in case you need to edit or remove it.

2. Choose the policy for this condition to connect to. If you have an **Existing policy**, select it from the drop-down. If not, you'll need to [create a policy](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/create-edit-or-find-alert-policy/#alert-policy-name) by selecting **New policy** and creating it from here.

3. Set any of the additional settings for the alert. This includes setting how long until open incidents are automatically closed and entering any [custom descriptions](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/alert-custom-violation-descriptions/) for the incidents.

4. Select **Save condition** to finish.
    </Step>
</Steps>

You've now created an alert condition, and helped to make identifying symptoms within your system that much easier! We offer lots of configuration options for alerts, so you can fine-tune them based on your team's needs and whatever you're monitoring. If that was your first alert condition, we suggest you create more from your charts with different options to get used to how they work. You can also create conditions from NRQL queries, which is the next topic in this tutorial series.

## Get more from alerting

We have some advanced alerting features that can help you make alerting more efficent and manageable for your team:

* If you want greater control over where and when you receive alerts and ensure the right people are notified about issues, see how to use [Workflows](/docs/alerts-applied-intelligence/applied-intelligence/incident-workflows/incident-workflows/).
* If you want to enable near-instant notification of critical alerts, helping you reduce your mean time to resolution (MTTR), see how to use our [anomaly detection with applied intelligence](/docs/alerts-applied-intelligence/applied-intelligence/anomaly-detection/anomaly-detection-applied-intelligence/).
* If you want to use APIs to manage New Relic alerts, you can do so with either [REST API or GraphQL](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/alerts-api/intro-alerts-api/).

<DocTiles>
  <DocTile title='Getting started with New Relic alerts' path="/docs/tutorial-create-alerts/create-new-relic-alerts/"> Learn about New Relic alerts</DocTile>
  <DocTile title='Create alerts from charts' path="/docs/tutorial-create-alerts/create-chart-alerts/" label={{text: "You are here", color: "orange"}}> Learn how to create alerts from a chart</DocTile>
  <DocTile title='Create NRQL alerts' path="/docs/tutorial-create-alerts/create-nrql-alerts/"> Learn how to create alerts from NRQL queries</DocTile>
  <DocTile title='Improve your stack with alerts' path="/docs/tutorial-create-alerts/improve-with-alerts/"> Learn how to use alerts to improve your system</DocTile>
</DocTiles>
