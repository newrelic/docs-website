---
title: Control distributed tracing data volume
tags:
  - Understand dependencies
  - Distributed tracing
  - Span configuration
  - Cost optimization
metaDescription: Configure APM agent settings to control distributed tracing span volume and attributes, reducing costs by 50-85% while maintaining visibility.
freshnessValidatedDate: never
---

Distributed tracing is powerful for understanding your system, but it can become expensive at scale. You might face high costs, incomplete trace coverage, or an all-or-nothing dilemma where you either capture everything or nothing from each service.

APM agents give you precise control over your distributed tracing data. You can reduce tracing costs by 50-85% while maintaining complete service-to-service visibility. These aren't separate products—they're configuration options you set in your APM agent files.

## Common distributed tracing challenges [#challenges]

Customers tell us they face three main challenges with distributed tracing:

* **High costs at scale:** Full distributed tracing data can get expensive as your system grows. A typical microservices architecture generating 100GB of trace data per day costs around $30/day in data ingest alone ($0.30/GB). That's $900/month or $10,800/year—and many systems generate far more.

* **Incomplete instrumentation:** When only some services send traces, you get fragmented maps and broken trace paths. It's hard to justify the cost of full instrumentation everywhere, but partial coverage leaves blind spots in your system understanding.

* **All-or-nothing problem:** You're forced to choose: capture everything from a service (expensive) or capture nothing (no visibility). There's no middle ground that gives you connectivity data without the full cost.

## How APM agent configuration solves these problems [#solution]

APM agents let you control exactly how much trace data each service sends to New Relic. You can:

* **Reduce costs by 50-85%** by filtering which spans get captured and what attributes they contain
* **Instrument all services at lower cost** using lightweight trace data for supporting services
* **Mix configurations across services** keeping full detail for critical services while reducing costs elsewhere

These are configuration settings in your APM agent files (Java, .NET, Node.js, Python, Go, Ruby, PHP). Not separate products, not OpenTelemetry—just APM agent settings.

**Cost example:** If you're spending $30/day ($900/month) on 100GB of trace data, these configurations could reduce that to:
* Option 1: $13.50/day ($405/month) - 55% savings
* Option 2: $10.50/day ($315/month) - 65% savings
* Option 3: $7.50/day ($225/month) - 75% savings

## Three configuration options [#options]

APM agents provide three sequential configuration levels. Each option includes the previous level's reductions plus additional filtering:

### Option 1: Remove in-process spans

Drop internal method-level spans, keeping only entry spans (incoming requests) and exit spans (calls to other services, databases, etc.).

**What you get:**
* Service-to-service visibility intact
* Database and external API calls visible
* Complete distributed trace paths

**What you lose:**
* Internal method-level execution detail
* Step-by-step code path within each service

**Typical reduction:** 50-80% fewer spans

**Best for:** Services with deep call stacks (Java, .NET) where you rarely need internal method detail

### Option 2: Remove in-process spans + reduce attributes

Everything from Option 1, plus strip most attributes from entry and exit spans. Keeps only attributes needed for trace connectivity and service relationships.

**What you get:**
* Everything from Option 1
* Service names and relationships
* Duration and timing data
* Error tracking

**What you additionally lose:**
* Detailed span metadata (HTTP headers, query parameters, etc.)
* Most custom attributes

**Typical reduction:** 60-75% total data volume

**Best for:** Supporting services where you need connectivity but not detailed diagnostics

### Option 3: Remove in-process spans + reduce attributes + compress duplicates

Everything from Option 2, plus compress duplicate exit spans. If your service calls the same database or downstream service multiple times in a request, only the first call gets reported as a span.

**What you get:**
* Everything from Option 2
* Proof that Service A calls Service B
* Complete service relationship graph

**What you additionally lose:**
* Count of how many times each service was called
* Individual timing for repeated calls

**Typical reduction:** 70-85% total data volume

**Best for:** High-volume services or non-production environments where you need connectivity proof but not call frequency

## What you preserve vs what you lose [#tradeoffs]

All three options preserve the fundamentals you need for troubleshooting distributed systems:

<table>
  <thead>
    <tr>
      <th width={200}>
        You always keep
      </th>
      <th>
        You lose (varies by option)
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        * Complete end-to-end traces
        * Service-to-service relationships
        * Request flow visualization
        * Dynamic Flow Map connectivity
        * Transaction data
        * Error tracking across services
      </td>
      <td>
        * In-process method-level detail
        * Detailed span attributes
        * Custom attributes (unless opted in)
        * Internal code path visibility
        * Duplicate call tracking (Option 3 only)
      </td>
    </tr>
  </tbody>
</table>

**Key insight:** You maintain the "what" and "where" of your system behavior while reducing the "how" details.

## When to use these configurations [#when-to-use]

Consider these configurations if you're facing any of these situations:

<CollapserGroup>
  <Collapser
    id="high-costs"
    title="You need to reduce distributed tracing costs"
  >
    Prioritize your services and apply different configurations based on business importance. Keep full distributed tracing on customer-facing transactions, use Option 1 on important APIs, and Options 2-3 on supporting services.

    **Recommended approach:** Tiered strategy based on service priority

    **Example:** 20% of services keep full tracing, 30% use Option 1, 50% use Option 2

    **Expected savings:** 45-60% overall cost reduction
  </Collapser>

  <Collapser
    id="fragmented-traces"
    title="You have gaps in your trace coverage"
  >
    Use Option 2 on uninstrumented services to fill the gaps. You'll get complete end-to-end traces without the cost of full instrumentation everywhere. This fixes broken-looking service maps and incomplete trace paths.

    **Recommended approach:** Option 2 on previously uninstrumented services

    **Result:** Complete service visibility at approximately 70% lower cost than full instrumentation
  </Collapser>

  <Collapser
    id="new-customer"
    title="You're new to distributed tracing and want to start small"
  >
    Start with Option 2 across all services to prove value quickly at low cost. You'll see complete traces and service maps immediately. Then upgrade critical services to full tracing as needed.

    **Recommended approach:** Start with Option 2 everywhere, expand selectively

    **Benefit:** Fast proof-of-value with controlled costs
  </Collapser>

  <Collapser
    id="large-deployment"
    title="You have many microservices generating lots of span data"
  >
    Use Option 1 on services with deep call stacks (especially Java and .NET). This dramatically reduces span volume while maintaining service-to-service visibility. You rarely need internal method detail across dozens of services.

    **Recommended approach:** Option 1 on high-span-count services

    **Example:** Java services with 10+ spans per request drop to 2-3 spans
  </Collapser>
</CollapserGroup>

## Understanding span types [#span-types]

To understand these options, you need to know about the three types of spans in distributed tracing:

* **Entry spans:** The first span when a request enters your service. This represents the incoming request.
  * Example: An HTTP request arriving at your API service

* **Exit spans:** Spans representing calls from your service to downstream dependencies—databases, external services, other microservices, caches, etc.
  * Examples: Database queries, HTTP calls to other services, cache operations (Redis, Memcached), external API calls

* **In-process spans:** Spans representing internal method calls within your service. These show the detailed execution path inside your code.
  * Examples: Individual function calls, internal service layers, business logic methods, framework operations

Together, all spans within one service form a **transaction trace**. When you connect transaction traces across multiple services, you get a **distributed trace**.

**How the options work:**
* **Option 1** removes in-process spans, keeps entry and exit spans
* **Option 2** removes in-process spans + strips attributes from entry and exit spans
* **Option 3** removes in-process spans + strips attributes + compresses duplicate exit spans

## Requirements [#requirements]

To use these configuration options, you need:

* **APM agent monitoring:** These configurations only work with New Relic APM agents (Java, .NET, Node.js, Python, Go, Ruby, PHP). They do not apply to OpenTelemetry instrumentation.
* **Distributed tracing enabled:** Your services must already have distributed tracing enabled.
* **Compatible agent versions:** See configuration docs for minimum agent versions.
* **Configuration file access:** You'll modify your agent's configuration file (typically YAML format).

When services with different configurations communicate, trace context propagates correctly—you can mix configuration levels across your architecture (some services with Option 1, others with full tracing, etc.).

**Note:** You can also adjust adaptive sampling rates independently of these options to further control how many traces are captured per minute.

## Get started [#get-started]

Ready to configure span filtering for your APM agents? Start here:

* [Configure span filtering](/docs/distributed-tracing/core-tracing/configure-minimal-spans-tracing): Implement Options 1-3 for your agent language
* [Configure span attributes](/docs/distributed-tracing/core-tracing/configure-low-granularity-tracing): Fine-tune attribute inclusion for Options 2-3
* [Cost optimization strategies](/docs/distributed-tracing/core-tracing/cost-optimization-strategies): Strategic guide for reducing costs while maintaining visibility

<Callout variant="tip">
  Start conservative. Enable Option 1 on 2-3 non-critical services first, monitor the impact for 1-2 weeks, then expand based on results. You can always adjust or revert configurations.
</Callout>
