---
title: Control distributed tracing data volume
tags:
  - Understand dependencies
  - Distributed tracing
  - Span configuration
  - Cost optimization
metaDescription: Configure APM agent settings to control distributed tracing span volume and attributes, reducing costs by 50-85% while maintaining visibility.
freshnessValidatedDate: never
---

Distributed tracing is powerful for understanding your system, but it can become expensive at scale. You might face high costs, incomplete trace coverage, or an all-or-nothing dilemma where you either capture everything or nothing from each service.

APM agents give you precise control over your distributed tracing data. You can reduce tracing costs by 50-85% while maintaining complete service-to-service visibility. These aren't separate products—they're configuration options you set in your APM agent files.

## Common distributed tracing challenges [#challenges]

Customers tell us they face three main challenges with distributed tracing:

* **High costs at scale:** Full distributed tracing data can get expensive as your system grows. A typical microservices architecture generating 100GB of trace data per day costs around $30/day in data ingest alone ($0.30/GB). That's $900/month or $10,800/year—and many systems generate far more.

* **Incomplete instrumentation and fragmentation:** When only some services send traces, you get fragmented maps and broken trace paths. Gaps in instrumentation create blind spots where you can't see the complete request flow. It's hard to justify the cost of full instrumentation everywhere, but partial coverage makes it difficult to understand your full system behavior.

* **All-or-nothing problem:** You're forced to choose: capture everything from a service (expensive) or capture nothing (no visibility). There's no middle ground that gives you connectivity data without the full cost.

## How APM agent configuration solves these problems [#solution]

APM agents let you control exactly how much trace data each service sends to New Relic. You can:

* **Reduce costs by 50-85%** by filtering which spans get captured and what attributes they contain
* **Instrument all services at lower cost** using lightweight trace data for supporting services
* **Mix configurations across services** keeping full detail for critical services while reducing costs elsewhere

These are configuration settings in your APM agent files (Java, .NET, Node.js, Python, Go, Ruby, PHP). Not separate products, not OpenTelemetry—just APM agent settings.

**Cost example:** If you're spending $30/day ($900/month) on 100GB of trace data, these configurations can provide more cost optimization controls to customize your needs. Results vary by application.

## Three configuration options [#options]

APM agents provide three sequential configuration levels called **partial granularity** options. Each option includes the previous level's reductions plus additional filtering:

### Reduced granularity: Remove in-process spans

Drop internal method-level spans, keeping only entry spans (incoming requests) and exit spans (calls to other services, databases, etc.). Note that AI spans are still captured even with this option enabled.

**What you get:**
* Service-to-service visibility intact
* Database and external API calls visible
* Complete distributed trace paths
* AI monitoring spans preserved

**What you lose:**
* Internal method-level execution detail
* Step-by-step code path within each service

**Typical reduction:** 50-80% fewer spans

**Best for:** Services with deep call stacks (Java, .NET) where you rarely need internal method detail

### Essential granularity: Remove in-process spans + reduce attributes

Everything from reduced granularity, plus strip most attributes from entry and exit spans. Keeps only attributes needed for trace connectivity and service relationships.

**What you get:**
* Everything from reduced granularity
* Service names and relationships
* Duration and timing data
* Error tracking

**What you additionally lose:**
* Detailed span metadata (HTTP headers, query parameters, etc.)
* Most custom attributes

**Important:** If your team relies on specific custom attributes or traced functions for monitoring or troubleshooting, verify these will still be available before enabling this option.

**Typical reduction:** 60-75% total data volume

**Best for:** Supporting services where you need connectivity but not detailed diagnostics

### Compact granularity: Remove in-process spans + reduce attributes + compress duplicates

Everything from essential granularity, plus compress duplicate exit spans. If your service calls the same database or downstream service multiple times in a request, only the first call to the same URL or database gets reported as a span.

**What you get:**
* Everything from essential granularity
* Proof that Service A calls Service B
* Complete service relationship graph

**What you additionally lose:**
* Count of how many times each service was called
* Individual timing for repeated calls

**Typical reduction:** 70-85% total data volume

**Best for:** High-volume services or non-production environments where you need connectivity proof but not call frequency

## What you preserve vs what you lose [#tradeoffs]

All three options preserve the fundamentals you need for troubleshooting distributed systems:

<table>
  <thead>
    <tr>
      <th width={200}>
        You always keep
      </th>
      <th>
        You lose (varies by option)
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        * Complete end-to-end traces
        * Service-to-service relationships
        * Request flow visualization
        * Dynamic Flow Map connectivity
        * Transaction data
        * Error tracking across services
      </td>
      <td>
        * In-process method-level detail
        * Detailed span attributes
        * Custom attributes (unless opted in)
        * Internal code path visibility
        * Duplicate call tracking (Option 3 only)
      </td>
    </tr>
  </tbody>
</table>

**Key insight:** You maintain the "what" and "where" of your system behavior while reducing the "how" details.

## When to use these configurations [#when-to-use]

Consider these configurations if you're facing any of these situations:

<CollapserGroup>
  <Collapser
    id="high-costs"
    title="You need to reduce distributed tracing costs"
  >
    Prioritize your services and apply different configurations based on business importance. Keep full distributed tracing on customer-facing transactions, use reduced granularity on important APIs, and essential/compact granularity on supporting services.

    **Recommended approach:** Tiered strategy based on service priority

    **Example:** 20% of services keep full tracing, 30% use reduced granularity, 50% use essential granularity

    **Expected outcome:** Customized cost optimization that fits your needs
  </Collapser>

  <Collapser
    id="fragmented-traces"
    title="You have gaps in your trace coverage"
  >
    Use essential granularity on uninstrumented services to fill the gaps. You'll get complete end-to-end traces without the cost of full instrumentation everywhere. This fixes broken-looking service maps and incomplete trace paths.

    **Recommended approach:** Essential granularity on previously uninstrumented services

    **Result:** Complete service visibility with lower data volume than full instrumentation
  </Collapser>

  <Collapser
    id="new-customer"
    title="You're new to distributed tracing and want to start small"
  >
    Start with essential granularity across all services to prove value quickly at low cost. You'll see complete traces and service maps immediately. Then upgrade critical services to full tracing as needed, or consider a hybrid sampling approach.

    **Recommended approach:** Start with essential granularity everywhere, expand selectively

    **Benefit:** Fast proof-of-value with controlled costs
  </Collapser>

  <Collapser
    id="large-deployment"
    title="You have many microservices generating lots of span data"
  >
    Use reduced granularity on services with deep call stacks (especially Java and .NET). This dramatically reduces span volume while maintaining service-to-service visibility. You rarely need internal method detail across dozens of services.

    **Recommended approach:** Reduced granularity on high-span-count services

    **Example:** Java services with 10+ spans per request drop to 2-3 spans
  </Collapser>
</CollapserGroup>

## Understanding span types [#span-types]

To understand these options, you need to know about the three types of spans in distributed tracing:

* **Entry spans:** The first span when a request enters your service. This represents the incoming request.
  * Example: An HTTP request arriving at your API service

* **Exit spans:** Spans representing calls from your service to downstream dependencies—databases, external services, other microservices, caches, etc.
  * Examples: Database queries, HTTP calls to other services, cache operations (Redis, Memcached), external API calls

* **In-process spans:** Spans representing internal method calls within your service. These show the detailed execution path inside your code.
  * Examples: Individual function calls, internal service layers, business logic methods, framework operations

Together, all spans within one service form a **transaction trace**. When you connect transaction traces across multiple services, you get a **distributed trace**.

**How the options work:**
* **Reduced granularity** removes in-process spans, keeps entry and exit spans
* **Essential granularity** removes in-process spans + strips attributes from entry and exit spans
* **Compact granularity** removes in-process spans + strips attributes + compresses duplicate exit spans

## Hybrid sampling strategy [#hybrid-sampling]

You can enable **both full and partial granularity tracing** on the same application simultaneously. This powerful approach lets you:

* Sample a percentage of traces at full granularity for detailed troubleshooting
* Sample the remaining traces at partial granularity for cost-efficient coverage
* Achieve 100% sampling coverage with mixed detail levels

**Example:** Configure your service to capture:
* 10% full granularity traces (complete detail for deep analysis)
* 90% partial granularity traces (reduced, essential, or compact)
* Result: 100% trace coverage with significant cost savings

This hybrid approach is ideal when you need occasional detailed traces but can't justify full granularity for all requests. See your agent's configuration documentation for how to implement sampling strategies alongside partial granularity options.

## Requirements [#requirements]

To use these configuration options, you need:

* **APM agent monitoring:** These configurations only work with New Relic APM agents (Java, .NET, Node.js, Python, Go, Ruby, PHP). They do not apply to OpenTelemetry instrumentation.
* **Distributed tracing enabled:** Your services must already have distributed tracing enabled.
* **Compatible agent versions:** See configuration docs for minimum agent versions.
* **Configuration file access:** You'll modify your agent's configuration file (typically YAML format).

When services with different configurations communicate, trace context propagates correctly—you can mix configuration levels across your architecture (some services with Option 1, others with full tracing, etc.).

**Note:** You can also adjust adaptive sampling rates independently of these options to further control how many traces are captured per minute.

## Get started [#get-started]

Ready to configure span filtering for your APM agents? Start here:

* [Configure span filtering](/docs/distributed-tracing/core-tracing/configure-minimal-spans-tracing): Implement Options 1-3 for your agent language
* [Configure span attributes](/docs/distributed-tracing/core-tracing/configure-low-granularity-tracing): Fine-tune attribute inclusion for Options 2-3
* [Cost optimization strategies](/docs/distributed-tracing/core-tracing/cost-optimization-strategies): Strategic guide for reducing costs while maintaining visibility

<Callout variant="tip">
  Start conservative. Enable Option 1 on 2-3 non-critical services first, monitor the impact for 1-2 weeks, then expand based on results. You can always adjust or revert configurations.
</Callout>
