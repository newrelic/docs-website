---
title: Navigate from Error Inbox to relevant traces
tags:
  - Understand dependencies
  - Distributed tracing
  - Intelligent Exemplar
  - Error Inbox
  - Error investigation
metaDescription: Find traces that actually contain specific errors from Error Inbox using Intelligent Exemplar's prioritized trace selection.
freshnessValidatedDate: never
---

{/* TODO: Add screenshot of Error Inbox with trace navigation points highlighted */}

Error Inbox groups and prioritizes errors across your services. When you need to investigate a specific error occurrence, Intelligent Exemplar ensures you see traces that actually contain that error—not just traces from the same group. This guide shows you how to navigate from Error Inbox to actionable traces.

## Before you begin [#before-you-begin]

Ensure you have:

* **Error Inbox configured** for your services
* **Distributed tracing enabled** on relevant services
* **Intelligent Exemplar access** (Limited Preview)
* **Recent error occurrences** with associated trace data

<Callout variant="tip">
  New to Error Inbox? See [Error Inbox introduction](/docs/errors-inbox/errors-inbox) to understand how errors are grouped and managed.
</Callout>

## View traces from Error Inbox charts [#from-charts]

Error Inbox displays error trends through charts showing error counts and rates. Clicking on these charts with Intelligent Exemplar takes you directly to relevant traces.

### From error count chart [#error-count-chart]

The error count chart shows the number of error occurrences over time.

1. **Navigate to Error Inbox:**
   * Go to **[one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > Errors > Error Inbox**
   * Select your workload or service group

2. **Select an error group:**
   * Click on any error in the list to see its details
   * View the **Error count** chart showing occurrences over time

   {/* TODO: Add screenshot of Error Inbox with error count chart */}

3. **Click on a spike or time period:**
   * Click directly on a spike in the chart
   * Or drag to select a time range

4. **View intelligent trace selection:**
   * Redirected to traces that contain this specific error
   * Traces filtered to your selected time window
   * Only traces with matching error messages/types shown
   * Prioritized by relevance and completeness

{/* TODO: Add screenshot of filtered trace list from error count navigation */}

**What makes this intelligent:**

Traditional navigation would show all traces from the error group, including:
* ❌ Traces without the error
* ❌ Traces from different time periods
* ❌ Fragmented or incomplete traces

Intelligent Exemplar shows:
* ✅ Only traces containing the specific error
* ✅ From the exact time window you selected
* ✅ Preferring complete, non-fragmented traces
* ✅ Ranked by how representative they are of the error pattern

### From error rate chart [#error-rate-chart]

The error rate chart shows errors per minute or as a percentage of total requests.

1. **View error rate trends:**
   * In Error Inbox, select an error group
   * Look at the **Error rate** chart

   {/* TODO: Add screenshot of error rate chart */}

2. **Identify rate spikes:**
   * Notice periods of elevated error rates
   * Click on spikes or concerning trends

3. **Investigate with context:**
   * Intelligent Exemplar shows traces from that rate spike
   * Includes context about request volume
   * Highlights if errors affect all requests or specific patterns

**Example:**

```
Error rate spike from 0.1% to 15% at 4:30 PM

Intelligent Exemplar shows:
✓ 47 error traces from 4:28-4:33 PM
✓ All containing "UnauthorizedException"
✓ Pattern: 95% affect API endpoint /user/login
✓ Insight: Spike correlates with authentication service deployment
```

## Understanding associated distributed traces [#associated-traces]

Error Inbox shows an "Associated distributed trace" link for each error occurrence. This is where Intelligent Exemplar provides the most value.

### The traditional problem [#traditional-problem]

**Before Intelligent Exemplar:**

Clicking "View trace" from an error occurrence would:
1. Take you to the trace group page
2. Show all traces in that group (hundreds or thousands)
3. Include traces both with and without the error
4. Require manual searching to find the right trace
5. Provide no indication which trace corresponds to the error you clicked on

**Result:** Time-consuming and frustrating experience.

### The Intelligent Exemplar solution [#intelligent-solution]

**With Intelligent Exemplar:**

Clicking the trace link from an error occurrence:
1. Takes you directly to relevant traces
2. Shows only traces containing this specific error
3. Prioritizes the most representative traces
4. Prefers complete (non-fragmented) traces
5. Provides clear connection between error and trace

{/* TODO: Add before/after comparison screenshots */}

### Navigate to the right trace [#navigate-right-trace]

1. **From an error occurrence:**
   * In Error Inbox, expand an error group
   * View individual error occurrences in the list
   * Each occurrence has an associated trace link

   {/* TODO: Add screenshot showing error occurrence list with trace links */}

2. **Click the trace link:**
   * Click "View trace" or the trace ID
   * Intelligent Exemplar identifies the most relevant trace(s)

3. **View prioritized traces:**
   * Traces containing this exact error are shown first
   * Time window matches the error occurrence
   * Complete traces prioritized over fragmented ones
   * Metadata shows error details prominently

{/* TODO: Add screenshot of intelligent trace view with error highlighted */}

### Prioritized trace list features [#prioritized-list-features]

When viewing traces from an error occurrence, you'll see:

**Relevance indicators:**
* **Exact match:** Traces with the exact error message and type
* **Timing match:** Traces from the error occurrence timestamp (±30 seconds)
* **Completeness:** Complete traces marked and prioritized
* **Service involvement:** Shows which services participated in the error

**Error-specific metadata:**
* Error message highlighted in trace details
* Span where error occurred clearly marked
* Stack trace readily available
* Related error attributes shown

**Prioritization factors:**

<table>
  <thead>
    <tr>
      <th width={200}>Factor</th>
      <th>Description</th>
      <th>Impact</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>**Error match**</td>
      <td>Contains the exact error type and message</td>
      <td>Critical - must match</td>
    </tr>
    <tr>
      <td>**Time proximity**</td>
      <td>How close to the error occurrence timestamp</td>
      <td>High - within ±30s preferred</td>
    </tr>
    <tr>
      <td>**Completeness**</td>
      <td>All services present, no missing spans</td>
      <td>High - complete traces first</td>
    </tr>
    <tr>
      <td>**Representativeness**</td>
      <td>Has typical attributes for this error pattern</td>
      <td>Medium - helps pattern analysis</td>
    </tr>
    <tr>
      <td>**Recency**</td>
      <td>More recent occurrences</td>
      <td>Medium - for current state</td>
    </tr>
  </tbody>
</table>

## View error propagation paths [#error-propagation]

Intelligent Exemplar provides visualization of how errors propagate through your services.

### Understand propagation paths [#understand-propagation]

When investigating an error, you need to know:
* **Where did it originate?** (source service)
* **How did it spread?** (propagation path)
* **What's affected?** (downstream impact)
* **Which paths are most common?** (contribution percentages)

{/* TODO: Add diagram showing error propagation visualization */}

### Read the propagation diagram [#read-diagram]

The propagation visualization shows:

1. **Source service** (where error originated):
   ```
   payment-db ← Error source
   ```

2. **Propagation paths** (how error spreads):
   ```
   payment-db → payment-service → checkout-api → web-frontend
                → admin-api      → admin-frontend
   ```

3. **Contribution percentages** (which paths account for most errors):
   ```
   checkout-api path: 70% of errors
   admin-api path: 30% of errors
   ```

4. **Affected entities** (services experiencing the error):
   ```
   ⚠️ payment-db (source)
   ⚠️ payment-service (propagates)
   ⚠️ checkout-api (affected)
   ⚠️ admin-api (affected)
   ```

**Example visualization:**

{/* TODO: Add real screenshot of propagation visualization */}

```
Error: DatabaseConnectionTimeout in payment-db

Propagation paths:

Path 1 (70% of errors):
payment-db (source) → payment-service → checkout-api → web-frontend
├─ 520 traces
├─ Average duration: 2.4s
└─ Affects: checkout flow

Path 2 (30% of errors):
payment-db (source) → payment-service → admin-api → admin-frontend
├─ 220 traces
├─ Average duration: 1.8s
└─ Affects: admin operations
```

### Use propagation insights [#use-propagation-insights]

**Identify the source:**
* The originating service where the error first occurs
* Focus remediation efforts here for maximum impact

**Understand impact:**
* See which downstream services are affected
* Prioritize fixes based on impact scope
* Communicate to affected service teams

**Analyze patterns:**
* Some paths may be more affected than others
* Certain operations or endpoints may correlate with errors
* Time-based patterns may emerge

## Affected transaction analysis [#affected-transactions]

Intelligent Exemplar analyzes which types of transactions are impacted by the error.

### View transaction breakdown [#transaction-breakdown]

For any error, see:

**Transaction types affected:**
* Which API endpoints experience the error
* Which operations are impacted
* Which customer actions fail

**Error frequency by transaction:**
* How many traces of each type contain the error
* Percentage of each transaction type that fails
* Comparison with normal success rates

**Duration patterns:**
* How long erroring transactions take
* Comparison with successful transactions
* Whether errors correlate with slow requests

{/* TODO: Add screenshot of transaction analysis panel */}

**Example analysis:**

```
Error: PaymentProcessingException

Affected transactions:

POST /checkout/complete
├─ 850 traces with error
├─ 92% error rate (normally 0.2%)
├─ Average duration: 3.2s (vs 0.8s normal)
└─ Most affected

GET /orders/{id}/status
├─ 220 traces with error
├─ 15% error rate (normally 0.1%)
├─ Average duration: 1.1s (vs 0.5s normal)
└─ Secondarily affected

PATCH /payment/method
├─ 80 traces with error
├─ 8% error rate (normally 0%)
├─ Average duration: 0.9s (vs 0.4s normal)
└─ Minimally affected
```

### Use transaction insights [#use-transaction-insights]

**Scope the impact:**
* Understand which user-facing features are broken
* Communicate specific impact to stakeholders
* Prioritize fixes for most-used transactions

**Find patterns:**
* All affected transactions may share common attributes
* Certain transactions may be more vulnerable
* Helps narrow down root cause

**Test remediation:**
* After fix, verify specific transactions
* Monitor these transaction types closely
* Ensure error rate returns to baseline

## Common error investigation workflows [#common-workflows]

### Investigate new error type [#new-error-investigation]

1. **Notice new error** in Error Inbox
2. **Click on first occurrence** to see details
3. **View associated trace** with Intelligent Exemplar
4. **Examine error span** to understand what failed
5. **Check propagation path** to see scope
6. **Analyze affected transactions** to understand impact
7. **Identify root cause** from trace details
8. **Implement fix** and monitor error rate

### Understand error spike [#error-spike-investigation]

1. **Notice spike** in error count/rate chart
2. **Click on spike time period**
3. **View exemplar traces** from that period
4. **Look for common attributes:**
   * Same customer segment?
   * Same geographic region?
   * Same deployment version?
   * Same external dependency?
5. **Compare with baseline period**
6. **Determine trigger** (deployment, config change, external issue)
7. **Remediate and monitor**

### Track intermittent error [#intermittent-error-tracking]

1. **Select error group** with intermittent occurrences
2. **View error count over extended period** (24 hours or more)
3. **Click on occurrence periods**
4. **Analyze patterns:**
   * Time-of-day correlation?
   * Request volume correlation?
   * Specific customer correlation?
5. **Use transaction analysis** to find commonalities
6. **Identify trigger condition**
7. **Implement fix or mitigation**

## Troubleshooting [#troubleshooting]

### "No trace data available" for an error [#no-trace-data]

**Possible causes:**
* Distributed tracing not enabled on service where error occurred
* Error occurred on a span that wasn't sampled
* Trace data not yet available (1-2 minute delay)

**Solutions:**
* Verify distributed tracing is enabled
* Check [sampling configuration](/docs/distributed-tracing/concepts/how-new-relic-distributed-tracing-works#sampling)
* Wait 2-3 minutes and refresh
* Check for [missing trace data](/docs/distributed-tracing/troubleshooting/missing-trace-data)

### Trace shown doesn't contain the error [#trace-missing-error]

**Possible causes:**
* Trace data incomplete or fragmented
* Error occurred in a different instance of the same trace pattern
* Timing mismatch between error event and trace

**Solutions:**
* Check if trace is marked as "fragmented"
* Look at the propagation path to see if error should have appeared
* Try viewing other traces from the same time period
* Report issue if consistently incorrect

### Too many traces to review [#too-many-traces]

**Solutions:**
* Focus on top 3-5 traces—they're most representative
* Use the propagation path to understand patterns without viewing every trace
* Apply additional filters (service, duration, attributes)
* Use [Attribute Correlation Analysis](/docs/distributed-tracing/attribute-correlation-analysis/introduction-aca) for automated analysis

### Propagation path unclear [#propagation-unclear]

**Possible causes:**
* Complex, branching service architecture
* Some services missing trace instrumentation
* Async or queued operations breaking visual path

**Solutions:**
* Focus on most common paths (highest percentages)
* Check service map to understand architecture
* Verify all services have distributed tracing enabled
* Look at individual traces to understand full flow

## What's next? [#whats-next]

* [Navigate from APM Summary to relevant traces](/docs/distributed-tracing/intelligent-exemplar/navigate-from-apm-summary)
* [Understand trace relevance and pattern analysis](/docs/distributed-tracing/intelligent-exemplar/understand-trace-relevance)
* [Use Attribute Correlation Analysis to find root causes](/docs/distributed-tracing/attribute-correlation-analysis/introduction-aca)

{/*
WRITING NOTES FOR COMPLETION:
- Add all TODO screenshots from actual Error Inbox UI
- Get product team confirmation on prioritization algorithm details
- Add real propagation path diagrams from staging
- Include video walkthrough of error investigation
- Add more customer examples if available
- Verify all troubleshooting scenarios with engineering
- Update with any UI changes from product team
- Add links to related Error Inbox documentation
*/}
