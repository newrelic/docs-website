---
title: "Quality foundation: Optimize web performance to improve your customers' digital experience" 
tags:
  - Observability maturity
  - Customer experience
  - Digital customer experience
  - Implementation guide
  - Quality Foundation
metaDescription: "New Relic observability maturity series: this guide gives you a quality foundation for optimizing your web application's performance to improve your customer experience."
redirects:
  - /docs/new-relic-solutions/observability-maturity/qf-implementation-guide
  - /docs/new-relic-solutions/observability-maturity/customer-experience/quality-foundation-implementation-guide
---

import cxWhatYouCanMeasureNr from 'images/cx_diagram_what-you-can-measure-nr.webp'

import cxSegmentWhitelistInvestigation from 'images/cx_screenshot-full_segment-whitelist-investigation.webp'

import cxQfDashboard from 'images/cx_screenshot-full_qf-dashboard.webp'

import cxQfKpiProgress from 'images/cx_diagram_qf_kpi_progress.webp'

This guide walks you through establishing a high-quality foundation for understanding, measuring, and improving your customer experience. It's part of our [series on observability maturity](/docs/new-relic-solutions/observability-maturity/introduction). 

## Overview [#overview]

Digital customer experience refers to your end users' experience across all your digital touch points. There are four core factors that impact a user's experience:

* Availability: Is it reachable?
* Performance: Does it perform well enough to be usable?
* Content quality: Does it have what users need and can they find it?
* Product and content relevance: Does it have what users care about?

<img
  title="Digital Customer Experience (DCX): What you can measure with New Relic"
  alt="Digital customer experience"
  src={cxWhatYouCanMeasureNr}
/>

Digital customer experience includes web, mobile, and IoT. The first version of this guide is focused on measuring the end user web experience.

This **Quality Foundation** guide is about creating a standard practice to help you understand your digital customer experience in a meaningful way. This implementation guide will help you:
* Look at customer experience in relation to:
  * Global functions, such as search and login
  * Lines of business
  * Regions
* Report back to business stakeholders on what they care about
* Prioritize what you work on
* Create a repeatable practice

### Desired outcome

Improve customer engagement and retention by measuring and improving performance in a way that better aligns to the end user experience.

## Key performance indicators

This guide measures the following KPIs:

<CollapserGroup>
  <Collapser
    id="availability-kpi"
    title="Availability"
  >
This KPI measures whether or not your application or its pages can be accessed by your users.

**Goal:** Improve uptime and availablity

**Thresholds:**
* &lt; 99% warning
* &lt; 95% critical

99%, or "2 9s," is a good minimum standard of availability, even for employee applications or sub-pages.  We configure these default thresholds into the dashboards. You can easily change this to better suit expectations for your application.
  </Collapser>

  <Collapser
    id="core-web-lcp-kpi"
    title="Largest contentful paint (LCP)"
  >
This KPI is part of [Core Web Vitals](https://web.dev/vitals/). Largest Contentful Paint (LCP) measures the time it takes to load the largest image after a user has navigated to a new page.

**Goal:**
* Reduce LCP to 2.5 seconds or better for the 75% percentile for all pages or at least the most critical pages.

**Thresholds:**
* Warning: > 2.5 seconds
* Critical: > 4.0 seconds

LCP thresholds are defined by the team at Google.  The thresholds and the supporting logic behind them can be found on [Google's web.dev site](https://web.dev/defining-core-web-vitals-thresholds/).
  </Collapser>

  <Collapser
    id="core-web-kpi"
    title="First input delay (FID)"
  >
This KPI is part of [Core Web Vitals](https://web.dev/vitals/). It measures the interactivity of a page by tracking the time between user interaction (such as clicking a link or entering text) when the browser begins processing the event.

**Goal:** Reduce FID to 100 milliseconds or better for the 75% percentile for all pages or at least the most critical pages.

**Thresholds:**
* Warning: > 100 milliseconds
* Critical: > 300 milliseconds

FID thresholds are defined by the team at Google.  The thresholds and the supporting logic behind them can be found on [Google's web.dev site](https://web.dev/defining-core-web-vitals-thresholds/).
  </Collapser>

  <Collapser
    id="layout-shift-kpi"
    title="Cumulative layout shift (CLS)"
  >
This KPI is part of [Core Web Vitals](https://web.dev/vitals/). It measures how much the page layout shifts during render.

**Goal:** Maintain a score of 0.1 or less for the 75% percentile for all pages or at least the most critical pages.

**Thresholds:**
* Warning: > 0.1 score
* Critical: > 0.25 score

CLS thresholds are defined by the team at Google.  The thresholds and the supporting logic behind them can be found on [Google's web.dev site](https://web.dev/defining-core-web-vitals-thresholds/).

  </Collapser>

  <Collapser
    id="ttfb-kpi"
    title="Time to first byte (TTFB)"
  >
This KPI measures the time from navigation start (a user clicking a link) to the browser receiving the first byte of the response from the server. Google considers TTFB secondary to Core Web Vitals. We recommend measuring it for a more complete picture. It can be revealing if you see a change in LCP, because it answers the question as to whether the change occurred server side or client side.

**Goal:** Reduce the time to first byte by improving CDN, network, and service performance.

**Thresholds:**
* Warning > 0.5 seconds
* Critical > 1.0 seconds

According to Google and the organization Search Engine People, 500 milliseconds is a decent TTFB for pages with dynamic content. You can find mention of these recommendations in this [Search Engine People post](https://www.searchenginepeople.com/blog/16081-time-to-first-byte-seo.html).
  </Collapser>

  <Collapser
    id="ajax-response-times-kpi"
    title="AJAX response times"
  >
Slow AJAX calls can make the user feel as though nothing is happening or the page is broken. If the response time is slow enough, users may even abandon the journey.

**Goal:** Measure and improve AJAX response times.

**Thresholds:**
* Warning > 2 seconds
* Critical > 2.5 seconds

These thresholds come from experience with customers across a variety of industries.
  </Collapser>

  <Collapser
    id="http-errors-kpi"
    title="HTTP error rate"
  >
HTTP errors (or HTTP `4xx` and `5xx` responses) happen when calls to the backend are not successful.

**Goal:** Measure and reduce the HTTP error rate to ensure your customers are able to do what they came to your site to do.

**Thresholds:**
* Warning &lt; 99% of requests are successful
* Critical &lt; 97% of requests are successful

These thresholds come from experience with customers across a variety of industries.

We made the assumption that every AJAX request is associated with something the user is trying to achieve and treat it accordingly. Because users will often retry failed actions, we allowed for space between warning and critical thresholds.

* If the AJAX requests being measured are an important part of the user journey, we recommended aiming for higher success rates, such as 99.5% or 99.9%.
* If the AJAX requests are tied to login requests, separate 4xx response codes from 5xx response codes and set a much lower threshold for the 4xx responses. You can look to historical response code rates to determine a reasonable threshold.
  </Collapser>

  <Collapser
    id="js-errors-kpi"
    title="JavaScript error rate"
  >
This KPI measures the number of JavaScript errors per page view.

**Goals:**
* Remove irrelevant JavaScript errors being tracked either by tuning ingest or using filtering.
* Reduce JavaScript errors that impact customer performance.

**Thresholds:**
* Warning: > 5% errors per page view
* Critical: > 10% errors per page view

These thresholds come from experience with customers across a variety of industries.
  </Collapser>
</CollapserGroup>

For each KPI, we defined thresholds: one for a warning state, another for a critical state. You might ask where these values come from or how you can be sure they should apply to your application. Our thresholds are the ones recommended by Google (as with Core Web Vitals) or by us, based on our experience across a large number of customers and applications. If you feel strongly that they should be different, you can adjust them, but you should do this at the organizational level rather than on an application-by-application basis.

This **Quality foundation** guide helps you identify where in your application you need to make improvements that will optimize user retention, conversion, and satisfaction. It's less about where things are and more about where to get to.

It also shows you what you should be measuring going forward. You can use this to define [service level objectives (SLOs)](/docs/service-level-management/intro-slm) (in a service level dashboard) and alert on them.

## Prerequisites

### Required knowledge

You should have some basic knowledge of: 
* [New Relic synthetic monitoring](/docs/synthetics/synthetic-monitoring/getting-started/get-started-synthetic-monitoring/)
* [New Relic browser monitoring](/docs/browser/browser-monitoring/getting-started/)
* [New Relic browser monitoring UI](/docs/browser/browser-monitoring/getting-started/introduction-browser-monitoring/)
* [New Relic SPA data in the browser monitoring UI](/docs/browser/single-page-app-monitoring/use-spa-data/view-spa-data-browser-ui/)

### Required installation and configuration

* [The Browser Pro agent version installed on all pages](/docs/browser/browser-monitoring/installation)
* [SPA enabled for single page applications](/docs/new-relic-solutions/best-practices-guides/full-stack-observability/browser-monitoring-best-practices-guide/#how-to-do-it)
* Synthetics monitors configured:
  * [Ping monitors configured for anonymous users](/docs/synthetics/synthetic-monitoring/using-monitors/add-edit-monitors)
  * [Scripted synthetics check configured for login flow](/docs/synthetics/synthetic-monitoring/using-monitors/store-secure-credentials-scripted-browsers-api-tests)
  * Monitors should be configured to [test from all regions applicable to your users](/docs/synthetics/synthetic-monitoring/using-monitors/add-edit-monitors/#setting-location)
  * Monitors should be configured for each domain and each login flow
* [Data retention](/docs/data-apis/manage-data/manage-data-retention) for browser events greater than or equal to 2x an average sprint

## Establish current state [#current-state]

To establish the current state: 
* [Review instrumented pages](#review-instrumented-pages)
* [Validate browser URL grouping](#validate-browser-url-grouping)
* [Understand how you will segment your data](#understand-how-you-will-segment-your-data)
* [Import the quality foundation dashboard](#import-the-quality-foundation-dashboard)
* [Capture current performance for each dashboard page](#capture-current-performance-for-each-dashboard-page)

These steps are explained in more detail below. 

### Review instrumented pages

Review browser apps and pages to make sure that everything you expect to be reporting to New Relic is actually doing so. You can do this by reviewing the **Page Views** tab in the browser monitoring UI or running the following query:

```
SELECT uniques(pageUrl) from PageView LIMIT MAX 
```

You may need to filter out URLs that contain request or customer ID.

### Validate browser URL grouping

Ensure browser segments are captured correctly so user experience performance is measurable in both the New Relic UI as well as at the aggregate level when querying via NRQL.

A segment is the text between two `/` in a URL or between `.` of a domain name. For example, in the URL `website.com/product/widget-name`, the segments are:
* `website`
* `.com`
* `product`
* `widget-name`

When there are a lot of URLs with a lot of segments, URLs can get abridged, so that `website.com/product/widget-name` becomes `website.com/` **or** `website.com/product/`. In this example, the first abridged URL is not particularly useful, but the second one may be a useful way of aggregating customer experience data for the product.

Not sure whether you need to tune your configuration? Import the [Segment allow-list investigation dashboard](https://newrelic.com/instant-observability/browser-segment-investigation/eb24e234-90aa-4908-972d-64d6d56b557e) to help.

<img
  title="Segment allow-list (aka white-list) investigation"
  alt="Segment allow-list (aka white-list) investigation"
  src={cxSegmentWhitelistInvestigation}
/>

Once you've identified which segments to add, you can add them using [Segment allow-lists in browser](/docs/browser/new-relic-browser/configuration/group-browser-metrics-urls/#adding).

### Understand how you will segment your data

Make customer experience data understandable and actionable by breaking it out into different segments. In this case, segments refer to groups of data. It does not refer to sections of URLs, as in [segment allow lists](https://docs.newrelic.com/docs/browser/new-relic-browser/configuration/group-browser-metrics-urls/#adding).

Consider the following statements:
* Most of our users experience 3 seconds or better to first input delay.
* On average, we see 2 seconds to the largest contentful paint.
* Last week, there were 1 million page views.

Compared to:
* Most of the users in the US, Canada, and EMEA experience 2 seconds or better to first input delay. Malaysia and Indonesia users experience 4 seconds; we are looking into this.
* Customers buying car insurance typically see 1 second to largest contentful paint. For home insurance, it's 4 seconds.
* Last week, there were 700,000 page views on mobile browser apps compared to 300,000 on desktop. Let's make sure we're optimizing our mobile experience.

Typical segmentation involves breaking down user experience into the following categories:

<table>
  <thead>
    <tr>
      <th style={{ width: "200px" }}>
        Segment
      </th>

      <th>
        Guidance
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        Region/Location
      </td>

      <td>
**Basic:** Group by country. Browser events automatically contain the country code of requests, so there is nothing you need to do to break it out further.

**Advanced:** Make regional grouping match regional SLO groups by creating your own region attribute using [custom attributes](/docs/telemetry-data-platform/custom-data/custom-events/report-browser-monitoring-custom-events-attributes/) in browser monitoring.

Facet by `countryCode`.

Related attributes:
* `regionCode`
* `city`
* `asnLatitude`
* `asnLongitude`
      </td>
    </tr>

    <tr>
      <td>
        Device
      </td>

      <td>
Break out performance and engagement device type so you can understand:

* Typical breakdown of desktop vs mobile browser users

* Experience of desktop vs mobile browser users

Facet by `deviceType`.

Related attributes:
* `userAgentName`
* `userAgentOS`
* `userAgentVersion`
      </td>
    </tr>

    <tr>
      <td>
        Product/line of business
      </td>

      <td>
In this scenario, a product is a separate line of business or service provided by your organization. Some examples of industries and respective products:
* An insurance company that sells both car and house insurance
* A media company that has multiple streaming services or channels
* A travel company that provides car rental as well as hotel bookings

**Basic:** Break out performance by product by:
* Faceting on `pageUrl`: Use this approach when multiple products are grouped into one browser app in New Relic.
* Faceting by `appName`: Use this approach when each product is instrumented as a separate web app.
* Grouping by `appName` and then facet: Use this approach when there are multiple apps in browser supporting one product.

**Advanced:** Add product offering as a custom attribute to browser pages using [custom attributes](/docs/telemetry-data-platform/custom-data/custom-events/report-browser-monitoring-custom-events-attributes).
      </td>
    </tr>

    <tr>
      <td>
        Environment
      </td>

      <td>
During instrumentation or afterwards, follow a naming convention that specifies the environment for browser apps. Well named browser apps specify product and/or function as well as environment. Examples:
* `account-management.prod`
* `hotels-book.prod`
* `car-insurance.uat`

Using app naming conventions to specify the environment supports filtering data in both the UI and in dashboards. For more information, see the documentation for [how to rename browser apps](/docs/browser/new-relic-browser/configuration/rename-browser-apps/).
      </td>
    </tr>

    <tr>
      <td>
        Team
      </td>

      <td>
In some organizations, a single team supports multiple products, while in others, a product is big enough to be supported by multiple teams. Report on team performance against customer experience or engagement by either adding the team name to the browser app name in New Relic (for example, `account-management.prod.unicorn-squad`) or by using [custom attributes](/docs/telemetry-data-platform/custom-data/custom-events/report-browser-monitoring-custom-events-attributes).
      </td>
    </tr>
  </tbody>
</table>

### Import the quality foundation dashboard

<img
  title="Customer experience quality foundation dashboard"
  alt="Customer experience quality foundation dashboard"
  src={cxQfDashboard}
/>

This step creates the dashboard that you'll use to measure your customer experience and improve it. To do this: 

1. Go to the [Quality foundation quickstart](https://newrelic.com/instant-observability/customer-experience-quality-foundation/7a5739bf-30ee-4be9-9705-14871cafd7f4).
2. Follow the instructions in the quickstart to download the dashboard and customize it to fit how you plan to break out the data.
3. Make sure to align the dashboard to lines of business or customer facing offerings rather than teams. This ensures optimization time is spent where it is most impactful.

### Capture current performance for each dashboard page

<img
  title="Capture current performance for each dashboard page"
  alt="Capture current performance for each dashboard page"
  src={cxQfKpiProgress}
/>

1. Follow the instructions on our [Quality foundation GitHub README](https://github.com/newrelic/oma-resource-center/tree/main/src/content/docs/oma/value-drivers/customer-experience/use-cases/quality-foundation).
2. Use the dashboard from the previous step to understand the overall performance for each line of business. If relevant, apply filters to see performance across region or device. If values drop below targets and it matters, add it to the sheet as a candidate for improvement. Examples of tracking-value: 
   * Not worth tracking: A company that sells insurance in the US only notices poor performance in Malaysia.
   * Worth tracking: A company that sells insurance in the US only notices poor performance with respect to mobile users in the US.

## Improvement process [#improvement-process]

Here are the key steps involved in improving this process: 
* [Plan your work](#plan-your-work)
* [Decide which KPIs to improve](#decide-which-kpis-to-improve)
* [Improve targeted KPIs](#improve-targeted-kpis)
* [Improve page load performance](#improve-page-load-performance)
* [Improve AJAX response times](#improve-ajax-response-times)
* [Improve the AJAX error rate](#improve-the-ajax-error-rate)
* [Improve JavaScript errors](#improve-javascript-errors)

These steps are explained in more detail below. 

### Plan your work

Whether you have a dedicated initiative to improve performance or classifying as ongoing maintenance, you need to track your progress at the end of every sprint.

### Decide which KPIs to improve

You now know what your user experience looks like across multiple lines of business. Where should you be improving? 
1. Start with business priorities. If you have clear business directives or have access to a senior manager above who does, you should focus on what matters most to your organization. For example, let's say your company has recently launched a new initiative around a line of business but the KPIs associated with the UI are below target. This is where you should focus your time initially.
2. Next, focus on KPIs for each line of business.
3. Finally, filter each line of business by device, region, etc., to see if additional focus is needed for specific regions or devices.

### Improve targeted KPIs

To track your progress, create a new dashboard or add a new page to the existing dashboard and name it `Quality Foundation KPI Improvement`. For more information, see [Improve web uptime](/docs/new-relic-solutions/observability-maturity/customer-experience/cx-improve-web-uptime).

### Improve page load performance

Narrow your focus to specific pages that aren't meeting target KPI values.

For each page load KPI result that's out of bounds in the quality foundation dashboard, remove the `COMPARE WITH` clause and add `FACET pageUrl/targetGroupedUrl LIMIT MAX` to find which pages are the poor performers.

Use `targetGroupedUrl` when there are many results; for example, when the customer ID is part of the URL. Otherwise, use `pageUrl`.

Original dashboard query:

```
FROM PageViewTiming SELECT percentile(largestContentfulPaint, 75) WHERE appName ='WebPortal' AND pageUrl LIKE '%phone%' SINCE 1 week AGO COMPARE WITH 1 week AGO 
```

New query to identify problem pages:

```
FROM PageViewTiming SELECT percentile(largestContentfulPaint, 75) WHERE appName ='WebPortal' AND pageUrl LIKE '%phone%' FACET targetGroupedUrl LIMIT MAX
```

Once you've identified pages to improve, see the guidance in [Improve page load performance](/docs/new-relic-solutions/observability-maturity/customer-experience/cx-improve-page-load).

### Improve AJAX response times

Find the slow requests using these steps:
1. Go to the AJAX duration widget on the dashboard.
2. View query, then open in query builder.
3. Add `facet requestUrl LIMIT MAX` to the end of the query.
4. Run the query.
5. View the results as a table and save to your KPI improvement dashboard as `LOB - AjaxResponseTimes`.
6. Focus improving requests with a `timeToSettle` \> 2.5s.

Use New Relic's recommended best practices to improve response times. See [AJAX troubleshooting tips](/docs/browser/browser-monitoring/browser-pro-features/ajax-page-identify-time-consuming-calls/).

### Improve the AJAX error rate

Find the failing requests: 
1. Go to <InlinePopover type="dashboards" /> > Query builder.
2. Enter and run the following query: 
    ```
    FROM AjaxRequest 
    SELECT percentage(count(*), WHERE httpResponseCode >= 400) 
    WHERE httpResponseCode >= 200 AND <Ajax Request filter>
    SINCE 1 week AGO facet pageUrl, appName
    ```
3. View the results as a table and save to your KPI improvement dashboard as `LOB - Pages with AjaxErrors`.
4. Run the query again for the most problematic pages to find the requests that are failing:
    ```
    FROM AjaxRequest 
    SELECT percentage(count(*), WHERE httpResponseCode >= 400) 
    WHERE httpResponseCode >= 200 AND pageUrl=PROBLEMATIC_PAGE AND appName = YOUR_APP_NAME <Ajax Request filter> 
    SINCE 1 week AGO facet requestUrl
    ```

Use New Relic's recommended best practices to improve response times. See [AJAX troubleshooting tips](/docs/browser/browser-monitoring/browser-pro-features/ajax-page-identify-time-consuming-calls).

### Improve JavaScript errors

Find the most common failures: 
1. Go to <InlinePopover type="dashboards" /> > Query builder
2. Enter and run the following query: 
    ```
    FROM JavaScriptError 
    SELECT count(errorClass) 
    SINCE 1 week AGO WHERE <PageView filter>  
    FACET transactionName, errorClass, errorMessage, domain
    ```
3. View the results as a table and save to your KPI improvement dashboard as `LOB - Javascript Errors`.
4. Use this information to figure out which errors need to be addressed.
   Use New Relic's recommended best practices to resolve errors that need addressing. See [JavaScript errors page: Detect and analyze errors](/docs/browser/new-relic-browser/browser-pro-features/javascript-errors-page-detect-analyze-errors/).
5. Remove third party errors that do not add value.

You may be using a third party JavaScript that is noisy but works as expected. You can take a couple of approaches:
* Remove the domain name from the JavaScript error/Pageview ratio widget and add it as its own widget so you can see unexpected changes. You can alert on this using [Anomaly NRQL](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-baseline-alert-conditions/) alerts.
* Drop the JavaScript error using [drop filters](/docs/logs/log-management/ui-data/drop-data-drop-filter-rules). Only use this option if the volume of errors is impacting your data ingest in a significant way. Be as specific as you can in the drop filter.

## Conclusion [#conclusion]

**Best practices to adopt:**
* Revisit performance metrics (shared in this document as quality foundation KPIs) at the end of each sprint.
* Incorporate performance improvements into developer sprints.
* Openly share metrics with the lines of the business you support as well as other internal stakeholders.
* Define customer experience SLOs.
* Create alerts for [business critical drops](/docs/new-relic-solutions/observability-maturity/aqm-implementation-guide/) in quality foundation KPIs.

**Value realization**

At the end of this process you should:
* Have an understanding of your end user experience in a way that is tangible, actionable, and easy for engineers as well as the business to understand.
* Know how releases impact your end users.
* Know how your customers are impacted by service, infrastructure, or network level events.
* See latency issues caused by backend services, if they exist.
* Have created, or be on the path to create, a common language with business owners so you're working together. This can open new avenues for recognition and sponsorship for new projects.
