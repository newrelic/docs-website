---
title: "Alert quality management: optimize your alerts and reduce alert fatigue" 
tags:
  - Observability maturity
  - Uptime, performance, and reliability
  - Alert quality management
  - Implementation guide
metaDescription: "New Relic observability maturity series: the alert quality management guide helps reduce alert fatique by ensuring that fewer and more valuable incidents are created in New Relic."
redirects:
  - /docs/new-relic-solutions/best-practices-guides/alerts-applied-intelligence/alert-quality-management-tutorial/
  - /docs/new-relic-solutions/observability-maturity/aqm-implementation-guide
  - /docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/aqm-implementation-guide
---

import nrAqmIncidentFlow from 'images/nrAQMIncidentFlow.png'

This guide walks you through improving and optimizing the quality of your alerting. It's part of our [series on observability maturity](/docs/new-relic-solutions/observability-maturity/introduction). 

## Overview [#overview]

Teams suffer from alert fatigue when they experience high alert volumes and alerts that aren't aligned to business impact. As they start to perceive that many alerts are false and unhelpful, they may prioritize easy-to-resolve alerts over others. Also, they may close unresolved incidents so they can stay within their SLA targets.

The result will be slower incident responses, magnified issue scope, and increased severity when true business impacting issues occur.

This **Alert quality management** (AQM) implementation guide focuses on reducing the number of nuisance incidents so that you focus only on those alerts with true business impact. This reduces alert fatigue and ensures that you and your team focus your attention on the right places at the right times.

You're a good candidate for AQM if:
* You have too many alerts.
* You have alerts that stay open for long time periods.
* Your alerts are not relevant.
* Your customers discover your issues before your monitoring tools do.
* You can't see the value of your observability tool(s).

## Desired outcome [#desired-outcome]

An alert strategy based on measuring business impact will result in faster response times and greater proactive awareness of critical events. An improved alert signal to noise ratio reduces confusion and improves rapid identification and problem isolation.

The overall goal of this **Alert quality management** practice is to ensure that fewer, more valuable, incidents are created, resulting in:
* Increased uptime and availability
* Reduced MTTR
* Decreased alert volume
* The ability to easily identify alerts that are not valuable, so you can either make them valuable or remove them.

The process described in this guide generates the key performance indicators and metrics that you will use to measure progress towards these goals. The metrics are measured in real time, published in a dashboard, and are used to drive a continuous improvement process that identifies and reduces nuisance alerts and increases user engagement in incident investigation.

Our **Alert quality management** practice does not encompass anomaly detection or AIOps, which are designed to detect unknown or unexpected modes of failure. The two practices (AQM and ML/AI) work hand in hand: they're not mutually exclusive.

## Key performance indicators [#key-perf-indicators]

You'll use the AQM process to collect and measure the following KPIs:

[**Incident volume**](#kpi-incident-volume), which includes: 
* Incident count
* Accumulated incident time
* Mean time to close (MTTC)
* Percent under 5 minutes

[**User engagement**](#kpi-user-engagement), which includes: 
* Mean time to investigate (MTTI)
* % of incidents investigated

These KPIs will help you to find the noisiest and least valuable alerts so you can improve their value or eliminate them. You will then use the long term metric trends to show real business impact to management and stakeholders. Detailed information on these metrics follows.

### Incident volume [#kpi-incident-volume]

You should treat incidents (with or without alerts) like a queue of tasks. Just like a queue, the number of alerts should spend time near zero. Each incident should be a trigger for action to resolve the condition. If an alert does not result in action, then you should question the value of the alert condition.

If you see a constant rate of incidents or specific incidents that are "always on," then you should question why. Are you in a constant state of business impact, or do you simply have a large volume of noise?  The alert volume KPIs help you to answer those questions and to measure progress towards a healthy state of high quality alerting.

<CollapserGroup>
  <Collapser
    id="kpi-incident-count"
    title="Incident count KPI"
  >
Incident count is the number of incidents generated over a period of time. Typically you should compare the current and previous weeks.

**Goal:** Reduce the number of low value / nuisance incidents.

**Best practices:**
* Ensure condition settings are intended to detect real business impact.
* Ensure condition settings are detecting abnormal behavior.
* Communicate that the incident details "Acknowledge" feature helps measure meaningful and actionable alerts. See [Percentage incident acknowledge KPI](#kpi-user-engagement).
* Report AQM KPIs to all stakeholders.
  </Collapser>

  <Collapser
    id="kpi-incident-duration"
    title="Accumulated incident duration KPI"
  >
Accumulated incident duration is the total sum of minutes that all the incidents accumulated over a period of time. Typically you should compare the current and previous weeks.

**Goal:** Reduce the total accumulated minutes of incidents.

**Best practices:**
* Do not manually close incidents. Manual closure will skew the real duration of incident length.
* Eliminate alerts that do not result in any remediation actions from the recipients.
* Improve percent investigated and mean-time-to-investigate KPIs by communicating their importance in improving detection and response times.
* Report AQM KPIs to all stakeholders.
  </Collapser>

  <Collapser
    id="kpi-mttc"
    title="Mean time to close (MTTC) KPI"
  >
Average duration of incidents within the period of time measured.

**Goal:** Reduce MTTC

**Best practices:**
* Do not manually close incidents. Manual closure will skew the real duration of incident length.
* Improve reliability engineering skills.
* Report AQM KPIs to all stakeholders.
  </Collapser>

  <Collapser
    id="kpi-pct-under-five"
    title="Percent under 5 minutes KPI"
  >
Percentage of incidents where the duration of the incident is under five minutes. This can be an indicator of incident flapping.

**Goal:** Minimize percentage of incidents with short durations.

**Best practices:**
* Ensure that conditions are detecting legitimate deviations from expected behavior.
* Understand service level management.
* Ensure that conditions are detecting legitimate deviations that correlate to business impact or impending business impact.
  </Collapser>
</CollapserGroup>

### User engagement [#kpi-user-engagement]

You should measure the value of an incident by the amount of attention it receives. Engagement in this context is measured by whether or not an incident has been acknowledged.

The amount of engagement an individual alert receives is a direct measurement of its value. More engagement implies a valuable alert. Less (or zero) engagement implies a nuisance alert that should be modified or disabled.

There is a significant difference between measuring the moment of incident awareness vs. acknowledging the moment resolution activity begins. If you are using an integration with New Relic alerts, be sure that the "acknowledge" event that is sent to New Relic is triggered when resolution activity begins, not when the incident is sent to the external incident management tool. For more information regarding the standard incident management process, see "[Incident management process: 5 steps to effective resolution, posted on August 31, 2020 by OnPage Corporation.](https://www.onpage.com/incident-management-process-5-steps-to-effective-resolution) -- in reference to [ITIL4](https://itsm.tools/its-here-itil-4-explained)"

<CollapserGroup>
  <Collapser
    id="kpi-pct-ack"
    title="Percentage acknowledged KPI"
  >
Incidents acknowledged identifies the percentage of incidents that have been engaged with and had their acknowledged property set to true. Typically you should compare the current and previous weeks.

**Goal:** Increase the percentage of incident engagement.

**Best practices:**
* Educate the DevOps team on when it is appropriate to acknowledge an incident alert.
* Gamify alert acknowledgement to drive usage.
* Discourage mass acknowledgement exercises.
  </Collapser>

  <Collapser
    id="kpi-mtti"
    title="Mean time to investigate (MTTI) KPI"
  >
Mean time to investigate identifies the average time it takes for an incident to be triaged. Typically you should compare the current and previous weeks.

**Goal:** Reduce the mean time to investigate.

**Best practices:**
* Work at building incident responder's confidence in alerts.
* Ensure that valuable alerts are acknowledged.
* Incentivize response teams to respond quickly to alerts.
  </Collapser>
</CollapserGroup>

## Prerequisites [#prerequisites]

Before you begin, if you don't have equivalent experience, complete the [New Relic University (NRU) Overview Course](https://learn.newrelic.com/overview-course). 

Also, you should have at least a basic understanding of:
* New Relic alert policy and conditions configuration
* New Relic incident notification channel webhook configuration
* NRQL (our query language)
* Our alerting best practices
* New Relic APM and infrastructure monitoring
* How to baseline data in order to determine anomalies versus normal behavior

## Establish current state [#current-state]

As with any continuous improvement process, the first step of AQM is to establish the current state of your KPIs. To do so, perform the following tasks:
* [Install and configure the incident event webhook](#install-webhook)
* [Install the AQM dashboard](#install-dashboard)
* [Perform initial AQM orientation and enablement](#perform-enablement-one)
* [Accumulate AQM data](#accumulate-data)
* [Perform second enablement session](#perform-enablement-two)

### Install and configure the incident event webhook [#install-webhook]

The webhook will create New Relic events for each incident as it proceeds through its lifecycle (open, acknowledge, close). To ensure that the AQM process generates accurate and valuable findings, this webhook must be added as a notification channel to every alert policy.

The AQM process requires incident, not violation data. This is why you will not be using the default `NrAiIncident` event, which provides violation data only. Instead, you will use this webhook to send the required incident data to New Relic.

To use the webhook, do the following:
1. Identify your primary production account and each of your accounts that you will be analyzing with the AQM process.
2. Install the incident event webhook into each account that will participate in the AQM process and configure the webhook to report `nrAQMIncident` events to your primary production account.
3. Assign the webhook as a notification channel to every alert policy in each account.

<img
  title="Incident alert webhook for alert quality management in New Relic"
  alt="Incident alert webhook for alert quality management in New Relic"
  src={nrAqmIncidentFlow}
/>

<figcaption>
  This example shows a webhook notification channel assigned to each alert policy for a New Relic account with multiple sub-accounts.
</figcaption>

The webhook, AQM dashboard, and detailed installation instructions can be [found in the New Relic OMA resource center on GitHub.](https://github.com/newrelic/oma-resource-center/tree/main/src/content/docs/oma/value-drivers/uptime-performance-and-reliability/use-cases/alert-quality-management)

### Install the AQM dashboard [#install-dashboard]

The AQM dashboard is the primary asset that drives the AQM process. You need to install the AQM dashboard into the primary production account you identified in the [Install and configure incident event webhook](#install-webhook) step you previously performed by doing the following:

1. Download the dashboard definition JSON file from our [observability maturity resource center on GitHub.](https://github.com/newrelic/oma-resource-center/tree/main/src/content/docs/oma/value-drivers/uptime-performance-and-reliability/use-cases/alert-quality-management)
2. Import the definition into your primary production account.

For more details on importing dashboards, see [Introduction to dashboards](/docs/query-your-data/explore-query-data/dashboards/introduction-dashboards/#dashboards-import).

### Perform initial AQM orientation and enablement [#perform-enablement-one]

During this phase, your incident management team(s) and other stakeholders will learn the goals of the AQM process and the scope of their involvement in it.

The most critical portion of this task is educating your team on the importance of acknowledging incident alerts, because that's how the alert's value is determined. In general, instruct them to follow these guidelines:
* If you look at an alert and decide to take any sort of further investigative action, acknowledge the alert.
* If you typically close an alert without doing anything else, do not acknowledge the alert.
* If the incident alert is always on, do not close or acknowledge it. For further details, see [Second enablement session](#perform-enablement-two).

You can use the [first session template presentation](https://docs.google.com/presentation/d/1X1HHEfDx30XpaB65bgn_OCLB-lG72zuRKQ8HnW3GJw4/edit?usp=sharing) to communicate this material to your stakeholders.

### Accumulate AQM data [#accumulate-data]

The overall process requires at least two weeks of data before it can proceed. During this time, you should periodically check the following items:
* Confirm that incident alert event data is accumulating.
* Confirm that the webhook is attached to every alert policy.
* Ensure that incident responders are following the alert acknowledgement guidelines.

### Perform second enablement session [#perform-enablement-two]

During this phase, you'll introduce incident management teams and other stakeholders to the initial AQM data and the ongoing continuous improvement process you'll be following.

The process consists of four activities:
1. Review AQM dashboard and KPI trends: Here you and the stakeholders will look at the AQM KPIs and identify their week over week trends. The team should identify areas where KPIs are not improving and develop strategies to drive improvement.
2. Identify achievements, challenges, and opportunities: Here you and the stakeholders will map the current state of alert quality to business impact, identifying areas where improvement has resulted in better business outcomes and areas where problems are impacting business outcomes.
3. Incident policy review: Using the AQM dashboard, you and the stakeholders will identify the noisiest incident policies. Once identified, those policies should be evaluated as detailed in step 4 below.
4. Alert policy recommendations: In this step, you and the stakeholders will review the noisiest policies using the following criteria:
  * Do the alerts have any business impact?
  * Are the policies properly configured?
    * Are they telling us something about the resource that needs to be fixed?
    * Are the policies necessary? Do they have business impact?
    * Are the thresholds set properly?
5. Technical recommendations: Here, you and the stakeholders will review any technical recommendations, including:
   * Are there application / system problems for engineering to review?
   * Are there poorly constructed policies that need to be fixed?
   * Are there instrumentation gaps?

You can use the [second session template presentation](https://docs.google.com/presentation/d/1FflqMMVejwn6HMjEni8igZh6UR0a3_JbuMcS5xlZiJU/edit?usp=sharing) to keep this part of the AQM process organized.

## Improvement process [#improvement-process]

This is the ongoing phase of the continuous improvement process where you periodically review your accumulated AQM data and make adjustments as needed to alert policies. You should perform this step once a week until your alert volume is acceptable. You can then perform it less frequently.

During this phase you should:
* Report your KPIs each week to upper management to ensure that the stakeholder teams are appropriately prioritizing the work and to show that progress towards the promised business outcomes are being reached.
* Record and retain your weekly KPIs over periods of months to years to establish a baseline and to show the rate of improvement.

You should keep in mind that this is a continuous improvment process. You will continue to collect and evaluate the KPIs over long periods of time to ensure you are meeting your AQM goals.

## Value realization [#value-realization]

Once the AQM process is established, you will see significant reductions in the volume of alerts while reliability and stability remain the same or improve. In addition, you should see that your alerts have a clear and unambiguous business impact. Your AQM KPIs will provide quantifiable proof of these improvements.

Once you are firmly on the path to meeting the goals of AQM, consider moving to other use cases within the **Uptime, performance, and reliability** value stream, such as [Service level management](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/slm-implementation-guide), or reliability engineering. You can also move to other observability maturity value streams, such as [Customer experience](/docs/new-relic-solutions/observability-maturity/customer-experience/quality-foundation-implementation-guide).

## KPI reference [#kpi-reference]

Following are the descriptions of each KPI as well as sample NRQL queries that will extract them from the New Relic platform. These KPIs are also included in the AQM dashboard that can be downloaded from our [observability maturity resource center on GitHub.](https://github.com/newrelic/oma-resource-center/tree/main/src/content/docs/oma/value-drivers/uptime-performance-and-reliability/use-cases/alert-quality-management)

### Incident volume

<CollapserGroup>
  <Collapser
    id="kpi-incident-count-query"
    title="Incident count KPI"
  >
Incident count is the number of incidents generated over a period of time. Typically you should compare the current and previous weeks.

NRQL query: 

```
FROM nrAQMIncident SELECT count(*) AS 'Incident Count' WHERE current_state='open' AND severity='CRITICAL' SINCE 1 WEEK AGO COMPARE WITH 1 WEEK AGO
```
  </Collapser>

  <Collapser
    id="kpi-incident-duration-query"
    title="Accumulated incident duration KPI"
  >
Accumulated incident duration is the total sum of minutes that all the incidents accumulated over a period of time. Typically you should compare the current and previous weeks.

NRQL query: 

```
FROM nrAQMIncident SELECT sum(duration)/(1000*60) AS 'Incident Minutes' WHERE current_state='closed' AND severity='CRITICAL' SINCE 1 WEEK AGO COMPARE WITH 1 WEEK AGO
```
  </Collapser>

  <Collapser
    id="kpi-mttc-query"
    title="Mean time to close (MTTC) KPI"
  >
Average duration of incidents within the period of time measured.

NRQL query: 

```
FROM nrAQMIncident SELECT average(duration/(1000*60)) AS 'Incident MTTC (minutes)' WHERE current_state='closed' AND severity='CRITICAL' SINCE 1 WEEK AGO COMPARE WITH 1 WEEK AGO
```
  </Collapser>

  <Collapser
    id="kpi-pct-under-five-query"
    title="Percent under 5 minutes KPI"
  >
Percentage of incidents where the duration of the incident is under five minutes. This can be an indicator of incident flapping.

NRQL query: 

```
FROM nrAQMIncident SELECT percentage(count(*), WHERE duration <= 5 * 60 * 100) AS '% Under 5min' WHERE current_state='closed' AND severity='CRITICAL' SINCE 1 WEEK AGO COMPARE WITH 1 WEEK AGO
```
  </Collapser>
</CollapserGroup>

### Incident engagement

<CollapserGroup>
  <Collapser
    id="kpi-pct-ack-query"
    title="Percentage acknowledged KPI"
  >
Incidents acknowledged identifies the percentage of incidents that have been engaged with and had their acknowledged property set to true. Typically you should compare the current and previous weeks.

NRQL query: 

```
FROM nrAQMIncident SELECT filter(count(*), WHERE current_state='acknowledged')/filter(count(*), WHERE current_state='open')*100 AS '% Investigated' WHERE severity='CRITICAL' SINCE 1 WEEK AGO COMPARE WITH 1 WEEK AGO
```
  </Collapser>

  <Collapser
    id="kpi-mtti-query"
    title="Mean time to investigate (MTTI) KPI"
  >
Mean time to investigate identifies the average time it takes for an incident to be triaged. Typically you should compare the current and previous weeks.

NRQL query: 

```
FROM nrAQMIncident SELECT average(duration/(1000*60)) AS 'Incident MTTI (minutes)' WHERE current_state='acknowledged' AND severity='CRITICAL' SINCE 1 WEEK AGO COMPARE WITH 1 WEEK AGO
```
  </Collapser>
</CollapserGroup>

## Additional resources

Want to get your hands dirty before you start implementing this in your account?  Check out the [alert quality management lab](https://learn.newrelic.com/hands-on-lab-alert-quality-management)
