---
title: "Service level management: Successfully influence reliability decisions"
tags:
  - Observability maturity
  - Uptime, performance, and reliability
  - Service level management
  - Implementation guide
metaDescription: "New Relic observability maturity series: our service level management guide shows you how to easily measure and communicate the overall health, performance, and quality of your digital products and services to all stakeholders."
redirects:
  - /docs/1-establish-objectives-baselines-define-team-slos
  - /docs/1-adopt-easily-establish-objectives-baselines-define-team-slos
  - /docs/establish-objectives-baselines-define-team-slos
  - /docs/new-relic-solutions/best-practices-guides/alerts-applied-intelligence/service-level-management-tutorial/
  - /docs/new-relic-solutions/observability-maturity/slm-implementation-guide/
  - /docs/using-new-relic/welcome-new-relic/optimize-your-cloud-native-environment/establish-objectives-baselines-define-team-slos
  - /docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/slm-implementation-guide
---

import omSlTheWholeIsGreater from 'images/om-sl_diagram_theWholeIsGreater.png'

import apmOverviewScreen from 'images/apm_screenshot-full_overview-screen.png'

import apmServiceBoundary from 'images/apm_screenshot-crop_service-boundary.png'

import apmLatencyBaseline from 'images/apm_screenshot-full_latency-baseline.png'

import apmSLStartFromApm from 'images/apm_screenshot-full_sl-start-from-apm.png'

import apmAddInputServiceLevel from 'images/apm_screenshot-crop_add-input-service-level.png'

import apmSyntheticsSetup from 'images/apm_screenshot-crop_synthetics-setup.png'

import queriesnrqlTransactionListNrql from 'images/queries-nrql_screenshot-full_transaction-list-nrql.png'

import apmCapabilityFilterClause from 'images/apm_screenshot-crop_capability-filter-clause.png'


This guide walks you through how to improve and optimize the quality of your service level management. It's part of our [series on observability maturity](/docs/new-relic-solutions/observability-maturity/introduction).

## Overview [#overview]

Service level management is the practice of standardizing data into a universal language that can be communicated easily to all stakeholders. IT does not usually speak business and business does not usually speak IT, so an observability language barrier must be resolved first in order to improve reliability.

This need for a universal language to articulate reliability is what has re-popularized service level management. Service level management is known best in the practice of **Uptime, performance and reliability**; however, service level management also applies to the other practices of **Customer experience**, **Innovation and growth**, and **Operational efficiency** ([learn more about these practices](/docs/new-relic-solutions/observability-maturity/introduction)).

This implementation guide will teach you the practice of service level management in the context of the **Uptime, performance and reliability** practice.

## Desired outcomes [#desired-outcomes]

### Business outcome [#business-outcomes]

The required business outcome in the practice of reliability is to reduce the number of business-impacting incidents, their duration, and the number of people involved in those incidents.

1. Reduce the number of business-disrupting incidents
2. Reduce mean time to resolution (MTTR)
3. Reduce average people engaged (FTEs) per severe incident

### Operational outcome [#operational-outcomes]

The required operational outcome of service level management within a reliability practice is to communicate digital product health successfully. Operational success is measured by what percentage of critical product applications are covered by standard service levels and percentage of adoption by primary stakeholders. This is achieved by staying focused on what is important to the stakeholders, standardization, ensuring simplicity, a sprinkle of consulting, and proving the effectiveness of service level management.

## Terminology [#terminology]

Here are some terms it may help you to understand before using this guide:

<CollapserGroup>
  <Collapser
    id="slm-terminology-service-level"
    title="Service level"
  >
    A [service level](https://en.wikipedia.org/wiki/Service_level) measures the performance of a system, not exclusive to technology. Service levels are measured by indicators and compared against objectives set for expected performance and outcomes.
  </Collapser>

  <Collapser
    id="slm-terminology-service-level-management"
    title="Service level management"
  >
    Service level management is the practice of managing and reporting on service level compliance and attainment.
  </Collapser>

  <Collapser
    id="slm-terminology-sli"
    title="Service level indicator"
  >
A service level indicator (SLI) is a measure of a service level. An SLI identifies the thing or things to be measured. An SLI can be one datapoint or a composite of datapoints. In technology an SLI is most commonly represented as a percentage (%) of successful transactions or events. The mathematical representation is the total good events over the total valid events calculated over a specified period of time such as 1-hour, 1-day, 1-week.

**Examples SLIs:**
* Percentage of login API transactions that complete within 500 milliseconds.
* Percentage of login API transactions that complete without an internal server error.
* Percentage of login API transactions that complete within 500 milliseconds and without an internal server error.
* Percentage of synthetic check Pings against the Login API that are successful.
* Percentage of browser logins that transition to the landing page within 3 seconds.
* Percentage of metrics ingested that are available for query within 3 seconds.
* Some or all of the above aggregated into a single SLI.
  </Collapser>

  <Collapser
    id="slm-terminology-slo"
    title="Service level objective"
  >
A service level objective (SLO) is a statement describing the expected service level of a system, job, and/or capability. An SLO usually describes the required percentage of success of one or more SLIs within a given period of time.

**Example SLOs:**
* The login API will respond within 500 milliseconds and error free 99.99% of the time each week.
* The mobile login capability will transition to the landing page within 3 seconds 99.99% of the time each week.
* The data ingest of metrics will be available for query within 3 seconds of consumption from the API 99.99% of the time each day.
  </Collapser>

  <Collapser
    id="slm-terminology-service-boundary"
    title="Service boundary"
  >
A service boundary is the point in a system where the consumer client connects to make requests. This is most commonly referred to as the external API or endpoint.

Service boundaries can also be internal between two distinct systems, where the collection of applications serves the requests of another collection of applications. For example, an identity management system can be a dependency for both login requests from the client and permissions authorization for different features.

Types of boundaries:
* **Internal service boundary**: These are usually middleware dependencies behind the customer-facing GraphQL API. Each dependency API that responds to the GraphQL tier would be considered its own internal service boundary. Internal service boundaries do not represent overall output performance health in a 1:1 ration. For example, a customer request to the GraphQL API could hit seven internal dependencies. One slow dependency will not be the total sum response time of the original request.
* **Customer-facing service boundary**: The customer-facing GraphQL API. This boundary will give you the total sum response time and success metrics required for measuring performance health, without the need to measure each dependency.
  </Collapser>

  <Collapser
    id="slm-terminology-error-budget"
    title="Error budget"
  >
An error budget is an advanced method to measure and communicate your service level objective compliance. New Relic is currently exploring different methods of error budgeting as described in the references below. Please refer to our [service level management product documentation](/docs/service-level-management/intro-slm) for the most recent information.

An error budget is how much you are allowed to fail before you are considered "out of compliance." There are multiple methods for defining and measuring error budgets. The word "error" in "error budget" does not mean HTTP request errors or application errors as in a "500 Internal Server Error" but simply "any bad request or event" as defined in your service level objectives.

**Note**: It's recommended to first build competency in defining, calculating, and communicating service level compliance before implementing error budgets. See [operational outcomes](#operational-outcomes) above. The really important part is that you will establish a target for you SLIs (objective) and you will measure your success of how well your SLI meets that target. For example, are you within 500 milliseconds and error-free on 99.99% each 24-hour period of your login requests? If your 24-hour SLI compliance result is 99.99%, then you are meeting your objective.

**References**

Google defines error budget as:

"In a nutshell, an error budget is the amount of error that your service can accumulate over a certain period of time before your users start being unhappy. Once you define an objective for each (SLI) for your service-level objective (SLO) — the error budget is the remainder, up to 100." -- [Google Cloud - How maintenance windows affect your error budget—SRE tips](https://cloud.google.com/blog/products/management-tools/sre-error-budgets-and-maintenance-windows)

Atlassian defines error budget as:

"An error budget is the maximum amount of time that a technical system can fail without contractual consequences. For example, if your service level agreement (SLA) specifies that systems will function 99.99% of the time before the business has to compensate customers for the outage, that means your error budget (or the time your systems can go down without consequences) is 52 minutes and 35 seconds per year." -- [Atlassian - What is an error budget and why does it matter?](https://www.atlassian.com/incident-management/kpis/error-budget)
  </Collapser>
</CollapserGroup>

## Key performance indicators [#key-performance-indicators]

### What is service health?

Health is the sum of the performance of all your services measured by the final response, connectivity, and renderability at the client. The reality is that your service is a "digital product" and that product is only as good as how well it's received by your users. Your technology, as complex as it can be, is a closed system mostly unseen by the internet except for your external APIs.

The most critical health KPIs are gathered by answering the following questions:

1. How fast and successfully can we deliver responses to the user?
2. Can users connect to our service?
3. Can our client apps render content fast and successfully?
4. Can we process critical data fast and successfully?

### What is __not__ service health?

Traditionally IT was able to correlate health from hardware data such as CPU, RAM, Disk, Network, etc. However today's infrastructure technology compensates for much of that, especially load balancing and orchestrated infrastructure like Kubernetes. Today these "metrics" are considered diagnostic data and are just a portion of the failure points to look at when application performance issues are detected.

Service health is not exclusively infrastructure data, nor is it exclusively end-user client performance data. Interestingly, many look first to real user monitoring (RUM) or end-to-end transaction data (distributed tracing) to identify health but are left with many more questions.

### Learn how to react (reactive) before you pro-act (proactive)

It's common to debate reactive and proactive practices when establishing initial health data. For example, if I know the hardware performance of our infrastructure, I can predict failure. There was a time the previous statement was true when monolith architecture was dominant. This is not entirely true today because distributed systems do not have a linear relationship (1:1) between hardware performance and the output performance of applications that sit on that hardware.

In order to truly be proactive you must first establish real input and output datapoints. You must first know what to measure before you can react. You must first learn how to react before you can pro-act (be proactive). Keep it simple and build your skills incrementally. This guide will show you the fastest path to proactive behaviors.

The reality is that starting at your application layer, closest to the customer but before the client device, is the fastest path to observing health data from the customer's perspective.

### Primary health datapoints and their KPIs

You'll learn to establish and measure service level objectives for output performance and input performance in this guide as defined below.

Client performance is covered in our [implementation guide on Customer experience: Quality foundation](/docs/new-relic-solutions/observability-maturity/customer-experience/quality-foundation-implementation-guide).

Data quality is not covered in this guide because each use case varies greatly depending on the data inputs, outputs, and desired results.

It's strongly recommended that you first accomplish the **Output performance** and **Input performance** steps before proceeding to the **Client performance** step. Output and input SLOs are very easy to create, and there is a much greater return on your investment to have output and input health data first. In addition to being easy to establish, input and output datapoints will provide you with a remediation path much sooner in your reliability journey.

<CollapserGroup>
  <Collapser
    id="slm-health-output"
    title="Output performance"
  >
    <img
      title="The whole is greater than the sum of its parts. - Aristotle"
      alt="The whole is greater than the sum of its parts. - Aristotle"
      src={omSlTheWholeIsGreater}
    />

Output performance refers specifically to how well your back-end technology can respond to requests.

One of the core capabilities of New Relic is the ability to consume and query vast quantities of data very quickly. You can imagine this requires extraordinary architecture with mind-boggling levels of parallel asynchronous distributed transactions. New Relic measures query output health the same as you'll measure most client-facing capabilities: by the standard of how fast and error-free HTTP query requests receive responses. For every NRQL query there's one request to the back-end and one response to the client. For the great majority of applications and digital products this is the simple fact.

Output Performance is measured by:

* Response times (a.k.a., latency)
* Error-free percentage (a.k.a quality, or availability)
  </Collapser>

  <Collapser
    id="slm-health-input"
    title="Input performance"
  >
Input performance is measured simply by the ability to connect to your application. This is traditionally known as "uptime." We'll use the same example above: NRQL queries.

Output performance measures responses after the fact a connection has been made. What if the connection between the client and the backend fails? That connection failure will not be reflected in output performance because a connection is required before we can measure the output.

New Relic uses [synthetic monitoring](/docs/synthetics/synthetic-monitoring/getting-started/get-started-synthetic-monitoring/) to simulate connections from all over the world. New Relic has synthetic checks established against the NRQL query API. If there is any interruption between the source point and the output, these checks will fail and be reflected in our service level objective scores, as well as triggering very critical alerts of course.

Input performance is measured by:
* Percentage of successful test against your endpoints from an external source.
  </Collapser>

  <Collapser
    id="slm-health-client"
    title="Client performance"
  >
Client performance is the ability of your client, if you have one, to render content quickly and error-free. This is not to be confused with general customer experience where we measure and improve conversion rates, reoccurring users, retention, abandonment, and overall customer satisfaction of the product.

For more information on client performance, please see our [implementation guide on Customer experience: Quality foundation](/docs/new-relic-solutions/observability-maturity/customer-experience/quality-foundation-implementation-guide).
  </Collapser>

  <Collapser
    id="slm-health-data"
    title="Data quality"
  >
Data quality measures the ability of a process to provide the expected data in a timely manner. New Relic must first ingest your data before you can query it. We expect that process to be completed in near-real-time. New Relic measures the time your data is ingested and samples to determine when that data is available for query.

Data quality is measured by knowing the input data, knowing the output data, and measuring if the desired result is achieved. This may include auditing the results to determine accuracy.
  </Collapser>
</CollapserGroup>

## Prerequisites [#prerequisites]

### Technical enablements

* [APM instrumentation](/docs/apm/new-relic-apm/getting-started/introduction-apm) of your primary application
* [Synthetic test](/docs/synthetics/synthetic-monitoring/getting-started/get-started-synthetic-monitoring) of your application

### Learning enablements

* **Highly recommended**: our free [interactive online course on this service levels guide](https://learn.newrelic.com/self-paced-service-level-management-in-new-relic/1233749/scorm/3p40vqhz6d4f8)
* Achieve basics skills with our [dashboards and NRQL](https://learn.newrelic.com/webinar-getting-started-with-dashboards-nrql)
* Review the [service level management UI](/docs/service-level-management/intro-slm)

## Establishing an output performance SLI [#establishing-output-sli]

Here's an overview of the steps for establishing an output performance SLI:
1. Identify your service
2. Identify your service boundary
3. Establish your baseline
4. Create your service level

We'll now describe these steps in more detail.

### 1. Identify your service

The following is assumed:
* Your primary applications are instrumented with [New Relic APM agents](/docs/apm/new-relic-apm/getting-started/introduction-apm).
* Your application names follow a familiar naming convention, as outlined in our [Service characterization guide](/docs/new-relic-solutions/observability-maturity/operational-efficiency/sc-implementation-guide/#improvement-process).
* You're familiar with how to [find your application in the New Relic Explorer](/docs/new-relic-one/core-concepts/new-relic-explorer-view-performance-across-apps-services-hosts/#find).

In the New Relic Explorer, find your application (entity type of "Services - APM") and select it. You should see the overview screen below. **Don't click "Service levels" yet.**

<img
  alt="APM Overview screenshot"
  src={apmOverviewScreen}
/>

### 2. Identifying your service boundary [#identifying-service-boundary]

Be sure to read the definition of [service boundaries](#slm-terminology-service-boundary) in the [terminology section](#terminology) above.

The goal here is to ensure you're measuring the output of your service, first. While dependencies of that application each play a part in response times and success rates, the final and total response time and success is easily measured at the point where the request is received and responded to.

Use [service maps](/docs/new-relic-one/ui-data/service-maps/how-use-service-maps/) and [automaps](/docs/new-relic-one/ui-data/automaps) to help determine the application you're looking at is a dependency of your application or the application that runs the endpoint API.

**Example**

In the screenshot below you are responsible for all applications that support order processing. You selected #2 (Order-Composer) to start, clicked **Service maps**, and discovered that Order-Composer is really a dependency; therefore, you will need to select #1 (Order-Processing) in order to establish a true health service level.

Your team may only be accountable for the dependency, Order-Composer. If that's the case, then your own service level on Order-Composer is perfectly acceptable for your own self-monitoring of performance. Be sure to tag your own non-customer facing service levels as `customer-facing:false` to allow for better filtering in health reports. Also, consider collaborating with the customer-facing endpoint (#1 Order-Processing) in your observability journey in order to establish true output performance, an input connectivity service level, and client service levels.

<img
  alt="Service map example"
  src={apmServiceBoundary}
/>

### 3. Establishing your baseline

Establishing a baseline is a critical step to accelerating adoption and implementation of service levels. It's more challenging to determine what the design specifications are or **should have been** for services. Establishing a baseline allows you to measure the current performance of a service and then, through the service level reports, you will know if you are hitting that baseline or degrading.

You can create a baseline for virtually any dataset; however, there are different formulas and recommendations for different use cases. For example, you should use the average for some datasets, percentiles for others, and max for others.

When starting service levels you should start with [output performance](#key-performance-indicators) of your applications. For this we use response times (latency) and percentage of non-errors (success).

**How much history should be considered?** Not much, in fact. You're establishing a reliability health metric. Seasonality and peak usage is not a handicap for good performance. Also, the more history you include in your measurement, the more likely you are including different codebases from releases. Previous deployments, no matter how small, could skew your results.

The recommended history is one to two weeks of performance data to establish a fair baseline.

#### Example baselines

Here's an example NRQL query that represents the recommended target for a 7-day service level objective for latency:

```
FROM Transaction SELECT percentile(duration, 95) AS 'Latency Baseline SLI' WHERE appName='Order-Processing' SINCE 1 WEEK AGO
```

<img
  alt="Latency Baseline"
  src={apmLatencyBaseline}
/>

For a success (error-free) baseline, try the following query. Be sure to substitute `Order-Processing` for your own application name.

```
FROM Transaction SELECT percentage(count(*), WHERE error is false) AS 'Success Baseline SLI' SINCE 1 WEEK AGO WHERE appName='Order-Processing'
```

### 4. Create your service level

The New Relic platform will automatically calculate recommended APM and browser baselines for you.

**Note:** If you don't see the **Add a service level** button, check with your New Relic administrator about your permissions.

The "[Identifying your service](#1-identifying-your-service)" section above shows you how to find your application APM data. You'll see #2 in the screenshot in that same section, called "Service levels." Find your application APM data and click **Service levels**. You should see the view below.

<img
  alt="Service levels start from APM"
  src={apmSLStartFromApm}
/>

Click **Add baseline service level objectives** and almost instantly you will have both your Latency SLI and Success SLI and their respective objectives created for you.

You can view and change all the settings by clicking the three dot icon in the upper-right corner of each SLO scorecard.

**Note:** It will take approximately 10 minutes for data to populate the SLO scores. This is because we use the [events-to-metrics service](/docs/data-apis/convert-to-metrics/create-metrics-other-data-types/) for data longevity and query performance. It takes a moment for the conversion to take place and begin to populate the data.

## Establishing an input performance SLI [#establishing-input-sli]

An overview of the process of establishing an input performance SLI:
1. Create your synthetic check.
2. Create your service level indicator.

Below are more details on these steps.

### 1. Create your synthetic check

The most common input performance service level is often referred to as "connectivity" or "uptime." This is a simple check against a health API endpoint or simply loading a URL. Both of these can be done easily by using our synthetic monitoring service. Please refer to [Add simple browser monitor](/docs/synthetics/synthetic-monitoring/using-monitors/add-edit-monitors#simple)  and [Add scripted API test](/docs/synthetics/synthetic-monitoring/using-monitors/add-edit-monitors#complex) to learn how to begin reporting data.

### 2. Create your service level indicator

After completing that first step, you should now have data.

Now you will use the service level management service to create an input indicator and objective.

Use the New Relic Explorer menu to select **Service levels**, and then click **+ Add a service level indicator**.

**Note:** If you don't see the **Add a service level** button, check with your New Relic administrator about your permissions.

Next filter your entity types to `Synthetic monitors`. See screenshot below.

<img
  alt="Filter entity types in service levels"
  src={apmAddInputServiceLevel}
/>

Next steps:

1. Find your synthetic monitor in that list and click it. This will enable the **Continue** button in the left panel. Click **Continue**.
2. You'll see a button for the recommended settings for a **Success** service level (shown below). Click it.
3. Make appropriate changes to the tags, title and description as needed.
4. Click **Save**.

<img
  alt="Synthetics input service level guided flow"
  src={apmSyntheticsSetup}
/>

## Establishing a capability SLI [#capability-SLI]

This is where you will really accelerate adoption of service levels!

You do not need to have intimate knowledge of an application or service in order to complete this task. You simply need to know where the consumer-facing API (service boundary) is, and follow the steps below.

This is a major step in your observability maturity journey. Having service levels on critical business capabilities, like login or authorize payment, will rapidly close the language barrier between IT and business. Service level scores on capabilities also provide you with a more precise remediation path when their service levels begin to degrade. For example, if the login service level begins to degrade, you'll know to look at identity mangement dependencies and workflows starting at the consumer-facing API.

**Note:** in this task you're building on the skills you learned in the "[Establishing an output SLI](#establishing-output-sli)" section.

Here's an overview of this process:
1. Assess application capabilities.
2. Baseline a capability.
3. Create your capability service level.

These steps will be explained in more detail below.

Identify the service boundary application as described in the [Establishing an output SLI](#establishing-output-sli) section above.

Run the following NRQL query to identify baselines on the most frequently used transactions. Be sure to replace `Order-Processing` with the application name you identified.

```
FROM Transaction SELECT count(*), percentile(duration, 95) WHERE appName='Order-Processing' FACET name SINCE 1 WEEK AGO
```

You should see something similar to the screenshot below.

<img
  alt="transaction list query"
  src={queriesnrqlTransactionListNrql}
/>

You'll see the first transaction states it has something to do with "purchase." You can now create a "purchase" capability service level.

**Note:** Even if you're not sure that this transaction represents the purchase capability, this exercise makes a great example to show the application team and your leadership the value of capability service levels. Remember, your goal here is to start a conversation with the stakeholders by showing the art-of-the-possible.

Add `WHERE name='Controller/Sinatra//purchase'` to the end of your query, replacing `Controller/Sinatra//purchase` with your transaction name. Run the query to make sure it works. You should now see only the one transaction in your result. Copy this query and the `DURATION (95%)` result into a notepad. You'll need both in a moment.

Create a new service level in the platform. Starting a new service level is described in [Establishing an input performance SLI](#establishing-input-sli).

In this case you want to find your application (APM `Entity` type) in the list so we can retain the metadata (tags) through the entity GUID. Instead of "Synthetic monitor" as in section above, select "APM" in the entity filter dropdown.

Select the "Latency" guided workflow so the good and valid queries are auto-populated for you.

Use your notepad to copy just the `name='your/transaction/name/here'`.

Add this condition to both queries in service levels, **preceded with an `AND `**, as underlined in the screenshot below.

<img
  alt="capability query clause"
  src={apmCapabilityFilterClause}
/>


Simply adjust the `duration < 1.78` portion of the second query to match the `DURATION (95%)` result in your original baseline copied to your notepad.

Proceed to name this service level, update the description, and save the service level.

It's recommended to set up a few of these capability service levels and present to the application team and your leadership for feedback.

## Improvement process [#improvement-process]

### Alert quality management

[Alert quality management](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/aqm-implementation-guide) is another observability maturity practice that marries really well with service level management. The value of both alerting quality data side-by-side with service level data is that you can see if alert policies are aligned with real impact or are just creating noise. You'll be able to validate good alerts, missing alerts, and just noisy alerts.

You can do this by creating a custom dashboard with an SLI compliance query side-by-side with an alerting quality query.

If you haven't already, check out our [Alert quality management guide](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/aqm-implementation-guide).

### Adoption and continuous improvement

Improvement of service levels and reliability requires adoption of the practice by all the stakeholders of the service. This includes, but is not limited to, engineering management, product management, and executive management. The primary goal is to quickly demonstrate the power and value of service levels to stakeholders in order to start a meaningful discussion on what really matters to those stakeholders. The steps in this guide will get you those meaningful discussions very quickly.

A proven method, with a high rate of adoption, is to first establish output performance and input performance service levels for one digital product and its critical capabilities. This usually involves one overall output and input service level for each endpoint application (usually one or two), and then approximately 4-7 output performance service levels for assumed critical capabilities measured at the endpoint transaction.

This method includes **not** surveying each stakeholder for what should and shouldn't be measured. Surveys usually result in long wait times, lots of questions, frustration, lack of demonstrated value, and suboptimal answers. Remember, start with baselines and key transactions as "capabilities."

Freely make assumptions of what these endpoints are, and what endpoint transactions make up what capabilities, as demonstrated above. Accuracy is not the key at first. What is key to a successful kick-off is demonstrating the ability to easily measure and communicate health. That initial demonstration will show the value in investing more time to refine what is and what isn't measured in primary service levels.

Don't wait. The sooner you provide that demonstration and the more complete that demonstration is, the sooner you will achieve broader adoption and begin the reliability improvement process in collaboration with all the stakeholders!

### Automation

Once you have established what works (and what doesn't) for your stakeholders, you can then begin to design SLM at scale with automation. You can start learning about automating service level management by studying the [New Relic Terraform library](https://registry.terraform.io/providers/newrelic/newrelic/latest/docs/resources/service_level).

## Business value

As stated in the "[Desired outcomes](#desired-outcomes)" section above; the preverbial bottom-line result is to reduce the cost of business impacting incidents.

However, service levels can also help quantify both estimated revenue loss during violations as well as estimated revenue at risk for subscription-based businesses.

**Revenue loss** can be easily estimated for revenue generated by transaction, such as online retail, as well as penalties paid if your business has service level agreement contracts with penalties built-in.

**Revenue at risk** is for subscription-based (SaaS) business models where each customer has a monthly or annual subscription value. You can easily estimate the number of customers impacted and their subscription revenue by period to calculate "revenue at risk." Note: subscription businesses can also have penalties within a service level agreement contract, which should be included as stated below.

### Quantifying the direct cost of service level agreement violations

Determine the cost of previous violations. For example, online retail businesses know the estimated revenue loss per minute during service loss (downtime). Legal can tell you the penalty costs of service level agreement (SLA) contract breaches. Both losses can be easily *estimated* in real-time using New Relic data on service level breaches.

### Quantifying the revenue opportunity costs of service level violations

Determine the three variables below.

* (A) number of violations that trigger penalties or revenue loss
* (B) average duration of violations
* (C) average penalty or revenue loss per minute/hour

Multiply those three variables (A * B * C) to calculate total revenue opportunity to recover.

### Quantifying revenue leakage

Determine the two variables below.

* (A) Total revenue (per period)
* (B) Total penalties payments made to customers (per same period as A)

Divide B / A to calculate revenue leakage % rate.

## Tracking your Service Level Objectives

Service levels are a practice, just like testing, alerting, game days etc. You could think of them as a scientific instrument that helps you to measure the "health" of your systems. But like all tools, service levels require calibration.

Include the service level practice in your team's process. The below recommendations were distilled from the experience using service levels within teams at New Relic, and you should adjust them for your specific team requirements:

- Do a periodic review of the service levels, and pay close attention to:
    + Do the SLIs reflect incidents & pages?
    + What's your error budget for a week?
        * If it's too low, investigate what caused a drop, use "Analyze" feature to find bad events that caused it
        * If it's 100%, make sure your indicator is correct and SLO is aggressive enough. Being at 100% indicates the SLO is too safe
        * What is the trend that you observe in various time periods (1d/7d/28d)
- Keep an eye on SLIs during game days. SLIs should reflect the impact, just like your alerts do
- When you have a drop of error budget on production, evaluate why it didn't happen on staging

## Next steps [#next-steps]

The next step in our observability maturity practice is to add in customer experience service levels measured at the client browser or mobile device. Again, it's important you first prove value as described in the improvement process above. Remember: observability is a journey, and maturity takes time, practice, and patience.

To proceed on your journey, see:
* Our guide on [Customer experience: Quality foundation](/docs/new-relic-solutions/observability-maturity/customer-experience/quality-foundation-implementation-guide).
* If you haven't already, see our guide on [Alert quality management](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/aqm-implementation-guide).
