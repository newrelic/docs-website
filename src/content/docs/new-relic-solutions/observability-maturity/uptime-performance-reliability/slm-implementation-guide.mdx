---
title: Service level management use case implementation guide
tags:
  - Observability maturity
  - Uptime, performance, and reliability
  - Service level management
  - Implementation guide
metaDescription: Service Level Management's overall goal is to easily measure and communicate the overall health, performance, and quality of your digital products and services to all stakeholders.
redirects:
  - /docs/1-establish-objectives-baselines-define-team-slos
  - /docs/1-adopt-easily-establish-objectives-baselines-define-team-slos
  - /docs/establish-objectives-baselines-define-team-slos
  - /docs/new-relic-solutions/best-practices-guides/alerts-applied-intelligence/service-level-management-tutorial/
  - /docs/new-relic-solutions/observability-maturity/slm-implementation-guide/
  - /docs/using-new-relic/welcome-new-relic/optimize-your-cloud-native-environment/establish-objectives-baselines-define-team-slos/
---

## Overview [#overview]
Service Level Management is the practice of standardizing data into a universal language that can be communicated easily to all stakeholders. IT does not usually speak business and business does not usually speak IT, so an observability language barrier must be resolved first in order to improve reliability.

This need for a universal language to articulate reliability is what has re-popularized Service Level Management. Service level management is known best in the practice of Uptime, Performance and Reliability; however, service level management also applies to Customer Experience, Innovation and Growth, and Operational Efficiency.

This implementation guide will teach you the practice of service level management in the context of Uptime, Performance and Reliability.

## Desired outcomes [#desired-outcomes]

### Business outcome [#business-outcomes]

The required business outcome in the practice of reliability is to reduce the number of business-impacting incidents, their duration, and the number of people involved in those incidents.

1. Reduce the number of business-disrupting incidents
2. Reduce Mean-Time-to-Resolve (MTTR)
3. Reduce average people engaged (FTEs) per severe incident

### Operational outcome [#operational-outcomes]
The required operational outcome of Service Level Management within a reliability practice is to communicate digital product health successfully. Operational success is measured by what percentage of critical product applications are covered by standard service levels and percentage of adoption by primary stakeholders. This is achieved by staying focused on what is important to the stakeholders, standardization, ensuring simplicity, a sprinkle of consulting, and proving the effectiveness of service level management.


## Terminology [#terminology]

<CollapserGroup>

  <Collapser
    id="slm-terminology-service-level"
    title="Service Level"
  >

  A [service level](https://en.wikipedia.org/wiki/Service_level) measures the performance of a system, not exclusive to technology. Service levels are measured by indicators and compared against objectives set for expected performance and outcomes.

  </Collapser>

  <Collapser
    id="slm-terminology-service-level-management"
    title="Service Level Management"
  >

  Service Level Management is the practice of managing and reporting on service level compliance and attainment.

  </Collapser>

  <Collapser
    id="slm-terminology-sli"
    title="Service Level Indicator"
  >

  A Service Level Indicator (SLI) is a measure of a Service Level. An SLI identifies the thing or things to be measured. An SLI can be one datapoint or a composite of datapoints. In technology an SLI is most commonly represented as a percentage (%) of successful transactions or events. The mathematical representation is the total good events over the total valid events calculated over a specified period of time such as 1-hour, 1-day, 1-week.

    **Examples SLIs:**

    * Percentage of login API transactions that complete within 500 milliseconds.
    * Percentage of login API transactions that complete without an internal server error.
    * Percentage of login API transactions that complete within 500 milliseconds and without an internal server error.
    * Percentage of synthetic check Pings against the Login API that are successful.
    * Percentage of browser logins that transition to the landing page within 3 seconds.
    * Percentage of metrics ingested that are available for query within 3 seconds.
    * Some or all of the above aggregated into a single SLI.

  </Collapser>

  <Collapser
    id="slm-terminology-slo"
    title="Service Level Objective"
  >
    A Service Level Objective (SLO) is a statement describing the expected service level of a system, job, and/or capability. An SLO usually describes the required percentage of success of one or more SLIs within a given period of time.

    **Example SLOs:**

    * The Login API will respond within 500 milliseconds and error free 99.99% of the time each week.
    * The Mobile login capability will transition to the landing page within 3 seconds 99.99% of the time each week.
    * The data ingest of metrics will be available for query within 3 seconds of consumption from the API 99.99% of the time each day.

  </Collapser>

  <Collapser
    id="slm-terminology-service-boundary"
    title="Service Boundary"
  >
    A service boundary is the point in a system where the consumer client connects to make requests. This is most commonly referred to as the external API or endpoint.

    Service boundaries can also be internal between two distinct systems, where the collection of applications serves the requests of another collection of applications. For example, an identity management system can be a dependency for both login requests from the client and permissions authorization for different features.

    * **Internal service boundary**: These are usually middleware dependencies behind the customer-facing GraphQL API. Each dependency API that responds to the GraphQL tier would be considered its own internal service boundary. Internal service boundaries do not represent overall output performance health in a 1:1 ration. For example, a customer request to the GraphQL API could hit seven internal dependencies. One slow dependency will not be the total sum response time of the original request.

    * **Customer-facing service boundary**: The customer-facing GraphQL API. This boundary will give you the total sum response time and success metrics required for measuring performance health, without the need to measure each dependency.

  </Collapser>

  <Collapser
    id="slm-terminology-error-budget"
    title="Error Budget"
  >

    An error budget is an advanced method to measure and communicate your service level objective compliance. New Relic is currently exploring different methods of error budgeting as described in the references below. Please refer to our [service level management product documentation](/docs/service-level-management/intro-slm) for the most recent information.

    An error budget is how much you are allowed to fail before you are considered "out of compliance." There are multiple methods for defining and measuring error budgets. The word "error" in "error budget" does not mean HTTP request errors or application errors as in a "500 Internal Server Error" but simply "any bad request or event" as defined in your service level objectives.

    **NOTE**: It is recommended to first build competency in defining, calculating, and communicating service level compliance before implementing error budgets. See [operational outcomes](#operational-outcomes) above. The really important part is that you will establish a target for you SLIs (objective) and you will measure your success of how well your SLI meets that target. For example, are you within 500 milliseconds and error-free on 99.99% each 24-hour period of your Login requests? If your 24-hour SLI compliance result is 99.99%, then you are meeting your objective.

    **References**

    Google defines error budget as:

    "In a nutshell, an error budget is the amount of error that your service can accumulate over a certain period of time before your users start being unhappy. Once you define an objective for each (SLI) for your service-level objective (SLO) — the error budget is the remainder, up to 100." -- [Google Cloud - How maintenance windows affect your error budget—SRE tips](https://cloud.google.com/blog/products/management-tools/sre-error-budgets-and-maintenance-windows)

    Atlassian defines error budget as:

    "An error budget is the maximum amount of time that a technical system can fail without contractual consequences. For example, if your Service Level Agreement (SLA) specifies that systems will function 99.99% of the time before the business has to compensate customers for the outage, that means your error budget (or the time your systems can go down without consequences) is 52 minutes and 35 seconds per year." -- [Atlassian - What is an error budget and why does it matter?](https://www.atlassian.com/incident-management/kpis/error-budget)
  </Collapser>

</CollapserGroup>

## Key performance indicators [#key-performance-indicators]

### What is service health?

The reality is that your service is a "digital product" and that product is only as good as how well it is received by your users. Your technology, as complex as it can be, is a closed system mostly unseen by the internet save your external APIs. The totality of your digital product health is the ability of that product to respond to requests, render content, and process data.

The most critical questions to answering digital product (service) health are:

  1. How fast and successfully can we deliver responses?
  2. Can clients connect to our service?
  2. Can our clients render content fast and successfully?
  3. Can we process data fast and successfully?

### What is not service health?

Service health is not the same as diagnostic data. Service health is not exclusively infrastructure data, nor is it exclusively end-user client performance data. Interestingly, many look first to real user monitoring (RUM) or end-to-end transaction data (distributed tracing) to identify health but are left with many more questions.


### Learn how to react (reactive) before you pro-act (proactive)

It is common to debate reactive and proactive practices when establishing initial health data. For example, if I know hardware performance of our infrastructure I can predict failure. There was a time the previous statement was true when monolith architecture was dominant. This is not entirely true today because distributed systems do not have a linear relationship (1:1) between hardware performance and the output performance of applications that sit on that hardware.

In order to truly be proactive you must first establish real input and output datapoints. You must first know what to measure before you can react. You must first learn how to react before you can pro-act (be proactive). Keep it simple and build your skills incrementally. This guide will show you the fastest path to proactive behaviors.

The reality is that starting at your application layer, closest to the customer but before the client device, is the fastest path to observing health data from the customer's perspective.

### Primary health datapoints and their KPIs

You will learn to establish and measure service level objectives for output performance and input performance in this guide as defined below.

Client performance is covered in our [Customer Experience - Quality Foundation implementation guide](/docs/new-relic-solutions/observability-maturity/customer-experience/quality-foundation-implementation-guide).

Data quality is not covered in this guide because each use case varies greatly depending on the data inputs, outputs, and desired results.

It is strongly recommended that you first accomplish Output Performance and Input Performance before proceeding to Client Performance. Output and input SLOs are very easy to create, and there is a much greater return on your investment to have Output and Input health data first. In addition to be easy to establish, input and output datapoints will provide you with a remediation path much sooner in your reliability journey.


<CollapserGroup>
  <Collapser
    id="slm-health-output"
    title="Output Performance"
  >

    ![The whole is greater than the sum of its parts. - Aristotle](./images/theWholeIsGreater.png "The whole is greater than the sum of its parts. - Aristotle")

    Output performance refers specifically to how well your back-end technology can respond to requests.

    One of the core capabilities of New Relic is the ability to consume and query vast quantities of data very quickly. You can imagine this requires extraordinary architecture with mind-boggling levels of parallel asynchronous distributed transactions. New Relic measures query output health the same as you will measure most client-facing capabilities -- by the standard of how fast and error-free HTTP query requests receive responses. For every NRQL query there is one request to the back-end and one response to the client. For the great majority of applications and digital products this is the simple fact.

    Output Performance is measured by:

    1. Response Times (a.k.a. Latency)
    2. Error-free percentage (a.k.a Quality or Availability)


  </Collapser>

  <Collapser
    id="slm-health-input"
    title="Input Performance"
  >

    Input performance is measured simply by the ability to connect to your application. This is traditionally known as "Uptime". We'll use the same example above, NRQL Queries.

    Output performance measures responses after the fact a connection has been made. What if the connection between the client and the backend fails? That connection failure will not be reflected in output performance because a connection is required before we can measure the output.

    New Relic uses [synthetic monitoring](/docs/synthetics/synthetic-monitoring/getting-started/get-started-synthetic-monitoring/) to simulate connections from all over the world. New Relic has synthetic checks established against the NRQL Query API. If there is any interruption between the source point and the output, these checks will fail and be reflected in our Service Level Objective scores, as well as triggering very critical alerts of course.

    Input Performance is measure by:

    1. Percentage of successful test against your endpoints from an external source.

  </Collapser>


  <Collapser
    id="slm-health-client"
    title="Client Performance"
  >

    Client performance is the ability of your client, if you have one, to render content quickly and error-free. This is not to be confused with general customer experience where we measure and improve conversion rates, reoccurring users, retention, abandonment, and overall customer satisfaction of the product.

    For more information on client performance, please see our [Quality Foundation Use Case Implementation Guide](/docs/new-relic-solutions/observability-maturity/customer-experience/quality-foundation-implementation-guide).

  </Collapser>

  <Collapser
    id="slm-health-data"
    title="Data Quality"
  >

    Data quality measures the ability of a process to provide the expected data in a timely manner. New Relic must first ingest your data before you can query it. We expect that process to be completed in near-real-time. New Relic measures the time your data is ingested and samples to determine when that data is available for query.

    Data quality is measured by knowing the input data, knowing the output data, and measuring if the desired result is achieved. This may include auditing the results to determine accuracy.

  </Collapser>

</CollapserGroup>


## Prerequisites [#prerequisites]

### Technical enablements

- [APM Instrumentation](/docs/apm/new-relic-apm/getting-started/introduction-apm) of your primary application
- [Synthetic test](/docs/synthetics/synthetic-monitoring/getting-started/get-started-synthetic-monitoring) of your application

### Learning enablements
- Achieve basics skills in [New Relic One Dashboards and NRQL](https://learn.newrelic.com/webinar-getting-started-with-dashboards-nrql)
- Complete the [Service Level Management Free Online Course](https://learn.newrelic.com/self-paced-service-level-management)
- Review the [Service Level Management UI Product](https://docs.newrelic.com/docs/service-level-management/intro-slm)

## Establishing an output performance SLI [#establishing-output-sli]

### Process overview

  1. Identify your service
  2. Identify your service boundary
  3. Establish your baseline
  4. Create your service level

### 1) Identifying your service

  The following is assumed:

  1. Your primary applications are instrumented with [New Relic APM agents](/docs/apm/new-relic-apm/getting-started/introduction-apm).
  2. Your application names follow a familiar naming convention as outlined in our [Service Characterization Use Case Guide](docs/new-relic-solutions/observability-maturity/operational-efficiency/sc-implementation-guide/#improvement-process).
  3. You are familiar with how to [find your application in the New Relic Explorer](/docs/new-relic-one/core-concepts/new-relic-explorer-view-performance-across-apps-services-hosts/#find).


  Find your application (APM entity type) and select it. You should see the overview screen below.

  **Do not yet click Service Levels.**

  ![APM Overview screenshot](./images/apm-overview-screen.png)


### 2) Identifying your service boundary [#identifying-service-boundary]

  Be sure to read the definition of [service boundaries](#slm-terminology-service-boundary) in the [Terminology](#terminology) section above.

  The goal here is to ensure you are measuring the output of your service, first. While dependencies of that application each play a part in response times and success rates, the final and total response time and success is easily measured at the point where the request is received and responded to.

  Use [service maps](/docs/new-relic-one/ui-data/service-maps/how-use-service-maps/) and [automaps](/docs/new-relic-one/ui-data/automaps) to help determine the application you are looking at is a dependency of your application or the application that runs the endpoint API.

  **Example**

  In the screenshot below you are responsible for all applications that support order processing. You selected #2 (Order-Composer) to start, clicked **Service maps**, and discovered that Order-Composer is really a dependency; therefore, you will need to select #1 (Order-Processing) in order to establish a true health service level.

  Your team may only be accountable for the dependency, Order-Composer. If that's the case, then your own service level on Order-Composer is perfectly acceptable for your own self-monitoring of performance. Be sure to tag your own non-customer facing service levels as `customer-facing:false` to allow for better filtering in health reports. Also, consider collaborating with the customer-facing endpoint (#1 Order-Proessing) in your observability journey in order to establish true output performance, an input connectivity service level, and client service levels.

  ![Service Map Example](./images/service-boundary.png)

### 3) Establishing your baseline

  Establishing a baseline is a critical step to accelerating adoption and implementation of service levels. It's more challenging to determine what the design specifications are or **should have been** for services. Establishing a baseline allows you to measure the current performance of a service and then, through the service level reports, you will know if you are hitting that baseline or degrading.

  You can create a baseline for virtually any dataset; however, there are different formulas and recommendations for different use cases. For exampl,e you should use the average for some datasets, percentiles for others, and max for others.

  When starting service levels you should start with [output performance](#key-performance-indicators) of your applications. For this we use response times (Latency) and percentage of non-errors (Success).

  #### How much history should be considered?

  Not much in fact. You are establishing a reliability health metric. Seasonality and peak usage is not a handicap for good performance. Also, the more history you include in your measurement, the more likely you are including different codebases from releases. Previous deployments, no matter how small, could skew your results.

  The recommended history is 1-2 weeks of performance data to establish a fair baseline.

  #### Example baselines

  Below is an example NRQL query that represents the recommended target for a 7-day service level objective for latency.

  ```
  FROM Transaction SELECT percentile(duration, 95) AS 'Latency Baseline SLI' WHERE appName='Order-Processing' SINCE 1 WEEK AGO
  ```

  ![Latency Baseline](./images/latency-baseline.png)

  For a success (error-free) baseline, try the following query. Be sure to substitute `appName='Order-Processing'` for your own application name.

  ```
  FROM Transaction SELECT percentage(count(*), WHERE error is false) AS 'Success Baseline SLI' SINCE 1 WEEK AGO WHERE appName='Order-Processing'
  ```


### 4) Create your service level

  Great news! The New Relic platform will calculate recommended APM and browser baselines for you.

  **NOTE:** If you do not see the **Add a service level** button, you will need to check with your administrator regarding your permissions.

  The "[Identifying your service](#1-identifying-your-service)" section above shows you how to find your application APM data. You'll see #2 in the screenshot in that same section, called "Service Levels." Find your application APM data and click **Service levels**. You should see the view below.

  ![Service levels start from APM](./images/service-levels-start-from-apm.png)

  Click **Add baseline service level objectives** and almost instantly you will have both your Latency SLI and Success SLI and their respective objectives created for you.

  You can view and change all the settings by clicking the three dot icon in the upper-right corner of each SLO scorecard.

  **NOTE:** It will take approximately 10 minutes for data to populate the SLO scores. This is because we use [Events-to-Metrics](/docs/data-apis/convert-to-metrics/create-metrics-other-data-types/) for data longevity and query performance. It takes a moment for the conversion to take place and begin to populate the data.


## Establishing an input performance SLI [#establishing-input-sli]

### Process overview

  1. Create your synthetic check.
  2. Create your service level indicator.

  #### 1) Create your synthetic check

  The most common input performance service level is often referred to as "Connectivity" or "Uptime." This is a simple check against a health API endpoint or simply loading a URL. Both of these can be done easily by using our synthetic monitoring service. Please refer to our [add simple browser monitor](docs/synthetics/synthetic-monitoring/using-monitors/add-edit-monitors#simple)  and [add scripted api test](/docs/synthetics/synthetic-monitoring/using-monitors/add-edit-monitors#complex) documentation to begin collecting data.

  #### 2) Create your service level indicator

  You should now have data after completing step 1.

  Now you will use the Service Level Management service to create an input indicator and objective.

  Use the Explorer menu to select **Service levels**, and then click **+ Add a service level indicator**.

  **NOTE:** If you do not see the **Add a service level** button, you will need to check with your administrator regarding your permissions.

  Next filter your entity types to `Synthetic monitors`. See screenshot below.

  ![Filter entity types in service levels](./images/add-input-service-level.png)

  * Now find your synthetic monitor in the list from above and click it.
  * This will enable the **Continue** button in the left panel. Click Continue.

  1. You will see a button for the recommended settings for a Success service level (below). Click it.
  2. Make appropriate changes to the tags, title and description as needed.
  3. Click Save!

  ![Synthetics input service level guided flow](./images/synthetics-setup-2022-03-30.png)

## Establishing a capability SLI

  This is where you will really accelerate adoption of service levels!

  You do not need to have intimate knowledge of an application or service in order to complete this task. You simply need to know where the consumer-facing API (service boundary) is, and follow the steps below.

  This is a major step in your observability maturity journey. Having service levels on critical business capabilities, like login or authorize payment, will rapidly close the language barrier between IT and business. Service level scores on capabilities also provide you with a more precise remediation path when their service levels begin to degrade. For example, if the login service level begins to degrade, you will know to look at identity mangement dependencies and workflows starting at the consumer-facing API.

  **NOTE:** in this task you are building on the skills you learned in the "[Establishing and output SLI](#establishing-output-sli)" section.


### Process overview

  1. Assess application capabilities.
  2. Baseline a capability.
  3. Create your capability service level.

#### 1) Assessing your application capabilities

Identify the service boundary application as described in the [Establishing and output SLI](#establishing-output-sli) section above.

Now run the following NRQL query to identify baselines on most frequently used transactions. Be sure to replace `appName='Order-Processing'` with the application name you identified.

```
FROM Transaction SELECT count(*), percentile(duration, 95) WHERE appName='Order-Processing' FACET name SINCE 1 WEEK AGO
```

You should see something similar to the screenshot below.

![transaction list query](./images/transaction-list-nrql.png)

You'll see the first transaction (#3) states it has something to do with "purchase." You can now create a "purchase" capability service level.

**NOTE:** Even if you are not sure that this transaction represents the purchase capability, this exercise makes a great example to show the application team and your leadership the value of capability service levels. Remember, your goal here is to start a conversation with the stakeholders by showing the art-of-the-possible.

Add `WHERE name='Controller/Sinatra//purchase'` to the end of your query, replacing `Controller/Sinatra//purchase` with your transaction name. Run the query to make sure it works. You should now see only the one transaction in your result. Copy this query and the `DURATION (95%)` result into a notepad. You'll need both in a moment.

Create a new service level in the platform. Starting a new service level is described in [Establishing an input performance SLI](#establishing-input-sli).

In this case you want to find your application (APM `Entity` type) in the list so we can retain the metadata (tags) through the entity guid. Instead of "Synthetic monitor" as in section above, select "APM" in the entity filter pulldown.

Select the "Latency" guided workflow so the good and valid queries are auto-populated for you.

Use your notepad to copy just the `name='your/transaction/name/here'`.

Add this condition to both queries in service levels, __preceeded with an `AND `__, as underlined in the screenshot below.

![capability query clause](./images/capability-filter-clause.png)

Almost there!

Simply adjust the `duration < 1.78` portion of the second query to match the `DURATION (95%)` result in your original baseline copied to your notepad.

Add ` AND error is false` to the same query.

Congratulations! You now have a combined Latency and Success service level for a capability in the language of the business.

Proceed to name this service level, update the description, and save the service level.

It's recommended to set up a few of these capability service levels and present to the application team and your leadership for feedback.


## Improvement process [#improvement-process]

### Alert quality management
[Alert quality management](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/aqm-implementation-guide) is another observability maturity practice that marries really well with service level management. The value of both alerting quality data side-by-side with service level data is that you can see if alert policies are aligned with real impact or are just creating noise. You will find be able to validate good alerts, missing alerts, and just noisy alerts.

You can do this by creating a custom dashboard with an SLI compliance query side-by-side with an alerting quality query.

Be sure to check-out [alerting quality management](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/aqm-implementation-guide) next.


### Adoption and continuous improvement
Improvement of service levels and reliability requires adoption of the practice by all the stakeholders of the service. This includes, but is not limited to, engineering management, product management, and executive management. The primary goal is to quickly demonstrate the power and value of service levels to stakeholders in order to start a meaningful discussion on what really matters to those stakeholders. The steps in this guide will get you those meaningful discussions very quickly.

A proven method, with a high rate of adoption, is to first establish output performance and input performance service levels for one digital product and its critical capabilities. This usually involves one overall output and input service level for each endpoint application (usually one or two), and then approximately 4-7 output performance service levels for assumed critical capabilities measured at the endpoint transaction.

This method includes __not__ surveying each stakeholder for what should and shouldn't be measured. Surveys usually result in long wait times, lots of questions, frustration, lack of demonstrated value, and not so good answers. Remember, start with baselines and key transactions as "capabilities."

Freely make assumptions of what these endpoints are and what endpoint transactions make up what capabilities as demonstrated above. Accuracy is not the key at first. What is key to a successful kick-off is demonstrating the ability to easily measure and communicate health. That initial demonstration will show the value in investing more time to refine what is and what isn't measured in primary service levels.

Don't wait. The sooner you provide that demonstration and the more complete that demonstration is, the sooner you will achieve broader adoption and begin the reliability improvement process in collaboration with all the stakeholders!

### Automation

Once you have established what works (and what doesn't) for your stakeholders, you can then begin to design SLM at scale with automation. You can start learning about automating service level management by studying the [New Relic Terraform library](https://registry.terraform.io/providers/newrelic/newrelic/latest/docs/resources/service_level).

## Next steps [#next-steps]

The next step is to add in customer experience service levels measured at the client browser or mobile device. Again, it's important you first prove value as described in the improvement process above. Remember, observability is a journey, and maturity takes time, practice, and patience.

See [Quality Foundation](docs/new-relic-solutions/observability-maturity/customer-experience/quality-foundation-implementation-guide) in our Customer Experience section to proceed on your journey.

Also see [alerting quality management](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/aqm-implementation-guide).
