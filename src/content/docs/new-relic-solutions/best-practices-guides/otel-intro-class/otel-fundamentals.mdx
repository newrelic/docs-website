---
title: 'OpenTelemetry fundamentals'
metaDescription: 'OpenTelemetry fundamentals'
freshnessValidatedDate: never
---

In this chapter, you learn the fundamentals of OpenTelemetry. Specifically, you're introduced to core concepts and components of the OpenTelemetry standard and implementation. This will help you learn learn how, when, and why you use it in your applications.


## Objectives [#objectives]

In this chapter, you learn answers to questions like:

- What is OpenTelemetry?
- Why should I be interested in it?
- How do I use it?

## Prerequisite knowledge [#prerequisite-knowledge]

Before reading this chapter, you need to be familiar with telemetry concepts, including:

- What telemetry is
- How telemetry works
- What data types telemetry supports
- What traces are

You should also be familiar with:

- General programming concepts, like key-value pairs, functions, and classes
- Programming organizational structures, like libraries, frameworks, and repositories
- Common networking concepts like requests, responses, and status codes

<Steps>
    <Step>
    ## OpenTelemetry [#otel]

    [OpenTelemetry](https://opentelemetry.io) is an exciting new standard for open instrumentation, one that's supported by a large developer community composed of end users, cloud providers, and observability leaders (including New Relic). Like its predecessor, [OpenTracing](https://opentracing.io), it's a CNCF project, currently in the [incubating](https://landscape.cncf.io/card-mode?project=incubating) phase of maturity.

    OpenTelemetry aims to:

        - Standardize how applications collect and send their telemetry data to backend platforms. With this standard, clients and observability platforms all agree on what the data looks like, so you no longer have to be locked in to a particular platform's ecosystem to make use of your data.

        - Provide better end-to-end visibility of telemetry data. OpenTelemetry defines how libraries and frameworks generate telemetry data in a way that is platform-agnostic and implementation-independent. This means that more tools can adopt OpenTelemetry, which leads to better observability, especially in distributed systems that use different technologies.

    OpenTelemetry accomplishes these goals by providing both a specification, including what telemetry data should look like and how it's used, as well as an out-of-the-box implementation of that spec in many of the most popular programming languages.

        * Specification. OpenTelemetry is, first and foremost, a specification. It provides a standard definition of what telemetry data looks like and a description of how to build and use OpenTelemetry instrumentation.

        * Implementation. OpenTelemetry's API and SDK specs define classes, functions, and configuration mechanisms for working with telemetry data. But OpenTelemetry also supplies implementations that comply with the standard in many of the most popular programming languages.

    <Callout variant="tip">
        Platform-specific solutions often support more features of their backend platforms than OpenTelemetry does, and New Relic is no exception. While this is true today, as OpenTelemetry matures, this disparity will shrink.

        Read our [documentation](/docs/more-integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-introduction//#choice) to compare how our platform supports our proprietary agents versus how we support OpenTelemetry.

    </Callout>

    </Step>

    <Step>
    
    ## The OpenTelemetry specification [#otel-specification]

    The specification provides a standard definition of what telemetry data looks like and a description of how to build OpenTelemetry instrumentation.

    Specifically, it describes:

        - A **data model** for each telemetry data type it supports. This specification includes a protocol for sending data over the wire and conventional attributes for describing data for common operations and technologies.
        - An application programming interface (**API**) that consists of classes and functions that ensure telemetry data is generated in a consistent way.
        - A software development kit (**SDK**) with mechanisms for configuring, processing, and exporting telemetry data generated by the API.

    Standardized data models are useful because developers and observability platforms can agree on what telemetry data structures look like, how they're represented in code, and how they're serialized and transmitted between services. You, the end user, can choose between competing platforms and tools that adhere to the spec with confidence that the data will be generated and formatted consistently.

    A standard way to build instrumentation provides consistency across all supported programming languages. If you instrument your application with tools that adhere to the OpenTelemetry specification, you'll know how they work and that they generate telemetry data in a consistent way.

    <Callout variant="tip">
        It's helpful to compartmentalize the different components of the OpenTelemetry specification. Developers can create instrumentation libraries that comply with the data spec, but that don't comply with the API or SDK specs. In other words, their libraries may have different interfaces and configuration mechanisms while still generating and sending data that matches the OpenTelemetry specification.

        You learn more specifics about the specification and its components in a later lesson.

    </Callout>

    </Step>

    <Step>

    ## The OpenTelemetry data model specification [#otel-data-model]    

    The data model specifies what telemetry data structures look like. In OpenTelemetry, these data structures are called **signals**. The signals that it supports align mostly with the telemetry data types you learned about earlier in this course: traces, metrics, and logs.

    Each signal has its own data model specification that includes:

        - **Semantic conventions**: Conventional attributes that describe common operations, technologies, events, and protocols.
        - **An OpenTelemetry Protocol (OTLP) specification**: A description of how each data signal should be encoded and transferred over OpenTelemetry's exchange protocol.

    To learn more specifically what a data model provides, look at the trace data model.

    </Step>

    <Step>

    ## Trace data model semantic conventions [#trace-data-model]     

    The OpenTelemetry trace signal comprises a hierarchical collection of linked spans. Each span consists of metadata fields, such as the span's name and its start time and end time, plus a list of attributes and a list of events. This definition of what a span is and the fields it contains is universal for all telemetry tooling that adheres to the OpenTelemetry data specification for traces.

    The name field identifies the operation that the span represents. For example, the name might be a web service endpoint or function.

    The start time and end time fields represent the time that the operation started and the operation ended, respectively. These times represent the total elapsed time of the operation, including any and all sub operations.

    There are other important [span fields](https://opentelemetry.io/docs/reference/specification/trace/api/#span) as well, some of which you learn more about in later lessons because they're particularly relevant to the implementation of traces in OpenTelemetry. One simple, but really important component of all OpenTelemetry data models, including the span model, is the **attribute**.

    ### Attributes [#attributes]

    Attributes are key-value pairs that represent features that you want to highlight. For example, if your span represents a web request to your online store for a user adding a product to their cart, you might want to store the product's ID as an attribute on the span. This is an example of a custom attribute.

    There are also conventional attributes for describing common operations, technologies, concepts, and protocols. For example, a span that represents a database query should have an attribute named `db.user` that represents the user of the database. A span that represents a web service fielding a request should have an attribute named `net.peer.port` that stores the node's port number. These conventions, called semantic conventions, standardize the keys and values of these common features.

    Libraries that support popular technologies, like HTTP, gRPC, and messaging protocols, must use these semantic conventions when describing their operations. These keys aren't all required, but if a library captures the data in an attribute, it must follow the conventions.

    <Callout variant="tip">

    While these conventions unify tool developers on what to call these characteristics, they're also helpful to backend observability platforms. Conventional attributes mean observability platforms can expect consistency from OpenTelemetry implementations. They can rely on that consistency to build specific user experiences and tools that utilize those attributes.

    For example, because New Relic knows that spans representing database operations contain these widely-adopted attributes, it can programmatically find attributes in the data and craft a specific experience with them.

    </Callout>

    Like attributes, span events are another important component to the trace data model.

    ### Span Events [#span-events]

    A span event represents something that happens during a span's operation. For example, if a function raises an exception, you can capture details about that exception in a span event.

    Like its associated span, a span event has metadata, specifically a name and a timestamp, as well as some attributes that highlight the features of the event.

    In the case of an exception event, some of its attributes correspond to semantic conventions. For example, exception events store their stack trace in an attribute called `exception.stacktrace`, if they have one. They store their message in an attribute called `exception.message`, if they have one. Span events are really useful in understanding what happens during a span's operation.

    <Callout variant="tip">

    Some services, like New Relic, may treat an event as a first class data structure. In OpenTelemetry, events are directly related to spans.

    </Callout>

    You've learned that the OpenTelemetry data model specification defines the metadata and the semantic conventions of the signal. In the case of spans, this includes things like the span's name, its timestamps, its attributes, its semantic conventions, and its events. But the specification also defines how the signal should be serialized so that it can be transmitted from telemetry clients.

    </Step>

    <Step>
    ## The OpenTelemetry Protocol (OTLP) [#otel-protocol]

    The OpenTelemetry specification includes an exchange protocol, OTLP, that defines how data is serialized, deserialized, and transported between networked services. The goal of this protocol is to specify a serialization schema that closely adheres to the data models and solves [problems that exist with other telemetry protocols](https://opentelemetry.io/docs/reference/specification/protocol/requirements/#known-issues-with-existing-protocols). It also aims to define how to generally implement a client that can serialize and send the data, and how to implement a server that can accept the serialized data and indicate success or failure in its response.

    OTLP uses protocol buffers to serialize telemetry data and gRPC or HTTP for sending it. The protocol works in all nodes from your telemetry client to the observability platform that receives your data and any services, agents, collectors, and forwarders in between.

    OTLP is relevant to the OpenTelemetry data specification because the spec doesn't only define what the data for each signal looks like at a high level, but it also specifies what it looks like at the code level. Specifically, it defines the protocol buffer schemas you use to serialize your data and send it over the wire.

    Here's a snippet from the [trace protocol buffer schema](https://github.com/open-telemetry/opentelemetry-proto/blob/main/opentelemetry/proto/trace/v1/trace.proto):

    ```
    message Span {
    bytes trace_id = 1;
    bytes span_id = 2;
    string trace_state = 3;
    bytes parent_span_id = 4;
    string name = 5;
    SpanKind kind = 6;
    fixed64 start_time_unix_nano = 7;
    fixed64 end_time_unix_nano = 8;
    repeated opentelemetry.proto.common.v1.KeyValue attributes = 9;
    uint32 dropped_attributes_count = 10;
    repeated Event events = 11;
    uint32 dropped_events_count = 12;
    repeated Link links = 13;
    uint32 dropped_links_count = 14;
    Status status = 15;
    }
    ```

    This message represents a trace's span. You may not recognize the first few fields, because you haven't yet learned about how traces are implemented, but most of the other fields should look familiar:

        - The **fifth** field on the message is the span's name. As you learned previously, the span's name is a string of characters that identifies the operation the span represents.
        - The **seventh** and **eighth** fields are the start and end timestamps of the span, respectively. These are stored as eight-byte integers that represent the number of nanoseconds it has been since the UNIX epoch (January 1, 1970). It's more efficient to send integers over the wire than it is to send formatted timestamp strings.
        - The **ninth** field represents the span's attributes as a collection—denoted by the “repeated” data type—of `KeyValue` elements.
        - The **eleventh** field stores a collection of `Event` objects.

    In the schema, the `Event` and other custom data types that are more complex than primitive ones have their own definitions. In this case, you have:

    ```
    message Event {
    fixed64 time_unix_nano = 1;
    string name = 2;
    repeated opentelemetry.proto.common.v1.KeyValue attributes = 3;
    uint32 dropped_attributes_count = 4;
    }
    ```

    This should look mostly familiar to you as it matches almost one-to-one with the data model you learned in a previous lesson.

    When you reference or work with the data specification, it's important to keep in mind that OpenTelemetry is still an evolving product. Some of the data signals are stable, have long-term support, and are guaranteed to not have subsequent minor releases with backward-incompatible changes. Others, however, are experimental or are currently being tested where there is no such guarantee of stability. Check out the **Maturity Level** chart in the [opentelemetry-proto repository](https://github.com/open-telemetry/opentelemetry-proto#maturity-level) to see how mature each schema is.

    Awesome! You've learned all about the OpenTelemetry data specification, including:

    - What signals are
    - What the spec defines about each signal
    - What the trace data model looks like
    - What attributes are in the scope of OpenTelemetry
    - What span events are
    - How the spec defines the OTLP schemas according to the data models

    The specification doesn't stop with defining telemetry data, though. It also specifies an interface for interacting with that data.

    </Step>

    <Step>
    ## The OpenTelemetry API specification [#otel-api-spec]

    So far, you've learned how the OpenTelemetry specification defines the data models, semantic conventions, and OTLP schemas for each of the telemetry data signals. But the specification defines more than what telemetry data looks like. It also defines how to work with that data.

    The OpenTelemetry API specification describes the classes and functions you use to interact with the data signal. For example, it describes how an OpenTelemetry implementation can generate a span or report metric measurements.

    For OpenTelemetry instrumentation developers, this specification provides the blueprint for building an API that's consistent with other OpenTelemetry-compliant tools. For framework, library, and app developers who use the API, the specification guarantees consistency and provides documentation for how you can expect to interact with all compliant tools.

    There are two essential components of the API specification:

    - A **definition of the interface** you use to work with telemetry data
    - A **minimal implementation** of that interface

    </Step>

    <Step>
    ## Defining the interface [#defining-interface]

    The first thing that the API specification provides is a definition of the classes and functions for interacting with a signal.

    For example, the specification defines three primary classes for working with traces:

        - `TracerProvider` is a class that, as you may have guessed, provides tracers. Some apps may use multiple tracer providers, but generally, you'll use a single, global one. A tracer provider holds certain configurations, such as how to process spans and how to sample spans.
        - `Tracer` is a class you use to create spans that collect information about processes. Remember, the tracer provider, not the tracer, holds the configurations that dictate how the tracer creates spans and what it does with them when they're recorded.
        - `Span` captures data about an operation. Multiple spans make up a trace. You learned a lot about what an OpenTelemetry span looks like when you learned about the data model specification. However, there's more to a span than the data it holds.

    For example, a span has start and end methods that determine its duration. It has a method you use to see if you can update the span's values, called `IsRecording()`. It also has methods for updating those values, such as for adding attributes and events. All of these interfaces for interacting with the signal are defined by the API specification.

    <Callout variant="tip">

    While the spec defines the classes and functions you use to interact with a signal, the language-level details, like the casing of a method's name, are subject to differences between implementations.

    </Callout>

    Another aspect that the API specification defines is what it calls the “minimal implementation.”

    </Step>

    <Step>
    ## Minimal implementation [minimal-implementation]

    The API defines the interface with which you work with data, but it also requires an implementation that does effectively nothing. This “minimal” implementation has very low overhead and no side effects.

    The API is used by programs that deal with telemetry data, including instrumentation libraries; frameworks for interacting with certain technologies, like network requests or databases; and applications that use those tools. Only the application developer, however, knows how they want to use the data generated by the OpenTelemetry APIs. Having a minimal implementation allows developers to build API interactions into their code regardless of whether or not the application developer wants to use OpenTelemetry.

    For example, imagine a database library developer wants to instrument their code so that their users have the option of using the telemetry data it generates. The developer configures their tracer provider, uses it to create a tracer, then uses that tracer to create spans throughout their code. They can do all of this, and more, without knowing or caring if the user even wants telemetry data, but with peace of mind that the instrumentation will be performant and without side effects.

    If the user wants to use OpenTelemetry, they configure an implementation. The library, which uses the APIs, automatically starts generating and sending the appropriate data the way the user wanted it to.
    If the user doesn't want to use OpenTelemetry, the API uses its minimal implementation, which doesn't actually generate any telemetry data.

    This provides consistency for developers using OpenTelemetry APIs. It also relieves some burden off developers by not requiring some kind of special, instrumented version of their library. By removing these barriers, you can adopt OpenTelemetry much more easily, and greater adoption means less gaps and better telemetry data for everyone.

    Like with the data model specification, it's important to remember that the API specifications for each signal are at different stages of maturity. View the [OpenTelemetry status page](https://opentelemetry.io/status/) for the most up-to-date information of the maturity of API specs.

    Now that you've seen some examples of what the API specification defines, it's important to take a step back in order to be clear about what it doesn't define.

    </Step>

    <Step>
    ## Separating concerns [separating-concerns]

    The API defines the interface you use to interact with OpenTelemetry signals. With it, you set up your code to generate telemetry data. The API, itself, does not actually generate data; that's the responsibility of the implementation.

    OpenTelemetry separates the signal's interface and its implementation, which is the logic that runs when you interact with the interface. This protects the user of the API from having to know or care about exactly how the API is implemented. It also allows the implementation to be configurable.

    Because the user of the API can't know the logic behind the interface, that logic can take a number of different forms, depending on how the user wants to use the data. Configuring the implementation of the API is the purpose of the SDK.

    </Step>

    <Step>
    ## The OpenTelemetry SDK [#otel-sdk]
    OpenTelemetry provides specifications for each of the signals it supports. You've already seen that these include descriptions of the data model and API, but they also include a description of the SDK. The SDK specification defines the requirements for implementing the logic behind the API. Before you learn more specifics about the SDK specification, you may find an illustration of how you might use the SDK valuable.

    Imagine you're building an application that is supported by libraries that are instrumented with OpenTelemetry. Until now, you haven't defined how you want to use the telemetry data, so the API that those libraries interact with are using the minimal implementation. In other words, the libraries aren't generating or sending data, but they're equipped to. You're now ready to configure the SDK to make use of that instrumentation.

    You start by deciding how you want to use your telemetry data. For traces, you decide how you want to process spans, how you want to sample them, and how and where you want to send them, once collected. Once you know how you want the API to treat your data, you configure an implementation using interfaces from the SDK.

    The specification defines three mechanisms for configuring the SDK:

        - **Plugins** that provide interchangeable implementations
        - **Constructors**, such as environment variables and objects, that hold configurations, accept plugins, or create resources
        - A **Resource** that identifies your environment that produces telemetry data

    Once you apply your plugins and configure your constructors, API interactions produce results the way you want them to. They no longer use the minimal implementation; they use the implementation you configured instead. Because the API doesn't know or care what implementation is used, the instrumentation requires no code changes.

    Now that you've seen a high level example of how the SDK works, it's time to learn how the specification defines the plugin and constructor interfaces.

    </Step>

    <Step>
    ## SDK plugins [#sdk-plugins]

    One of the ways that you define how you want the API to treat your data is with plugins. For example, if you want to sample your data a particular way, you can use a sampler plugin to meet your needs. If you want to change to a different sampler, you can swap out the plugin.

    The SDK specification defines both what types of plugins the SDK accepts as well as the interface that those plugins must implement. Because of this interface definition, it's easy for developers to create new plugins. And if you need custom behaviors, you can even use the specification to write your own.

    The trace SDK accepts several types of plugins, such as:

        - **Span exporter:** Encodes data and transmits it to a consumer
        - **Span processor:** Enriches and filters span data, then converts it into a form that exporters can work with
        - **Sampler:** Lets users control span sampling

    For each of these plugin types, the SDK spec defines the interface, or the required methods.

    For example, span exporter plugins must implement two functions:

        - **Export:** Receives spans, serializes them, and transmits them to a designated consumer
        - **Shutdown:** Cleans up the export environment

    API interactions can call these functions at the appropriate time regardless of what plugin is being used because the specification requires them.

    The spec also defines the plugins that must be included in any implementation of the SDK. For example, it declares that the SDK must provide a simple span processor and a batch processor. The simple span processor processes spans as soon as they finish. The batch processor batches finished spans before passing them to the exporter.

    SDK plugins are modular logical components that implement API interfaces in a particular way. You choose the plugins that best suit your goals for telemetry data. Once you choose your plugins, you can further configure them and assign them to the API using SDK constructors.

    </Step>

    <Step>
    ## SDK constructors [#sdk-constructors]
    The specification defines constructors as a way to configure your OpenTelemetry implementation. Constructors include environment variables and configuration classes.

    For example, the SDK provides a built-in exporter plugin for transmitting data over OTLP, OpenTelemetry's exchange protocol you learned about in a previous lesson. The OTLP exporter spec defines some environment variables you use to configure it, such as `OTEL_EXPORTER_OTLP_TRACES_ENDPOINT`, which specifies the url where the exporter should send trace data.

    The specification also defines classes you use to configure the API implementation. For traces, this is the tracer provider you already learned about. You use the tracer provider to create tracers that create spans. The tracer provider holds the configuration for how spans are generated, processed, and exported. It's how you supply the plugins to implement the API.

    Along with plugins and constructors, the OpenTelemetry SDK specification defines a way for you to identify the environment in which you're producing telemetry data. This SDK component is called a **resource**.

    </Step>

    <Step>
    ## Resources [#resources]
    You use a resource to identify your project's environment by means of attributes. For example, to describe your host, you can include attributes for your host name and a virtual machine image. To describe your service, you can include a service namespace and a version. To describe your telemetry instrumentation, you can include an SDK name and a version.

    The OpenTelemetry specification provides resource semantic conventions, like it does for signals. The one and only required conventional attribute for a resource is `service.name`. You can find the optional conventional attributes in the [specification](https://opentelemetry.io/docs/reference/specification/resource/semantic_conventions/#semantic-attributes-with-sdk-provided-default-value).

    Once you've created a resource for your environment, you can include it with a constructor. For traces, this means that you attach the resource to your tracer provider. When you create spans, they are associated with your environment's resource.

    Like with the data model and API specifications, some SDK specs are more mature than others. The resource spec, for example, is stable, while the resource semantic conventions are still experimental. View the [OpenTelemetry status page](https://opentelemetry.io/status/) for the most up-to-date information on the maturity of SDK specifications.

    You've learned a lot about OpenTelemetry as a specification. This is the foundation for understanding OpenTelemetry because everything built around it follows the spec. But OpenTelemetry also includes official code repositories that implement the spec in many popular programming languages.

    </Step>

    <Step>
    ## OpenTelemetry implementations [#otel-implementations]
    
    OpenTelemetry is more than a specification. It also provides official implementations in many of the most popular programming languages:

            - [.NET](https://github.com/open-telemetry/opentelemetry-dotnet)
            - [C++](https://github.com/open-telemetry/opentelemetry-cpp)
            - [Erlang/Elixir](https://github.com/open-telemetry/opentelemetry-erlang)
            - [Go](https://github.com/open-telemetry/opentelemetry-go)
            - [Java](https://github.com/open-telemetry/opentelemetry-java)
            - [JavaScript](https://github.com/open-telemetry/opentelemetry-js)
            - [PHP](https://github.com/open-telemetry/opentelemetry-php)
            - [Python](https://github.com/open-telemetry/opentelemetry-python)
            - [Ruby](https://github.com/open-telemetry/opentelemetry-ruby)
            - [Rust](https://github.com/open-telemetry/opentelemetry-rust)
            - [Swift](https://github.com/open-telemetry/opentelemetry-swift)


    Just as with the spec, these implementations are evolving. Learn more about how far along your preferred language is by checking the [compliance matrix](https://github.com/open-telemetry/opentelemetry-specification/blob/main/spec-compliance-matrix.md).

    OpenTelemetry provides manual and automatic forms of instrumentation that use the API to generate data.

    ### Manual instrumentation [#manual-instrumentation]

    Manual instrumentation requires you, the developer, to use the SDK to configure your API and to use the API to create and export the data. In terms of traces, this means that you create a tracer provider, you create tracers, and you create spans for each operation that you want to collect data about. If you're trying to natively instrument your library or framework, you use manual instrumentation.

    ### Automatic instrumentation [#automatic-instrumentation]

    Automatic instrumentation provides the same telemetry data as manual instrumentation without you having to instrument code yourself. Automatic instrumentation is limited to popular libraries and frameworks, so you can only use it if you're an application developer using one of those projects. Fortunately, you can find automatic instrumentation for many of the popular open source libraries and frameworks.

    <Callout variant="tip">
        OpenTelemetry developers can provide automatic instrumentation by programatically manipulating and instrumenting code. For example, the OpenTelemetry Java agent rewrites an application's bytecode to add instrumentation. However, they can also provide library instrumentation. This can take a few forms:

            - OpenTelemetry can be built directly into a library
            - Instrumentation code can utilize special purpose extension hooks that are present in the library it instruments
            - Wrapped versions of libraries can include an instrumentation layer
    </Callout>

    ### Repository structure [#repository-structure]

    OpenTelemetry implementations include many components, including SDK interfaces, API interfaces, plugins, and semantic conventions, and their implementations don't all look the same. Not only do they make decisions that are idiomatic to the language they're implemented in, but they separate concepts differently.

    For example, one language may have one repository for all OpenTelemetry code. Another language may separate manual and automatic instrumentation into their own repositories. Review the [OpenTelemetry documentation](https://opentelemetry.io/docs/instrumentation/) to learn more about your preferred language.

    </Step>
</Steps>




