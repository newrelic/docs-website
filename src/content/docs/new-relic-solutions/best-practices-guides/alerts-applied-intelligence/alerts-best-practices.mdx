---
title: Alerts best practices
tags:
  - New Relic solutions
  - Best practices guides
  - Alerts and Applied Intelligence
translate:
  - jp
metaDescription: 'Best practices for deciding what to alert on, when to trigger notifications, and who receives them.'
redirects:
  - /docs/alerts/new-relic-alerting/configuring-alert-policies/best-practices-alert-policies
  - /docs/alerts/new-relic-alerting/getting-started/best-practices-alert-policies
  - /docs/alerts/new-relic-alerts-beta/getting-started/best-practices-alert-policies
  - /docs/alerts/new-relic-alerts/getting-started/best-practices-alert-policies
  - /docs/alerts/new-relic-alerts/getting-started/alerts-best-practices
  - /docs/alerts-applied-intelligence/new-relic-alerts/get-started/alerts-best-practices
---

Improve your Alerts coverage by implementing the following recommendations and get the most out of your alerts configuration. 

Read on to learn the best practices for:
- Policies
- Notification channels
- Incident preferences
- Thresholds and violations
- Muting rules


## Recommended alerts [#recommend-alerts]

Use [recommended alerts conditions](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/condition-recommendations/) conditions if you are new to Alerts or if you want suggestions that optimize your alert coverage.


## Organize your policies [#policy-practices]

A policy is a container for similar [conditions](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-alert-conditions/).

If you’re new to Alerts, learn how to [create, edit, or find policies](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/create-edit-or-find-alert-policy/).

Organize your policy's scope to a single [entity](/docs/new-relic-one/use-new-relic-one/core-concepts/what-entity-new-relic/) when possible. Assign your policy to the essential team or teams that need to be notified when an [incident](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/specify-when-alerts-create-incidents/) occurs. This way, you keep policies centralized and focused.

If a team is monitoring several groups of the same entity type, combine those entity clusters (like servers) together into one policy. This way, your team can be notified from one policy rather than navigating several policies at once.

You can use Alerts to monitor all of your entities. Consider your role and priorities when assigning yourself to a policy. 

For example:

* **Software developers** may need [notifications](/docs/alerts/new-relic-alerts/managing-notification-channels/notification-channels-controlling-where-send-alerts) for both front-end and back-end performance, such as webpage response time and page load JavaScript errors.
* **Operations personnel** may need notifications for poor back-end performance, such as server memory and load averages.
* **The product owner** may need notifications for positive front-end performance, such as improved end user Apdex scores or sales being monitored in [dashboards](/docs/query-your-data/explore-query-data/dashboards/introduction-new-relic-one-dashboards).

## Set your condition thresholds and violations [#threshold-practices]

Set meaningful [threshold](/docs/alerts/new-relic-alerts-beta/configuring-alert-policies/define-thresholds-trigger-alert) levels to optimize Alerts for your business. Here are some suggested guidelines:

<table>
  <thead>
    <tr>
      <th width={200}>
        Action
      </th>

      <th>
        **Recommendations**
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        Set threshold levels
      </td>

      <td>
        **Avoid setting thresholds too low.** For example, if you set a CPU condition threshold of 75% for 5 minutes on your production servers, and it routinely goes over that level, this will increase the likelihood of un-actionable alerts or false positives.
      </td>
    </tr>

    <tr>
      <td>
        Experimenting with settings
      </td>

      <td>
        **You do not need to edit files or restart software**, so feel free to make quick changes to your threshold levels and adjust as necessary.
      </td>
    </tr>

    <tr>
      <td>
        Adjust settings
      </td>

      <td>
        **Adjust your conditions over time.**

        * As you use our products to help you optimize your entity's performance, tighten your thresholds to keep pace with your improved performance.
        * If you are rolling out something that you know will negatively impact your performance for a period of time, loosen your thresholds to allow for this.
      </td>
    </tr>

    <tr>
      <td>
        Disable settings
      </td>

      <td>
        You can **disable any condition** in a policy. This is useful, for example, if you want to continue using other conditions in the policy while you experiment with other metrics or thresholds.
      </td>
    </tr>
  </tbody>
</table>

In most of our products (except Infrastructure), the color-coded [health status indicator](/docs/using-new-relic/welcome-new-relic/get-started/glossary/#health-status) in the user interface changes as the alerting threshold escalates or returns to normal. This allows you to monitor a situation through our UI before a critical threshold passes, without needing to receive specific notifications about it.
 
There are two violation thresholds: critical (red) and warning (yellow). Define these thresholds with different criteria, keeping in mind the suggestions above. 

<Callout variant='important'>
Warning violations do not open incidents. A critical violation can open incidents, but you must define that decision through your [incident preferences](/docs/new-relic-solutions/best-practices-guides/alerts-applied-intelligence/alerts-best-practices/#incident-practices).
</Callout>


## Define your incident preferences [#incident-practices]

Decide when you get incident notifications so you can respond to incidents when they happen.

If you’re new to Alerts, learn more about your [incident preferences options](https://discuss.newrelic.com/t/relic-solution-alert-incident-preferences-are-the-key-to-consistent-alert-notifications/40867).

The default incident preference setting combines all conditions within a policy into one incident. Change your default incident preference setting to increase or decrease the number of incidents and incident notifications you receive.

Each team within your organization will have different needs. Ask your team two important questions when deciding your incident preferences:
- Do we want to be notified every time something goes wrong? 
- Do we want to group all similar notifications together and be notified once?

When a policy and its conditions have a broader scope (like managing the performance of several entities), increase the amount of incidents you receive. You will need more notifications because two incidents will not necessarily relate to each other.

When a policy and its conditions have a focused scope (like managing the performance of one entity), opt for the default incident preference. You will need less notifications when two incidents are related to each other or when the team is already notified and fixing an existing problem.

Decide how you get incident notifications by using our [notification channel best practices](/docs/new-relic-solutions/best-practices-guides/alerts-applied-intelligence/alerts-best-practices/#channel-practices).


## Select your notification channels [#channel-practices]

Tailor notifications to the most useful channel and policy so you can avoid alert fatigue and help the right personnel receive and respond to incidents they care about in a systematic way.

If you’re new to Alerts, learn how to [set up notification channels](/docs/alerts-applied-intelligence/new-relic-alerts/alert-notifications/notification-channels-control-where-send-alerts/).

Notify teams and individuals who needs to stay updated on or resolve a problem when an incident arises.

To stay updated, select a notification channel that is less intrusive, like email.

For vital notifications and incident responses, select a notification channel that is more intrusive, like PagerDuty or Slack.

Do not rely on email for quick notifications in case of delays.


## Understand muting rules [#mute-practices]

Mute alerts during routine events, such as maintenance or planned downtime.

You can also silence a policy, a specific entity, and a condition when needed. Incidents can still be opened, but you won't be notified.

If you're new to Alerts, learn how to [create and manage muting rules](/docs/alerts-applied-intelligence/new-relic-alerts/alert-notifications/muting-rules-suppress-notifications/).


## What's next?

To learn more about using alerts:

* Learn about the [API](/docs/alerts/rest-api-alerts/new-relic-alerts-rest-api/rest-api-calls-new-relic-alerts).
* Read technical details about [min/max limits and rules](/docs/alerts/new-relic-alerts/getting-started/minimum-maximum-values).
