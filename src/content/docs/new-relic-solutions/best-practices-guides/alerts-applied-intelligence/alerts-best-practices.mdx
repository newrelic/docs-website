---
title: Alerts best practices
tags:
  - New Relic solutions
  - Best practices guides
  - Alerts and applied intelligence
translate:
  - jp
metaDescription: 'Best practices for deciding what to alert on, when to trigger notifications, and who receives them.'
redirects:
  - /docs/alerts/new-relic-alerting/configuring-alert-policies/best-practices-alert-policies
  - /docs/alerts/new-relic-alerting/getting-started/best-practices-alert-policies
  - /docs/alerts/new-relic-alerts-beta/getting-started/best-practices-alert-policies
  - /docs/alerts/new-relic-alerts/getting-started/best-practices-alert-policies
  - /docs/alerts/new-relic-alerts/getting-started/alerts-best-practices
  - /docs/alerts-applied-intelligence/new-relic-alerts/get-started/alerts-best-practices
  - /docs/new-relic-solutions/new-relic-solutions/optimize-your-cloud-native-environment
  - /set-proactive-alerts-align-teams-tools-processes-incident-response/
  - /docs/4-fail-fast-set-proactive-alerts-align-teams-tools-processes-incident-response
  - /docs/set-proactive-alerts-align-teams-tools-processes-incident-response
  - /docs/using-new-relic/welcome-new-relic/optimize-your-cloud-native-environment/set-proactive-alerts-align-teams-tools-processes-incident-response
  - /docs/new-relic-solutions/new-relic-solutions/optimize-your-cloud-native-environment/set-proactive-alerts-align-teams-tools-processes-incident-response/
---

Improve your alerts coverage by implementing the following recommendations and get the most out of your alerts configuration.

<Callout variant="tip">
  See [Alert quality management](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/alert-quality-management-guide) for a guide on how to measure and improve your alerting quality.
</Callout>

And check out this video on finding the root cause for an alert (5:01 minutes):

<Video
  id="I0mg9Wcep1Q"
  type="youtube"
/>

Read on to learn the best practices for:

* Policies
* Notification channels
* Issue preferences
* Thresholds and incidents
* Muting rules

## Recommended alerts [#recommend-alerts]

Use [recommended alerts conditions](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/condition-recommendations/) if you are new to alerts or if you want suggestions that optimize your alert coverage.

## Organize your policies [#policy-practices]

A policy is a container for similar [conditions](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-alert-conditions/).

If you’re new to alerts, learn how to [create, edit, or find policies](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/create-edit-or-find-alert-policy/).

Organize your policy's scope to a single [entity](/docs/new-relic-one/use-new-relic-one/core-concepts/what-entity-new-relic/) when possible. Assign your policy to the essential team or teams that need to be notified when an [issue](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/specify-when-alerts-create-incidents/) occurs. This way, you keep policies centralized and focused.

If a team is monitoring several groups of the same entity type, combine those entity clusters (like servers) together into one policy. This way, your team can be notified from one policy rather than navigating several policies at once.

You can use alerts to monitor all of your entities. Consider your role and priorities when assigning yourself to a policy.

For example:

* **Software developers** may need [notifications](/docs/alerts/new-relic-alerts/managing-notification-channels/notification-channels-controlling-where-send-alerts) for both front-end and back-end performance, such as webpage response time and page load JavaScript errors.
* **Operations personnel** may need notifications for poor back-end performance, such as server memory and load averages.
* **The product owner** may need notifications for positive front-end performance, such as improved end user Apdex scores or sales being monitored in [dashboards](/docs/query-your-data/explore-query-data/dashboards/introduction-new-relic-one-dashboards).

## Set your condition thresholds and incidents [#threshold-practices]

Set meaningful [threshold](/docs/alerts/new-relic-alerts-beta/configuring-alert-policies/define-thresholds-trigger-alert) levels to optimize alerts for your business. Here are some suggested guidelines:

<table>
  <thead>
    <tr>
      <th width={200}>
        Action
      </th>

      <th>
        **Recommendations**
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        Set threshold levels
      </td>

      <td>
        **Avoid setting thresholds too low.** For example, if you set a CPU condition threshold of 75% for 5 minutes on your production servers, and it routinely goes over that level, this will increase the likelihood of un-actionable alerts or false positives.
      </td>
    </tr>

    <tr>
      <td>
        Experimenting with settings
      </td>

      <td>
        **You do not need to edit files or restart software**, so feel free to make quick changes to your threshold levels and adjust as necessary.
      </td>
    </tr>

    <tr>
      <td>
        Adjust settings
      </td>

      <td>
        **Adjust your conditions over time.**

        * As you use our products to help you optimize your entity's performance, tighten your thresholds to keep pace with your improved performance.
        * If you are rolling out something that you know will negatively impact your performance for a period of time, loosen your thresholds to allow for this.
      </td>
    </tr>

    <tr>
      <td>
        Disable settings
      </td>

      <td>
        You can **disable any condition** in a policy. This is useful, for example, if you want to continue using other conditions in the policy while you experiment with other metrics or thresholds.
      </td>
    </tr>
  </tbody>
</table>

In most of our products (except Infrastructure), the color-coded [health status indicator](/docs/using-new-relic/welcome-new-relic/get-started/glossary/#health-status) in the user interface changes as the alerting threshold escalates or returns to normal. This allows you to monitor a situation through our UI before a critical threshold passes, without needing to receive specific notifications about it.

There are two incident thresholds: critical (red) and warning (yellow). Define these thresholds with different criteria, keeping in mind the suggestions above.

## Decide what happens when there's no signal

Loss of signal occurs when New Relic stops receiving data for a while; technically, we detect loss of signal after a significant amount of time has elapsed since data was last received in a time series. Loss of signal can be used to trigger or resolve an incident, which you can use to set up alerts.

You can configure loss of signal settings by condition in the UI or [configure loss of signal via the NerdGraph API](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/alerts-nerdgraph/nerdgraph-api-loss-signal-gap-filling/#loss-of-signal).

## Set up an alert condition to ensure your daily batch jobs run

Assuming you're sending an event to New Relic as part of your batch job, you can set up an alert condition to notify you if your batch jobs fail to run.

1. Set up a simple count query on the event (`SELECT count(*) FROM MyBatchEvent`)
2. Set Loss of Signal to open a new incident after 24 hours and 30 minutes (you can adjust this, but it's a good idea to allow for a late-running batch job)
3. Make sure to use the Event Timer streaming aggregation method. Since you will only ever get 1 data point every 24 hours, you can set the Timer to its lowest setting, 5 seconds.

## Use non-null values when there's no signal

By default, gaps in data signals are filled with null values. In cases where you need to be able to create conditions based on those data gaps, you can fill gaps with a custom value or the last known value. You can configure this setting by condition in the UI or [configure gap filling values via NerdGraph](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/alerts-nerdgraph/nerdgraph-api-loss-signal-gap-filling/#customize)

## Define your issue preferences [#incident-practices]

Decide when you get issue notifications so you can respond to issues when they happen.

If you’re new to alerts, learn more about your [issue preferences options](https://discuss.newrelic.com/t/relic-solution-alert-incident-preferences-are-the-key-to-consistent-alert-notifications/40867).

The default issue preference setting combines all conditions within a policy into one issue. Change your default issue preference setting to increase or decrease the number of issues and issue notifications you receive.

Each team within your organization will have different needs. Ask your team two important questions when deciding your issue preferences:

* Do we want to be notified every time something goes wrong?
* Do we want to group all similar notifications together and be notified once?

When a policy and its conditions have a broader scope (like managing the performance of several entities), increase the amount of issues you receive. You will need more notifications because two issues will not necessarily relate to each other.

When a policy and its conditions have a focused scope (like managing the performance of one entity), opt for the default issue preference. You will need less notifications when two issues are related to each other or when the team is already notified and fixing an existing problem.

Decide how you get issue notifications by using our [notification channel best practices](/docs/new-relic-solutions/best-practices-guides/alerts-applied-intelligence/alerts-best-practices/#channel-practices).

## Select your notification channels [#channel-practices]

Tailor notifications to the most useful channel and policy so you can avoid alert fatigue and help the right personnel receive and respond to issues they care about in a systematic way.

If you’re new to alerts, learn how to [set up notification channels](/docs/alerts-applied-intelligence/new-relic-alerts/alert-notifications/notification-channels-control-where-send-alerts/).

Notify teams and individuals who needs to stay updated on or resolve a problem when an issue arises.

To stay updated, select a notification channel that is less intrusive, like email.

For vital notifications and issue responses, select a notification channel that is more intrusive, like PagerDuty or Slack.

Do not rely on email for quick notifications in case of delays.

## Understand muting rules [#mute-practices]

Mute alerts during routine events, such as maintenance or planned downtime.

You can also silence a policy, a specific entity, and a condition when needed. Issues can still be opened, but you won't be notified.

If you're new to alerts, learn how to [create and manage muting rules](/docs/alerts-applied-intelligence/new-relic-alerts/alert-notifications/muting-rules-suppress-notifications/).

## What's next?

To learn more about using alerts:

* Learn about the [API](/docs/alerts/rest-api-alerts/new-relic-alerts-rest-api/rest-api-calls-new-relic-alerts).
* Read technical details about [min/max limits and rules](/docs/alerts/new-relic-alerts/getting-started/minimum-maximum-values).
* Read more about about [when you might want to use loss-of-signal and gap-filling settings](https://discuss.newrelic.com/t/relic-solution-how-can-i-figure-out-when-to-use-gap-filling-and-loss-of-signal/120401).
