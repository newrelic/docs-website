---
title: "Implementation part 4: Alerting and other proactive solutions"
tags:
  - New Relic solutions
  - Best practices guides
metaDescription: "Part 4 of the New Relic implementation guide, where you think about your alerting strategies, and set up alerting, synthetic monitors, and errors inbox."
---

import alertsAlertingUi from 'images/alerts_screenshot-full_alerting-ui.webp'

import syntheticMonitorIndex from 'images/synthetic_screenshot-full_monitor-index.webp'

import errorsinboxErrorsUi from 'images/errors-inbox_screenshot-full_errors-ui.webp'

*This is the fourth and final part of [our implementation guide](/docs/new-relic-solutions/get-started/implementation-guide-intro).*

In previous implementation stages, you [instrumented your stack](/docs/new-relic-solutions/get-started/implementation-guide-instrument) and [became acquainted with the New Relic platform](/docs/new-relic-solutions/get-started/implementation-guide-organize-data). Now is a good time to think about proactive solutions that will notify you of problems early and help you avoid worst-case scenarios. In this stage, you'll learn about some important solutions in this area, including: 

* Alerting
* Synthetic monitors
* Errors inbox

## Think about your alerting strategy [#alert-strategy]

<img
  title="Alerts UI"
  alt="Alerts UI"
  src={alertsAlertingUi}
/>

<figcaption>
The New Relic alerts UI gives you a view of the state of alert conditions in an account.
</figcaption>

Before setting up alerts, we recommend spending some time thinking about your alerting goals and strategy. The larger your organization is, the more important that is.

When you don't have an alerting strategy, and instead set up alerts quickly and haphazardly to solve one-off problems, that can result in too many alert notifications going out. When that happens, your team will suffer from alert fatigue and start to ignore alerts. Spending some time thinking about your alerting strategy will ensure you set up alerts in a smart way that can scale as your organization grows or as you add more data to New Relic. 

To route alert notification messages to you, we use **workflows** (the rules of how violations create notifications and what data gets sent) and **notification destinations** (where notifications are sent). We recommend you plan out how these will be set up to be consistent and maintainable across your organization. If you're integrating with another service, such as Slack or PagerDuty, then consider how you'll control and maintain these integrations in the long term. 

Avoiding alert fatigue should be a central goal of your alerting strategy. One strategy you could apply is to categorize your alerts by severity of business impact. Those that are more severe or critical should make the loudest noise and be delivered to the stakeholders that are in a position to respond, while those that are less business-impacting should deliver more quietly, with a smaller "blast radius."

For example, you might consider defining some alert severity protocols that you can apply across the organization and use workflows to ensure the alerts are routed correctly. Teams may apply slightly different routing for each severity, but introducing a common language and understanding of impact across the organization can pay dividends as your alerting efforts scale out.

      <table>
        <thead>
          <tr>
            <th>
              Severity
            </th>

            <th>
              Impact
            </th>
            <th>
              Audience
            </th>
            <th>
              Integrations
            </th>

          </tr>
        </thead>

        <tbody>
          <tr>
            <td>
              Sev 1 / P1
            </td>
            <td>
              Critical
            </td>
            <td>
              On call SRE, C-Level Manager / Incident Commander /, Relevant Product Owner and DevOps teams
            </td>
            <td>
              Pagerduty, Slack, Email
            </td>
          </tr>

          <tr>
            <td>
              Sev 2 / P2
            </td>

            <td>
              High
            </td>
            <td>
              Relevant Product Owner and DevOps teams
            </td>
            <td>
              Pagerduty, Slack
            </td>
          </tr>

          <tr>
            <td>
              Sev 3 / P3
            </td>

            <td>
              Medium
            </td>
            <td>
              DevOps teams
            </td>
            <td>
              Slack
            </td>
          </tr>

          <tr>
            <td>
              Sandbox / Sev 4 / P4
            </td>

            <td>
              Low / None
            </td>
            <td>
              DevOps teams
            </td>
            <td>
              Sandbox Slack
            </td>
          </tr>
        </tbody>
      </table>

<figcaption>
An example of how an organization might define some alert security protocols. 
</figcaption>

In order to ensure the long-term quality of alerts, you may want to consider planning regular reviews of your alert conditions to ensure that any alert fatigue is addressed and that alerts are being correctly categorized. This will involve analyzing how often alerts fire and what the response and resolution times are. 

For ways to get started with alerting: 

* To get started quickly with setting up an alert condition and a notification destination, see [our docs on creating your first alert](/docs/alerts-applied-intelligence/new-relic-alerts/get-started/your-first-nrql-condition).
* For in-depth guidance on planning out and implementing an alerting strategy, see our [Alert quality management guide](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/alert-quality-management-guide).

Here are some docs on automating your alerts: 
* [Use our NerdGraph API to configure alerts](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/alerts-nerdgraph/nerdgraph-api-examples)
* [Configure alerts using Terraform](https://newrelic.com/blog/how-to-relic/observability-as-code-new-relic-terraform-provider)

## Synthetic monitoring [#synthetics]

Our synthetic monitoring gives you a suite of automated, scriptable tools to monitor your websites, critical business transactions, and API endpoints. These tools let you run simple monitors to check on uptime and basic functionality, or create complex scripts that mimic the actions and workflows of real users. 

To use synthetics well, your team should identify business-critical customer journeys and dependent APIs, and set up synthetic monitors to track them. Your synthetic monitor reports can be part of your workloads or other dashboards. 

<img
  title="Synthetic monitoring - Monitors index"
  alt="Synthetic monitoring - Monitors index"
  src={syntheticMonitorIndex}
/>

<figcaption>
  You can check the status and metrics of your monitors with the monitors index.
</figcaption>

To get started with synthetics, see [Introduction to synthetics](/docs/synthetics/synthetic-monitoring/getting-started/get-started-synthetic-monitoring) and [Create a monitor](/docs/synthetics/synthetic-monitoring/using-monitors/add-edit-monitors).

## Errors inbox [#errors-inbox]

Our errors inbox feature helps you proactively detect, prioritize, and take action on errors before they impact your end users. You'll receive alerts whenever a critical, customer-impacting error arises via your preferred communication channel, like Slack.

<img
  title="ui-main"
  alt="This is an image of the main errors inbox UI"
  src={errorsinboxErrorsUi}
/>
<figcaption>
The errors inbox UI lets you easily review the errors of your workloads. 
</figcaption>

To use errors inbox, you'll need to have set up some workloads. Resources for getting started: 

* Read the [errors inbox docs](/docs/errors-inbox/errors-inbox) 
* Watch [a short video on setting up errors inbox](https://www.youtube.com/watch?v=HEbX0dgeGGw)

## What's next? [#whats-next]

This guide has helped you set up a strong observability foundation, but that's only the first step in moving towards observability excellence. Next you may want to focus on learning the finer points of New Relic and optimizing your setup. Some ideas for next steps: 

* If you think you still need more instrumentation, [browse and install more observability tools](https://newrelic.com/instant-observability). 
* Read [the docs for the tools and features you're using](http://docs.newrelic.com) to learn about configuration and customization options.
* [Understand and optimize your data ingest.](/docs/data-apis/manage-data/manage-your-data) 
* Complete [a New Relic University class on querying data](https://learn.newrelic.com/writing-nrql-queries), and take [other classes](https://learn.newrelic.com).
* To go in-depth on planning out your observability goals and achieving observability excellence, see our [Observability maturity series](/docs/new-relic-solutions/observability-maturity/introduction). It includes guides for [ensuring optimal instrumentation](/docs/new-relic-solutions/observability-maturity/operational-efficiency/service-characterization-optimize-telemetry-data-guide), [observability-as-code](/docs/new-relic-solutions/observability-maturity/operational-efficiency/observability-as-code-guide), [alert quality management](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/alert-quality-management-guide), and more. 
