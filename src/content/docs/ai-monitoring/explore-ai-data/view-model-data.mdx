---
title: 'Analyze model data'
metaDescription: 'AI monitoring lets you observe the AI-layer of your tech stack, giving you a holistic overview of the health and performance of your AI-powered app.'
freshnessValidatedDate: 2024-06-12
---

AI monitoring surfaces data about your AI models so you can analyze AI model performance alongside AI app performance. You can find data about your AI models in two areas:

* <DNT>**Model inventory**</DNT>: A centralized view that shows performance and completion data about all AI models in your account. Isolate token usage, keep track of overall performance, or dig into the individual completions your models make.
* <DNT>**Compare models**</DNT>: Conduct comparative performance analysis between two models over time. This page displays data for aggregated analysis of your model performance over time.

<img
  title="Model data overview"
  alt="A screenshot of the model inventory page"
  src="/images/ai_screenshot-crop_intro-to-model-data.webp"
/>

<figcaption>
  Go to **<DNT>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring</DNT>**: From AI monitoring, you can choose between model inventory or model comparison.
</figcaption>

## Model inventory page [#model-inventory]

<img
  title="Model inventory overview"
  alt="A screenshot of the overview page when you go to Model inventory"
  src="/images/ai_screenshot-full_model-inventory-overview-page.webp"
/>

<figcaption>
  Go to **<DNT>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model inventory</DNT>**: View data about interactions with your AI model.
</figcaption>

The <DNT>model inventory</DNT> page provides insight into the overall performance and usage of your AI models. You can analyze data from calls made to your model, so you can understand how AI models affect your AI app.

From the overview tab, explore the number of requests made to a model against its response times, or analyze time series graphs to see when model behavior changed. From there, investigate the errors, performance, or cost tabs.

### Errors tab [#errors-inventory]

<img
  title="Model inventory: Errors"
  alt="A screenshot of the Errors time series and chart"
  src="/images/ai_screenshot-crop_model-inventory-errors.webp"
/>

<figcaption>
  Go to **<DNT>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model inventory > Errors</DNT>**: View data about AI model errors.
</figcaption>

The errors tab uses time series graphs and tables to organize model errors.

* <DNT>**Response errors**</DNT>: Track the number of errors in aggregate that come from your AI model.
* <DNT>**Response errors by model**</DNT>: Determine if one specific model produces more errors on average, or if one specific error is occurring across your models.
* <DNT>**Response errors by type**</DNT>: View how often certain errors appear.
* <DNT>**Errors table**</DNT>: View error type and message in context of the request and response.

### Performance tab [#performance-inventory]

<img
  title="Model inventory: Performance"
  alt="A screenshot of the Errors time series and chart"
  src="/images/ai_screenshot-crop_model-inventory-performance-page.webp"
/>

<figcaption>
  Go to **<DNT>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model inventory > Performance</DNT>**: View data about your AI model's performance.
</figcaption>

The performance tab aggregates response and request metrics across all your models. Overview the models that take the most time to process a request or create a response with the pie charts, or refer to the time series graphs to track upticks in request or response times. You can use performance charts to locate outliers across your models.

### Cost tab [#cost-inventory]

<img
  title="Model inventory: Performance"
  alt="A screenshot of the Errors time series and chart"
  src="/images/ai_screenshot-crop_model-inventory-cost.webp"
/>

<figcaption>
  Go to **<DNT>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model inventory > Cost</DNT>**: View data about your AI model's cost.
</figcaption>

The cost tab uses a combination of time series graphs and pie charts to identify cost drivers amongst your models. Determine the number of tokens came from either prompts or completions, or if certain models cost more on average than others.

* <DNT>**Tokens used and token limit**</DNT>: Evaluate how often your models approach a given token limit.
* <DNT>**Total tokens by models**</DNT>: Determine which of your models use the most tokens on average.
* <DNT>**Total usage by prompt and completion tokens**</DNT>: Understand what ratio of tokens comes from prompts your model accepts against tokens used per completion.

Understanding cost allows you to improve how your AI app uses one or more of your models so your AI toolchain is more cost effective.

## Model comparison page [#model-comparison]

<img
  title="Model inventory overview"
  alt="A screenshot of the overview page when you go to Model inventory"
  src="/images/ai_screenshot-full_ai-model-comparison-page.webp"
/>

<figcaption>
  Go to **<DNT>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model comparison</DNT>**: Compare data about the different AI models in your stack.
</figcaption>

The model comparison page organizes your AI monitoring data to help you conduct comparative analysis. This page scopes your model comparison data to a single account, giving you aggregated data about model cost and performance across one or more apps. To generate data:

1. Choose your models from the drop down.
2. Scope to one service to see performance in the context of a particular app, or keep the query to `Service = All` to see how a model behaves on average.
3. Choose time parameters. This tool is flexible: you can make comparisons across different time periods, which lets you see how performance or cost changed before and after a deployment.

### Compare model performance [#compare-performance]

<img
  title="Model comparison page: Performance"
  alt="A screenshot of model comparison"
  src="/images/ai_screenshot-crop_model-comparison-performance.webp"
/>

<figcaption>
  Go to **<DNT>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model comparison</DNT>**: Compare performance between different AI models in your stack.
</figcaption>

To start conducting comparative analysis, choose a specific service, model, and time range. When you compare models, you can evaluate different metrics aggregated over time, depending on your own settings. Here are some use case examples for comparative analysis:

* **Compare two models in the same service**: Service X uses model A during week one, but then uses model B on week two. You can compare performance by choosing service X, selecting model A, and setting the dates for week one. On the second side, choose service X, select model B, and set the dates for week two.
* **Compare the performance of one model over time**: Select choose service X, select model A, and set the dates for week one. On the second side, choose service X, select model A, and set the dates for week two.
* **Evaluate model performance in two different services**: You have two different applications that use two different models. To compare the total tokens during the last month, choose the relevant parameters for the specific service and specific models, then set the dates for the same time frame.
* **Compare two models**: You have application that uses model A and you want to measure model A against model B. For every user's prompt, you call model B as a background process. Compare how model A performs relative to model B on the same service during the same time frame.

### Compare model cost [#compare-cost]

<img
  title="Model comparison page: Cost"
  alt="A screenshot of model comparison"
  src="/images/ai_screenshot-crop_model-comparison-cost.webp"
/>

<figcaption>
  Go to **<DNT>[one.newrelic.com](https://one.newrelic.com) > All Capabilities > AI monitoring > Model comparison</DNT>**: Compare cost between different AI models in your stack.
</figcaption>

The model cost column breaks down completion events into two parts: the prompt given to the model and the final response the model delivers to the end user.

* <DNT>**Tokens per completion**</DNT>: The token average for all completion events.
* <DNT>**Prompt tokens**</DNT>: The token average for prompts. This token average includes prompts created by prompt engineers and end users.
* <DNT>**Completion tokens**</DNT>: The number of tokens consumed by the model when it generates the response delivered to the end user.

When analyzing this column, the value for completion tokens and prompt tokens should equal the value in tokens per completion.

## What's next? [#whats-next]

Now that you know how to find your data, you can explore other features that AI monitoring has to offer.

* Want to analyze the performance of your AI app? Check out our doc about [the AI app response pages](/docs/ai-monitoring/explore-ai-data/view-ai-responses).
* Concerned about sensitive information? [Learn to set up drop filters](/docs/ai-monitoring/drop-sensitive-data).
* If you want forward user feedback information about your app's AI responses to New Relic, follow our procedures to [update your app code to get user feedback in the UI](/docs/ai-monitoring/customize-agent-ai-monitoring).
