---
title: Respond to service outages
metaDescription: Learn how to monitor your system so you can quickly identify and resolve many error occurrences fast. 
---

import allEntitiesView from 'images/apm_screenshot-crop_all_entities.webp'

import apiGateway from 'images/apm_screenshot-crop_api-gateway-summary.webp'

import apmErrorsInboxPage from 'images/apm_screenshot-crop_errors-inbox-page.webp'

import apmErrorsAnomaly from 'images/apm_screenshot-crop_errors-anomaly.webp'

Errors are bound to happen. Even with an observability tool, finding the source of an error isn't as straightforward as you might assume. Think about a flooded yard. You notice water flowing near your hose, but the cause of the flood is actually a crack somewhere in your water main. If you assumed that the leaking hose caused the flood, you'd end up with a fixed hose but a ruined lawn: a costly mistake. 

Error analysis takes you to the source of the issue so you can fix the issue before the flood happens. When a team makes a new deployment or a service fails upstream, you need to dig deeper before implementing any solutions. There's no room for assumptions in error analysis.

## Objectives [#objective]

This tutorial series shows you how to solve critical errors, then guides you into reducing your overall error count. This doc covers:

* Choosing a service to begin error analysis
* Choosing an error group that indicates an outage 

## Prerequisites [#prereq]

To monitor your application's performance, you'll use an agent created specifically for your app's language. Clicking a logo sends you to New Relic platform where you'll be guided through installing and configuring the agent.

<TechTileGrid>
  
  <TechTile
    name="Go agent"
    icon="logo-go"
    to="https://one.newrelic.com/nr1-core?state=985d4005-ba90-a8c7-1da1-2af34539b03b"
  />

  <TechTile
    name="Java agent"
    icon="logo-java"
    to="https://one.newrelic.com/nr1-core?state=80d18bcb-4919-1fcb-2b77-9406838eb916"
  />

  <TechTile
    name=".NET agent"
    icon="logo-dotnet"
    to="https://one.newrelic.com/nr1-core?state=30e93090-6dfa-6b70-8e75-472f54414355"
  />

  <TechTile
    name="Node.js agent"
    icon="logo-nodejs"
    to="https://one.newrelic.com/marketplace/install-data-source?state=be2e62fa-cc3b-c428-27c4-8d662c9e80a1"
  />

  <TechTile
    name="PHP agent"
    icon="logo-php"
    to="https://one.newrelic.com/nr1-core?state=aa633b41-72d4-009c-3abf-55dcf64894fe"
  />

  <TechTile
    name="Python agent"
    icon="logo-python"
    to="https://one.newrelic.com/nr1-core?state=20fda75b-58fb-a92a-f9e1-7b052035c6e8"
  />

  <TechTile
    name="Ruby agent"
    icon="logo-ruby"
    to="https://one.newrelic.com/nr1-core?state=d69143ab-605c-579b-25bf-cc6e5fee5b80"
  />

</TechTileGrid>

Once you've installed an agent, go to **[one.newrelic.com](https://one.newrelic.com/nr1-core?filters=(domain%3D'APM'ANDtype%3D'APPLICATION'))** and select your app. If you don't see much data just yet, step away for a while and let the agent gather real-time data as your application runs.

## Find the critical error [#critical-error]

Now that your apps are instrumented, New Relic is capturing data about your services. This includes data about error occurrences in your app. 

<Steps>
    <Step>
        ### Think about the end user

Alerts let you know a problem exists: they're the water on your lawn. But alerts won't provide you all the context. That's where your errors inbox comes in. 

Imagine you're responsible for some apps on an ecommerce site. You've received two alerts for two components, one for checking out and one for searching inventory. You're only receiving reports that the search functionality is failing for end users, but the check out component works fine. You may believe that the check out function is more important, but it's critical to separate your beliefs from your observability practices. 

This practice applies even if the end user hasn't reported anything. When you notice services failing, you can ask yourself these questions:

* Is the end user experiencing a problem? 
* How should their experience look?
* What behavior are they currently experiencing?

    </Step>
    <Step>
        ### Choose a service

Let's see how this might look in practice. When you view the **All entities** page, you notice four services are alerting.

<img
title="Overview errors affecting your services"
alt="A screenshot showing an app with many errors"
src={allEntitiesView}
/>

After asking yourself the questions from step one, you know that: 

* The end user is struggling with purchase actions.
* Your site should only display in-stock items.
* Your site is displaying all products, so customers are able to purchase out-of-stock items.

After performing your audit, you know that `api-gateway` is a critical dependency for your inventory. This is where you'll begin your error analysis. 

    </Step>
    <Step>

### Locate what changed [#source] 

You have your entry point into your system, so now you can look into the errors affecting your app. From the `api-gateway` summary page, click the **Errors** tab under **Triage**. Your errors page filters your data to an errors-only view. 

<img
title="Overview errors affecting your services"
alt="A screenshot showing an app with many errors"
src={apmErrorsInboxPage}
/>

`api-gateway` has at least six error groups reporting with anywhere from a dozen to thousands of occurrences in your app. Don't make any assumptions about what error is critical. Instead, start with the time series on the far right of the waterfall view. 

At first this seems to lack granularity, but your time series gives you enough information to point to what changed over time. We'll break this down:

* Based on number of occurrences alone, your first instinct might tell you to start with `ActivemModel:::ValidationError` as it has 4,000 occurrences. If you look at the time series, though, its peaks and troughs are relatively consistent. This could be an expected error, but let's look at the other five. 
* The `Net::OpenTimeOut` error group has a similar pattern, and it actually makes up four of the six reporting groups. Across each error group, you can see consistent peaks and troughs that extend before the incident. With the same name and similar patterns, we can infer this is an expected error as well.
* Our last option is `JsonapiClient:::Notfound`. Like `ActivemModel:::ValidationError`, it has a distinct shape and is consistently reporting. While it doesnâ€™t have many occurrences, the timeseries is anomalous enough that it might be worth digging a bit deeper. 
    </Step>
    <Step>
    ### Adjust the timeseries [#timeseries]

To be certain, adjust the time parameter to show patterns from the last 12 hours:

<img
title="Overview errors affecting your services"
alt="A screenshot showing an app with many errors"
src={apmErrorsAnomaly}
/>

With the adjustment, you see that `ActivemModel:::ValidationError`has an unchanging pattern of peaks and troughs, but your  `JsonapiClient:::Notfound` has a dramatic change in behavior. This is a good ground zero. 

Knowing when something happened is a critical piece for getting closer to the source. Having a complete understanding of the problem space, you can now dig into the source. 
</Step>
</Steps>

<UserJourneyControls
    nextStep={{path: "/docs/tutorial-errors/solve-critical-errors", title: "Next step", body: "After you've selected your error groups, the errors summary page displays attribute data about failures in your system."}}
/>
