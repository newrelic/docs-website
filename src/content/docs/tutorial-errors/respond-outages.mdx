---
title: Respond to service outages
metaDescription: Learn how to monitor your system so you can quickly identify and resolve many error occurrences fast. 
---

import allEntitiesView from 'images/apm_screenshot-crop_all_entities.webp'

import apiGateway from 'images/apm_screenshot-crop_api-gateway-summary.webp'

import apmErrorsInboxPage from 'images/apm_screenshot-crop_errors-inbox-page.webp'

import apmErrorsAnomaly from 'images/apm_screenshot-crop_errors-anomaly.webp'

Errors are bound to happen. Even with an observability tool, finding the source of an error isn't as straightforward as you might assume. Think about a flooded yard: you notice water flowing near your hose bib, but the cause of the flood is actually a crack somewhere in your water main. If you assumed that the leaking hose caused the flood, you'd end up with a fixed hose but a ruined lawn. The mistake would be costly. 

Error analysis is similar. When a team makes a new deployment or a service fails upstream, you need to dig deeper before implementing any solutions. There's no room for assumptions in error analysis.

## Objectives [#objective]

This tutorial series shows you how to solve critical errors, then guides you into reducing your overall error count. This doc covers these concepts:

* Choosing a service to begin error analysis
* Choosing an error group that indicates an outage 

## Prerequisites [#prereq]

To monitor your application's performance, you will use an agent created specifically for your app's language.

* Click a logo to install an agent. This will send you to the New Relic platform where you will be guided through installing and configuring the agent.

<TechTileGrid>
  
  <TechTile
    name="Go agent"
    icon="logo-go"
    to="https://one.newrelic.com/nr1-core?state=985d4005-ba90-a8c7-1da1-2af34539b03b"
  />

  <TechTile
    name="Java agent"
    icon="logo-java"
    to="https://one.newrelic.com/nr1-core?state=80d18bcb-4919-1fcb-2b77-9406838eb916"
  />

  <TechTile
    name=".NET agent"
    icon="logo-dotnet"
    to="https://one.newrelic.com/nr1-core?state=30e93090-6dfa-6b70-8e75-472f54414355"
  />

  <TechTile
    name="Node.js agent"
    icon="logo-nodejs"
    to="https://one.newrelic.com/marketplace/install-data-source?state=be2e62fa-cc3b-c428-27c4-8d662c9e80a1"
  />

  <TechTile
    name="PHP agent"
    icon="logo-php"
    to="https://one.newrelic.com/nr1-core?state=aa633b41-72d4-009c-3abf-55dcf64894fe"
  />

  <TechTile
    name="Python agent"
    icon="logo-python"
    to="https://one.newrelic.com/nr1-core?state=20fda75b-58fb-a92a-f9e1-7b052035c6e8"
  />

  <TechTile
    name="Ruby agent"
    icon="logo-ruby"
    to="https://one.newrelic.com/nr1-core?state=d69143ab-605c-579b-25bf-cc6e5fee5b80"
  />

</TechTileGrid>

Once you've installed an agent, go to **[one.newrelic.com](https://one.newrelic.com/nr1-core?filters=(domain%3D'APM'ANDtype%3D'APPLICATION'))** and select your app. If you don't see much data just yet, step away for a while and let the agent gather real-time data as your application runs.

* [Integrate your logs](/docs/logs/get-started/get-started-log-management/#integrate-logs). The APM agent provides some logging details with logs in context, but certain key attributes could be missing from your log details. We recommend that you invest some time in setting up our full logs solution to expedite the error analysis process.

## Find the critical error [#critical-error]

With your apps instrumented, New Relic can capture data about your services. This includes data about error occurrences in your app. 

<Steps>
    <Step>
        ### Think about the end user

Similar to our flood metaphor, it's likely that when an outage occurs, the alerting services only indicate a problem exists somewhere. When you first receive an alert, you won't have enough context to make a decision about where to start in your system. 

For example, imagine two services that have begun alerting, one for logging in and one for searching inventory. You're only receiving reports that the search functionality is failing for end users, but the login function works fine. You know in this case that, while logging in is critical to your funnel, you should prioritize the service that directly affects your end user. You may believe that the log in function is more important, but it's critical to separate your beliefs from your observability practices. 

This practice applies even if the end user hasn't reported anything. When you notice services failing, you can ask yourself these questions:

* Is the end user experiencing a problem? 
* How should their experience look?
* What behavior are they currently experiencing?

    </Step>
    <Step>
        ### Choose a service

Let's see how this might look in practice. When you view the **All entities** page, you notice four services are alerting.

<img
title="Overview errors affecting your services"
alt="A screenshot showing an app with many errors"
src={allEntitiesView}
/>

After asking yourself the questions from step one, you know that: 

* The end user is struggling with purchase actions.
* Your site should only display in-stock items.
* Your site is displaying all products, so customers are able to purchase out-of-stock items.

Having worked out the nature of the problem, you know that `api-gateway` is a dependency for other services that keep track of your inventory. This is the start of your observability journey. 

    </Step>
    <Step>

### Locate what changed [#source] 

You have your entry point into your system, so now you can look into the errors affecting your app. From the `api-gateway` summary page, click the **Errors** tab under **Triage**. Your errors page filters your data to an errors-only view. 

<img
title="Overview errors affecting your services"
alt="A screenshot showing an app with many errors"
src={apmErrorsInboxPage}
/>

`api-gateway` has least six error groups reporting with anywhere from a dozen to thousands of occurrences in your app. Don't make any assumptions about what error is critical. Instead, start with the time series on the far right of the waterfall view. 

At first this seems to lack granularity, but your time series gives you enough information to point to what changed over time. We'll break this down:

* Based on number of occurrences alone, your first instinct might tell you to start with `ActivemModel:::ValidationError` as it has 4,000 occurrences. If you look at the time series, though, its peaks and troughs are relatively consistent. This could be an expected error, but let's look at the other five. 
* The `Net::OpenTimeOut` error group has a similar pattern, and it actually makes up four of the six reporting groups. Across each error group, you can see consistent peaks and troughs that extend before the incident. With the same name and similar patterns, we can infer this is an expected error as well.
* Our last option is `JsonapiClient:::Notfound`. Like `ActivemModel:::ValidationError`, it has a distinct shape and is consistently reporting. While it doesnâ€™t have many occurrences, the timeseries is anomalous enough that it might be worth digging a bit deeper. 
    </Step>
    <Step>
    ### Adjust the timeseries [#timeseries]

To be certain, adjust the time parameter from the last 12 hours:

<img
title="Overview errors affecting your services"
alt="A screenshot showing an app with many errors"
src={apmErrorsAnomaly}
/>

With the adjustment, you see that `ActivemModel:::ValidationError`has an unchanging pattern of peaks and troughs, but your  `JsonapiClient:::Notfound` has a dramatic change in behavior. This is a good ground zero. 

Knowing when something happened is a critical piece for getting closer to the source. Having a complete understanding of the problem space, we can now dig into the source. 
</Step>
</Steps>

<UserJourneyControls
    nextStep={{path: "/docs/tutorial-errors/solve-critical-errors", title: "Next step", body: "After you've selected your error groups, the errors summary page displays attribute data about failures in your system."}}
/>
