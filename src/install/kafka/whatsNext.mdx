---
componentType: default
headingText: Metrics collected by the integration
---

## Configuration options [#configuration-options]

You can edit and change the configuration in the integration's YAML config file, `kafka-config.yml`. An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The configuration file has common settings applicable to all integrations like `interval`, `timeout`, `inventory_source`. To read all about these common settings see the [On-host integrations: Standard configuration format](/docs/infrastructure/host-integrations/infrastructure-integrations-sdk/specifications/host-integrations-standard-configuration-format/#configuration-basics) document.

<Callout variant="important">
  If you are still using our legacy configuration and definition files, refer to this [document](/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integrations-standard-configuration-format/) for help.
</Callout>

As with other integrations, one `kafka-config.yml` configuration file can have many instances of the integration collecting different brokers, consumers, and producers metrics. You can see configuration examples with one or multiple instances in the [`kafka-config.yml` sample files](#examples)

You can define specific settings related to Kafka in the `env` section of each instance in the `kafka-config.yml` configuration file. These settings control the connection to your Brokers, Zookeeper, and JMX as well as other security settings and features. You can see the list of valid settings in the [Kafka configuration settings](/docs/infrastructure/host-integrations/host-integrations-list/kafka/kafka-config) document.

The integration has 2 modes of operation on each instance, which are mutually exclusive, that you can set up with the `CONSUMER_OFFSET` parameter:

* Consumer offset collection: set `CONSUMER_OFFSET = true` to collect [`KafkaOffsetSample`](/docs/infrastructure/host-integrations/host-integrations-list/kafka/kafka-config/#KafkaOffsetSample-collection).

* Core collection mode: set `CONSUMER_OFFSET = false` to collect the rest of the samples: [`KafkaBrokerSample`, `KafkaTopicSample`](/docs/infrastructure/host-integrations/host-integrations-list/kafka/kafka-config/#broker-collection), [`KafkaProducerSample`, `KafkaConsumerSample`](/docs/infrastructure/host-integrations/host-integrations-list/kafka/kafka-config/#KafkaConsumerSample-collection).

<Callout variant="important">
  These modes are mutually exclusive because consumer offset collection takes a long time to run and has high performance requirements, in order to collect both groups of samples, set 2 instances, one with each mode.
</Callout>

You can define the values for these settings in several ways:

* Adding the value directly in the config file. This is the most common way.

* Replacing the values from environment variables using the `{{ }}` notation. Read more about [using environment variable passthroughs with on-host integrations](/docs/infrastructure/install-infrastructure-agent/configuration/configure-infrastructure-agent/#passthrough) or see the example for [environment variables replacement](/docs/infrastructure/host-integrations/host-integrations-list/elasticsearch/elasticsearch-integration#envvar-replacement).

* Using secrets management. Use this to protect sensitive information, such as passwords that would be exposed in plain text on the configuration file. For more information, see [secrets management](/docs/integrations/host-integrations/installation/secrets-management).

### Offset monitoring [#offset-monitoring]

When setting `CONSUMER_OFFSET = true`, by default, only the metrics from consumer groups with active consumers (and consumer metrics) will be collected.
To also collect the metrics from consumer groups with inactive consumers you must set `INACTIVE_CONSUMER_GROUP_OFFSET = true`.

When a consumer group is monitoring more than one topic, it's valuable to have consumer group metrics separated by topics, specially if one of the topics have inactive consumers, because then it's possible to spot in which topic the consumer group is having lag and if there are active consumers for that consumer group and topic.

To get consumer group metrics separated by topic, you must set `CONSUMER_GROUP_OFFSET_BY_TOPIC` to `true` (default is `false`).

For more on how to set up offset monitoring, see [Configure `KafkaOffsetSample` collection](/docs/infrastructure/host-integrations/host-integrations-list/kafka/kafka-config/#KafkaOffsetSample-collection).

<Callout variant="important">
  See [Kafka configuration settings](/docs/infrastructure/host-integrations/host-integrations-list/kafka/kafka-config/) for more information.
</Callout>

## `kafka-config.yml` sample files [#examples]

See these examples of the `kafka-config.yml` file.

<CollapserGroup>
  <Collapser
    id="zookeeper"
    title="Zookeeper discovery"
  >
    This configuration collects Metrics and Inventory including all topics discovering the brokers from two different JMX hosts :

    ```yml
    integrations:
      - name: nri-kafka
        env:
          CLUSTER_NAME: testcluster1
          KAFKA_VERSION: "1.0.0"
          AUTODISCOVER_STRATEGY: zookeeper
          ZOOKEEPER_HOSTS: '[{"host": "localhost", "port": 2181}, {"host": "localhost2", "port": 2181}]'
          ZOOKEEPER_PATH: "/kafka-root"
          DEFAULT_JMX_USER: username
          DEFAULT_JMX_PASSWORD: password
          TOPIC_MODE: all
        interval: 15s
        labels:
          env: production
          role: kafka
        inventory_source: config/kafka
    ```
  </Collapser>

  <Collapser
    id="zookeeper-jmx-ssl"
    title="Zookeeper discovery with SSL based JMX connection"
  >
    This configuration collects Metrics and Inventory discovering the brokers from a JMX host with SSL :

    ```yml
    integrations:
      - name: nri-kafka
        env:
          CLUSTER_NAME: testcluster1
          KAFKA_VERSION: "1.0.0"
          AUTODISCOVER_STRATEGY: zookeeper
          ZOOKEEPER_HOSTS: '[{"host": "localhost", "port": 2181}]'
          ZOOKEEPER_PATH: "/kafka-root"
          DEFAULT_JMX_USER: username
          DEFAULT_JMX_PASSWORD: password

          KEY_STORE: "/path/to/your/keystore"
          KEY_STORE_PASSWORD: keystore_password
          TRUST_STORE: "/path/to/your/truststore"
          TRUST_STORE_PASSWORD: truststore_password

          TIMEOUT: 10000  #The timeout for individual JMX queries in milliseconds.
        interval: 15s
        labels:
          env: production
          role: kafka
        inventory_source: config/kafka
    ```
  </Collapser>

  <Collapser
    id="bootstrap"
    title="Bootstrap discovery"
  >
    This configuration collects Metrics and Inventory including all topics discovering the brokers from one bootstrap broker :

    ```yml
    integrations:
      - name: nri-kafka
        env:
          CLUSTER_NAME: testcluster1
          AUTODISCOVER_STRATEGY: bootstrap
          BOOTSTRAP_BROKER_HOST: localhost
          BOOTSTRAP_BROKER_KAFKA_PORT: 9092
          BOOTSTRAP_BROKER_KAFKA_PROTOCOL: PLAINTEXT
          BOOTSTRAP_BROKER_JMX_PORT: 9999  # This same port will be used to connect to all discover broker JMX
          BOOTSTRAP_BROKER_JMX_USER: admin
          BOOTSTRAP_BROKER_JMX_PASSWORD: password

          LOCAL_ONLY_COLLECTION: false

          COLLECT_BROKER_TOPIC_DATA: true
          TOPIC_MODE: "all"
          COLLECT_TOPIC_SIZE: false
        interval: 15s
        labels:
          env: production
          role: kafka
        inventory_source: config/kafka
    ```
  </Collapser>

  <Collapser
    id="bootstrap-tls"
    title="Bootstrap discovery TLS"
  >
    This configuration collects only Metrics discovering the brokers from one bootstrap broker listening with TLS protocol :

    ```yml
    integrations:
      - name: nri-kafka
        env:
          METRICS: true
          CLUSTER_NAME: testcluster1
          AUTODISCOVER_STRATEGY: bootstrap
          BOOTSTRAP_BROKER_HOST: localhost
          BOOTSTRAP_BROKER_KAFKA_PORT: 9092
          BOOTSTRAP_BROKER_KAFKA_PROTOCOL: SSL
          BOOTSTRAP_BROKER_JMX_PORT: 9999
          BOOTSTRAP_BROKER_JMX_USER: admin
          BOOTSTRAP_BROKER_JMX_PASSWORD: password

          # Kerberos authentication arguments
          TLS_CA_FILE: "/path/to/CA.pem"
          TLS_CERT_FILE: "/path/to/cert.pem"
          TLS_KEY_FILE: "/path/to/key.pem"
          TLS_INSECURE_SKIP_VERIFY: false
        interval: 15s
        labels:
          env: production
          role: kafka
        inventory_source: config/kafka
    ```
  </Collapser>

  <Collapser
    id="boostrap-kerberos"
    title="Bootstrap discovery kerberos auth"
  >
    This configuration collects only Metrics discovering the brokers from one bootstrap broker in a Kerberos Auth Cluster :

    ```yml
    integrations:
      - name: nri-kafka
        env:
          METRICS: true
          CLUSTER_NAME: testcluster1
          AUTODISCOVER_STRATEGY: bootstrap
          BOOTSTRAP_BROKER_HOST: localhost
          BOOTSTRAP_BROKER_KAFKA_PORT: 9092
          BOOTSTRAP_BROKER_KAFKA_PROTOCOL: PLAINTEXT # Currently support PLAINTEXT and SSL
          BOOTSTRAP_BROKER_JMX_PORT: 9999
          BOOTSTRAP_BROKER_JMX_USER: admin
          BOOTSTRAP_BROKER_JMX_PASSWORD: password

          # Kerberos authentication arguments
          SASL_MECHANISM: GSSAPI
          SASL_GSSAPI_REALM: SOMECORP.COM
          SASL_GSSAPI_SERVICE_NAME: Kafka
          SASL_GSSAPI_USERNAME: kafka
          SASL_GSSAPI_KEY_TAB_PATH: /etc/newrelic-infra/kafka.keytab
          SASL_GSSAPI_KERBEROS_CONFIG_PATH: /etc/krb5.conf
          SASL_GSSAPI_DISABLE_FAST_NEGOTIATION: false
        interval: 15s
        labels:
          env: production
          role: kafka
        inventory_source: config/kafka
    ```
  </Collapser>

  <Collapser
    id="zookeeper-topic-bucket"
    title="Zookeeper dicsovery topic bucket"
  >
    This configuration collects Metrics splitting topic collection between 3 different instances:

    ```yml
    integrations:
      - name: nri-kafka
        env:
          METRICS: true
          CLUSTER_NAME: testcluster1
          KAFKA_VERSION: "1.0.0"
          AUTODISCOVER_STRATEGY: zookeeper
          ZOOKEEPER_HOSTS: '[{"host": "host1", "port": 2181}]'
          ZOOKEEPER_AUTH_SECRET: "username:password"
          ZOOKEEPER_PATH: "/kafka-root"
          DEFAULT_JMX_USER: username
          DEFAULT_JMX_PASSWORD: password
          TOPIC_MODE: regex
          TOPIC_REGEX: 'topic\d+'
          TOPIC_BUCKET: '1/3'
        interval: 15s
        labels:
          env: production
          role: kafka
        inventory_source: config/kafka
      - name: nri-kafka
        env:
          METRICS: true
          CLUSTER_NAME: testcluster2
          KAFKA_VERSION: "1.0.0"
          AUTODISCOVER_STRATEGY: zookeeper
          ZOOKEEPER_HOSTS: '[{"host": "host2", "port": 2181}]'
          ZOOKEEPER_AUTH_SECRET: "username:password"
          ZOOKEEPER_PATH: "/kafka-root"
          DEFAULT_JMX_USER: username
          DEFAULT_JMX_PASSWORD: password
          TOPIC_MODE: regex
          TOPIC_REGEX: 'topic\d+'
          TOPIC_BUCKET: '2/3'
        interval: 15s
        labels:
          env: production
          role: kafka
        inventory_source: config/kafka
      - name: nri-kafka
        env:
          METRICS: true
          CLUSTER_NAME: testcluster3
          KAFKA_VERSION: "1.0.0"
          AUTODISCOVER_STRATEGY: zookeeper
          ZOOKEEPER_HOSTS: '[{"host": "host3", "port": 2181}]'
          ZOOKEEPER_AUTH_SECRET: "username:password"
          ZOOKEEPER_PATH: "/kafka-root"
          DEFAULT_JMX_USER: username
          DEFAULT_JMX_PASSWORD: password
          TOPIC_MODE: regex
          TOPIC_REGEX: 'topic\d+'
          TOPIC_BUCKET: '3/3'
        interval: 15s
        labels:
          env: production
          role: kafka
        inventory_source: config/kafka
    ```
  </Collapser>

  <Collapser
    id="java-consumer-producer"
    title="Java consumer and producer"
  >
    This gives an example for collecting JMX metrics from Java consumers and producers:

    ```yml
    integrations:
      - name: nri-kafka
        env:
          METRICS: "true"
          CLUSTER_NAME: "testcluster3"
          PRODUCERS: '[{"host": "localhost", "port": 24, "username": "me", "password": "secret"}]'
          CONSUMERS: '[{"host": "localhost", "port": 24, "username": "me", "password": "secret"}]'
          DEFAULT_JMX_HOST: "localhost"
          DEFAULT_JMX_PORT: "9999"
        interval: 15s
        labels:
          env: production
          role: kafka
        inventory_source: config/kafka
    ```
  </Collapser>

  <Collapser
    id="consumer-offset"
    title="Consumer offset"
  >
    This configuration collects consumer offset Metrics and Inventory for the cluster:

    ```yml
    integrations:
      - name: nri-kafka
        env:
          CONSUMER_OFFSET: true
          CLUSTER_NAME: testcluster3
          AUTODISCOVER_STRATEGY: bootstrap
          BOOTSTRAP_BROKER_HOST: localhost
          BOOTSTRAP_BROKER_KAFKA_PORT: 9092
          BOOTSTRAP_BROKER_KAFKA_PROTOCOL: PLAINTEXT
          # A regex pattern that matches the consumer groups to collect metrics from
          CONSUMER_GROUP_REGEX: '.*'
        interval: 15s
        labels:
          env: production
          role: kafka
        inventory_source: config/kafka
    ```
  </Collapser>
</CollapserGroup>


## Metrics collected by the integration [#metrics]

The Kafka integration collects the following metrics. Each metric name is prefixed with a category indicator and a period, such as `broker.` or `consumer.`.

<CollapserGroup>
  <Collapser
    id="broker-sample"
    title={<><InlineCode>KafkaBrokerSample</InlineCode> event</>}
  >
    <table>
      <thead>
        <tr>
          <th style={{ width: "350px" }}>
            Metric
          </th>

          <th>
            Description
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `broker.bytesWrittenToTopicPerSecond`
          </td>

          <td>
            Number of bytes written to a topic by the broker per second.
          </td>
        </tr>

        <tr>
          <td>
            `broker.IOInPerSecond`
          </td>

          <td>
            Network IO into brokers in the cluster in bytes per second.
          </td>
        </tr>

        <tr>
          <td>
            `broker.IOOutPerSecond`
          </td>

          <td>
            Network IO out of brokers in the cluster in bytes per second.
          </td>
        </tr>

        <tr>
          <td>
            `broker.logFlushPerSecond`
          </td>

          <td>
            Log flush rate.
          </td>
        </tr>

        <tr>
          <td>
            `broker.messagesInPerSecond`
          </td>

          <td>
            Incoming messages per second.
          </td>
        </tr>

        <tr>
          <td>
            `follower.requestExpirationPerSecond`
          </td>

          <td>
            Rate of request expiration on followers in evictions per second.
          </td>
        </tr>

        <tr>
          <td>
            `net.bytesRejectedPerSecond`
          </td>

          <td>
            Rejected bytes per second.
          </td>
        </tr>

        <tr>
          <td>
            `replication.isrExpandsPerSecond`
          </td>

          <td>
            Rate of replicas joining the ISR pool.
          </td>
        </tr>

        <tr>
          <td>
            `replication.isrShrinksPerSecond`
          </td>

          <td>
            Rate of replicas leaving the ISR pool.
          </td>
        </tr>

        <tr>
          <td>
            `replication.leaderElectionPerSecond`
          </td>

          <td>
            Leader election rate.
          </td>
        </tr>

        <tr>
          <td>
            `replication.uncleanLeaderElectionPerSecond`
          </td>

          <td>
            Unclean leader election rate.
          </td>
        </tr>

        <tr>
          <td>
            `replication.unreplicatedPartitions`
          </td>

          <td>
            Number of unreplicated partitions.
          </td>
        </tr>

        <tr>
          <td>
            `request.avgTimeFetch`
          </td>

          <td>
            Average time per fetch request in milliseconds.
          </td>
        </tr>

        <tr>
          <td>
            `request.avgTimeMetadata`
          </td>

          <td>
            Average time for metadata request in milliseconds.
          </td>
        </tr>

        <tr>
          <td>
            `request.avgTimeMetadata99Percentile`
          </td>

          <td>
            Time for metadata requests for 99th percentile in milliseconds.
          </td>
        </tr>

        <tr>
          <td>
            `request.avgTimeOffset`
          </td>

          <td>
            Average time for an offset request in milliseconds.
          </td>
        </tr>

        <tr>
          <td>
            `request.avgTimeOffset99Percentile`
          </td>

          <td>
            Time for offset requests for 99th percentile in milliseconds.
          </td>
        </tr>

        <tr>
          <td>
            `request.avgTimeProduceRequest`
          </td>

          <td>
            Average time for a produce request in milliseconds.
          </td>
        </tr>

        <tr>
          <td>
            `request.avgTimeUpdateMetadata`
          </td>

          <td>
            Average time for a request to update metadata in milliseconds.
          </td>
        </tr>

        <tr>
          <td>
            `request.avgTimeUpdateMetadata99Percentile`
          </td>

          <td>
            Time for update metadata requests for 99th percentile in milliseconds.
          </td>
        </tr>

        <tr>
          <td>
            `request.clientFetchesFailedPerSecond`
          </td>

          <td>
            Client fetch request failures per second.
          </td>
        </tr>

        <tr>
          <td>
            `request.fetchTime99Percentile`
          </td>

          <td>
            Time for fetch requests for 99th percentile in milliseconds.
          </td>
        </tr>

        <tr>
          <td>
            `request.handlerIdle`
          </td>

          <td>
            Average fraction of time the request handler threads are idle.
          </td>
        </tr>

        <tr>
          <td>
            `request.produceRequestsFailedPerSecond`
          </td>

          <td>
            Failed produce requests per second.
          </td>
        </tr>

        <tr>
          <td>
            `request.produceTime99Percentile`
          </td>

          <td>
            Time for produce requests for 99th percentile.
          </td>
        </tr>

        <tr>
          <td>
            `topic.diskSize`
          </td>

          <td>
            Topic disk size per broker and per topic. Only present if `COLLECT_TOPIC_SIZE` is enabled.
          </td>
        </tr>

        <tr>
          <td>
            `topic.offset`
          </td>

          <td>
            Topic offset per broker and per topic. Only present if `COLLECT_TOPIC_OFFSET` is enabled.
          </td>
        </tr>
      </tbody>
    </table>
  </Collapser>

  <Collapser
    id="consumer-sample"
    title={<><InlineCode>KafkaConsumerSample</InlineCode> event</>}
  >
    <table>
      <thead>
        <tr>
          <th style={{ width: "350px" }}>
            Metric
          </th>

          <th>
            Description
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `consumer.avgFetchSizeInBytes`
          </td>

          <td>
            Average number of bytes fetched per request for a specific topic.
          </td>
        </tr>

        <tr>
          <td>
            `consumer.avgRecordConsumedPerTopic`
          </td>

          <td>
            Average number of records in each request for a specific topic.
          </td>
        </tr>

        <tr>
          <td>
            `consumer.avgRecordConsumedPerTopicPerSecond`
          </td>

          <td>
            Average number of records consumed per second for a specific topic in records per second.
          </td>
        </tr>

        <tr>
          <td>
            `consumer.bytesInPerSecond`
          </td>

          <td>
            Consumer bytes per second.
          </td>
        </tr>

        <tr>
          <td>
            `consumer.fetchPerSecond`
          </td>

          <td>
            The minimum rate at which the consumer sends fetch requests to a broke in requests per second.
          </td>
        </tr>

        <tr>
          <td>
            `consumer.maxFetchSizeInBytes`
          </td>

          <td>
            Maximum number of bytes fetched per request for a specific topic.
          </td>
        </tr>

        <tr>
          <td>
            `consumer.maxLag`
          </td>

          <td>
            Maximum consumer lag.
          </td>
        </tr>

        <tr>
          <td>
            `consumer.messageConsumptionPerSecond`
          </td>

          <td>
            Rate of consumer message consumption in messages per second.
          </td>
        </tr>

        <tr>
          <td>
            `consumer.offsetKafkaCommitsPerSecond`
          </td>

          <td>
            Rate of offset commits to Kafka in commits per second.
          </td>
        </tr>

        <tr>
          <td>
            `consumer.offsetZooKeeperCommitsPerSecond`
          </td>

          <td>
            Rate of offset commits to ZooKeeper in writes per second.
          </td>
        </tr>

        <tr>
          <td>
            `consumer.requestsExpiredPerSecond`
          </td>

          <td>
            Rate of delayed consumer request expiration in evictions per second.
          </td>
        </tr>
      </tbody>
    </table>
  </Collapser>

  <Collapser
    id="producer-sample"
    title={<><InlineCode>KafkaProducerSample</InlineCode> event</>}
  >
    <table>
      <thead>
        <tr>
          <th style={{ width: "350px" }}>
            Metric
          </th>

          <th>
            Description
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `producer.ageMetadataUsedInMilliseconds`
          </td>

          <td>
            Age in seconds of the current producer metadata being used.
          </td>
        </tr>

        <tr>
          <td>
            `producer.availableBufferInBytes`
          </td>

          <td>
            Total amount of buffer memory that is not being used in bytes.
          </td>
        </tr>

        <tr>
          <td>
            `producer.avgBytesSentPerRequestInBytes`
          </td>

          <td>
            Average number of bytes sent per partition per-request.
          </td>
        </tr>

        <tr>
          <td>
            `producer.avgCompressionRateRecordBatches`
          </td>

          <td>
            Average compression rate of record batches.
          </td>
        </tr>

        <tr>
          <td>
            `producer.avgRecordAccumulatorsInMilliseconds`
          </td>

          <td>
            Average time in ms record batches spent in the record accumulator.
          </td>
        </tr>

        <tr>
          <td>
            `producer.avgRecordSizeInBytes`
          </td>

          <td>
            Average record size in bytes.
          </td>
        </tr>

        <tr>
          <td>
            `producer.avgRecordsSentPerSecond`
          </td>

          <td>
            Average number of records sent per second.
          </td>
        </tr>

        <tr>
          <td>
            `producer.avgRecordsSentPerTopicPerSecond`
          </td>

          <td>
            Average number of records sent per second for a topic.
          </td>
        </tr>

        <tr>
          <td>
            `producer.AvgRequestLatencyPerSecond`
          </td>

          <td>
            Producer average request latency.
          </td>
        </tr>

        <tr>
          <td>
            `producer.avgThrottleTime`
          </td>

          <td>
            Average time that a request was throttled by a broker in milliseconds.
          </td>
        </tr>

        <tr>
          <td>
            `producer.bufferMemoryAvailableInBytes`
          </td>

          <td>
            Maximum amount of buffer memory the client can use in bytes.
          </td>
        </tr>

        <tr>
          <td>
            `producer.bufferpoolWaitTime`
          </td>

          <td>
            Faction of time an appender waits for space allocation.
          </td>
        </tr>

        <tr>
          <td>
            `producer.bytesOutPerSecond`
          </td>

          <td>
            Producer bytes per second out.
          </td>
        </tr>

        <tr>
          <td>
            `producer.compressionRateRecordBatches`
          </td>

          <td>
            Average compression rate of record batches for a topic.
          </td>
        </tr>

        <tr>
          <td>
            `producer.iOWaitTime`
          </td>

          <td>
            Producer I/O wait time in milliseconds.
          </td>
        </tr>

        <tr>
          <td>
            `producer.maxBytesSentPerRequestInBytes`
          </td>

          <td>
            Max number of bytes sent per partition per-request.
          </td>
        </tr>

        <tr>
          <td>
            `producer.maxRecordSizeInBytes`
          </td>

          <td>
            Maximum record size in bytes.
          </td>
        </tr>

        <tr>
          <td>
            `producer.maxRequestLatencyInMilliseconds`
          </td>

          <td>
            Maximum request latency in milliseconds.
          </td>
        </tr>

        <tr>
          <td>
            `producer.maxThrottleTime`
          </td>

          <td>
            Maximum time a request was throttled by a broker in milliseconds.
          </td>
        </tr>

        <tr>
          <td>
            `producer.messageRatePerSecond`
          </td>

          <td>
            Producer messages per second.
          </td>
        </tr>

        <tr>
          <td>
            `producer.responsePerSecond`
          </td>

          <td>
            Number of producer responses per second.
          </td>
        </tr>

        <tr>
          <td>
            `producer.requestPerSecond`
          </td>

          <td>
            Number of producer requests per second.
          </td>
        </tr>

        <tr>
          <td>
            `producer.requestsWaitingResponse`
          </td>

          <td>
            Current number of in-flight requests awaiting a response.
          </td>
        </tr>

        <tr>
          <td>
            `producer.threadsWaiting`
          </td>

          <td>
            Number of user threads blocked waiting for buffer memory to enqueue their records.
          </td>
        </tr>
      </tbody>
    </table>
  </Collapser>

  <Collapser
    id="topic-sample"
    title={<><InlineCode>KafkaTopicSample</InlineCode> event</>}
  >
    <table>
      <thead>
        <tr>
          <th style={{ width: "350px" }}>
            Metric
          </th>

          <th>
            Description
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `topic.partitionsWithNonPreferredLeader`
          </td>

          <td>
            Number of partitions per topic that are not being led by their preferred replica.
          </td>
        </tr>

        <tr>
          <td>
            `topic.respondMetaData`
          </td>

          <td>
            Number of topics responding to meta data requests.
          </td>
        </tr>

        <tr>
          <td>
            `topic.retentionSizeOrTime`
          </td>

          <td>
            Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time.
          </td>
        </tr>

        <tr>
          <td>
            `topic.underReplicatedPartitions`
          </td>

          <td>
            Number of partitions per topic that are under-replicated.
          </td>
        </tr>
      </tbody>
    </table>
  </Collapser>

  <Collapser
    id="offset-sample"
    title={<><InlineCode>KafkaOffsetSample</InlineCode> struct</>}
  >
    <table>
      <thead>
        <tr>
          <th style={{ width: "350px" }}>
            Metric
          </th>

          <th>
            Description
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `consumer.offset`
          </td>

          <td>
            The last consumed offset on a partition by the consumer group.
          </td>
        </tr>

        <tr>
          <td>
            `consumer.lag`
          </td>

          <td>
            The difference between a broker's high water mark and the consumer's offset (`consumer.hwm` - `consumer.offset`).
          </td>
        </tr>

        <tr>
          <td>
            `consumer.hwm`
          </td>

          <td>
            The offset of the last message written to a partition (high water mark).
          </td>
        </tr>

        <tr>
          <td>
            `consumer.totalLag`
          </td>

          <td>
            The sum of lags across partitions consumed by a consumer.
          </td>
        </tr>

        <tr>
          <td>
            `consumerGroup.totalLag`
          </td>

          <td>
            The sum of lags across all partitions consumed by a `consumerGroup`.
          </td>
        </tr>

        <tr>
          <td>
            `consumerGroup.maxLag`
          </td>

          <td>
            The maximum lag across all partitions consumed by a `consumerGroup`.
          </td>
        </tr>

        <tr>
          <td>
            `consumerGroup.activeConsumers`
          </td>

          <td>
            The number of active consumers in this `consumerGroup`.
          </td>
        </tr>
      </tbody>
    </table>
  </Collapser>
</CollapserGroup>

## Troubleshooting [#troubleshooting]

<CollapserGroup>
  <Collapser
    id="duplicate-info"
    title="Duplicate data being reported"
  >
    For agents monitoring producers and/or consumers, and that have `Topic mode` set to `All`:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option `Collect topic size` is set to false.
  </Collapser>

  {
    ' '
  }

  <Collapser
    id="zookeeper-node-not-found"
    title="Integration is logging errors 'zk: node not found'"
    title={<>Integration is logging errors <InlineCode>zk: node not found</InlineCode></>}
  >
    Ensure that `zookeeper_path` is set correctly in the configuration file.
  </Collapser>

  <Collapser
    id="jmx-connection-errors"
    title="JMX connection errors"
  >
    The Kafka integration uses a JMX helper tool called `nrjmx` to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port.

    To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the `PORT`, `USERNAME`, and `PASSWORD` variables with the corresponding JMX settings for the brokers:

    ```shell
    echo "*:*" | nrjmx -hostname MY_HOSTNAME -port MY_PORT -v -username MY_USERNAME -password MY_PASSWORD
    ```

    The command should generate the output showing a long series of metrics without any errors.
  </Collapser>

  <Collapser
    id="kerberos-authentication"
    title="Kerberos authentication failing"
  >
    The integration might show an error like the following:

    ```shell
    KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database
    ```

    Check the keytab with kinit command. Replace the highlighted fields with your values:

    ```shell
    $ kinit -k -t KEY_TAB_PATH USERNAME
    ```

    If the username and keytab combination is correct, the command above should finish without printing any errors.

    Check the realm using the `klist` command:

    ```shell
    $ klist |grep "Default principal:"
    ```

    You should see something like this:

    ```shell
    Default principal: johndoe@a_realm_name
    ```

    Check that the printed user name and realm match the `sasl_gssapi_realm` and `sasl_gssapi_username` parameters in the integration configuration.
  </Collapser>
</CollapserGroup>
