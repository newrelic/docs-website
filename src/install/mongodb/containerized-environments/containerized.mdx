---
componentType: default
headingText: Install the MongoDB integration
---

There is available a [Docker image](https://hub.docker.com/r/bitnami/mongodb-exporter/tags) that packages the very same Prometheus exporter used by the integration. Therefore, once the exporter is configured and scraped, the metrics are the same as the ones provided by the integration.

The steps bellow provide guidance to set up the integration in environments such as Kubernetes, where you may report prometheus metrics from different sources in a centralized way.

### Running the exporter [#running-exporter]

In order to monitor a MongoDB instance in Kubernetes or ECS, you need to run the MongoDB exporter image as a sidecar or as a separate workload.

Notice that both MongoDB Bitnami Helm charts provide a standard way to deploy and configure the MongoDB exporter you can use as a reference.

For example, the following `values.yaml` of the `bitnami/mongodb` chart configures each MongoDB workload with an exporter running as a sidecar configured as the NewRelic integration:

```yaml
metrics:
  enabled: true
  extraFlags: "--collect-all --discovering-mode=true"
```

Notice that, instead of `--collect-all`, you can also pass collectors one by one and disable any of them if needed: `--collector.dbstats --collector.collstats --collector.indexstats --collector.replicasetstatus --collector.topmetrics --collector.diagnosticdata`.

### Scrape the metrics [#scrape-metrics]

There are different ways to scrape Prometheus metrics and forward them to New Relic, check [our docs to know more](/docs/infrastructure/prometheus-integrations/install-configure-prometheus-agent/setup-prometheus-agent).

Running and scraping the Bitnami exporter or `percona/mongodb_exporter`, the metrics scraped are the same and the <InlinePopover type="dashboards" /> work. However, the integration adds a common label to all the Metrics scraped from a particular MongoDB cluster `mongodb_cluster_name` to be able to group metrics coming from different clusters.

<Callout variant="tip">
The MongoDB dashboard needs the `mongodb_cluster_name` attribute to all the MongoDB metrics and you need to add it manually. Depending on the agent scraping the Prometheus metrics, there are different ways to add common labels.
</Callout>

For example, the following Prometheus agent configuration adds to all the Prometheus metrics matching `mongodb_.*` the label `mongodb_cluster_name=cluster-name-example`.

```yml
# (...)
newrelic_remote_write:
  extra_write_relabel_configs:
    - source_labels: [__name__]
      regex: 'mongodb_.*'
      target_label: mongodb_cluster_name
      replacement: 'cluster-name-example'
```

If you're using multiple `percona/mongodb_exporter` pods to serve metrics from multiple MongoDB clusters, you can label each with a separate `mongodb_cluster_name` using a configuration like this:

```yml
# (...)
newrelic_remote_write:
  extra_write_relabel_configs:
    - source_labels: [pod]
      regex: 'mongodb-exporter-clst-1.*'
      target_label: mongodb_cluster_name
      replacement: 'cluster-name-example-1'
    - source_labels: [pod]
      regex: 'mongodb-exporter-clst-2.*'
      target_label: mongodb_cluster_name
      replacement: 'cluster-name-example-2'
```

On the other hand, in a Prometheus server, you can add custom attributes to all metrics with `external_labels`, or per job with `static_configs`, or for more complex scenarios `metric_relabel_config`.