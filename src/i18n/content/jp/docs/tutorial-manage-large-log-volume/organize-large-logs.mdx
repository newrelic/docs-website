---
title: 大規模なログの取り込みを整理する
metaDescription: Organize your logs into managable partitions and tag their attributes with logs parsing
freshnessValidatedDate: never
translationType: machine
---

どのログを取り込んで保存するかを決めたら、ログを整理します。おそらく、依然として数百ギガバイトまたは数十テラバイトのログを取り込むことになります。当初よりはかなり減りましたが、効果的に使用しようとすると、やはり苦労が必要になります。

これを解決するために、これらの残りのログをテーマ別のパーティションにグループ化し、ログを解析して貴重な属性を取り出してタグ付けします。この方法でログを整理すると、次のことが可能になります。

* ログ内の任意の属性をクエリします
* フロントエンド ログとバックエンド ログなど、特定のパーティションを一度に管理
* クエリのロード時間を短縮する

## ログを分割する理由

データ パーティションを適切に使用することで、パフォーマンスを大幅に向上できます。 データを個別のパーティションに整理すると、必要なデータだけをクエリできるようになります。 単一のパーティションまたはコンマで区切られたパーティションのリストをクエリできます。 データをパーティション分割する目的は次のとおりです。

* 環境または組織内の静的または頻繁に変更されないカテゴリ (たとえば、ビジネス ユニット、チーム、環境、サービスなど) に合わせてデータ パーティションを作成します。
* パーティションを作成して、最も一般的なクエリに対してスキャンする必要があるイベントの数を最適化します。 確立されたルールはありませんが、一般的に、スキャンされたログイベントは、 `common`書き込みに対して 5 億以上（特に 10 億以上）になるため、パーティショニングの調整を検討することをお勧めします。

パーティションの名前空間は、その保持期間を決定します。 2つの保持オプションを提供します。

* <DNT>**Standard:**</DNT> New Relic サブスクリプションによって決定されるアカウントのデフォルトの保持期間。 これは、アカウントで利用可能な最大保持期間であり、ほとんどのパーティションに対して選択するネームスペースです。
* <DNT>**Secondary:**</DNT> 30日間保持されます。 セカンダリ ネームスペースのメンバーであるパーティションに送信されたすべてのログは、取り込まれてから 30 日後に定期的に削除されます。

## ログを解析する理由

取り込み時にログを解析することは、あなたや組織内の他のユーザーがログ データをより使いやすくするための最良の方法です。たとえば、Grok 解析ルールを使用して、次の 2 つのログを解析前と解析後で比較します。

<SideBySide>
  <Side>
    事前解析:

    ```json
    {
      "message": "32 4329 store_Portland"
    }


    ```
  </Side>

  <Side>
    事後解析:

    ```json
    {
      "transaction_total": "32",
      "purchase_number": "4329",
      "store_location": "store_Portland",
    }
    ```
  </Side>
</SideBySide>

これにより、ダッシュボードやアラートで `transaction_total` などの新しく定義された属性を簡単にクエリできるようになります。

## ログを整理する

ACME Corp が今後数か月間で約 2 TB のログを取り込むことが分かっているとします。Java アプリとインフラストラクチャ エージェントからのログのパーティションを作成したいと考えています。彼らは、遠い将来に Java ログをクエリする必要があるかもしれないと考え、標準の保存期間を使用することにしました。一方、最近のインフラストラクチャ ログのみが必要なため、それらに対しては二次保持を使用します。

そのためには

<Steps>
  <Step>
    ## UIに移動します

    に行く <DNT>**[one.newrelic.com &gt; Logs](https://one.newrelic.com/launcher/logger.log-launcher)**</DNT>
  </Step>

  <Step>
    ## ログを分割する

    <SideBySide>
      <Side>
        1. 左側のナビゲーションの<DNT>**Manage data**</DNT>から<DNT>**Data partitions**</DNT>をクリックし、次に<DNT>**Create partition rule**</DNT>をクリックします。
        2. パーティション名を、 `Log_`で始まる英数字の文字列として定義します。この場合、 `Log_java`。
        3. 任意の説明を追加します。
        4. パーティションの標準の名前空間保持を選択します。
        5. ルールの一致基準を設定します。このパーティションに保存するログと一致する有効な NRQL `WHERE` 句を入力します。この場合、 `logtype=java`。
        6. 新しいパーティションを保存するには、 <DNT>**Create**</DNT>をクリックします。
      </Side>

      <Side>
        <img title="log-partition" alt="An image displaying New Relic's log partion UI" src="/images/logs_screenshot_full-partition.webp" />
      </Side>
    </SideBySide>

    これにより、すべての Java ログに対して標準のデータ保持を持つデータ パーティションが作成されます。インフラストラクチャ ログを整理するには、上記と同じ手順に従い、保持をセカンダリに変更し、クエリを `logtype=infrastructure`に変更するだけです。
  </Step>

  <Step>
    ## ログを解析する

    ログが分割されたので、今度はログを解析します。それらを解析すると、ログ内の関連データを抽出して、クエリとアクセスを容易にすることができます。

    ログを解析するには:

    <SideBySide>
      <Side>
        1. ログ UI の左側のナビゲーションの<DNT>**Manage Data**</DNT>から、 <DNT>**Parsing**</DNT>をクリックし、次に<DNT>**Create parsing rule**</DNT>をクリックします。
        2. 新しい解析規則の名前を入力します。
        3. 解析する既存のフィールドを選択するか (デフォルトは `message`)、または新しいフィールド名を入力します。
        4. 解析するログに一致する有効な NRQL `WHERE`句を入力してください。
        5. 一致するログが存在する場合はそれを選択するか、「ログの貼り付け」タブをクリックしてサンプル ログを貼り付けます。
        6. 解析ルールを入力し、 <DNT>**Output**</DNT>セクションの結果を表示してそれが機能していることを確認します。 （下記の例を参照）
        7. カスタム解析ルールを有効にして保存します。
      </Side>

      <Side>
        <img title="log-parsing" alt="An image displaying New Relic's log parsing UI" src="/images/logs_screenshot_full-parsing.webp" />
      </Side>
    </SideBySide>

    次の例は、解析ルールを作成する具体的な例を示しています。

    <CollapserGroup>
      <Collapser id="example" title="ログの解析例">
        このドキュメントの前半で使用した例を使ってみましょう。次のパターンに従うログがあります。

        ```json
        {
          "message": "32 4329 store_Portland"
        }
        ```

        取引金額、注文番号、店舗の場所を取得したいとします。解析ルールは Grok を使用して構築され、パターン `%{SYNTAX:SEMANTIC}`が使用されます。`SYNTAX` はテキストの検索に使用されるパターン、 `SEMANTIC` は一致した結果に与えられる識別子または属性です。

        この場合、解析ルールは次のようになります。

        ```
        %{INT:transaction_total} %{INT:purchase_number} store%{DATA:store_location}
        ```

        上記のパターンで解析ルールが作成されると、次の方法でログが返されます。

        ```json
        {
          "transaction_total": "32",
          "purchase_number": "4329",
          "store_location": "store_Portland",
        }
        ```
      </Collapser>
    </CollapserGroup>

    ログを解析するための Grok パターンの作成の詳細については、 [ブログ投稿を参照してください](https://newrelic.com/blog/how-to-relic/how-to-use-grok-log-parsing)。
  </Step>
</Steps>

## 次のステップ

ログの真の価値を明らかにし、ログに関するチームのストレス時間を節約できたことをお祝いします。 システムが成長し、データを取り込むにつれて、解析ルールとパーティションの維持管理を確実に行う必要があります。 New Relic <InlinePopover type="logs" />で何ができるかについて詳しく知りたい場合は、次のドキュメントをご覧ください。

* [ログ データの解析](/docs/logs/ui-data/parsing): Grok を使用したログの解析を詳しく調べ、GraphQL API である NerdGraph を使用してログ解析ルールを作成、クエリ、管理する方法を学びます。
* [ログ パターン](/docs/logs/ui-data/find-unusual-logs-log-patterns/): ログ パターンは、検索せずにログ データの値を発見する最速の方法です。
* [ログの難読化](/docs/logs/ui-data/obfuscation-ui/): ログの難読化ルールを使用すると、特定の種類の情報が New Relic に保存されないようにすることができます。
* [長いログ (BLOB) でデータを検索する](/docs/logs/ui-data/long-logs-blobs/): 大量のログ データは、問題のトラブルシューティングに役立ちます。しかし、ログ内の属性に数千の文字が含まれている場合はどうなるでしょうか?New Relic はこのデータのうちどれくらいの量を保存できますか?そして、このすべてのデータから有用な情報を見つけるにはどうすればよいでしょうか?

<DocTiles numbered>
  <DocTile title="Get started" path="/docs/tutorial-large-logs/get-started-managing-large-logs" />

  <DocTile title="Filter and reduce your log ingest" path="/docs/tutorial-large-logs/filter-large-logs" />

  <DocTile title="Organize your logs" label={{text: 'You are here', color: '#FCD672'}} path="/docs/tutorial-large-logs/organize-large-logs" />
</DocTiles>