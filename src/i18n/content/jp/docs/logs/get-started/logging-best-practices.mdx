---
title: ログ機能のベストプラクティスガイド
tags:
  - New Relic solutions
  - Best practices guides
  - Logs
  - Logging
metaDescription: Best practices for using New Relic logs
freshnessValidatedDate: never
translationType: machine
---

New Relicログ機能のベストプラクティスガイドへようこそ。ここでは、ログ機能を最適化し、データ消費を管理する方法に関する詳細な推奨事項をご覧いただけます。

<Callout variant="tip">
  ログがたくさんありますか? [それらを最適化および管理する方法については、チュートリアル](/docs/tutorial-large-logs/get-started-managing-large-logs/)をご覧ください。
</Callout>

## ログの転送 [#forwarding-logs]

[ログ転送ドキュメント](/docs/logs/forward-logs/enable-log-management-new-relic)を補足するためのログ転送のヒントは次のとおりです。

* ログを転送する場合は、[New Relic Infrastructureエージェント](/docs/logs/forward-logs/forward-your-logs-using-infrastructure-agent)および/または[APMエージェント](/docs/apm/new-relic-apm/getting-started/get-started-logs-context#agents)を使用することをお勧めします。New Relicエージェントを使用できない場合は、他の対応エージェント（FluentBit、Fluentd、Logstashなど）を使用してください。

  以下は、対応ロギングエージェントのGithub設定例です。

  * [FluentBitの例](https://github.com/newrelic/fluentbit-examples)

  * [Fluentdの例](https://github.com/newrelic/fluentd-examples/)

  * [Logstashの例](https://github.com/newrelic/logstash-examples)

    <Callout variant="important">
      ログが基盤となるホスト/コンテナ上のディレクトリに保存され、ログを収集するためにインフラストラクチャエージェントによってインストゥルメントされている場合、インフラストラクチャエージェントと<InlinePopover type="apm"/>エージェントの両方によって収集された重複したログが表示されることがあります。 重複したログの送信を避けるには、[このガイド](/docs/logs/logs-context/upgrade-to-automatic-logs-context)の推奨事項を参照してください。
    </Callout>

* 転送するすべてのデータに`logtype`属性を追加します。 属性は<DNT>**required**</DNT>で、組み込みの解析ルールを使用するほか、データ型に基づいてカスタム解析ルールを作成するためにも使用できます。 `logtype`属性はよく知られた属性とみなされ、ログの概要情報用のクイックスタート ダッシュボードで使用されます。

* 既知のログタイプには、[組み込みの解析ルール](/docs/logs/ui-data/built-log-parsing-rules)を使用します。関連する`logtype`属性を設定すると、さまざまな既知のログタイプからログが自動的に解析されます。

  以下は、Infrastructureエージェントが転送したログに`logtype`属性を追加する方法の例です。

  ```yml
  logs:
    - name: mylog
      file: /var/log/mylog.log
      attributes:
        logtype: mylog
  ```

* New Relicインテグレーションを使用して、次のような一般的な他のデータ型のログを転送します。

  * コンテナ環境：[Kubernetes（K8S）](/docs/kubernetes-pixie/kubernetes-integration/get-started/introduction-kubernetes-integration)
  * クラウドプロバイダーの統合：[AWS](/docs/infrastructure/amazon-integrations/get-started/introduction-aws-integrations/)、[Azure](/docs/infrastructure/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations)、[GCP](/docs/infrastructure/google-cloud-platform-integrations/get-started/introduction-google-cloud-platform-integrations)
  * [ロギングでサポートされているその他のオンホストインテグレーション](/docs/infrastructure/host-integrations/get-started/introduction-host-integrations)

## データパーティション [#partitions]

1 日あたり 1 TB 以上のログ データを消費している場合は、機能的およびテーマ別のグループ化を行う方法でデータを分割する計画など、ログの取り込みガバナンス計画に確実に取り組む必要があります。パーティションごとに 1 TB 以下のベスト プラクティスをお勧めします。これにより、パフォーマンスと使いやすさの両方が実現されます。すべてのログを 1 つのアカウント内の 1 つの巨大な「バケット」 (デフォルトのログ パーティション) に送信すると、クエリが遅くなったり、クエリが失敗したりする可能性があります。詳細については、 [「NRQL クエリ レート制限」](/docs/query-your-data/nrql-new-relic-query-language/get-started/rate-limits-nrql-queries/#query-limits)を参照してください。

クエリのパフォーマンスを向上させる 1 つの方法は、検索される時間範囲を制限することです。長期間にわたってログを検索すると、より多くの結果が返され、より多くの時間がかかります。可能な限り 1 週間を超える検索は避け、時間範囲セレクターを使用して検索をより小さく、より具体的な時間枠に絞り込みます。

検索パフォーマンスを向上させるもう1つの方法は、[データパーティション](/docs/logs/ui-data/data-partitions/)を使用することです。データパーティションのベストプラクティスは次のとおりです。

* ログのオンボーディングプロセスの早い段階で、パーティションを使用するようにしてください。ユーザーが特定のログを検索して見つけた場所を把握できるように、パーティションを使用する方策を作成します。そうすることで、ログジャーニーの後半でパーティションを実装する場合、アラート、ダッシュボード、保存済みビューを変更する必要がなくなります。

* 制限を回避してクエリ時間を短縮するには、1日あたりの総取り込み量が1TBを超える場合に、特定のデータ型に[データパーティションルール](/docs/logs/ui-data/data-partitions/)を作成します。特定の大量のインフラストラクチャログ（KK8Sログ、CDNログ、VPCログ、Syslogログ、Webログなど）は、独自のパーティションに適した候補となる場合があります。

* 取り込み量が少ない場合でも、データパーティションを使用してデータを論理的に分離したり、個別のデータ型間でクエリのパフォーマンスを改善したりすることもできます。

* 保持時間を短縮したいデータには、[セカンダリデータパーティションのネームスペース](/docs/logs/ui-data/data-partitions#namespace)を使用します。セカンダリデータパーティションには、デフォルトで30日間の保持期間があります。

* <DNT>**Logs**</DNT> UI で[データ パーティションを検索する](/docs/logs/ui-data/data-partitions#search)には、適切なパーティションを選択し、パーティション セレクターを開いて、検索するパーティションをチェックする必要があります。 NRQL を使用している場合は、 `FROM`句を使用して、検索する`Log`または`Log_<partion>`を指定します。 例えば：

  ```sql
  FROM Log_<my_partition_name> SELECT * SINCE 1 hour ago
  ```

  または、複数のパーティションのログを検索するには：

  ```sql
  FROM Log, Log_<my_partition_name> SELECT * SINCE 1 hour ago
  ```

## 解析ログ [#parsing-logs]

ログを取り込み時に解析することは、ログ データを自分や組織内の他のユーザーがより使いやすくするための最善の方法です。 属性を解析すると、クエリ時にデータを解析しなくても、 <DNT>**Logs**</DNT> UI と NRQL で簡単に検索できるようになります。 これにより、 <InlinePopover type="alerts"/>やダッシュボードでも簡単に使用できるようになります。

ログを解析するには、以下を推奨します。

* 取り込み時にログを解析して`attributes` (またはフィールド) を作成します。これは、 <InlinePopover type="dashboards"/>やアラートを検索または作成するときに使用できます。 属性は、データの文字列または数値にすることができます。

* 取り込み時にログに追加した`logtype`属性と他のNRQL`WHERE`句を使用して、解析するデータに一致させます。ログをできるだけ正確にフィルターする特定の一致ルールを記述します。例：

  ```sql
  WHERE logtype='mylog' AND message LIKE '%error%'
  ```

* 可能な限り、[組み込みの解析ルール](/docs/logs/ui-data/built-log-parsing-rules/)と関連する`logtype`属性を使用してください。組み込みルールがデータに対して機能しない場合は、別の`logtype`属性名 (つまり、 `apache_logs`対`apache` 、 `iis_w3c_custom`対`iis_w3c` ) を使用して、新しい解析ルールを作成します。 UI は、ログ データ形式で機能するように、組み込みルールの修正バージョンを使用します。

* <DNT>**Parsing**</DNT> UI を使用して、Grok ルールをテストおよび検証します。 `Paste log`オプションを使用すると、永続的な解析ルールを作成して保存する前に、ログメッセージの 1 つを貼り付けて Grok 式をテストできます。

  <img
    title="parsing example"
    alt="Parsing example in the UI"
    src="/images/logs_screenshot-full_parsing-example.webp"
  />

* 外部FluentBit設定を使用すると、複数行のログを解析したり、New Relicに取り込む前に、より広範な解析を事前に実行したりできます。当社のInfrastructureエージェントによる複数業の解析の詳細と設定については、[このブログ記事](https://newrelic.com/blog/how-to-relic/parse-multiline-log-messages-fluent-bit-plugin)を参照してください。

* フィルターされたログに一致するように最適化されたGrokパターンを作成し、属性を抽出します。GREEDYDATAのような高価なGrokパターンを過剰に使用しないでください。準最適な解析ルールの特定についてサポートが必要な場合は、New Relicのアカウント担当者にお問い合わせください。

### Grokに関するベストプラクティス

* Grok 型を使用して、抽出する属性値の型を指定します。省略した場合、値は文字列として抽出されます。これらの属性で NRQL 関数 (つまり、 `monthOf()` 、 `max()` 、 `avg()` 、 `>` 、 `<`など) を使用できるようにする場合、これは特に数値の場合に重要です。

* <DNT>
    **Parsing**
  </DNT>

  UI を使用して Grok パターンをテストします。

  <DNT>
    **Parsing**
  </DNT>

  UI にサンプル ログを貼り付けて、Grok または Regex パターンを検証し、期待どおりに属性が抽出されているかどうかを確認できます。

* 行頭を示すために構文解析ロジック（つまり、`^`）にアンカーを追加するか、行末に`$`を追加します。

* パターンの前後に`()?`を使用して、オプションのフィールドを識別します。

* `'%{GREEDYDATA}`のような高価なGrokパターンを過剰に使用しないようにしてください。属性を抽出するときは、常に有効なGrokパターンとGrokタイプを使用してください。

## ドロップフィルタールール [#drop-rules]

### 取り込み時にログをドロップする

* [ドロップフィルタルール](/docs/logs/ui-data/drop-data-drop-filter-rules#create)を作成して、役に立たないログ、またはダッシュボード、アラート、トラブルシューティングのユースケースの要件に不要なログを削除します。

### 取り込み時にログから属性をドロップする

* ログから未使用の属性をドロップするドロップルールを作成します。
* 解析後に`message`属性を削除します。メッセージ属性を解析してデータから新しい属性を作成する場合は、メッセージフィールドを削除します。
* 例：AWSインフラストラクチャからデータを転送している場合、ドロップルールを作成して不要なデータの肥大化を引き起こす可能性のあるAWS属性をドロップできます。

## New Relicログのサイジング [#sizing]

* ストレージの請求方法は、一部の競合他社とは異なる場合があります。ログデータの測定方法は、[使用量の計算](/docs/accounts/accounts-billing/new-relic-one-pricing-billing/data-ingest-billing#usage-calculation)で定義されている他のタイプのデータの測定方法と請求方法と同様です。

* クラウドインテグレーション（AWS、Azure、GCP）がインストールされている場合、すべてのログレコードにクラウドメタデータが追加され、全体的な取込み請求に加算されます。ただし、このデータは、取り込みを削減するためにドロップできます。

* ログデータのオーバーヘッドの主な要因を、影響の大きい順に以下に示します。

  * クラウドインテグレーション

  * JSON形式

  * ログパターン（

    <DNT>
      [**Logs**](/docs/logs/ui-data/find-unusual-logs-log-patterns#availability)
    </DNT>

    [UI でパターンを無効/有効に](/docs/logs/ui-data/find-unusual-logs-log-patterns#availability)できます。）

## ログの検索 [#searching-logs]

* 一般的なログ検索の場合は、UI で<DNT>**Saved views**</DNT>を作成して使用します。 データの検索を作成し、 <DNT>**+ Add Column**</DNT>をクリックして UI テーブルに追加の属性を追加します。 次に、列を移動して希望の順序で表示し、プライベートまたはパブリックの権限を持つ保存済みビューとして保存します。 保存したビューを公開するように構成すると、自分や他のユーザーが関連するすべての属性データを表示しながら一般的な検索を簡単に実行できるようになります。 これは、Apache、nginx などのサードパーティ アプリケーションでは良い方法であり、ユーザーは検索せずにそれらのログを簡単に確認できます。
* [クエリビルダー](/docs/query-your-data/explore-query-data/query-builder/introduction-query-builder)を使用して、NRQLで検索を実行します。クエリビルダーを使用すると、NRQLで使用可能な高度な機能をすべて使用できます。
* <InlinePopover type="dashboards"/>を作成するか、利用可能な[クイックスタート](https://newrelic.com/instant-observability)を使用して、ログに関する一般的な質問に答えたり、時系列グラフでログ データを経時的に確認したりできます。 複数のパネルを備えたダッシュボードを作成し、ログ データをさまざまな方法で細分化します。
* [capture()](/docs/query-your-data/nrql-new-relic-query-language/get-started/nrql-syntax-clauses-functions#func-capture)や[aparse()](/docs/query-your-data/nrql-new-relic-query-language/get-started/nrql-syntax-clauses-functions#func-aparse)などの高度なNRQL関数を使用すると、検索時にデータを解析できます。
* <DNT>**Logs analysis**</DNT>および/または<DNT>**APM logs monitoring quickstart**</DNT>ダッシュボードをインストールして、ログ データへのインサイトを迅速に増やします。 これらのダッシュボードを追加するには、 <DNT>**[one.newrelic.com](https://one.newrelic.com) > Integrations & Agents > Logging > Dashboards**</DNT>に移動します。

## 次は何ですか？

[<InlinePopover type="logs"/>の使用開始を](/docs/logs/get-started/get-started-log-management/)参照してください。
