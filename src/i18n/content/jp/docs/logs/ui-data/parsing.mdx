---
title: ログデータの解析
tags:
  - Logs
  - Log management
  - UI and data
metaDescription: How New Relic uses parsing and how to send customized log data.
freshnessValidatedDate: never
translationType: human
---

ログ<DNT>parsing</DNT>は、定義したルールに基づいて、非構造化ログデータを属性（キーと値のペア）に変換するプロセスです。これらの属性をNRQLクエリで使用すると、ログを便利な方法でファセット化またはフィルタリングできます。

New Relicは、特定の解析ルールに従ってログデータを自動解析します。このドキュメントでは、ログ解析の仕組みと、独自のカスタム解析ルールを作成する方法について説明します。

また、GraphQL APIであるNerdGraphを使用して、ログ解析ルールを作成、クエリ、管理することもできます。このために役立つツールは、[Nerdgraph APIエクスプローラー](https://api.newrelic.com/graphiql)です。詳細については、[解析に関するNerdGraphチュートリアル](/docs/apis/nerdgraph/examples/nerdgraph-log-parsing-rules-tutorial/)を参照してください。

ログ解析に関する5分間のビデオはこちらです：

<Video id="xPWM46yw3bQ" type="youtube" />

## 解析例 [#parsing-defined]

良い例としては、構造化されていないテキストを含むデフォルトのNGINXアクセスログが挙げられます。検索には便利ですが、それ以外にはあまり役に立ちません。典型的な行の例を次に示します。

```
127.180.71.3 - - [10/May/1997:08:05:32 +0000] "GET /downloads/product_1 HTTP/1.1" 304 0 "-" "Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)"
```

解析されていない形式では、ほとんどの質問に答えるには全文検索を行う必要があります。解析後、ログは`response code`や`request URL`などの属性に編成されます。

```json
{
  "remote_addr":"93.180.71.3",
  "time":"1586514731",
  "method":"GET",
  "path":"/downloads/product_1",
  "version":"HTTP/1.1",
  "response":"304",
  "bytesSent": 0,
  "user_agent": "Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)"
}
```

解析により、それらの値に基づいてファセットする[カスタムクエリ](/docs/using-new-relic/data/understand-data/query-new-relic-data)を簡単に作成できるようになります。これにより、リクエストURLごとの応答コードの分布を理解し、問題のあるページをすばやく見つけることができます。

## ログ解析の仕組み [#how-it-works]

New Relicがログの解析を実装する方法の概要は次のとおりです。

<table>
  <thead>
    <tr>
      <th style={{ width: "100px" }}>
        ログ解析
      </th>

      <th>
        使用方法
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        対象
      </td>

      <td>
        * 解析は特定の選択されたフィールドに適用されます。デフォルトでは、`message`フィールドが使用されます。ただし、現在データに存在しないフィールド/属性も含め、任意のフィールド/属性を選択できます。
        * 各解析ルールは、ルールが解析を試みるログを決定するNRQL `WHERE`句を使用して作成されます。
        * 一致プロセスを簡素化するために、ログに[`logtype`](#logtype)属性を追加することをお勧めします。ただし、`logtype`の使用に限定されるわけではなく、1つ以上の属性をNRQL `WHERE`句の一致基準として使用できます。
      </td>
    </tr>

    <tr>
      <td>
        タイミング
      </td>

      <td>
        * 解析は各ログメッセージに1回だけ適用されます。複数の解析ルールがログに一致する場合、最初に成功したルールのみが適用されます。
        * 解析ルールは順序付けられていません。複数の解析ルールがログに一致する場合、ランダムに1つが選択されます。同じログと一致しないように解析ルールを構築してください。
        * 解析は、データがNRDBに書き込まれる前に、ログの取り込み中に行われます。データがストレージに書き込まれると、解析できなくなります。
        * パイプラインで解析が行われ、 <DNT>**before**</DNT>データの強化が行われます。解析ルールの一致基準を定義するときは注意してください。条件が解析またはエンリッチメントが行われるまで存在しない属性に基づいている場合、一致が発生したときにそのデータはログに存在しません。その結果、解析は行われません。
      </td>
    </tr>

    <tr>
      <td>
        方法
      </td>

      <td>
        * ルールは[Grok](#grok)、正規表現、またはその2つを組み合わせて記述できます。Grokは、複雑な正規表現を抽象化するパターンのコレクションです。
        * 解析UIではJava Regex構文をサポートしています。キャプチャグループ内の属性名またはフィールド名の場合、Java Regexでは\[A-Za-z0-9]のみが許可されます。
      </td>
    </tr>
  </tbody>
</table>

## Grokを使用して属性を解析する [#grok]

解析パターンは、ログメッセージ解析の業界標準であるGrokを使用して指定されます。`logtype`フィールドを持つすべての受信ログは、[組み込みの解析ルール](#built-in-rules)と照合され、可能な場合は、関連するGrokパターンがログに適用されます。

Grokは、リテラルな複雑な正規表現の代わりに使用される、組み込みの名前付きパターンを追加する正規表現のスーパーセットです。インスタンスの場合、整数が正規表現`(?:[+-]?(?:[0-9]+))`と一致していることを覚えておく必要はなく、同じ正規表現を表すGrokパターン`INT`を使用するために`%{INT}`と記述するだけです。

Grokパターンの構文は次のとおりです。

```
%{PATTERN_NAME[:OPTIONAL_EXTRACTED_ATTRIBUTE_NAME[:OPTIONAL_TYPE[:OPTIONAL_PARAMETER]]]}
```

対象箇所：

* `PATTERN_NAME` は、サポートされているGrokパターンの1 つです。パターン名は、正規表現を表すユーザーフレンドリな名前です。これらは対応する正規表現とまったく同じです。
* `OPTIONAL_EXTRACTED_ATTRIBUTE_NAME`を指定した場合、パターン名と一致する値とともにログメッセージに追加される属性の名前です。これは、正規表現を使用して名前付きキャプチャグループを使用するのと同じです。これが指定されていない場合、解析ルールは文字列の領域のみを照合し、属性とその値を抽出しません。
* `OPTIONAL_TYPE` は、抽出する属性値のタイプを指定します。省略すると、値は文字列として抽出されます。インスタンスの場合、値`123` `"File Size: 123"`から数値として属性`file_size`に抽出するには、 `value: %{INT:file_size:int}`を使用します。
* `OPTIONAL_PARAMETER` は、特定のタイプに対してオプションのパラメーターを指定します。現在、パラメーターを受け取るのは`datetime`型のみです。詳細については、以下を参照してください。

一致する文字列では、正規表現とGrokパターン名を組み合わせて使用することもできます。

サポートされている[Grokパターン](https://github.com/thekrakken/java-grok/tree/master/src/main/resources/patterns)のリストについてはこのリンクを、サポートされている[Grokタイプ](#grok-types)のリストについてはこちらをクリックしてください。

変数名は明示的に設定する必要があり、`%{URI:uri}`のように小文字にする必要があることに注意してください。`%{URI}`や`%{URI:URI}`などの式は機能しません。

<CollapserGroup>
  <Collapser id="grok-example" title="Grokの例：ログから有用なデータを取得する">
    ログレコードは次のようになります。

    ```json
    {
      "message": "54.3.120.2 2048 0"
    }
    ```

    この情報は正確ですが、それが何を意味するのかは直感的に分かりにくいです。Grokパターンは、必要なテレメトリーデータを抽出して理解するのに役立ちます。たとえば、次のようなログレコードははるかに使いやすくなります。

    ```json
    {
      "host_ip": "43.3.120.2",
      "bytes_received": 2048,
      "bytes_sent": 0
    }
    ```

    これを行うには、次の3つのフィールドを抽出するGrokパターンを作成します。

    ```
    %{IP:host_ip} %{INT:bytes_received} %{INT:bytes_sent}
    ```

    処理後、ログレコードにはフィールド`host_ip`、`bytes_received`、 `bytes_sent`が含まれます。これで、New Relicのこれらのフィールドを使用して、ログデータのフィルタリング、ファセット化、統計操作を実行できるようになります。New RelicでGrokパターンを使用してログを解析する方法の詳細については、[ブログ記事](https://newrelic.com/blog/how-to-relic/how-to-use-grok-log-parsing)をご覧ください。
  </Collapser>

  <Collapser id="grok-ui" title="UIの例：Grok解析ルールの作成">
    適切な権限がある場合は、UIで解析ルールを作成し、Grok解析を作成、テスト、有効化できます。たとえば、Inventory Servicesと呼ばれるマイクロサービスの1つについて特定のタイプのエラーメッセージを取得するには、特定のエラーメッセージと製品を検索するGrok解析ルールを作成します。これを行うには、以下を実行します。

    1. ルールに名前を付けます。たとえば、 `Inventory Services error parsing`です。

    2. 解析する既存のフィールドを選択するか（デフォルト = `message`）、新しいフィールド名を入力します。

    3. 受信ログの事前フィルターとして機能するNRQL `WHERE`句を識別します（例：`entity.name='Inventory Service'`）。この事前フィルターは、ルールで処理する必要があるログの数を絞り込み、不要な処理を削除します。

    4. 一致するログが存在する場合はそれを選択するか、\[ログの貼り付け]タブをクリックしてサンプルログを貼り付けます。

    5. Grok解析ルールを追加します。例：

       ```
       Inventory error: %{DATA:error_message} for product %{INT:product_id}
       ```

       対象箇所：

    * `Inventory error`：解析ルールの名前
    * `error_message`：選択するエラーメッセージ
    * `product_id`：Inventory Serviceの製品ID

    6. 解析ルールを有効にして保存します。

       すぐに、Inventory Serviceログに2つの新しいフィールド`error_message`と`product_id`が追加されていることがわかります。ここから、これらのフィールドに対してクエリの実行、グラフやダッシュボードの作成、アラートの設定などが行えます。

       詳細については、[UIでカスタム解析ルールを追加するためのドキュメント](#custom-parsing)を参照してください。
  </Collapser>

  <Collapser id="grok-types" title="サポートされているGrokタイプ">
    `OPTIONAL_TYPE`フィールドは、抽出する属性値のタイプを指定します。省略すると、値は文字列として抽出されます。

    サポートされているタイプは次のとおりです：

    <table>
      <thead>
        <tr>
          <th>
            Grokで指定されたタイプ
          </th>

          <th>
            New Relicデータベースに保存されているタイプ
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `boolean`
          </td>

          <td>
            `boolean`
          </td>
        </tr>

        <tr>
          <td>
            `byte` `short` `int` `integer`
          </td>

          <td>
            `integer`
          </td>
        </tr>

        <tr>
          <td>
            `long`
          </td>

          <td>
            `long`
          </td>
        </tr>

        <tr>
          <td>
            `float`
          </td>

          <td>
            `float`
          </td>
        </tr>

        <tr>
          <td>
            `double`
          </td>

          <td>
            `double`
          </td>
        </tr>

        <tr>
          <td>
            `string` （デフォルト） `text`
          </td>

          <td>
            `string`
          </td>
        </tr>

        <tr>
          <td>
            `date` `datetime`
          </td>

          <td>
            Time as a `long`

            デフォルトではISO 8601として解釈されます。`OPTIONAL_PARAMETER`が存在する場合、 `datetime`を解釈するために使用する[日付と時刻のパターン文字列](https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/text/SimpleDateFormat.html)を指定します。

            これは解析中にのみ使用できることに注意してください。取り込みパイプラインの後半で、すべてのログに対して実行される、追加の[個別タイムスタンプ解釈ステップ](/docs/logs/ui-data/timestamp-support)があります。
          </td>
        </tr>

        <tr>
          <td>
            `json`
          </td>

          <td>
            JSON構造化データ。詳細については、[プレーンテキストと混在するJSONの解析](#parsing-json)を参照してください。
          </td>
        </tr>

        <tr>
          <td>
            `csv`
          </td>

          <td>
            CSVデータ。詳細については、[CSVの解析](#parsing-csv)を参照してください。
          </td>
        </tr>

        <tr>
          <td>
            `geo`
          </td>

          <td>
            IPアドレスからの地理的位置。詳細については、[IPアドレスの位置情報の特定（GeoIP）](#geo)を参照してください。
          </td>
        </tr>

        <tr>
          <td>
            `key value pairs`
          </td>

          <td>
            キーの値のペア。詳細については、[キーの値ペアの解析](#parsing-key-value-pairs)を参照してください。
          </td>
        </tr>
      </tbody>
    </table>
  </Collapser>

  <Collapser id="grok-multiline" title="Grokの複数行の解析">
    複数行のログがある場合は、`GREEDYDATA` Grokパターンが改行と一致しないことに注意してください（`.*`に相当します）。

    したがって、`%{GREEDYDATA:some_attribute}`を直接使用するのではなく、その前に複数行フラグを追加する必要があります。 `(?s)%{GREEDYDATA:some_attribute}`
  </Collapser>

  <Collapser id="parsing-json" title="プレーンテキストと混在するJSONの解析">
    New Relic LogsパイプラインはデフォルトでログのJSONメッセージを解析しますが、プレーンテキストと混在するJSONログメッセージが含まれる場合があります。このような状況では、それらを解析し、JSON属性を使用してフィルタリングできるようにする必要がある場合があります。その場合は、grokパターンによってキャプチャされたJSONを解析する`json` [grokタイプ](#grok-syntax)を使用できます。この形式は、grok構文、解析されたjson属性に割り当てるプレフィックス、および`json` [grokタイプ](#grok-syntax)という3つの主要な部分に依存します。`json` [grokタイプ](#grok-syntax)を使用すると、適切にフォーマットされていないログからJSONを抽出して解析できます。たとえば、ログの先頭に日付/時刻文字列が付いている場合などです。

    ```json
    2015-05-13T23:39:43.945958Z {"event": "TestRequest", "status": 200, "response": {"headers": {"X-Custom": "foo"}}, "request": {"headers": {"X-Custom": "bar"}}}
    ```

    このログ形式からJSONデータを抽出して解析するには、次のGrok式を作成します。

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json}
    ```

    結果のログは次のようになります。

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    my_attribute_prefix.event: "TestRequest"
    my_attribute_prefix.status: 200
    my_attribute_prefix.response.headers.X-Custom: "foo"
    my_attribute_prefix.request.headers.X-Custom: "bar"
    ```

    オプション`keepAttributes`または`dropAttributes`を使用して、抽出または削除する属性のリストを定義できます。たとえば、次のGrok式の場合：

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json({"keepAttributes": ["my_attribute_prefix.event", "my_attribute_prefix.response.headers.X-Custom"]})}
    ```

    結果のログは次のようになります。

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    my_attribute_prefix.event: "TestRequest"
    my_attribute_prefix.request.headers.X-Custom: "bar"
    ```

    `my_attribute_prefix`プレフィックスを省略したい場合は、設定に`"noPrefix": true`を含めることができます。

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json({"noPrefix": true})}
    ```

    `my_attribute_prefix`プレフィックスを省略し、 `status`属性のみを保持する場合は、設定に`"noPrefix": true`と`"keepAttributes: ["status"]`を含めることができます。

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json({"noPrefix": true, "keepAttributes": ["status"]})}
    ```

    JSONがエスケープされている場合は、`isEscaped`オプションを使用して解析できます。JSONがエスケープされてから引用符で囲まれている場合は、以下に示すように引用符も一致させる必要があります。たとえば、次のGrok式の場合：

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} "%{GREEDYDATA:my_attribute_prefix:json({"isEscaped": true})}"
    ```

    エスケープされたメッセージを解析することができます。

    ```
    2015-05-13T23:39:43.945958Z "{\"event\": \"TestRequest\", \"status\": 200, \"response\": {\"headers\": {\"X-Custom\": \"foo\"}}, \"request\": {\"headers\": {\"X-Custom\": \"bar\"}}}"
    ```

    結果のログは次のようになります。

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    my_attribute_prefix.event: "TestRequest"
    my_attribute_prefix.status: 200
    my_attribute_prefix.response.headers.X-Custom: "foo"
    my_attribute_prefix.request.headers.X-Custom: "bar"
    ```

    `json` [Grokタイプ](#grok-syntax)を設定するには、 `:json(_CONFIG_)`を使用します。

    * `json({"dropOriginal": true})`：解析に使用されたJSONスニペットを削除します。`true`（デフォルト値）に設定すると、解析ルールによって元のJSONスニペットが削除されます。JSON属性はメッセージフィールドに残ることに注意してください。
    * `json({"dropOriginal": false})`：抽出されたJSONペイロードが表示されます。`false`に設定すると、完全なJSONのみのペイロードが上記の`my_attribute_prefix`で指定された属性の下に表示されます。ここでもJSON属性がメッセージフィールドに残り、ユーザーにJSONデータの3つの異なるビューが提供されることに注意してください。3つのバージョンすべてを保存する必要がある場合は、ここではデフォルトの`true`を使用することをお勧めします。
    * `json({"depth": 62})`：JSON値を解析する深さのレベル（デフォルトは62）。
    * `json({"keepAttributes": ["attr1", "attr2", ..., "attrN"]})`：JSONから抽出される属性を指定します。指定されたリストは空にできません。この設定オプションが設定されていない場合は、すべての属性が抽出されます。
    * `json({"dropAttributes": ["attr1", "attr2", ..., "attrN"]})`：JSONから削除する属性を指定します。この設定オプションが設定されていない場合、属性は削除されません。
    * `json({"noPrefix": true})`：JSONから抽出された属性からプレフィックスを削除するには、このオプションを`true`に設定します。
    * `json({"isEscaped": true})`：エスケープされたJSONを解析するには、このオプションを`true`に設定します（これは通常、JSONが文字列化されたときに表示されます。たとえば、`{\"key\": \"value\"}`）
  </Collapser>

  <Collapser id="parsing-csv" title="CSVの解析">
    システムがコンマ区切り値（CSV）ログを送信し、それをNew Relicで解析する必要がある場合は、GrokパターンによってキャプチャされたCSVを解析する`csv` [Grokタイプ](#grok-syntax)を使用できます。この形式は、Grok構文、解析されたCSV属性に割り当てるプレフィックス、および`csv` [Grokタイプ](#grok-syntax)という3つの主要な部分に依存します。`csv` [Grokタイプ](#grok-syntax)を使用すると、ログからCSVを抽出して解析できます。

    例として次のCSVログ行を示します。

    ```
    "2015-05-13T23:39:43.945958Z,202,POST,/shopcart/checkout,142,10"
    ```

    そして、次の形の解析ルール：

    ```
    %{GREEDYDATA:log:csv({"columns": ["timestamp", "status", "method", "url", "time", "bytes"]})}
    ```

    ログは次のように解析されます：

    ```
    log.timestamp: "2015-05-13T23:39:43.945958Z"
    log.status: "202"
    log.method: "POST"
    log.url: "/shopcart/checkout"
    log.time: "142"
    log.bytes: "10"
    ```

    「ログ」プレフィックスを省略する必要がある場合は、設定に`"noPrefix": true`を含めることができます。

    ```
    %{GREEDYDATA:log:csv({"columns": ["timestamp", "status", "method", "url", "time", "bytes"], "noPrefix": true})}
    ```

    ### 列の設定：

    * CSV Grokタイプ設定（有効なJSONである必要があります）で列を示すことが必須です。
    * 列名として「\_」（アンダースコア）を設定することで、結果のオブジェクトからその列を削除し、任意の列を無視できます。

    ### オプション設定のオプション：

    「列」の設定は必須ですが、次の設定でCSVの解析を変更することができます。

    * <DNT>**dropOriginal**</DNT>：（デフォルトは`true`）解析に使用されるCSVスニペットを削除します。`true`（デフォルト値）に設定すると、解析ルールによって元のフィールドが削除されます。
    * <DNT>**noPrefix**</DNT>：（デフォルトは`false`）結果のオブジェクトのプレフィックスとしてGrokフィールド名を含めません。
    * <DNT>**separator**</DNT>：（デフォルトは`,`）各列を分割する文字/文字列を定義します。
      * もう1つの一般的なシナリオはタブ区切り値（TSV）です。この場合、区切り文字として`\t`を指定する必要があります（例： `%{GREEDYDATA:log:csv({"columns": ["timestamp", "status", "method", "url", "time", "bytes"], "separator": "\t"})`
    * <DNT>**quoteChar**</DNT>：（デフォルトは`"`）列の内容をオプションで囲む文字を定義します。
  </Collapser>

  <Collapser id="geo" title="IPアドレスの位置情報（GeoIP）">
    システムがIPv4アドレスを含むログを送信する場合、 New Relicはそれらを地理的に特定し、指定された属性でログイベントを強化することができます。`geo` [Grokタイプ](#grok-syntax)を使用すると、GrokパターンによってキャプチャされたIPアドレスの位置が検索されます。この形式は、IPの都市、国、緯度/経度など、住所に関連する1つ以上のフィールドを返すように設定できます。

    次のログ行を例に挙げます。

    ```
    2015-05-13T23:39:43.945958Z 146.190.212.184
    ```

    そして、次の形の解析ルール：

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:ip:geo({"lookup":["city","region","countryCode", "latitude","longitude"]})}
    ```

    ログは次のように解析されます。

    ```
    ip: 146.190.212.184
    ip.city: North Bergen
    ip.countryCode: US
    ip.countryName: United States
    ip.latitude: 40.793
    ip.longitude: -74.0247
    ip.postalCode: 07047
    ip.region: NJ
    ip.regionName: New Jersey
    containerTimestamp:2015-05-13T23:39:43.945958Z
    ISO8601_TIMEZONE:Z
    ```

    ### Lookup設定：

    `geo`アクションによって返される必要な`lookup`フィールドを指定することが必須です。以下のオプションから少なくとも1つの項目が必要です。

    * <DNT>**city**</DNT>：都市名
    * <DNT>**countryCode**</DNT>：国の略称
    * <DNT>**countryName**</DNT>：国名
    * <DNT>**latitude**</DNT>：緯度
    * <DNT>**longitude**</DNT>：経度
    * <DNT>**postalCode**</DNT>：郵便番号、ZIPコード、または類似の番号
    * <DNT>**region**</DNT>：州、県、または地域の略語
    * <DNT>**regionName**</DNT>：州、県、または地域の名前
  </Collapser>

  <Collapser id="parsing-key-value-pairs" title="キーの値のペアを解析する">
    New Relic Logsパイプラインはデフォルトでログメッセージを解析しますが、キーの値のペアとしてフォーマットされたログメッセージが存在する場合があります。この状況では、それらを解析して、キーと値の属性を使用してフィルタリングできるようにする必要がある場合があります。

    その場合は、grokパターンによってキャプチャされたキーと値のペアを解析する`keyvalue` [grokタイプ](#grok-syntax)を使用できます。この形式は、grok構文、解析されたキーと値の属性に割り当てるプレフィックス、および`keyvalue` [grokタイプ](#grok-syntax)という3つの主要な部分に依存します。`keyvalue` [grokタイプ](#grok-syntax)を使用すると、適切にフォーマットされていないログからキーの値のペアを抽出して解析できます。たとえば、ログの先頭に日付/時刻文字列が付いている場合などです。

    ```json
      2015-05-13T23:39:43.945958Z key1=value1,key2=value2,key3=value3
    ```

    このログ形式からキーの値データを抽出して解析するには、次のGrok式を作成します。

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:keyvalue()}
    ```

    結果のログは次のようになります。

    ```
      containerTimestamp: "2015-05-13T23:39:43.945958Z"
      my_attribute_prefix.key1: "value1"
      my_attribute_prefix.key2: "value2"
      my_attribute_prefix.key3: "value3"
    ```

    カスタム区切り文字と区切り文字を定義して、必要なキーの値のペアを抽出することもできます。

    ```json
    2015-05-13T23:39:43.945958Z event:TestRequest request:bar
    ```

    たとえば、次のGrok式の場合：

    ```
      %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:keyvalue({"delimiter": " ", "keyValueSeparator": ":"})}
    ```

    結果のログは次のようになります。

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    my_attribute_prefix.event: "TestRequest"
    my_attribute_prefix.request: "bar"
    ```

    `my_attribute_prefix`プレフィックスを省略したい場合は、設定に`"noPrefix": true`を含めることができます。

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:keyValue({"noPrefix": true})}
    ```

    結果のログは次のようになります。

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    event: "TestRequest"
    request: "bar"
    ```

    カスタム引用符文字プレフィックスを設定する場合は、設定に「quoteChar」を含めることができます。

    ```json
    2015-05-13T23:39:43.945958Z nbn_demo='INFO',message='This message contains information with spaces ,sessionId='abc123'
    ```

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:keyValue({"quoteChar": "'"})}
    ```

    結果のログは次のようになります。

    ```
    "my_attribute_prefix.message": "'This message contains information with spaces",
    "my_attribute_prefix.nbn_demo": "INFO",
    "my_attribute_prefix.sessionId": "abc123"
    ```

    ### Grokパターンの問題

    ログ形式に合わせて、次のオプションを使用して解析動作をカスタマイズできます。

    * **区切り文字**

      * **説明：**各キーの値のペアを区切る文字列。
      * **デフォルト値：** `,`（カンマ）
      * **オーバーライド：**この動作を変更するには、フィールド`delimiter`を設定します。

    * **keyValueSeparator**

      * **説明：**キーに値を割り当てるために使用される文字列。
      * **デフォルト値：** `=`
      * **オーバーライド：**カスタム区切り文字を使用するためにフィールド`keyValueSeparator`を設定します。

    * **quoteChar**

      * **説明：**スペースまたは特殊文字を含む値を囲むために使用される文字。
      * **デフォルト値：** `"`（二重引用符）
      * **オーバーライド：**`quoteChar`を使用してカスタム文字を定義します。

    * **dropOriginal**

      * **説明：**解析後に元のログメッセージを削除します。ログの保存容量を削減するのに役立ちます。
      * **デフォルト値：** `true`
      * **オーバーライド：**元のログメッセージを保持するには、 `dropOriginal`を`false`に設定します。

    * **noPrefix**

      * **説明：**`true`の場合、結果のオブジェクトのプレフィックスとしてGrokフィールド名を除外します。
      * **デフォルト値：** `false`
      * **オーバーライド：**`noPrefix`を`true`に設定して有効にします。

    * **escapeChar**

      * **説明：**特殊なログ文字を処理するためのカスタムエスケープ文字を定義します。
      * **デフォルト値：** &amp;quot;&amp;quot;（バックスラッシュ）
      * **オーバーライド：**`escapeChar`でカスタマイズします。

    * **rimValues**

      * **説明：**空白を含む値をトリミングできます。
      * **デフォルト値：** `false`
      * **オーバーライド：**トリミングを有効にするには、 `trimValues`を`true`に設定します。

    * **trimKeys**

      * **説明：**空白を含むキーをトリミングできます。
      * **デフォルト値：** `true`
      * **オーバーライド：**トリミングを有効にするには、 `trimKeys`を`true`に設定します。
  </Collapser>
</CollapserGroup>

## ログタイプ別に整理 [#type]

New Relicのログ取り込みパイプラインは、ログイベントをログの解析方法を記述したルールと照合することでデータを解析できます。ログイベントを解析するには2つの方法があります。

* [組み込みルール](#built-in-rules)を使用します。
* [カスタムルール](#custom-parsing)を定義します。

ルールは、一致ロジックと解析ロジックの組み合わせです。マッチングは、ログの属性にクエリマッチを定義することによって行われます。ルールは遡及的に適用されません。ルールが作成される前に収集されたログは、そのルールによって解析されません。

ログとその解析方法を整理する最も簡単な方法は、ログイベントに`logtype`フィールドを含めることです。これにより、ログに適用する組み込みルールがNew Relicに通知されます。

<Callout variant="important">
  解析ルールがアクティブになると、そのルールによって解析されたデータは永続的に変更されます。これを元に戻すことはできません。
</Callout>

## 制限

解析には計算コストがかかるため、リスクが生じます。解析は、アカウントで定義されたカスタムルールと、ログへのパターンの一致に対して実行されます。パターンの数が多い場合やカスタムルールの定義が不十分な場合は、大量のメモリとCPUリソースが消費され、完了するまでに非常に長い時間がかかります。

問題を防ぐために、メッセージごと、ルールごと、およびアカウントごとの2つの解析制限が適用されます。

<table>
  <thead>
    <tr>
      <th style={{ width: "200px" }}>
        LIMIT:
      </th>

      <th>
        説明
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        メッセージごと、ルールごと
      </td>

      <td>
        メッセージごと、ルールごとの制限により、単一のメッセージの解析にかかる時間が100ミリ秒を超えないようにします。その制限に達すると、システムはそのルールを使用してログメッセージを解析する試みを停止します。

        取り込みパイプラインは、そのメッセージに対して他の適用可能な処理を実行しようとしますが、メッセージは取り込みパイプラインを通過し、NRDBに保存されます。ログメッセージは元の解析されていない形式になります。
      </td>
    </tr>

    <tr>
      <td>
        アカウントごと
      </td>

      <td>
        アカウントごとの制限は、アカウントが公平な割合を超えてリソースを使用することを防ぐために存在します。この制限では、アカウントの <DNT>**all**</DNT>のログメッセージの処理に費やされる1分あたりの合計時間が考慮されます。
      </td>
    </tr>
  </tbody>
</table>

<Callout variant="tip">
  レート制限に達したかどうかを簡単に確認するには、New Relic UIの[システム<DNT>**Limits**</DNT>ページ](/docs/telemetry-data-platform/ingest-manage-data/manage-data/view-system-limits#limits-ui)に移動します。
</Callout>

## ビルトイン解析ルール [#built-in-rules]

一般的なログ形式には、確立された解析ルールがすでに作成されています。組み込みの解析ルールの利点を活用するには、ログを転送するときに`logtype`属性を追加します。次の表に記載されている値を設定すると、その種類のログのルールが自動的に適用されます。

### 組み込みルールのリスト [#rulesets]

次の`logtype`属性値は、定義済みの解析ルールにマップされます。たとえば、アプリケーションロードバランサーを作成するには、次のようにします。

* New Relic UIからは、`logtype:"alb"`形式を使用します。
* [NerdGraph](/docs/apis/nerdgraph/examples/nerdgraph-log-parsing-rules-tutorial/)からは、`logtype = 'alb'`形式を使用します。

各ルールで解析されるフィールドについては、[組み込みの解析ルール](/docs/logs/ui-data/built-log-parsing-rules)に関するドキュメントをご覧ください。

<table>
  <thead>
    <tr>
      <th style={{ width: "200px" }}>
        `logtype`
      </th>

      <th>
        ログソース
      </th>

      <th>
        一致するクエリの例
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        [`apache`](/docs/logs/ui-data/built-log-parsing-rules#apache)
      </td>

      <td>
        Apacheアクセスログ
      </td>

      <td>
        `logtype:"apache"`
      </td>
    </tr>

    <tr>
      <td>
        [`apache_error`](/docs/logs/ui-data/built-log-parsing-rules#apache_error)
      </td>

      <td>
        Apacheエラーログ
      </td>

      <td>
        `logtype:"apache_error"`
      </td>
    </tr>

    <tr>
      <td>
        [`alb`](/docs/logs/ui-data/built-log-parsing-rules#application-load-balancer)
      </td>

      <td>
        アプリケーションロードバランサーログ
      </td>

      <td>
        `logtype:"alb"`
      </td>
    </tr>

    <tr>
      <td>
        [`cassandra`](/docs/logs/ui-data/built-log-parsing-rules#cassandra)
      </td>

      <td>
        Cassandraログ
      </td>

      <td>
        `logtype:"cassandra"`
      </td>
    </tr>

    <tr>
      <td>
        [`cloudfront-web`](/docs/logs/ui-data/built-log-parsing-rules#cloudfront)
      </td>

      <td>
        CloudFront（標準ウェブログ）
      </td>

      <td>
        `logtype:"cloudfront-web"`
      </td>
    </tr>

    <tr>
      <td>
        [`cloudfront-rtl`](/docs/logs/ui-data/built-log-parsing-rules#cloudfront-rtl)
      </td>

      <td>
        CloudFront（リアルタイムウェブログ）
      </td>

      <td>
        `logtype:"cloudfront-rtl"`
      </td>
    </tr>

    <tr>
      <td>
        [`elb`](/docs/logs/ui-data/built-log-parsing-rules#elastic-load-balancer)
      </td>

      <td>
        Elastic Load Balancerログ
      </td>

      <td>
        `logtype:"elb"`
      </td>
    </tr>

    <tr>
      <td>
        [`haproxy_http`](/docs/logs/ui-data/built-log-parsing-rules#haproxy)
      </td>

      <td>
        HAProxyログ
      </td>

      <td>
        `logtype:"haproxy_http"`
      </td>
    </tr>

    <tr>
      <td>
        [`ktranslate-health`](/docs/logs/ui-data/built-log-parsing-rules#ktranslate-health)
      </td>

      <td>
        KTranslateコンテナの健全性ログ
      </td>

      <td>
        `logtype:"ktranslate-health"`
      </td>
    </tr>

    <tr>
      <td>
        [`linux_cron`](/docs/logs/ui-data/built-log-parsing-rules/#linux_cron)
      </td>

      <td>
        Linux cron
      </td>

      <td>
        `logtype:"linux_cron"`
      </td>
    </tr>

    <tr>
      <td>
        [`linux_messages`](/docs/logs/ui-data/built-log-parsing-rules/#linux_messages)
      </td>

      <td>
        Linuxメッセージ
      </td>

      <td>
        `logtype:"linux_messages"`
      </td>
    </tr>

    <tr>
      <td>
        [`iis_w3c`](/docs/logs/ui-data/built-log-parsing-rules/#iis)
      </td>

      <td>
        Microsoft IISサーバーログ - W3C形式
      </td>

      <td>
        `logtype:"iis_w3c"`
      </td>
    </tr>

    <tr>
      <td>
        [`mongodb`](/docs/logs/ui-data/built-log-parsing-rules#mongodb)
      </td>

      <td>
        MongoDBログ
      </td>

      <td>
        `logtype:"mongodb"`
      </td>
    </tr>

    <tr>
      <td>
        [`monit`](/docs/logs/ui-data/built-log-parsing-rules#monit)
      </td>

      <td>
        Monitログ
      </td>

      <td>
        `logtype:"monit"`
      </td>
    </tr>

    <tr>
      <td>
        [`mysql-error`](/docs/logs/ui-data/built-log-parsing-rules#mysql-error)
      </td>

      <td>
        MySQLエラーログ
      </td>

      <td>
        `logtype:"mysql-error"`
      </td>
    </tr>

    <tr>
      <td>
        [`nginx`](/docs/logs/ui-data/built-log-parsing-rules#nginx)
      </td>

      <td>
        NGINXアクセスログ
      </td>

      <td>
        `logtype:"nginx"`
      </td>
    </tr>

    <tr>
      <td>
        [`nginx-error`](/docs/logs/ui-data/built-log-parsing-rules#nginx-error)
      </td>

      <td>
        NGINXエラーログ
      </td>

      <td>
        `logtype:"nginx-error"`
      </td>
    </tr>

    <tr>
      <td>
        [`postgresql`](/docs/logs/ui-data/built-log-parsing-rules#postgresql)
      </td>

      <td>
        PostgreSQLログ
      </td>

      <td>
        `logtype:"postgresql"`
      </td>
    </tr>

    <tr>
      <td>
        [`rabbitmq`](/docs/logs/ui-data/built-log-parsing-rules#rabbitmq)
      </td>

      <td>
        Rabbitmqログ
      </td>

      <td>
        `logtype:"rabbitmq"`
      </td>
    </tr>

    <tr>
      <td>
        [`redis`](/docs/logs/ui-data/built-log-parsing-rules#redis)
      </td>

      <td>
        Redisログ
      </td>

      <td>
        `logtype:"redis"`
      </td>
    </tr>

    <tr>
      <td>
        [`route-53`](/docs/logs/ui-data/built-log-parsing-rules#route53)
      </td>

      <td>
        Route 53ログ
      </td>

      <td>
        `logtype:"route-53"`
      </td>
    </tr>

    <tr>
      <td>
        [`syslog-rfc5424`](/docs/logs/ui-data/built-log-parsing-rules/#syslog-rfc5424)
      </td>

      <td>
        RFC5424形式のSyslog
      </td>

      <td>
        `logtype:"syslog-rfc5424"`
      </td>
    </tr>
  </tbody>
</table>

### を追加する `logtype` [#logattr]

ログを集約するときは、ログの整理、検索、解析を容易にするメタデータを提供することが重要です。これを行う簡単な方法の1つは、出荷時にログメッセージに属性`logtype`を追加することです。[組み込みの解析ルール](#built-in-rules)は、特定の`logtype`値にデフォルトで適用されます。

<Callout variant="tip">
  フィールド`logType`、 `logtype`、 `LOGTYPE`はすべて組み込みルールでサポートされています。検索を容易にするために、組織内で単一の構文に合わせることをお勧めします。
</Callout>

以下は、[サポートされている配送方法](/docs/logs/enable-new-relic-logs)によって送信されたログに`logtype`を追加する方法の例です。

<CollapserGroup>
  <Collapser className="freq-link" id="infrastructure-log-forwarder-example" title="New Relic Infrastructureエージェントの例">
    [`attribute`](/docs/logs/forward-logs/forward-your-logs-using-infrastructure-agent#attributes)として`logtype`を追加します。名前付きソースごとにログタイプを設定する必要があります。

    ```yml
    logs:
      - name: file-simple
        file: /path/to/file
        attributes:
          logtype: fileRaw
      - name: nginx-example
        file: /var/log/nginx.log
        attributes:
          logtype: nginx
    ```
  </Collapser>

  <Collapser className="freq-link" id="fluentd-example" title="Fluentdの例">
    `record_transformer`を使用して新しいフィールドを追加するフィルターブロックを、`.conf`ファイルに追加します。この例では、組み込みのNGINX解析ルールをトリガーするために、`nginx`の`logtype`を使用します。他の[Fluentdの例](https://github.com/newrelic/fluentd-examples)を確認してください。

    ```apacheconf
    <filter containers>
      @type record_transformer
      enable_ruby true
      <record>
        #Add logtype to trigger a built-in parsing rule for nginx access logs
        logtype nginx
        #Set timestamp from the value contained in the field "time"
        timestamp record["time"]
        #Add hostname and tag fields to all records
        hostname "#{Socket.gethostname}"
        tag ${tag}
      </record>
    </filter>
    ```
  </Collapser>

  <Collapser className="freq-link" id="fluentbit-example" title="Fluent Bitの例">
    `record_modifier`を使用して新しいフィールドを追加するフィルターブロックを、`.conf`ファイルに追加します。この例では、組み込みのNGINX解析ルールをトリガーするために、`nginx`の`logtype`を使用します。他の[Fluent Bitの例](https://github.com/newrelic/fluentbit-examples)を確認してください。

    ```ini
    [FILTER]
        Name   record_modifier
        Match  *
        Record logtype nginx
        Record hostname ${HOSTNAME}
        Record service_name Sample-App-Name
    ```
  </Collapser>

  <Collapser className="freq-link" id="logstash-example" title="Logstashの例">
    `add_field` mutateフィルターを使用して新しいフィールドを追加するフィルターブロックを、Logstash設定に追加します。この例では、組み込みのNGINX解析ルールをトリガーするために、`nginx`の`logtype`を使用します。他の[Logstashの例](https://github.com/newrelic/logstash-examples)を確認してください。

    ```ini
    filter {
      mutate {
        add_field => {
          "logtype" => "nginx"
          "service_name" => "myservicename"
          "hostname" => "%{host}"
        }
      }
    }
    ```
  </Collapser>

  <Collapser className="freq-link" id="api-example" title="ログAPIの例">
    New Relicに送信されるJSONリクエストに属性を追加できます。この例では、組み込みのNGINX解析ルールをトリガーするために、値`nginx`の`logtype`属性を追加します。[ログAPI](/docs/logs/new-relic-logs/log-api/introduction-log-api)の使用について詳しくは、こちらをご覧ください。

    ```
    POST /log/v1 HTTP/1.1
    Host: log-api.newrelic.com
    Content-Type: application/json
    X-License-Key: YOUR_LICENSE_KEY
    Accept: */*
    Content-Length: 133
    {
      "timestamp": TIMESTAMP_IN_UNIX_EPOCH,
      "message": "User 'xyz' logged in",
      "logtype": "nginx",
      "service": "login-service",
      "hostname": "login.example.com"
    }
    ```
  </Collapser>
</CollapserGroup>

## カスタム解析ルールの作成と表示 [#custom-parsing]

多くのログは、独自の方法でフォーマットまたは構造化されています。これらを解析するには、カスタムロジックを構築して適用する必要があります。

<img title="Log parsing rules" alt="Screenshot of log parsing in UI" src="/images/logs_screenshot-full_parsing-ui.webp" />

<figcaption>
  ログUIの左側のナビゲーションから<DNT>**Parsing**</DNT>を選択し、有効なNRQL `WHERE`句とGrokパターンを使用して独自のカスタム解析ルールを作成します。
</figcaption>

独自のカスタム解析ルールを作成および管理するには：

1. <DNT>**[one.newrelic.com &gt; All capabilities](https://one.newrelic.com/all-capabilities) &amp;gt; Logs**</DNT>に移動します。
2. ログUIの左側のナビゲーションの<DNT>**Manage data**</DNT>から、 <DNT>**Parsing**</DNT>をクリックし、次に<DNT>**Create parsing rule**</DNT>をクリックします。
3. 新しい解析ルールの名前を入力します。
4. 解析する既存のフィールドを選択するか（デフォルト = `message`）、新しいフィールド名を入力します。
5. 解析するログに一致する有効なNRQL `WHERE`句を入力します。
6. 一致するログが存在する場合はそれを選択するか、 <DNT>**Paste log**</DNT>タブをクリックしてサンプルログを貼り付けます。ログUIまたは書き込みビルダーからテキストをコピーして解析UIに貼り付ける場合は、それが <DNT>*Unformatted*</DNT> バージョンであることを確認してください。
7. 解析ルールを入力し、 <DNT>**Output**</DNT>セクションの結果を表示してそれが機能していることを確認します。Grokとカスタム解析ルールの詳細については、[Grokパターンを使用してログを解析する方法に関するブログ記事](https://blog.newrelic.com/product-news/how-to-use-grok-log-parsing)をお読みください。
8. カスタム解析ルールを有効にして保存します。

既存の解析ルールを表示するには：

1. <DNT>**[one.newrelic.com &gt; All capabilities](https://one.newrelic.com/all-capabilities) &amp;gt; Logs**</DNT>に移動します。
2. ログUIの左側のナビゲーションにある<DNT>**Manage data**</DNT>から、 <DNT>**Parsing**</DNT>をクリックします。

## トラブルシューティング [#troubleshooting]

解析が意図したとおりに機能しない場合は、次の原因が考えられます。

* <DNT>**Logic:**</DNT> 解析ルールの一致ロジックが、必要なログと一致しません。
* <DNT>**Timing:**</DNT> 解析一致ルールがまだ存在しない値である場合、失敗します。これは、エンリッチメントプロセスの一部として値がパイプラインの後半で追加された場合に発生する可能性があります。
* <DNT>**Limits:**</DNT> 解析、パターン、ドロップフィルターなどを使用してログを処理するために、1分ごとに一定の時間が利用できます。最大時間が経過した場合、追加のログイベントレコードの解析はスキップされます。

これらの問題を解決するには、[カスタム解析ルール](#custom-parsing)を作成または調整します。