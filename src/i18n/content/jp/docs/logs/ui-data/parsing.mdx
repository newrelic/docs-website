---
title: ログデータの解析
tags:
  - Logs
  - Log management
  - UI and data
metaDescription: How New Relic uses parsing and how to send customized log data.
translationType: machine
---

New Relic では、ログの**解析**とは、構造化されていないログ データから[属性](/docs/new-relic-solutions/get-started/glossary/#attribute)(キー:値のペア) を引き出すプロセスを指します。これらの属性を使用して、より実用的な方法でログを検索およびクエリできます。これにより、より優れたグラフとアラートを作成できます。

New Relic は、特定の解析ルールに従ってログ データを自動的に解析します。このドキュメントでは、ログの解析の仕組みと、独自のカスタム解析ルールを作成する方法について学習します。

NerdGraph (GraphQL API) を使用して、ログ解析ルールを作成、クエリ、管理することもできます。これに役立つツールは、 [Nerdgraph API Explorer](https://api.newrelic.com/graphiql)です。詳細については、 [解析に関する NerdGraph チュートリアルを](/docs/apis/nerdgraph/examples/nerdgraph-log-parsing-rules-tutorial/)参照してください。

ログの解析に関する 5 分間のビデオを次に示します。

<Video
  id="xPWM46yw3bQ"
  type="youtube"
/>

## 構文解析の例 [#parsing-defined]

良い例は、非構造化テキストを含むデフォルトの NGINX アクセス ログです。検索には便利ですが、それ以外はあまり役に立ちません。典型的な行の例を次に示します。

```
127.180.71.3 - - [10/May/1997:08:05:32 +0000] "GET /downloads/product_1 HTTP/1.1" 304 0 "-" "Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)"
```

解析されていない形式では、ほとんどの質問に答えるために全文検索を行う必要があります。解析後、ログは`response code`や`request URL`などの属性に整理されます。

```
{
  "remote_addr":"93.180.71.3",
  "time":"1586514731",
  "method":"GET",
  "path":"/downloads/product_1",
  "version":"HTTP/1.1",
  "response":"304",
  "bytesSent": 0,
  "user_agent": "Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)"
}
```

解析により、これらの値をファセットする[カスタムクエリ](/docs/using-new-relic/data/understand-data/query-new-relic-data)を簡単に作成できます。これは、リクエストURLごとの応答コードの分布を理解し、問題のあるページをすばやく見つけるのに役立ちます。

## ログ解析のしくみ [#how-it-works]

NewRelicがログの解析を実装する方法の概要は次のとおりです。

<table>
  <thead>
    <tr>
      <th style={{ width: "100px" }}>
        ログの解析
      </th>

      <th>
        使い方
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        何
      </td>

      <td>
        * 解析は、選択された特定のフィールドに適用されます。デフォルトでは、 `message`フィールドが使用されます。ただし、現在データに存在しないものであっても、任意のフィールド/属性を選択できます。
        * 各解析ルールは、ルールが解析を試みるログを決定する NRQL `WHERE`句を使用して作成されます。
        * 照合プロセスを簡素化するために、 [`logtype`](#logtype)属性をログに追加することをお勧めします。ただし、使用できるのは`logtype`に限定されません。 NRQL `WHERE`句で一致基準として1つ以上の属性を使用できます。
      </td>
    </tr>

    <tr>
      <td>
        いつ
      </td>

      <td>
        * 解析は、各ログメッセージに1回だけ適用されます。複数の解析ルールがログに一致する場合、成功した最初のルールのみが適用されます。
        * 解析ルールには順序がありません。複数の解析ルールがログに一致する場合、ランダムに 1 つが選択されます。同じログに一致しないように解析ルールを構築してください。
        * 解析は、データがNRDBに書き込まれる前に、ログの取り込み中に行われます。データがストレージに書き込まれると、それを解析することはできなくなります。
        * 解析は、データの強化が行わ**れる前**にパイプラインで行われます。解析ルールの一致基準を定義するときは注意してください。基準が、解析またはエンリッチメントが実行されるまで存在しない属性に基づいている場合、一致が発生したときにそのデータはログに存在しません。その結果、解析は行われません。
      </td>
    </tr>

    <tr>
      <td>
        どのように
      </td>

      <td>
        * ルールは、 [Grok](#grok) 、正規表現、またはその 2 つの組み合わせで記述できます。Grok は、複雑な正規表現を抽象化するパターンのコレクションです。
      </td>
    </tr>
  </tbody>
</table>

## Grokを使用して属性を解析する [#grok]

解析パターンは、ログ メッセージ解析の業界標準である Grok を使用して指定されます。`logtype`フィールドを持つ受信ログは、[組み込みの解析ルール](#built-in-rules)に照らしてチェックされ、可能であれば、関連する Grok パターンがログに適用されます。

Grokは正規表現のスーパーセットであり、リテラルの複雑な正規表現の代わりに使用される組み込みの名前付きパターンを追加します。たとえば、整数を正規表現`(?:[+-]?(?:[0-9]+))`と一致させることができることを覚えておく代わりに、 `%{INT}`と書くだけで、同じ正規表現を表すGrokパターン`INT`を使用できます。

Grokパターンの構文は次のとおりです。

```
%{PATTERN_NAME[:OPTIONAL_EXTRACTED_ATTRIBUTE_NAME[:OPTIONAL_TYPE]]}
```

どこ：

* `PATTERN_NAME` サポートされている Grok パターンの 1 つです。パターン名は、正規表現を表すわかりやすい名前です。それらは、対応する正規表現とまったく同じです。
* `OPTIONAL_EXTRACTED_ATTRIBUTE_NAME`、指定されている場合は、パターン名と一致する値でログメッセージに追加される属性の名前です。これは、正規表現を使用して名前付きキャプチャグループを使用するのと同じです。これが指定されていない場合、解析ルールは文字列の領域に一致するだけで、その値で属性を抽出しません。
* `OPTIONAL_TYPE` 抽出する属性値のタイプを指定します。省略した場合、値は文字列として抽出されます。たとえば、 `"File Size: 123"` }から値`123`を数値として属性`file_size`に抽出するには、 `value: %{INT:file_size:int}`を使用します。

一致する文字列で正規表現と Grok パターン名を組み合わせて使用することもできます。

サポートされている[Grok パターン](https://github.com/thekrakken/java-grok/tree/master/src/main/resources/patterns)のリストについては、このリンクをクリックしてください。サポートされている[Grok タイプ](#grok-types)のリストについては、ここをクリックしてください。

変数名は明示的に設定し、 `%{URI:uri}`のように小文字にする必要があることに注意してください。`%{URI}`や`%{URI:URI}`などの式は機能しません。

<CollapserGroup>
  <Collapser
    id="grok-example"
    title="Grokの例：ログから有用なデータを取得する"
  >
    ログレコードは次のようになります。

    ```
    {
      "message": "54.3.120.2 2048 0"
    }
    ```

    この情報は正確ですが、それが何を意味するのか正確には直感的ではありません。 Grokパターンは、必要なテレメトリデータを抽出して理解するのに役立ちます。たとえば、次のようなログレコードははるかに使いやすいです。

    ```
    {
      "host_ip": "43.3.120.2",
      "bytes_received": 2048,
      "bytes_sent": 0
    }
    ```

    これを行うには、これら3つのフィールドを抽出するGrokパターンを作成します。例えば：

    ```
    %{IP:host_ip} %{INT:bytes_received} %{INT:bytes_sent}
    ```

    処理後、ログレコードにはフィールド`host_ip` 、 `bytes_received` 、および`bytes_sent`が含まれます。 New Relicでこれらのフィールドを使用して、ログデータのフィルタリング、ファセット、統計操作を実行できるようになりました。 New RelicでGrokパターンを使用してログを解析する方法の詳細については[、ブログ投稿を参照して](https://newrelic.com/blog/how-to-relic/how-to-use-grok-log-parsing)ください。
  </Collapser>

  <Collapser
    id="grok-ui"
    title="UIの例：Grok解析ルールの作成"
  >
    適切な権限がある場合は、UIで解析ルールを作成して、Grok解析を作成、テスト、および有効にすることができます。たとえば、Inventory Servicesと呼ばれるマイクロサービスの1つについて特定の種類のエラーメッセージを取得するには、特定のエラーメッセージと製品を検索するGrok解析ルールを作成します。これをする：

    1. ルールに名前を付けます。たとえば、 `Inventory Services error parsing` 。

    2. 解析する既存のフィールドを選択するか (デフォルト = `message` )、新しいフィールド名を入力します。

    3. 着信ログのプレフィルターとして機能する NRQL `WHERE`句を特定します。たとえば、 `entity.name='Inventory Service'`です。このプレフィルターは、ルールで処理する必要があるログの数を絞り込み、不要な処理を削除します。

    4. 一致するログが存在する場合はそれを選択するか、\[ログの貼り付け] タブをクリックしてサンプル ログに貼り付けます。

    5. Grok解析ルールを追加します。例えば：

       ```
       Inventory error: %{DATA:error_message} for product %{INT:product_id}
       ```

       どこ：

    * `Inventory error`：解析ルールの名前
    * `error_message`：選択したいエラーメッセージ
    * `product_id`：在庫サービスの製品ID

    6. 解析ルールを有効にして保存します。

       すぐに、InventoryServiceログが2つの新しいフィールド`error_message`と`product_id`で強化されていることがわかります。ここから、これらのフィールドのクエリ、チャートやダッシュボードの作成、アラートの設定などを行うことができます。

       詳細について[は、UI でカスタム解析ルールを追加するためのドキュメントを](#custom-parsing)参照してください。
  </Collapser>

  <Collapser
    id="grok-types"
    title="サポートされている Grok 型"
  >
    `OPTIONAL_TYPE`フィールドは、抽出する属性値のタイプを指定します。省略した場合、値は文字列として抽出されます。

    サポートされているタイプは次のとおりです。

    <table>
      <thead>
        <tr>
          <th>
            Grokで指定されたタイプ
          </th>

          <th>
            NewRelicデータベースに保存されているタイプ
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `boolean`
          </td>

          <td>
            `boolean`
          </td>
        </tr>

        <tr>
          <td>
            `byte` `short` `int` `integer`
          </td>

          <td>
            `integer`
          </td>
        </tr>

        <tr>
          <td>
            `long`
          </td>

          <td>
            `long`
          </td>
        </tr>

        <tr>
          <td>
            `float`
          </td>

          <td>
            `float`
          </td>
        </tr>

        <tr>
          <td>
            `double`
          </td>

          <td>
            `double`
          </td>
        </tr>

        <tr>
          <td>
            `string` （デフォルト） `text`
          </td>

          <td>
            `string`
          </td>
        </tr>

        <tr>
          <td>
            `date` `datetime`
          </td>

          <td>
            ISO8601時間として `long`
          </td>
        </tr>

        <tr>
          <td>
            `string` （デフォルト） `text`
          </td>

          <td>
            `string`
          </td>
        </tr>

        <tr>
          <td>
            `date` `datetime`
          </td>

          <td>
            ISO8601時間として `long`
          </td>
        </tr>

        <tr>
          <td>
            `json`
          </td>

          <td>
            JSON 構造化データ。詳細について[は、プレーン テキストと混合した JSON の解析](#parsing-json)を参照してください。
          </td>
        </tr>

        <tr>
          <td>
            `csv`
          </td>

          <td>
            CSV データ。詳細については、 [CSV の解析](#parsing-csv)を参照してください。
          </td>
        </tr>

        <tr>
          <td>
            `geo`
          </td>

          <td>
            IP アドレスからの地理的位置。詳細については [、「地理位置情報 IP アドレス (GeoIP)」](#geo) を参照してください。
          </td>
        </tr>
      </tbody>
    </table>
  </Collapser>

  <Collapser
    id="grok-multiline"
    title="Grok の複数行の解析"
  >
    複数行のログがある場合は、 `GREEDYDATA` Grok パターンが改行と一致しないことに注意してください (これは `.*`と同等です)。

    したがって、 `%{GREEDYDATA:some_attribute}` を直接使用する代わりに、その前に複数行のフラグを追加する必要があります。 `(?s)%{GREEDYDATA:some_attribute}`
  </Collapser>

  <Collapser
    id="parsing-json"
    title="プレーンテキストと混合した JSON の解析"
  >
    New Relic ログ パイプラインは、デフォルトでログ JSON メッセージを解析しますが、JSON ログ メッセージがプレーン テキストと混在している場合があります。この状況では、それらを解析して、JSON 属性を使用してフィルタリングできるようにする必要がある場合があります。その場合は、 `json` [grok type](#grok-syntax)を使用できます。これは、grok パターンによってキャプチャされた JSON を解析します。この形式は、grok 構文、解析された json 属性に割り当てるプレフィックス、 `json` [grok タイプ](#grok-syntax)の 3 つの主要部分に依存しています。`json` [grok type](#grok-syntax)を使用すると、適切にフォーマットされていないログから JSON を抽出して解析できます。たとえば、ログに日付/時刻の文字列がプレフィックスとして付けられている場合:

    ```
    2015-05-13T23:39:43.945958Z {"event": "TestRequest", "status": 200, "response": {"headers": {"X-Custom": "foo"}}, "request": {"headers": {"X-Custom": "bar"}}}
    ```

    このログ形式から JSON データを抽出して解析するには、次の Grok 式を作成します。

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json}
    ```

    結果のログは次のとおりです。

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    my_attribute_prefix.event: "TestRequest"
    my_attribute_prefix.status: 200
    my_attribute_prefix.response.headers.X-Custom: "foo"
    my_attribute_prefix.request.headers.X-Custom: "bar"
    ```

    オプション `keepAttributes`を使用して、抽出する属性のリストを定義できます。たとえば、次の Grok 式を使用します。

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json({"keepAttributes": ["my_attribute_prefix.event", "my_attribute_prefix.response.headers.X-Custom"]})}
    ```

    結果のログは次のとおりです。

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    my_attribute_prefix.event: "TestRequest"
    my_attribute_prefix.request.headers.X-Custom: "bar"
    ```

    `my_attribute_prefix` 接頭辞を省略する必要がある場合は、構成に `"noPrefix": true` を含めることができます。

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json({"noPrefix": true})}
    ```

    `:json(_CONFIG_)`を使用して`json` [Grok タイプ](#grok-syntax)を構成することもできます。

    * `json({"dropOriginal": true})`: 解析で使用された JSON スニペットをドロップします。`true` (デフォルト値) に設定すると、解析ルールは元の JSON スニペットを削除します。JSON 属性はメッセージ フィールドに残ることに注意してください。
    * `json({"dropOriginal": false})`: 抽出された JSON ペイロードが表示されます。`false`に設定すると、完全な JSON のみのペイロードが上記の`my_attribute_prefix`で指定された属性の下に表示されます。ここでは、JSON 属性がメッセージ フィールドに残り、ユーザーに JSON データの 3 つの異なるビューを提供することに注意してください。3 つのバージョンすべての保存が懸念される場合は、ここでデフォルトの`true`を使用することをお勧めします。
    * `json({"depth": 62})`: JSON 値を解析する深さのレベル (デフォルトは 62)。
    * `json({"keepAttributes": ["attr1", "attr2", ..., "attrN"]})`: JSON から抽出する属性を指定します。提供されたリストを空にすることはできません。この構成オプションが設定されていない場合は、すべての属性が抽出されます。
    * `json({"noPrefix": true})`: JSON から抽出された属性からプレフィックスを削除するには、このオプションを `true` に設定します。
  </Collapser>

  <Collapser
    id="parsing-csv"
    title="CSV の解析"
  >
    システムがコンマ区切り値 (CSV) ログを送信し、それらを New Relic で解析する必要がある場合は、Grok パターンによってキャプチャされた CSV を解析する`csv` [Grok タイプ](#grok-syntax)を使用できます。この形式は、Grok 構文、解析された CSV 属性に割り当てる接頭辞、および`csv` [Grok タイプ](#grok-syntax)の 3 つの主要部分に依存しています。`csv` [Grok タイプ](#grok-syntax)を使用すると、ログから CSV を抽出して解析できます。

    例として、次の CSV ログ行があるとします。

    ```
    "2015-05-13T23:39:43.945958Z,202,POST,/shopcart/checkout,142,10"
    ```

    そして、次の形の解析規則:

    ```
    %{GREEDYDATA:log:csv({"columns": ["timestamp", "status", "method", "url", "time", "bytes"]})}
    ```

    ログを次のように解析します。

    ```
    log.timestamp: "2015-05-13T23:39:43.945958Z"
    log.status: "202"
    log.method: "POST"
    log.url: "/shopcart/checkout"
    log.time: "142"
    log.bytes: "10"
    ```

    「ログ」プレフィックスを省略する必要がある場合は、構成に`"noPrefix": true`を含めることができます。

    ```
    %{GREEDYDATA:log:csv({"columns": ["timestamp", "status", "method", "url", "time", "bytes"], "noPrefix": true})}
    ```

    ### 列の構成:

    * CSV Grok タイプ構成 (有効な JSON である必要があります) で列を示すことは必須です。
    * 列名として "\_" (アンダースコア) を設定して、結果のオブジェクトから削除することにより、任意の列を無視できます。

    ### オプションの構成オプション:

    「列」構成は必須ですが、次の設定で CSV の解析を変更することができます。

    * **dropOriginal** : (デフォルトは`true` ) 解析で使用される CSV スニペットをドロップします。`true` (デフォルト値) に設定すると、解析ルールは元のフィールドを削除します。

    * **noPrefix** : (デフォルトは`false` ) 結果のオブジェクトに Grok フィールド名をプレフィックスとして含めません。

    * **separator** : (デフォルトは`,` ) 各列を分割する文字/文字列を定義します。

      * もう 1 つの一般的なシナリオは、タブ区切り値 (TSV) です。そのため、セパレータとして`\t`を指定する必要があります。 `%{GREEDYDATA:log:csv({"columns": ["timestamp", "status", "method", "url", "time", "bytes"], "separator": "\t"})`

    * **quoteChar** : (デフォルトは`"` ) 必要に応じて列の内容を囲む文字を定義します。
  </Collapser>

  <Collapser
    id="geo"
    title="IP アドレスの位置情報 (GeoIP)"
  >
    システムが IPv4 アドレスを含むログを送信する場合、New Relic はそれらを地理的に特定し、指定された属性でログ イベントを充実させることができます。 `geo` [Grok タイプ](#grok-syntax)を使用できます。これは、Grok パターンによってキャプチャされた IP アドレスの位置を見つけます。この形式は、住所に関連する 1 つ以上のフィールド (都市、国、IP の緯度/経度など) を返すように構成できます。

    例として次のログ行があるとします。

    ```
    2015-05-13T23:39:43.945958Z 146.190.212.184
    ```

    そして、次の形の解析規則:

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:ip:geo({"lookup":["city","region","countryCode", "latitude","longitude"]})}
    ```

    ログは次のように解析されます。

    ```
    ip: 146.190.212.184
    ip.city: North Bergen
    ip.countryCode: US
    ip.countryName: United States
    ip.latitude: 40.793
    ip.longitude: -74.0247
    ip.postalCode: 07047
    ip.region: NJ
    ip.regionName: New Jersey
    containerTimestamp:2015-05-13T23:39:43.945958Z
    ISO8601_TIMEZONE:Z
    ```

    ### ルックアップ構成:

    `geo` アクションによって返される目的の `lookup` フィールドを指定することは必須です。次のオプションから少なくとも 1 つのアイテムが必要です。

    * **city**: 都市名
    * **countryCode**: 国の略称
    * **countryName**: 国名
    * **latitude**: 緯度
    * **longitude**: 経度
    * **postalCode**: 郵便番号、郵便番号など
    * **region**: 州、州、または地域の略語
    * **regionName**: 州、県、または地域の名前
  </Collapser>
</CollapserGroup>

## ログタイプによる整理 [#type]

New Relicのログ取り込みパイプラインは、ログイベントをログの解析方法を説明するルールと照合することでデータを解析できます。ログイベントを解析する方法は2つあります。

* [組み込みのルール](#built-in-rules)を使用します。
* [カスタムルール](#custom-parsing)を定義します。

ルールは、マッチング ロジックと解析ロジックの組み合わせです。照合は、ログの属性でクエリ照合を定義することによって行われます。ルールはさかのぼって適用されません。ルールが作成される前に収集されたログは、そのルールによって解析されません。

ログとその解析方法を整理する最も簡単な方法は、ログ イベントに`logtype`フィールドを含めることです。これにより、どの組み込みルールをログに適用するかが New Relic に伝えられます。

<Callout variant="important">
  解析ルールがアクティブになると、ルールによって解析されたデータは完全に変更されます。これは元に戻せません。
</Callout>

## 制限

解析は計算コストが高く、リスクが伴います。解析は、アカウントで定義されたカスタムルールと、パターンをログに一致させるために行われます。多数のパターンまたは不十分に定義されたカスタムルールは、完了するのに非常に長い時間がかかる一方で、大量のメモリとCPUリソースを消費します。

問題を防ぐために、ルールごととアカウントごとの2つの解析制限を適用します。

<table>
  <thead>
    <tr>
      <th style={{ width: "200px" }}>
        制限
      </th>

      <th>
        説明
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        ルールごとのメッセージごと
      </td>

      <td>
        ルールごとのメッセージごとの制限により、単一のメッセージの解析に費やされる時間が100ミリ秒を超えることはありません。その制限に達すると、システムはそのルールでログメッセージを解析しようとするのをやめます。

        取り込みパイプラインは、そのメッセージに該当する他のメッセージを実行しようとしますが、メッセージは引き続き取り込みパイプラインを通過してNRDBに保存されます。ログメッセージは、元の未解析の形式になります。
      </td>
    </tr>

    <tr>
      <td>
        アカウントごと
      </td>

      <td>
        アカウントごとの制限は、アカウントがリソースの公平なシェアを超えて使用することを防ぐために存在します。制限では、1分あたりのアカウントの**すべての**ログメッセージの処理に費やされた合計時間が考慮されます。

        制限は固定値ではありません。アカウントによって毎日保存されるデータの量と、その顧客をサポートするために後で割り当てられる環境サイズに比例してスケールアップまたはスケールダウンします。
      </td>
    </tr>
  </tbody>
</table>

<Callout variant="tip">
  レート制限に達しているかどうかを簡単に確認するには、NewRelicUIの[システム**制限**ページ](/docs/telemetry-data-platform/ingest-manage-data/manage-data/view-system-limits#limits-ui)に移動します。
</Callout>

## 組み込みの解析ルール [#built-in-rules]

一般的なログ形式には、確立された解析ルールがすでに作成されています。組み込みの解析ルールを利用するには、ログを転送するときに`logtype`属性を追加します。次の表にリストされている値に値を設定すると、そのタイプのログのルールが自動的に適用されます。

### 組み込みルールのリスト [#rulesets]

次の`logtype`属性値は、事前定義された解析ルールにマップされます。たとえば、アプリケーションロードバランサーをクエリするには、次のようにします。

* New Relic UIから、形式`logtype:"alb"`を使用します。
* [NerdGraph](/docs/apis/nerdgraph/examples/nerdgraph-log-parsing-rules-tutorial/)から、形式`logtype = 'alb'`を使用します。

各ルールで解析されるフィールドについては、[組み込みの解析ルールに関する](/docs/logs/ui-data/built-log-parsing-rules)ドキュメントを参照してください。

<table>
  <thead>
    <tr>
      <th style={{ width: "200px" }}>
        `logtype`
      </th>

      <th>
        ログソース
      </th>

      <th>
        マッチングクエリの例
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        [`apache`](/docs/logs/ui-data/built-log-parsing-rules#apache)
      </td>

      <td>
        Apacheアクセスログ
      </td>

      <td>
        `logtype:"apache"`
      </td>
    </tr>

    <tr>
      <td>
        [`apache_error`](/docs/logs/ui-data/built-log-parsing-rules#apache_error)
      </td>

      <td>
        Apacheエラーログ
      </td>

      <td>
        `logtype:"apache_error"`
      </td>
    </tr>

    <tr>
      <td>
        [`alb`](/docs/logs/ui-data/built-log-parsing-rules#application-load-balancer)
      </td>

      <td>
        アプリケーションロードバランサーログ
      </td>

      <td>
        `logtype:"alb"`
      </td>
    </tr>

    <tr>
      <td>
        [`cassandra`](/docs/logs/ui-data/built-log-parsing-rules#cassandra)
      </td>

      <td>
        カサンドラログ
      </td>

      <td>
        `logtype:"cassandra"`
      </td>
    </tr>

    <tr>
      <td>
        [`cloudfront-web`](/docs/logs/ui-data/built-log-parsing-rules#cloudfront)
      </td>

      <td>
        CloudFront (標準のウェブログ)
      </td>

      <td>
        `logtype:"cloudfront-web"`
      </td>
    </tr>

    <tr>
      <td>
        [`cloudfront-rtl`](/docs/logs/ui-data/built-log-parsing-rules#cloudfront-rtl)
      </td>

      <td>
        CloudFront (リアルタイム ウェブ ログ)
      </td>

      <td>
        `logtype:"cloudfront-rtl"`
      </td>
    </tr>

    <tr>
      <td>
        [`elb`](/docs/logs/ui-data/built-log-parsing-rules#elastic-load-balancer)
      </td>

      <td>
        ElasticLoadBalancerログ
      </td>

      <td>
        `logtype:"elb"`
      </td>
    </tr>

    <tr>
      <td>
        [`haproxy_http`](/docs/logs/ui-data/built-log-parsing-rules#haproxy)
      </td>

      <td>
        HAProxyログ
      </td>

      <td>
        `logtype:"haproxy_http"`
      </td>
    </tr>

    <tr>
      <td>
        [`ktranslate-health`](/docs/logs/ui-data/built-log-parsing-rules#ktranslate-health)
      </td>

      <td>
        KTranslateコンテナヘルスログ
      </td>

      <td>
        `logtype:"ktranslate-health"`
      </td>
    </tr>

    <tr>
      <td>
        [`linux_cron`](/docs/logs/ui-data/built-log-parsing-rules/#linux_cron)
      </td>

      <td>
        Linuxcron
      </td>

      <td>
        `logtype:"linux_cron"`
      </td>
    </tr>

    <tr>
      <td>
        [`linux_messages`](/docs/logs/ui-data/built-log-parsing-rules/#linux_messages)
      </td>

      <td>
        Linuxメッセージ
      </td>

      <td>
        `logtype:"linux_messages"`
      </td>
    </tr>

    <tr>
      <td>
        [`iis_w3c`](/docs/logs/ui-data/built-log-parsing-rules/#iis)
      </td>

      <td>
        MicrosoftIISサーバーログ-W3C形式
      </td>

      <td>
        `logtype:"iis_w3c"`
      </td>
    </tr>

    <tr>
      <td>
        [`mongodb`](/docs/logs/ui-data/built-log-parsing-rules#mongodb)
      </td>

      <td>
        MongoDBログ
      </td>

      <td>
        `logtype:"mongodb"`
      </td>
    </tr>

    <tr>
      <td>
        [`monit`](/docs/logs/ui-data/built-log-parsing-rules#monit)
      </td>

      <td>
        Monitログ
      </td>

      <td>
        `logtype:"monit"`
      </td>
    </tr>

    <tr>
      <td>
        [`mysql-error`](/docs/logs/ui-data/built-log-parsing-rules#mysql-error)
      </td>

      <td>
        MySQLエラーログ
      </td>

      <td>
        `logtype:"mysql-error"`
      </td>
    </tr>

    <tr>
      <td>
        [`nginx`](/docs/logs/ui-data/built-log-parsing-rules#nginx)
      </td>

      <td>
        NGINXアクセスログ
      </td>

      <td>
        `logtype:"nginx"`
      </td>
    </tr>

    <tr>
      <td>
        [`nginx-error`](/docs/logs/ui-data/built-log-parsing-rules#nginx-error)
      </td>

      <td>
        NGINXエラーログ
      </td>

      <td>
        `logtype:"nginx-error"`
      </td>
    </tr>

    <tr>
      <td>
        [`postgresql`](/docs/logs/ui-data/built-log-parsing-rules#postgresql)
      </td>

      <td>
        Postgresqlログ
      </td>

      <td>
        `logtype:"postgresql"`
      </td>
    </tr>

    <tr>
      <td>
        [`rabbitmq`](/docs/logs/ui-data/built-log-parsing-rules#rabbitmq)
      </td>

      <td>
        Rabbitmqログ
      </td>

      <td>
        `logtype:"rabbitmq"`
      </td>
    </tr>

    <tr>
      <td>
        [`redis`](/docs/logs/ui-data/built-log-parsing-rules#redis)
      </td>

      <td>
        Redisログ
      </td>

      <td>
        `logtype:"redis"`
      </td>
    </tr>

    <tr>
      <td>
        [`route-53`](/docs/logs/ui-data/built-log-parsing-rules#route53)
      </td>

      <td>
        Route53ログ
      </td>

      <td>
        `logtype:"route-53"`
      </td>
    </tr>

    <tr>
      <td>
        [`syslog-rfc5424`](/docs/logs/ui-data/built-log-parsing-rules/#syslog-rfc5424)
      </td>

      <td>
        RFC5424形式のSyslog
      </td>

      <td>
        `logtype:"syslog-rfc5424"`
      </td>
    </tr>
  </tbody>
</table>

### `logtype`属性を追加します [#logattr]

ログを集約するときは、それらのログの整理、検索、および解析を容易にするメタデータを提供することが重要です。これを行う簡単な方法の 1 つは、送信時にログ メッセージに属性`logtype`を追加することです。[組み込みの解析規則](#built-in-rules)は、デフォルトで特定の`logtype`値に適用されます。

<Callout variant="tip">
  フィールド`logType` 、 `logtype` 、および`LOGTYPE`はすべて、組み込みルールでサポートされています。検索を容易にするために、組織内の単一の構文に合わせるようにすることをお勧めします。
</Callout>

[サポートされている配送方法](/docs/logs/enable-new-relic-logs)のいくつかによって送信されたログに`logtype`を追加する方法の例を次に示します。

<CollapserGroup>
  <Collapser
    className="freq-link"
    id="infrastructure-log-forwarder-example"
    title="NewRelicインフラストラクチャエージェントの例"
  >
    `logtype`を[`attribute`](/docs/logs/forward-logs/forward-your-logs-using-infrastructure-agent#attributes) }として追加します。名前付きソースごとにログタイプを設定する必要があります。

    ```
    logs:
      - name: file-simple
        file: /path/to/file
        attributes:
          logtype: fileRaw
      - name: nginx-example
        file: /var/log/nginx.log
        attributes:
          logtype: nginx
    ```
  </Collapser>

  <Collapser
    className="freq-link"
    id="fluentd-example"
    title="流暢な例"
  >
    フィルタブロックを`.conf file`に追加します。これは、 `record_transformer`を使用して新しいフィールドを追加します。この例では、 `nginx`の`logtype`を使用して、組み込みのNGINX解析ルールをトリガーします。他の[Fluentdの例](https://github.com/newrelic/fluentd-examples)を確認してください。

    ```
    <filter containers>
      @type record_transformer
      enable_ruby true
      <record>
        #Add logtype to trigger a built-in parsing rule for nginx access logs
        logtype nginx
        #Set timestamp from the value contained in the field "time"
        timestamp record["time"]
        #Add hostname and tag fields to all records
        hostname "#{Socket.gethostname}"
        tag ${tag}
      </record>
    </filter>
    ```
  </Collapser>

  <Collapser
    className="freq-link"
    id="fluentbit-example"
    title="流暢なビットの例"
  >
    `record_modifier`を使用して新しいフィールドを追加するフィルターブロックを`.conf`ファイルに追加します。この例では、 `nginx`の`logtype`を使用して、組み込みのNGINX解析ルールをトリガーします。他の[FluentBitの例](https://github.com/newrelic/fluentbit-examples)を確認してください。

    ```
    [FILTER]
        Name record_modifier
        Match *
        Record logtype nginx
        Record hostname ${HOSTNAME}
        Record service_name Sample-App-Name
    ```
  </Collapser>

  <Collapser
    className="freq-link"
    id="logstash-example"
    title="Logstashの例"
  >
    `add_field`変異フィルターを使用して新しいフィールドを追加するLogstash構成にフィルターブロックを追加します。この例では、 `logtype`の`nginx`を使用して、組み込みのNGINX解析ルールをトリガーします。他の[Logstashの例](https://github.com/newrelic/logstash-examples)を確認してください。

    ```
    filter {
      mutate {
        add_field => {
          "logtype" => "nginx"
          "service_name" => "myservicename"
          "hostname" => "%{host}"
        }
      }
    }
    ```
  </Collapser>

  <Collapser
    className="freq-link"
    id="api-example"
    title="LogsAPIの例"
  >
    NewRelicに送信されるJSONリクエストに属性を追加できます。この例では、値`nginx`の`logtype`属性を追加して、組み込みのNGINX解析ルールをトリガーします。 [LogsAPI](/docs/logs/new-relic-logs/log-api/introduction-log-api)の使用の詳細をご覧ください。

    ```
    POST /log/v1 HTTP/1.1
    Host: log-api.newrelic.com
    Content-Type: application/json
    X-License-Key: YOUR_LICENSE_KEY
    Accept: */*
    Content-Length: 133
    {
      "timestamp": TIMESTAMP_IN_UNIX_EPOCH,
      "message": "User 'xyz' logged in",
      "logtype": "nginx",
      "service": "login-service",
      "hostname": "login.example.com"
    }
    ```
  </Collapser>
</CollapserGroup>

## カスタム解析ルールを作成して表示する [#custom-parsing]

多くのログは、独自の方法でフォーマットまたは構造化されています。それらを解析するには、カスタムロジックを構築して適用する必要があります。

<img
  title="Log parsing rules"
  alt="Screenshot of log parsing in UI"
  src="/images/logs_screenshot-full_parsing-ui.webp"
/>

<figcaption>
  ログ UI の左側のナビゲーションで **Parsing** \[解析]を選択し、有効な NRQL `WHERE` 句と Grok パターンを使用して独自のカスタム解析ルールを作成します。
</figcaption>

独自のカスタム解析ルールを作成および管理するには、次のようにします。

1. **[one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > Logs**に移動します。
2. ログ UI の左側のナビゲーションにある **Manage data** \[データの管理] で、 **Parsing** \[解析] をクリックし、 **Create parsing rule** \[解析ルールの作成]をクリックします。
3. 新しい解析規則の名前を入力します。
4. 解析する既存のフィールドを選択するか (デフォルト = `message` )、新しいフィールド名を入力します。
5. 解析するログに一致する有効な NRQL `WHERE`句を入力してください。
6. 一致するログが存在する場合はそれを選択するか、\[**ログの貼り付け**] タブをクリックしてサンプル ログに貼り付けます。
7. 解析ルールを入力し、\[**出力]**セクションで結果を表示して、それが機能していることを検証します。Grok とカスタム解析ルールについて学習[するには、Grok パターンを使用してログを解析する方法に関するブログ投稿](https://blog.newrelic.com/product-news/how-to-use-grok-log-parsing)をお読みください。
8. カスタム解析ルールを有効にして保存します。

既存の解析ルールを表示するには：

1. **[one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > Logs**に移動します。
2. ログ UI の左側のナビゲーションにある **Manage data** \[データの管理] で、 **Parsing** \[解析]をクリックします。

## トラブルシューティング [#troubleshooting]

解析が意図したとおりに機能しない場合は、次のことが原因である可能性があります。

* **ロジック:**解析ルールの一致ロジックが必要なログと一致しません。
* **タイミング：**解析一致ルールがまだ存在しない値を対象としている場合、失敗します。これは、濃縮プロセスの一部としてパイプラインの後半で値が追加された場合に発生する可能性があります。
* **制限：**解析、パターン、ドロップフィルターなどを介してログを処理するために、毎分一定の時間があります。最大時間が費やされた場合、追加のログイベントレコードのために解析がスキップされます。

これらの問題を解決するには、[カスタム解析ルール](#custom-parsing)を作成または調整します。

まだお持ちでない場合は、以下で無料の New Relic アカウントを作成して、今すぐデータの監視を開始してください。

<InlineSignup/>
