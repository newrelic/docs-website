---
title: Javaエージェントです。Kafkaメッセージキューの計測
tags:
  - Agents
  - Java agent
  - Instrumentation
metaDescription: 'New Relic for Java includes built-in Kafka monitoring, as well as advanced event and distributed tracing data collection.'
translationType: machine
---

New Relic Java エージェントは、 [Kafka](https://kafka.apache.org/documentation/)'s Java clients library からデータを自動的に収集します。Kafka は大量のデータを生成する高性能なメッセージングシステムであるため、アプリの特定のスループットやユースケースに合わせてエージェントをカスタマイズすることができます。

このドキュメントでは、3種類のKafkaデータを収集して表示する方法を説明しています。

* [Kafkaメトリクス](#view-kafka-metrics)
* [Kafkaイベント](#collect-kafka-events)
* [Kafkaの分散型トレース](#collect-kafka-distributed-traces)

Kafka のインスツルメンテーションは、Java エージェントのバージョン 4.12.0 以降で利用できます。サポートされているKafkaクライアントのバージョンについては、 [Javaの互換性と要件](/docs/agents/java-agent/getting-started/compatibility-requirements-java-agent) を参照してください。

<Callout variant="tip">
  Kafkaインテグレーションについては、 [Kafkaモニタリングインテグレーション](/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration) を参照してください。
</Callout>

## Kafkaメトリクスの表示

[インストール後](/docs/agents/java-agent/installation/install-java-agent) 、エージェントは、メッセージングレート、レイテンシー、ラグなどの情報を含むリッチなKafkaメトリクスを自動的にレポートします。Javaエージェントは、 [Kafkaのコンシューマーおよびプロデューサーのすべてのメトリクス](https://kafka.apache.org/documentation/#monitoring) を収集します（ただし、コネクトおよびストリームのメトリクスは収集しません）。

これらのメトリクスを表示するには、カスタムダッシュボードを作成します。

1. [New Relic metric explorer](/docs/insights/use-insights-ui/explore-data/metric-explorer-search-chart-metrics-sent-new-relic-agents) にアクセスします。

2. メトリクスエクスプローラーを使って、メトリクスを探します。Kafkaのメトリクスは、このmetricフォルダにあります。

   ```
   MessageBroker/Kafka/Internal/<var>KafkaMetricName</var>
   ```

   例えば、 `リクエストレート` メトリックは、次のように位置しています。

   ```
   MessageBroker/Kafka/Internal/consumer-metrics/request-rate
   ```

   <Callout variant="tip">
     Kafkaのコンシューマーとプロデューサーのメトリクスの全リストについては、 [Kafka documentation](https://kafka.apache.org/documentation/#monitoring) を参照してください。
   </Callout>

3. **Add to dashboard** をクリックして、モニターしたいメトリクスをダッシュボードに追加します。

## Kafkaイベントコレクションの有効化 [#collect-kafka-events]

メトリックのタイムスライスデータではなく、イベントデータを収集するようにエージェントを構成することができます（メトリックのタイムスライスとイベントデータの違いについては、 [データ収集](/docs/using-new-relic/metrics/analyze-your-metrics/data-collection-metric-timeslice-event-data#overview) ）。これにより、 [NRQL](/docs/insights/nrql-new-relic-query-language/using-nrql/introduction-nrql) を使用して、デフォルトの Kafka メトリクスをフィルタリングおよびファセットすることができます。有効にすると、エージェントは 30 秒ごとに 1 つの Kafka イベントを収集します。このイベントには、 [Kafka コンシューマーからのすべてのデータと、前回のイベント以降に取得したプロダク トメトリクス](https://kafka.apache.org/documentation/#monitoring) が含まれます。

<Callout variant="important">
  エージェントは、 [収穫サイクルごとに最大2000件のイベントを記録します。](/docs/using-new-relic/welcome-new-relic/getting-started/glossary#harvest-cycle) この値は、 [`max_samples_stored`](/docs/agents/java-agent/configuration/java-agent-configuration-config-file#ae-max_samples_stored) で変更することができます。Kafka のイベントデータもこのプールに含まれます。 `recordCustomEvent()` API コールを使用して [カスタムイベント](/docs/insights/insights-data-sources/custom-data/insert-custom-events-new-relic-apm-agents) を New Relic に送信した場合、2000 件を超えるイベントを送信すると、エージェントは一部の Kafka イベントまたはカスタムイベントを破棄します。
</Callout>

Kafkaのイベント収集を有効にするには

1. `kafka.metrics.as_events.enabled` 要素を `newrelic.yml` 設定ファイルに追加します。

   ```
   kafka.metrics.as_events.enabled: true
   ```

2. JVMを再起動します。

3. [イベントエクスプローラー](/docs/insights/use-insights-ui/explore-data/event-explorer-query-chart-your-event-data) を使用して、 `KafkaMetrics` イベントタイプにある Kafka イベントを表示します。または、 [NRQL](/docs/insights/nrql-new-relic-query-language/using-nrql/introduction-nrql) を使用して、イベントを直接照会します。例えば、以下のようになります。

   ```
   SELECT average('producer-metrics.record-send-rate') from KafkaMetrics SINCE 30 minutes ago timeseries
   ```

## Kafkaの分散型トレースの有効化 [#collect-kafka-distributed-traces]

Java エージェントは、Kafka クライアントから [分散型トレース](/docs/apm/distributed-tracing/getting-started/introduction-distributed-tracing) を収集することもできます。トレースを有効にしても、エージェントの通常の動作には影響しません。Kafkaからのメトリックデータやイベントデータの報告は継続されます。

実現する前に考慮すべき影響と要件

* **インスツルメンテーションは、メッセージヘッダーに150～200バイトのペイロードを追加します。** Kafkaメッセージが非常に小さい場合、トレースは大きな処理およびストレージのオーバーヘッドを追加します。このペイロードサイズの追加により、Kafkaのメッセージサイズ制限を超えると、Kafkaがメッセージをドロップする可能性があります。このため、本番環境で有効にする前に、開発環境でKafka分散トレースをテストすることをお勧めします。
* 分散トレーシングは、Kafkaクライアントのバージョン0.11.0.0以降でのみ利用可能です。
* **以前にアプリで** distributed tracing を有効にしていない場合は、有効にする前に [Transition guide](/docs/apm/distributed-tracing/getting-started/transition-guide-distributed-tracing) をお読みください。
* Kafkaメッセージヘッダーを介してW3Cトレースコンテキストを伝播するには、Java agent 6.4.0でリリースされたAPIの詳細について、 [distributed tracing API usage guide](/docs/agents/java-agent/api-guides/guide-using-java-agent-api#trace-calls) を参照してください。なお、Kafkaメッセージに追加のヘッダを追加すると、ペイロードサイズがさらに大きくなります。これらのAPIの動作を確認するには、 [Using Java agent trace APIs with Kafka](https://github.com/newrelic/newrelic-java-examples/tree/main/newrelic-java-agent/distributed-tracing/kafka-examples) を参照してください。

この機能を有効にするための完全なプロセスは以下のとおりですが、大まかには以下のような基本的なステップがあります。1) エージェントの設定でトレースを有効にする。2) [Java エージェント API](/docs/agents/java-agent/api-guides/guide-using-java-agent-api) を呼び出して、プロデューサ側とコンシューマ側の両方のトランザクションを計測する。

Kafkaから分散型トレースを収集する。

<CollapserGroup>
  <Collapser
    id="enable-dt-kafka"
    title="1.設定ファイルで分散型トレーシングを有効にする"
  >
    以前にアプリでdistributed tracingを有効にしていない場合は、有効にする前に [distributed tracing transition guide](/docs/apm/distributed-tracing/getting-started/transition-guide-distributed-tracing) をお読みください。

    Kafkaの分散トレーシングを有効にするには、この2つの設定が [`newrelic.yml` config file](/docs/agents/java-agent/configuration/java-agent-configuration-config-file#Structure) で有効になっている必要があります。

    * [`distributed_tracing`](/docs/agents/java-agent/configuration/java-agent-configuration-config-file#distributed-tracing) 要素が有効になっていることを確認してください。

      ```
      distributed_tracing:
        enabled: true
      ```

    * 以下を設定ファイルに追加して、Kafka固有の分散型トレーシング機能を有効にします。

      ```
      class_transformer:
        com.newrelic.instrumentation.kafka-clients-spans-0.11.0.0:
          enabled: true
      ```
  </Collapser>

  <Collapser
    id="instrument-kafka-producer"
    title="2.Kafkaプロデューサーのインストゥルメント"
  >
    Kafkaプロデューサーをインストルメント化するには、 `Producer.send(ProducerRecord<K, V> record)` の呼び出しの前にトランザクションを開始する必要があります。これを行うには、Javaエージェント `@Trace(dispatcher = true)` アノテーションをメソッドに追加します。

    たとえば、

    ```
    @Trace(dispatcher = true)
    public static void createAndSend(KafkaProducer<String, String> producer){
        ProducerRecord<String, String> data = new ProducerRecord<String, String>("topic", "key", "value");
        producer.send(data);
    }
    ```
  </Collapser>

  <Collapser
    id="instrument-kafka-consumer"
    title="3.Kafkaコンシューマーのインストゥルメント"
  >
    Kafkaコンシューマをインストルメントするには、メッセージが処理されるときにトランザクションを開始する必要があります。エージェントは `newrelic` キーの下、または W3C の `traceparent` と `tracestate` キーの下に分散型トレーシングペイロードのヘッダーを格納します。ヘッダーを取得した後、New Relic のトランザクション API を呼び出してペイロードを受け入れます。

    たとえば、

    ```
    @Trace(dispatcher = true)
    private static void <var>processMessage</var>(ConsumerRecord<String, String> rec) {
      // create a distributed trace headers map
      Headers dtHeaders = ConcurrentHashMapHeaders.build(HeaderType.MESSAGE);

      // Iterate through each record header and insert the trace headers into the dtHeaders map
      for (Header header : rec.headers()) {
        String headerValue = new String(header.value(), StandardCharsets.UTF_8);

        // using the newrelic key
        if (header.key().equals("newrelic")) {
          dtHeaders.addHeader("newrelic", headerValue);
        }

        // or using the W3C keys
        if (header.key().equals("traceparent")) {
          dtHeaders.addHeader("traceparent", headerValue);
        }

        if (header.key().equals("tracestate")) {
          dtHeaders.addHeader("tracestate", headerValue);
        }
      }

      // Accept distributed tracing headers to link this request to the originating request
      NewRelic.getAgent().getTransaction().acceptDistributedTraceHeaders(TransportType.Kafka, dtHeaders);
    }
    ```
  </Collapser>
</CollapserGroup>