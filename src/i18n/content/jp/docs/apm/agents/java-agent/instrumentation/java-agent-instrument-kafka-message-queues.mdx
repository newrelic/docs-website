---
title: Javaエージェントです。Kafkaメッセージキューの計測
tags:
  - Agents
  - Java agent
  - Instrumentation
metaDescription: 'New Relic for Java includes built-in Kafka monitoring, as well as advanced event and distributed tracing data collection.'
translationType: machine
---

New Relic Java エージェントは、 [Kafka](https://kafka.apache.org/documentation/)'s Java clients library からデータを自動的に収集します。Kafka は大量のデータを生成する高性能なメッセージングシステムであるため、アプリの特定のスループットやユースケースに合わせてエージェントをカスタマイズすることができます。

このドキュメントでは、3種類のKafkaデータを収集して表示する方法を説明しています。

* [Kafkaメトリクス](#view-kafka-metrics)
* [Kafkaイベント](#collect-kafka-events)
* [Kafka Streams トランザクションを有効にする](#collect-kafka-streams-transactions)
* [Kafkaの分散型トレース](#collect-kafka-distributed-traces)

<Callout variant="tip">
  また、Kafka との統合もあります。詳細については、 [Kafka 監視統合 を](/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration)参照してください。
</Callout>

## 要件 [#requirements]

Kafka クライアントのインスツルメンテーションは、Java エージェント バージョン 4.12.0 以降で使用できます。Kafka ストリーム インストルメンテーションは、Java エージェント バージョン 8.1.0 で使用できます。以上。サポートされているすべての Kafka ライブラリを確認するには、 [Java の互換性と要件のページ](/docs/agents/java-agent/getting-started/compatibility-requirements-java-agent)を確認してください。Kafka Streams は Kafka クライアント上で実行されるため、Kafka クライアントに適用されるすべてのインストルメンテーションは Streams にも適用されることに注意してください。

## Kafkaメトリクスの表示

[インストール後](/docs/agents/java-agent/installation/install-java-agent) 、エージェントは、メッセージングレート、レイテンシー、ラグなどの情報を含むリッチなKafkaメトリクスを自動的にレポートします。Javaエージェントは、 [Kafkaのコンシューマーおよびプロデューサーのすべてのメトリクス](https://kafka.apache.org/documentation/#monitoring) を収集します（ただし、コネクトおよびストリームのメトリクスは収集しません）。

これらのメトリクスを表示するには、カスタムダッシュボードを作成します。

1. [New Relic metric explorer](/docs/insights/use-insights-ui/explore-data/metric-explorer-search-chart-metrics-sent-new-relic-agents) にアクセスします。

2. メトリック エクスプローラーを使用してメトリックを見つけます。メトリクスを見つけることができるいくつかのフォルダーを次に示します。

   * Kafka メトリクス:

     ```
     MessageBroker/Kafka/Internal/KafkaMetricName
     ```

     たとえば、 `request-rate` 指標は次のようになります。

     ```
     MessageBroker/Kafka/Internal/consumer-metrics/request-rate
     ```

   * カフカストリーム:

     ```
     Kafka/Streams/KafkaStreamsMetricName
     ```

     たとえば、 `poll-latency-avg` 指標は次のようになります。

     ```
     Kafka/Streams/stream-thread-metrics/poll-latency-avg
     ```

   * カフカ接続:

     ```
     Kafka/Connect/KafkaConnectMetricName
     ```

     たとえば、 `connector-count` 指標は次のようになります。

     ```
     Kafka/Connect/connect-worker-metrics/connector-count
     ```

3. **Add to dashboard** をクリックして、モニターしたいメトリクスをダッシュボードに追加します。

<Callout variant="tip">
  Kafka のコンシューマー、プロデューサー、およびストリームのメトリックの完全なリストについては、 [Kafka のドキュメント を](https://kafka.apache.org/documentation/#remote_jmx)参照してください。これらのドキュメントのメトリックは、JMX を介して検索できます。ドキュメントに記載されているすべてのメトリクスが New Relic にエクスポートされるわけではないことに注意してください。これは、次のいずれかの理由による可能性があります。

  * このメトリクスは、実際には Kafka クライアントまたは Kafka ストリームによって生成されるわけではありません。これは、古いバージョンのクライアントまたはストリームを使用しているか、Kafka ライブラリの設定方法と使用方法に基づいていることが原因である可能性があります。
  * メトリックが数値でないか、その値が `NaN`です。New Relic は、数値を持つメトリクスのみを受け入れます。
</Callout>

## Kafkaイベントコレクションの有効化 [#collect-kafka-events]

メトリックのタイムスライスデータではなく、イベントデータを収集するようにエージェントを構成することができます（メトリックのタイムスライスとイベントデータの違いについては、 [データ収集](/docs/using-new-relic/metrics/analyze-your-metrics/data-collection-metric-timeslice-event-data#overview) ）。これにより、 [NRQL](/docs/insights/nrql-new-relic-query-language/using-nrql/introduction-nrql) を使用して、デフォルトの Kafka メトリクスをフィルタリングおよびファセットすることができます。有効にすると、エージェントは 30 秒ごとに 1 つの Kafka イベントを収集します。このイベントには、 [Kafka コンシューマーからのすべてのデータと、前回のイベント以降に取得したプロダク トメトリクス](https://kafka.apache.org/documentation/#monitoring) が含まれます。

Kafka ストリームを使用している場合、エージェントは、前のイベント以降にキャプチャされた [Kafka ストリーム メトリック](https://kafka.apache.org/documentation/#remote_jmx) からのすべてのデータを含む別のイベントを生成します。イベントも 30 秒ごとに収集されます。

<Callout variant="important">
  エージェントは[収集サイクル](/docs/using-new-relic/welcome-new-relic/getting-started/glossary#harvest-cycle)ごとに最大 2000 のイベントを記録しますが、この値は[`max_samples_stored`](/docs/agents/java-agent/configuration/java-agent-configuration-config-file#ae-max_samples_stored)で変更できます。Kafka イベント データは、このプールに含まれています。`recordCustomEvent()` API 呼び出しを使用して[カスタム イベント](/docs/insights/insights-data-sources/custom-data/insert-custom-events-new-relic-apm-agents)を New Relic に送信し、2000 を超えるイベントを送信すると、エージェントは一部の Kafka またはカスタム イベントを破棄します。
</Callout>

Kafkaのイベント収集を有効にするには

1. `kafka.metrics.as_events.enabled`要素を`newrelic.yml`構成ファイルに追加します。

   ```
   kafka.metrics.as_events.enabled: true
   ```

2. JVMを再起動します。

3. [イベント エクスプローラー](/docs/insights/use-insights-ui/explore-data/event-explorer-query-chart-your-event-data)を使用して、 `KafkaMetrics`イベント タイプにある Kafka イベントを表示します。または、 [NRQL](/docs/insights/nrql-new-relic-query-language/using-nrql/introduction-nrql)を使用してイベントを直接クエリします。例えば：

   ```
   SELECT average('producer-metrics.record-send-rate') from KafkaMetrics SINCE 30 minutes ago timeseries
   ```

   Kafka ストリーム メトリクスをクエリする場合は、 `KafkaStreamsMetrics` イベント タイプを使用してストリーム固有のメトリクスにアクセスします。

<Callout variant="important">
  タイムスライス メトリックとして New Relic に送信できる Kafka メトリックの種類に関する制限は、イベントにも適用されることに注意してください。つまり、非数値および NaN メトリックはイベント属性として含まれません。
</Callout>

## Kafka Streams トランザクションを有効にする [#collect-kafka-streams-transactions]

Kafka Streams を使用している場合、デフォルトではトランザクションは有効になりません。これは、Kafka アプリケーションのスループットが高くなる傾向があるため、不要なオーバーヘッドを防ぐためです。

JMS トランザクションとは異なり、Kafka Streams トランザクションはレコードごとに処理されません。代わりに、Kafka コンシューマーがレコードをポーリングし、結果のデータが処理されるときにトランザクションが開始されます。

トランザクションを作成したい場合は、 `kafka-streams-spans` モジュールを有効にする必要があります:

```
class_transformer:
  kafka-streams-spans:
    enabled: true
```

## Kafka Connect トランザクションを有効にしました [#collect-kafka-connect-transactions]

Kafka Connect を使用している場合、デフォルトではトランザクションは有効になりません。Kafka アプリケーションはスループットが高い傾向があるため、これは不要なオーバーヘッドを防ぐためです。

Kafka Connect トランザクションは、シンク/ソース タスクの反復ごとに記録されます。シンク タスクの場合、トランザクションは、Kafka コンシューマーのポーリング、各メッセージの変換、およびターゲットへのデータの送信で構成されます。ソース タスクの場合、トランザクションは、ターゲットからの読み取り、データのメッセージへの変換、Kafka プロデューサによる各メッセージの送信で構成されます。

これらのトランザクションを収集したい場合は、 `kafka-connect-spans` モジュールを有効にする必要があります。

```
class_transformer:
  kafka-connect-spans:
    enabled: true
```

## Kafkaの分散型トレースの有効化 [#collect-kafka-distributed-traces]

Java エージェントは、Kafka クライアントから [分散トレースを](/docs/apm/distributed-tracing/getting-started/introduction-distributed-tracing) 収集することもできます。Kafka Streams は Kafka クライアント上で実行されるため、分散トレースを管理する手順も適用されます。トレースを有効にしても、エージェントの通常の操作には影響しません。 Kafka からのメトリックまたはイベント データは引き続きレポートされます。

実現する前に考慮すべき影響と要件

* **インスツルメンテーションは、メッセージヘッダーに150～200バイトのペイロードを追加します。** Kafkaメッセージが非常に小さい場合、トレースは大きな処理およびストレージのオーバーヘッドを追加します。このペイロードサイズの追加により、Kafkaのメッセージサイズ制限を超えると、Kafkaがメッセージをドロップする可能性があります。このため、本番環境で有効にする前に、開発環境でKafka分散トレースをテストすることをお勧めします。
* 分散トレーシングは、Kafkaクライアントのバージョン0.11.0.0以降でのみ利用可能です。
* **以前にアプリで** distributed tracing を有効にしていない場合は、有効にする前に [Transition guide](/docs/apm/distributed-tracing/getting-started/transition-guide-distributed-tracing) をお読みください。
* Kafkaメッセージヘッダーを介してW3Cトレースコンテキストを伝播するには、Java agent 6.4.0でリリースされたAPIの詳細について、 [distributed tracing API usage guide](/docs/agents/java-agent/api-guides/guide-using-java-agent-api#trace-calls) を参照してください。なお、Kafkaメッセージに追加のヘッダを追加すると、ペイロードサイズがさらに大きくなります。これらのAPIの動作を確認するには、 [Using Java agent trace APIs with Kafka](https://github.com/newrelic/newrelic-java-examples/tree/main/newrelic-java-agent/distributed-tracing/kafka-examples) を参照してください。
* Kafka Streams を使用している場合は、スパン インストルメンテーション モジュールを有効にする必要があります ( [Kafka Streams トランザクション セクション](#collect-kafka-streams-transactions)を参照してください)。トランザクションはレコードごとに記録されないため、分散トレース ヘッダーの受け入れは 1 つのレコードに対してのみ機能します。

これを有効にする完全なプロセスを以下に説明しますが、大まかに言えば、次の基本的な手順が含まれます。1) エージェント構成を介してトレースを有効にし、2) [Java エージェント API](/docs/agents/java-agent/api-guides/guide-using-java-agent-api) を呼び出してプロデューサー側とコンシューマー側の両方でトランザクションを計測します。

Kafkaから分散型トレースを収集する。

<CollapserGroup>
  <Collapser
    id="enable-dt-kafka"
    title="1.設定ファイルで分散型トレーシングを有効にする"
  >
    以前にアプリでdistributed tracingを有効にしていない場合は、有効にする前に [distributed tracing transition guide](/docs/apm/distributed-tracing/getting-started/transition-guide-distributed-tracing) をお読みください。

    Kafka 分散トレースを有効にするには、 [`newrelic.yml`構成ファイル](/docs/agents/java-agent/configuration/java-agent-configuration-config-file#Structure)で次の 2 つの設定を有効にする必要があります。

    * [`distributed_tracing`](/docs/agents/java-agent/configuration/java-agent-configuration-config-file#distributed-tracing)要素が有効になっていることを確認します。

      ```
      distributed_tracing:
        enabled: true
      ```

    * 以下を設定ファイルに追加して、Kafka固有の分散型トレーシング機能を有効にします。

      ```
      class_transformer:
        com.newrelic.instrumentation.kafka-clients-spans-0.11.0.0:
          enabled: true
      ```
  </Collapser>

  <Collapser
    id="instrument-kafka-producer"
    title="2.Kafkaプロデューサーのインストゥルメント"
  >
    Kafka プロデューサーを計測するには、 `Producer.send(ProducerRecord<K, V> record)`を呼び出す前にトランザクションを開始する必要があります。これを行うには、Java エージェント`@Trace(dispatcher = true)`アノテーションをメソッドに追加します。

    例えば：

    ```
    @Trace(dispatcher = true)
    public static void createAndSend(KafkaProducer<String, String> producer){
        ProducerRecord<String, String> data = new ProducerRecord<String, String>("topic", "key", "value");
        producer.send(data);
    }
    ```

    <Callout variant="important">
      Kafka Streams を使用している場合、トランザクションを直接開始したり、プロデューサーにデータを送信したりする必要はありません。
    </Callout>
  </Collapser>

  <Collapser
    id="instrument-kafka-consumer"
    title="3.Kafkaコンシューマーのインストゥルメント"
  >
    Kafka コンシューマーを計測するには、メッセージの処理中にトランザクションを開始する必要があります。エージェントは、分散トレース ペイロード ヘッダーを`newrelic`キーまたは W3C の`traceparent`および`tracestate`キーの下に保存します。ヘッダーを取得し、New Relic トランザクション API を呼び出してペイロードを受け入れます。

    例えば：

    ```
    @Trace(dispatcher = true)
    private static void processMessage(ConsumerRecord<String, String> rec) {
      // create a distributed trace headers map
      Headers dtHeaders = ConcurrentHashMapHeaders.build(HeaderType.MESSAGE);

      // Iterate through each record header and insert the trace headers into the dtHeaders map
      for (Header header : rec.headers()) {
        String headerValue = new String(header.value(), StandardCharsets.UTF_8);

        // using the newrelic key
        if (header.key().equals("newrelic")) {
          dtHeaders.addHeader("newrelic", headerValue);
        }

        // or using the W3C keys
        if (header.key().equals("traceparent")) {
          dtHeaders.addHeader("traceparent", headerValue);
        }

        if (header.key().equals("tracestate")) {
          dtHeaders.addHeader("tracestate", headerValue);
        }
      }

      // Accept distributed tracing headers to link this request to the originating request
      NewRelic.getAgent().getTransaction().acceptDistributedTraceHeaders(TransportType.Kafka, dtHeaders);
    }
    ```

    <Callout variant="important">
      Kafka Streams でトランザクションを有効にした場合 ( [Kafka Streams トランザクション セクション](#collect-kafka-streams-transactions)を参照)、トランザクションが多くのレコードに適用される場合でも、分散トレース ヘッダーの受け入れは 1 つのレコードにのみ適用されます。
    </Callout>
  </Collapser>
</CollapserGroup>