---
title: コントロールプレーンモニタリングの設定
tags:
  - Integrations
  - Kubernetes integration
  - Installation
metaDescription: How to configure control plane monitoring for your Kubernetes integration with New Relic.
translationType: machine
---

import kubernetesIntegrationCp from 'images/kubernetes_diagram_integration-cp.webp'

import kubernetesIntegrationCpExternal from 'images/kubernetes_diagram_integration-cp-external.webp'

import kubernetesClusterExplorerControlPlane from 'images/kubernetes_screenshot-full_cluster-explorer-control-plane.webp'

[New Relic](/docs/infrastructure/new-relic-infrastructure/getting-started/introduction-new-relic-infrastructure) は、Kubernetes 統合のために [コントロールプレーン](https://kubernetes.io/docs/concepts/overview/components/#control-plane-components) をサポートしており、クラスタのコントロールプレーンコンポーネントからメトリクスを監視・収集することができます。これらのデータは、New Relic で、 [クエリやチャートの作成に使用することができます](/docs/using-new-relic/data/understand-data/query-new-relic-data) 。

<Callout variant="tip">
  このページでは、特に指定のない限り、 [Kubernetes integration v3](/docs/kubernetes-pixie/kubernetes-integration/get-started/changes-since-v3) を参照しています。v2のコントロールプレーン監視の設定方法の詳細は、以下の特定のセクションに記載されています。
</Callout>

## 特徴 [#control-plane-integration-features]

以下のコントロールプレーンコンポーネントから、 [メトリクス](http://docs.newrelic.com/docs/integrations/kubernetes-integration/understand-use-data/understand-use-data#metrics) を監視・収集しています。

* **etcd**: リーダー情報、常駐メモリサイズ、OSスレッド数、コンセンサスプロポーザルデータ、など。サポートされているメトリクスの一覧については、 [etcd data](/docs/integrations/kubernetes-integration/understand-use-data/understand-use-data#etcd-data) を参照してください。
* **APIサーバー**： `apiserver`リクエストの割合、HTTPメソッドとレスポンスコードによる`apiserver`リクエストの内訳など。サポートされている指標の完全なリストについては、 [APIサーバーデータ](/docs/integrations/kubernetes-integration/understand-use-data/understand-use-data#api-server-data)をご覧ください。
* **スケジューラ**: 要求されたCPU/メモリ対ノードで利用可能なCPU/メモリ、汚染に対する耐性、任意のセットのアフィニティまたはアンチアフィニティなど。サポートされているメトリクスの完全なリストについては、 [Scheduler data](/docs/integrations/kubernetes-integration/understand-use-data/understand-use-data#scheduler-data) を参照してください。
* **コントローラーマネージャー** ：常駐メモリサイズ、作成されたOSスレッド数、現在存在するゴルーチンなど。サポートされるメトリクスの完全なリストについては、 [Controller Manager data](/docs/integrations/kubernetes-integration/understand-use-data/understand-use-data#controller-manager-data) を参照してください。

## 互換性と要件 [#compatibility]

* AKS、EKS、GKEを含むほとんどのマネージドクラスターは、コントロールプレーンコンポーネントへの外部アクセスを許可していません。そのため、マネージドクラスターでは、New RelicはAPIサーバーのコントロールプレーンメトリックのみを取得でき、etcd、スケジューラー、またはコントローラーマネージャーのコントロールプレーンメトリックは取得できません。
* [非特権モード](/docs/integrations/kubernetes-integration/installation/kubernetes-installation-configuration#unprivileged) でソリューションを展開する場合、コントロールプレーンのセットアップには [追加の手順が必要となり、](#hostnetwork-privileged) いくつかの注意点が適用されます。
* [OpenShift](http://learn.openshift.com/?extIdCarryOver=true&sc_cid=701f2000001OH7iAAG) 4.xでは、コントロールプレーンコンポーネントのメトリックエンドポイントがデフォルトとは異なるものを使用しています。

## コントロールプレーンコンポーネント [#component]

Kubernetesコントロールプレーンを監視するタスクは、デフォルトでDaemonSetとしてデプロイされる`nrk8s-controlplane`コンポーネントの責任です。このコンポーネントは、 `node-role.kubernetes.io/control-plane`や`node-role.kubernetes.io/master`などのマスターノードを識別するために一般的に使用されるラベルを含む`nodeSelectorTerms`のデフォルトリストを使用して、マスターノードに自動的にデプロイされます。とにかく、このセレクターは`values.yml`ファイルで公開されているため、他の環境に合わせて再構成できます。

これらのセレクターに一致するノードがないクラスターでは、ポッドがスケジュールされないため、リソースを浪費することはなく、ヘルムチャートで`controlPlane.enabled`を`false`に設定することでコントロールプレーンの監視を完全に無効にすることと機能的に同等です。

コントロールプレーンの各コンポーネントには専用のセクションが設けられており、個別に対応することができます。

* そのコンポーネントのモニタリングを有効または無効にする
* そのコンポーネントを発見するための特定のセレクタと名前空間を定義します。
* そのコンポーネントのメトリクスを取得するために使用されるエンドポイントとパスの定義
* そのコンポーネントのメトリクスを取得するために使用する必要のある認証メカニズムの定義
* オートディスカバリーを完全にスキップするエンドポイントを手動で指定します。

<img
  title="Diagram showing a possible configuration scraping etcd with mTLS and API server with bearer Token."
  alt="Diagram showing a possible configuration scraping etcd with mTLS and API server with bearer Token. The monitoring is a DaemonSet deployed on master nodes only."
  src={kubernetesIntegrationCp}
/>

`controlPlane`キーの下のnri-kubernetesチャートの[values.yaml](https://github.com/newrelic/nri-kubernetes/blob/main/charts/newrelic-infrastructure/values.yaml)で使用可能なすべての構成オプションを確認できます。

`nri-bundle` チャートを介して統合をインストールする場合は [、対応するサブチャートに値を渡す](https://helm.sh/docs/chart_template_guide/subcharts_and_globals/)必要があります。たとえば、 `controlPlane` コンポーネントで `etcd` 監視を無効にするには、次のようにします。

```yaml
newrelic-infrastructure:
  controlPlane:
    config:
      etcd:
        enabled: false
```

### オートディスカバリーとデフォルト設定 [#autodiscovery-default]

デフォルトでは、 [Helm Chart](/docs/kubernetes-pixie/kubernetes-integration/installation/install-kubernetes-integration-using-helm)は、 `Kubeadm`や`minikube`など、クラスター内でコントロールプレーンを実行するオンプレミスディストリビューションの一部のコントロールプレーンコンポーネントに対して、すぐに使用できる構成を提供します。

自動検出は検出メカニズムとしてポッドラベルに依存しているため、クラウド環境や、コントロールプレーンコンポーネントがクラスター内で実行されていない場合は機能しないことに注意してください。ただし、これらのシナリオでは、コントロールプレーンコンポーネントに到達できる場合、[静的エンドポイント](#static-endpoints)を利用できます。

#### `hostNetwork` および `privileged` [#hostnetwork-privileged]

v3より前のバージョンでは、統合が`privileged: false`を使用してデプロイされると、コントロールプレーンコンポーネントの`hostNetwork`設定も`false`に設定されます。

バージョン3以降では、 `privileged`フラグは`securityContext`オブジェクトにのみ影響します。つまり、コンテナがホストメトリックにアクセスできるルートとして実行されているかどうかに影響します。ほとんどのディストリビューションでコントロールプレーンのエンドポイントに到達するために必要な`hostNetwork: true`を持つコントロールプレーンからメトリックを取得するポッドを除いて、すべての統合コンポーネントはデフォルトで`hostNetwork: false`になりました。すべてのコンポーネントの`hostNetwork`値は[、 `values.yaml`の`hostNetwork`トグル](https://github.com/newrelic/helm-charts/tree/master/library/common-library#values-managed-globally)を使用して、個別にまたはグローバルに変更できます。

クラスタまたはその他のポリシーのために、 `hostNetwork`でポッドを実行することがまったく受け入れられない場合、コントロールプレーンの監視は不可能であり、 `controlPlane.enabled`を`false`に設定して無効にする必要があります。

[カスタム自動検出](#autodiscovery-default)または[静的エンドポイント](#static-endpoints)を含む高度な構成がある場合は、 `hostNetwork`なしでコントロールプレーンを監視するために使用できます。 [プロジェクトのREADME](https://github.com/newrelic/nri-kubernetes/blob/main/charts/newrelic-infrastructure/README.md)を確認し、 [`values.yaml`](https://github.com/newrelic/nri-kubernetes/blob/main/charts/newrelic-infrastructure/values.yaml)で`controlPlane.hostNetwork`トゥーグルを探します。

#### カスタムオートディスカバリー [#autodiscovery-default]

自動検出に使用されるセレクターは、 `values.yaml`ファイルの構成エントリとして完全に公開されます。つまり、コントロールプレーンがクラスターの一部として実行されるほとんどすべての環境に合わせて、セレクターを微調整または置換できます。

オートディスカバリーのセクションは以下のようになります。

```yaml
autodiscover:
  - selector: "tier=control-plane,component=etcd"
    namespace: kube-system
    # Set to true to consider only pods sharing the node with the scraper pod.
    # This should be set to `true` if Kind is Daemonset, `false` otherwise.
    matchNode: true
    # Try to reach etcd using the following endpoints.
    endpoints:
      - url: https://localhost:4001
        insecureSkipVerify: true
        auth:
          type: bearer
      - url: http://localhost:2381
  - selector: "k8s-app=etcd-manager-main"
    namespace: kube-system
    matchNode: true
    endpoints:
      - url: https://localhost:4001
        insecureSkipVerify: true
        auth:
          type: bearer
```

`autodiscover`セクションには、自動検出エントリのリストが含まれています。各エントリには次のものがあります。

* `selector`：ポッドを検索するために使用される文字列でエンコードされたラベルセレクター。
* `matchNode`：trueに設定すると、検出を実行するDaemonSetの特定のインスタンスと同じノードで実行されているポッドに検出が追加で制限されます。
* `endpoints`：指定されたセレクターのポッドが見つかった場合に試行するエンドポイントのリスト。

さらに、各`endpoint`には次のものがあります。

* `url`：スキームを含む、ターゲットへのURL。 `http`または`https`にすることができます。
* `insecureSkipVerify`：trueに設定すると、証明書は`https` URLについてチェックされません。
* `auth.type`：リクエストの認証に使用するメカニズム。現在、次のメソッドがサポートされています。
* なし： `auth`が指定されていない場合、リクエストには認証がまったく含まれません。
* `bearer`：KubernetesAPIに対する認証に使用されたものと同じベアラトークンがこのリクエストに送信されます。
* `mtls`：mTLSはリクエストの実行に使用されます。

##### mTLS [#mtls]

`mtls`タイプの場合、以下を指定する必要があります。

```yaml
endpoints:
  - url: https://localhost:4001
    auth:
      type: mtls
      mtls:
        secretName: secret-name
        secretNamespace: secret-namespace
```

ここで、 `secret-name`は、 `secret-namespace`に存在するKubernetes TLSシークレットの名前であり、その特定のエンドポイントに接続するために必要な証明書、キー、およびCAが含まれています。

統合では、このシークレットをマウントするのではなく、実行時にフェッチします。つまり、このシークレットへのアクセスを許可するRBACロールが必要です。ヘルムチャートは、レンダリング時に`auth.mtls`エントリを自動的に検出し、 `rbac.create`がfalseに設定されていない限り、これらの特定のシークレットと名前空間のエントリを自動的に作成します。

私たちの統合は、以下のキーを持つ秘密を受け入れます。

* `cacert`: 署名に使用される PEM エンコードされた CA 証明書。 `cert`
* `cert`：etcdに提示されるPEMでエンコードされた証明書
* `key`：上記の証明書に対応するPEMでエンコードされた秘密鍵

これらの証明書は、etcdが動作に使用しているのと同じCAによって署名されている必要があります。

これらの証明書を生成する方法は、Kubernetesのディストリビューションによって大きく異なるため、このドキュメントの範囲外です。必要なetcdピア証明書を取得する方法については、ディストリビューションのドキュメントを参照してください。たとえば、Kubeadmでは、マスターノードの`/etc/kubernetes/pki/etcd/peer.{crt,key}`にあります。

etcdのピア証明書を見つけたり生成したりしたら、シークレットに含まれると思われるキーに合わせてファイル名を変更し、クラスタにシークレットを作成します。

```shell
mv peer.crt cert
mv peer.key key
mv ca.crt cacert

kubectl -n newrelic create secret tls newrelic-etcd-tls-secret --cert=./cert --key=./key --certificate-authority=./cacert
```

最後に、このセクションの冒頭に示されている構成スニペットにシークレット名（ `newrelic-etcd-tls-secret` ）と名前空間（ `newrelic` ）を入力できます。 Helm Chartはこの構成を自動的に解析し、RBACロールを作成して、 `nrk8s-controlplane`コンポーネントのこの特定のシークレットと名前空間へのアクセスを許可するため、その点で手動のアクションは必要ありません。

### 静的エンドポイント [#static-endpoints]

オートディスカバリーは、コントロールプレーンがKubernetesクラスター内に存在する場合をカバーする必要がありますが、ディストリビューションや洗練されたKubernetes環境の中には、可用性やリソースの分離など様々な理由から、コントロールプレーンを別の場所で実行するものもあります。

このような場合、コントロールプレーンラベルの付いたポッドがノードで見つかったかどうかに関係なく、任意の固定URLをスクレイプするように統合を構成できます。これは、 `staticEndpoint`エントリを指定することによって行われます。たとえば、外部etcdインスタンスの場合は次のようになります。

```yaml
controlPlane:
  etcd:
    staticEndpoint:
      url: https://url:port
      insecureSkipVerify: true
      auth: {}
```

<img
  title="Diagram showing a possible configuration scraping an external API server with bearer Token."
  alt="Diagram showing a possible configuration scraping an external API server with bearer Token. The monitoring is a Deployment with a single replica."
  src={kubernetesIntegrationCpExternal}
/>

`staticEndpoint` は`autodiscover`エントリの`endpoints`と同じタイプのエントリであり、そのフィールドは上記で説明されています。ここでは、認証メカニズムとスキーマがサポートされています。

`staticEndpoint`が設定されている場合、 `autodiscover`セクションは完全に無視されることに注意してください。

#### 制限 [#static-endpoints-limitations]

<Callout variant="important">
  ノード外を指す `staticEndpoint` を使用している場合 (つまり、 `localhost`ではありません) エンドポイントの場合、 `controlPlane.kind` `DaemonSet` から `Deployment`に変更する必要があります。
</Callout>

`staticEndpoint`を使用すると、すべての `nrk8s-controlplane` Pod が上記のエンドポイントに到達してスクレイピングしようとします。これは、 `nrk8s-controlplane` が DaemonSet (デフォルト) の場合、DaemonSet のすべてのインスタンスがこのエンドポイントをスクレイピングすることを意味します。 `localhost`を指している場合はこれで問題ありませんが、エンドポイントがノードに対してローカルでない場合、メトリックが重複し、課金対象の使用量が増える可能性があります。 `staticEndpoint` 使用して非ローカル URL を指している場合は、必ず `controlPlane.kind` を Deployment に変更してください。

上記と同様の理由により、現在のところ、あるコントロールプレーンコンポーネントにオートディスカバリーを使用し、他のコンポーネントにスタティックエンドポイントを使用することはできません。これは既知の制限事項であり、将来のバージョンでの対応を検討しています。

最後に、 `staticEndpoint`ではコンポーネントごとに1つのエンドポイントのみを定義できます。これは、異なるホストに複数のコントロールプレーンシャードがある場合、現在、それらを個別にポイントすることはできないことを意味します。これは、将来のバージョンで対処するために取り組んでいる既知の制限でもあります。当面の回避策は、別の場所にあるさまざまなシャードのメトリックを集約し、集約された出力を`staticEndpoint` URLにポイントすることです。

#### マネージド環境やクラウド環境のためのコントロールプレーン監視 [#cloud-control-plane]

EKSやGKEのような一部のクラウド環境では、Kubernetes API Serverからメトリクスを取得することができます。これは静的なエンドポイントとして簡単に設定できます。

```yaml
controlPlane:
  affinity: false  # https://github.com/helm/helm/issues/9136
  kind: Deployment
  # `hostNetwork` is not required for monitoring API Server on AKS, EKS
  hostNetwork: false
  config:
    etcd:
      enabled: false
    scheduler:
      enabled: false
    controllerManager:
      enabled: false
    apiServer:
     staticEndpoint:
       url: "https://kubernetes.default:443"
       insecureSkipVerify: true
       auth:
         type: bearer
```

なお、この機能はAPIサーバーにのみ適用され、クラウド環境ではetcd、スケジューラー、コントローラーマネージャーにはアクセスできませんのでご注意ください。

さらに、特定のマネージド環境またはクラウド環境によっては、KubernetesサービスがAPIサーバーのさまざまなインスタンス間でトラフィックの負荷を分散する可能性があることに注意してください。この場合、ロードバランサーによって選択されている特定のインスタンスに依存するメトリックは信頼できません。

#### ランチャーRKE1のコントロールプレーンモニタリング [#rancher]

Rancher Kubernetes Engine（RKE1）を利用してデプロイされたクラスターは、コントロールプレーンコンポーネントを、Kubeletによって管理されるポッドとしてではなくDockerコンテナーとして実行します。そのため、統合ではこれらのコンテナーを自動検出できず、統合構成の`staticEndpoint`セクションですべてのエンドポイントを手動で指定する必要があります。

統合は、直接接続するか、使用可能な認証方法（サービスアカウントトークン、カスタムmTLS証明書、またはなし）を使用するか、プロキシを介して、さまざまなエンドポイントに到達できる必要があります。

たとえば、スケジューラとコントローラマネージャのメトリックを到達可能にするために、 `--authorization-always-allow-paths`[フラグ](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/)を追加して、 `/metrics`または`--authentication-kubeconfig`と`--authorization-kubeconfig`でトークン認証を有効にする必要がある場合があります。

すべてのコンポーネントが指定されたポートで到達可能であると仮定すると、次の構成はAPIサーバー、スケジューラー、およびコントローラーマネージャーを監視します。

```yaml
controlPlane:
  kind: DaemonSet
  config:
    scheduler:
      enabled: true
      staticEndpoint:
        url: https://localhost:10259
        insecureSkipVerify: true
        auth:
          type: bearer
    controllerManager:
      enabled: true
      staticEndpoint:
        url: https://localhost:10257
        insecureSkipVerify: true
        auth:
          type: bearer
    apiServer:
      enabled: true
      staticEndpoint:
        url: https://localhost:6443
        insecureSkipVerify: true
        auth:
          type: bearer
```

この例では、統合は各`staticEndpoint`にローカルに接続しているため、 `hostNetwork`オプションがtrueに設定されている各コントロールプレーンコンポーネントの同じノードで実行する必要があります。したがって、 `controlPlane.kind`は`DaemonSet`として保持する必要があります。また、DaemonSetには、監視するすべてのコントロールプレーンノードですべてのインスタンスが実行されるように、アフィニティルール、nodeSelector、および許容値を構成する必要があります。

`node-role.kubernetes.io/controlplane`ラベルを確認すると、コントロールプレーンノードを認識できます。このラベルは、統合のデフォルトのアフィニティルールによってすでに考慮されています。

## 統合されたコントロールプレーンの監視 バージョン2 [#monitoring-control-plane]

ここでは、バージョン2以前のインテグレーションでコントロールプレーンモニタリングを設定する方法について説明します。

これらのバージョンでは、オートディスカバリーオプションの柔軟性が低く、外部エンドポイントをサポートしていないことにご注意ください。お早めに [バージョン3](/docs/kubernetes-pixie/kubernetes-integration/get-started/changes-since-v3) にアップデートされることを強くお勧めします。 [変更点をご覧ください](/docs/kubernetes-pixie/kubernetes-integration/get-started/changes-since-v3) Kubernetesとの統合の。

<CollapserGroup>
  <Collapser
    className="freq-link"
    id="controlplane-v2"
    title="統合バージョン2のコントロールプレーン監視"
  >
    ## マスターノードとコントロールプレーンコンポーネントの発見 [#discover-nodes-components]

    Kubernetes統合は、 [kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/)ラベル付け規則に依存して、マスターノードとコントロールプレーンコンポーネントを検出します。これは、マスターノードに`node-role.kubernetes.io/master=""`または`kubernetes.io/role="master"`のラベルを付ける必要があることを意味します。

    コントロールプレーンコンポーネントには、 `k8s-app`または`tier`および`component`のラベルが必要です。受け入れられるラベルの組み合わせと値については、次の表を参照してください。

    <table>
      <thead>
        <tr>
          <th style={{ width: "110px" }}>
            コンポーネント
          </th>

          <th>
            ラベル
          </th>

          <th style={{ width: "200px" }}>
            終点
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            APIサーバー
          </td>

          <td>
            **Kubeadm / Kops / ClusterAPI**

            `k8s-app=kube-apiserver`

            `tier=control-plane component=kube-apiserver`

            **OpenShift**

            `app=openshift-kube-apiserver apiserver=true`
          </td>

          <td>
            `localhost:443/metrics` デフォルトでは（構成可能）、要求が失敗した場合はにフォールバックします `localhost:8080/metrics`
          </td>
        </tr>

        <tr>
          <td>
            etcd
          </td>

          <td>
            **Kubeadm / Kops / ClusterAPI**

            `k8s-app=etcd-manager-main`

            `tier=control-plane component=etcd`

            **OpenShift**

            `k8s-app=etcd`
          </td>

          <td>
            `localhost:4001/metrics`
          </td>
        </tr>

        <tr>
          <td>
            スケジューラー
          </td>

          <td>
            **Kubeadm / Kops / ClusterAPI**

            `k8s-app=kube-scheduler`

            `tier=control-plane component=kube-scheduler`

            **OpenShift**

            `app=openshift-kube-scheduler scheduler=true`
          </td>

          <td>
            `localhost:10251/metrics`
          </td>
        </tr>

        <tr>
          <td>
            コントローラーマネージャー
          </td>

          <td>
            **Kubeadm / Kops / ClusterAPI**

            `k8s-app=kube-controller-manager`

            `tier=control-plane component=kube-controller-manager​`

            **OpenShift**

            `app=kube-controller-manager kube-controller-manager=true`
          </td>

          <td>
            `localhost:10252/metrics`
          </td>
        </tr>
      </tbody>
    </table>

    統合は、マスター・ノード内で実行されていることを検出すると、上の表に記載されているラベルに一致するポッドを探して、ノード上でどのコンポーネントが実行されているかを確認しようとします。実行中のコンポーネントごとに、統合はそのメトリクス・エンドポイントにリクエストを行います。

    ### 構成

    マスターノード内で動作するエージェントは、コントロールプレーンの監視が自動的に行われます。クライアントのリクエストに相互TLS認証（mTLS）を使用しているため、実行に余分な手順が必要なコンポーネントはetdのみです。APIサーバーは、 [Secure Port](https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/#api-server-ports-and-ips) を使って問い合わせを行うように設定することもできます。

    <Callout variant="important">
      [OpenShift](http://learn.openshift.com/?extIdCarryOver=true&sc_cid=701f2000001OH7iAAG) 4.x のコントロールプレーン監視には、追加の設定が必要です。詳細については、 [OpenShift 4.x Configuration](#openshift-4x-configuration) のセクションを参照してください。
    </Callout>

    #### etcd

    etcdへの問い合わせにmTLSを設定するには、2つの設定オプションを設定する必要があります。

    <table>
      <thead>
        <tr>
          <th style={{ width: "280px" }}>
            オプション
          </th>

          <th>
            価値
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `ETCD_TLS_SECRET_NAME`
          </td>

          <td>
            mTLSの設定を含むKubernetesのシークレットの名前。

            シークレットには以下のキーを入れてください。

            * `cert`：要求を行っているクライアントを識別する証明書。 etcdの信頼できるCAによって署名されている必要があります。

            * `key`：クライアント証明書の生成に使用される秘密鍵。

            * `cacert`：etcdサーバー証明書の識別に使用されるルートCA。

              `ETCD_TLS_SECRET_NAME`オプションが設定されていない場合、etcdメトリックはフェッチされません。
          </td>
        </tr>

        <tr>
          <td>
            `ETCD_TLS_SECRET_NAMESPACE`
          </td>

          <td>
            `ETCD_TLS_SECRET_NAME`で指定されたシークレットが作成されたネームスペース。設定されていない場合、 `default`名前空間が使用されます。
          </td>
        </tr>
      </tbody>
    </table>

    #### APIサーバー

    デフォルトでは、APIサーバーの指標は`localhost:8080`のセキュリティで保護されていないエンドポイントを使用してクエリされます。このポートが無効になっている場合は、セキュアポートを介してこれらのメトリックを照会することもできます。これを有効にするには、Kubernetes統合マニフェストファイルで次の設定オプションを設定します。

    <table>
      <thead>
        <tr>
          <th style={{ width: "250px" }}>
            オプション
          </th>

          <th>
            価値
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `API_SERVER_ENDPOINT_URL`
          </td>

          <td>
            メトリックをクエリするための（安全な）URL。 APIサーバーはデフォルトで`localhost:443`を使用します

            `ClusterRole`がマニフェストにある最新バージョンに更新されていることを確認してください

            バージョン1.15.0で追加
          </td>
        </tr>
      </tbody>
    </table>

    <Callout variant="important">
      なお、このポートは、APIサーバーが使用するセキュアポートに応じて異なる場合があります。

      たとえば、MinikubeではAPIサーバーのセキュアポートは8443であるため、 `API_SERVER_ENDPOINT_URL`は次のように設定する必要があります。 `https://localhost:8443`
    </Callout>
  </Collapser>
</CollapserGroup>

## OpenShiftの設定 [#openshift-4x-configuration]

Kubernetes Integrationのバージョン3には、OpenShiftクラスタ内のコントロールプレーンコンポーネントを自動検出するデフォルト設定が含まれているため、etdを除くすべてのコンポーネントですぐに動作するはずです。

OpenShift 環境ではメトリクスのエンドポイントが mTLS 認証を必要とするように構成されているため、Etcd はすぐにはサポートされません。当社の統合は、この構成でetcdのメトリクスを取得するためのmTLS認証をサポートしていますが、必要なmTLS証明書を手動で作成する必要があります。これは、ユーザーからの明示的な承認なしに当社の統合に広範な権限を与えることを避けるために必要です。

mTLSシークレットを作成するには、 [以下の本セクションの手順に従ってください。](#mtls-how-to-openshift) その後、 [mtlsセクション](#mtls) で説明されているように、新しく作成されたシークレットを使用するように統合を構成してください。

<CollapserGroup>
  <Collapser
    className="freq-link"
    id="openshift-v2"
    title="統合バージョン2でのOpenShiftの設定"
  >
    <Callout variant="important">
      Helmを介して`openshift`をインストールする場合は、これらのエンドポイントを自動的に含めるように構成を指定してください。 `openshift.enabled=true`と`openshift.version="4.x"`を設定すると、セキュアエンドポイントが含まれ、 `/var/run/crio.sock`ランタイムが有効になります。
    </Callout>

    [OpenShift](http://learn.openshift.com/?extIdCarryOver=true&sc_cid=701f2000001OH7iAAG) 4.x のコントロールプレーンコンポーネントは、SSL およびサービスアカウントベースの認証を必要とするエンドポイント URL を使用します。そのため、 [デフォルトの](/docs/integrations/kubernetes-integration/installation/configure-control-plane-monitoring#discover-nodes-components) エンドポイントURLは使用できません。

    OpenShift でコントロールプレーンモニタリングを構成するには、 [カスタマイズマニフェスト](/docs/integrations/kubernetes-integration/installation/kubernetes-integration-install-configure#customized-manifest) で以下の環境変数のコメントを解除します。URL 値は、 [OpenShift](http://learn.openshift.com/?extIdCarryOver=true&sc_cid=701f2000001OH7iAAG) 4.x のコントロールプレーン監視メトリクスエンドポイントのデフォルトベース URL にあらかじめ設定されています。

    ```
    - name: "SCHEDULER_ENDPOINT_URL"
      value: "https://localhost:10259
    - name: "ETCD_ENDPOINT_URL"
      value: "https://localhost:9979"
    - name: "CONTROLLER_MANAGER_ENDPOINT_URL"
      value: "https://localhost:10257"
    - name: "API_SERVER_ENDPOINT_URL"
      value: "https://localhost:6443"
    ```

    <Callout variant="important">
      カスタム`ETCD_ENDPOINT_URL`が定義されている場合でも、etcdではHTTPSおよびmTLS認証を構成する必要があります。 OpenShiftでのetcdのmTLSの設定の詳細については、OpenShiftで[のetcdのmTLSの設定を](#mtls-how-to-openshift)参照してください。
    </Callout>
  </Collapser>
</CollapserGroup>

### OpenShift での etcd 用 mTLS の設定 [#mtls-how-to-openshift]

以下の手順で、 [OpenShift](http://learn.openshift.com/?extIdCarryOver=true&sc_cid=701f2000001OH7iAAG) 4.xでetcdの相互TLS認証を設定してください。

1. etcdクライアント証明書をクラスターから不透明なシークレットにエクスポートします。デフォルトのマネージドOpenShiftクラスターでは、シークレットの名前は`kube-etcd-client-certs`で、 `openshift-monitoring`名前空間に保存されます。

   ```shell
   kubectl get secret etcd-client -n openshift-etcd -o yaml > etcd-secret.yaml
   ```

   etcd-secret.yaml の内容は次のようになります。

   ```yaml
    apiVersion: v1
    data:
      tls.crt: <CERT VALUE>
      tls.key: <KEY VALUE>
    kind: Secret
    metadata:
      creationTimestamp: "2023-03-23T23:19:17Z"
      name: etcd-client
      namespace: openshift-etcd
      resourceVersion: 
      uid: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
    type: kubernetes.io/tls   
   ```

2. 必要に応じて、シークレットの名前と名前空間を意味のあるものに変更します。次の手順では、シークレットの名前と名前空間がそれぞれ `mysecret` と `newrelic` に変更されていることを前提としています。

3. メタデータ部のこれらの不要なキーを削除します。

   * `creationTimestamp`
   * `resourceVersion`
   * `uid`

4. マニフェストを新しい名前と名前空間でインストールします。

   ```shell
   kubectl apply -n newrelic -f etcd-secret.yaml
   ```

5. [mtls セクション](#mtls)の説明に従って、新しく作成されたシークレットを使用するように統合を構成します。これは、 `nri-bundle` チャートを介して統合をインストールする場合、 `values.yaml` に次の構成を追加することで実行できます。

   ```yaml
   newrelic-infrastructure:
    controlPlane:
      enabled: true
      config:
        etcd:
          enabled: true
          autodiscover:
            - selector: "app=etcd,etcd=true,k8s-app=etcd"
              namespace: openshift-etcd
              matchNode: true
              endpoints:
                - url: https://<ETCD_ENDPOINT>:<PORT>
                  insecureSkipVerify: true
                  auth:
                    type: mTLS
                    mtls:
                      secretName: mysecret
                      secretNamespace: newrelic
   ```

## 自分のデータを見る [#check-integration-works]

統合が正しく設定されていれば、 [Kubernetes cluster explorer](/docs/integrations/kubernetes-integration/cluster-explorer/kubernetes-cluster-explorer) には、以下のように、すべてのコントロールプレーンコンポーネントとそのステータスが専用のセクションに表示されます。

<img
  title="new-relic-one-k8s-cluster-explorer-control-plane-parameters.png"
  alt="New Relic - Kubernetes Cluster Explorer - Control Plane section"
  src={kubernetesClusterExplorerControlPlane}
/>

<figcaption>
  **[one.newrelic.com](https://one.newrelic.com/all-capabilities) > Kubernetes Cluster Explorer** : Kubernetes クラスター エクスプローラーを使用して、クラスターのコントロール プレーン コンポーネントからメトリックを監視および収集します。
</figcaption>

また、この [NRQL](/docs/query-data/nrql-new-relic-query-language/getting-started/introduction-nrql) クエリでコントロールプレーンデータを確認することもできます。

```SQL
SELECT latest(timestamp) FROM K8sApiServerSample, K8sEtcdSample, K8sSchedulerSample, K8sControllerManagerSample FACET entityName where clusterName = '_MY_CLUSTER_NAME_'
```

<Callout variant="tip">
  それでもコントロールプレーンのデータが表示されない場合は、 [Kubernetesインテグレーションのトラブルシューティングで説明している解決方法を試してみてください。データが見えない](/docs/integrations/kubernetes-integration/troubleshooting/kubernetes-integration-troubleshooting-not-seeing-data) 。
</Callout>