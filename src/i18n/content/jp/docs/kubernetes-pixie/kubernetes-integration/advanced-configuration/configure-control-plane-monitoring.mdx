---
title: コントロールプレーンモニタリングの設定
tags:
  - Integrations
  - Kubernetes integration
  - Installation
metaDescription: How to configure control plane monitoring for your Kubernetes integration with New Relic.
freshnessValidatedDate: never
translationType: machine
---

[New Relic](/docs/infrastructure/new-relic-infrastructure/getting-started/introduction-new-relic-infrastructure) は、Kubernetes 統合のために [コントロールプレーン](https://kubernetes.io/docs/concepts/overview/components/#control-plane-components) をサポートしており、クラスタのコントロールプレーンコンポーネントからメトリクスを監視・収集することができます。これらのデータは、New Relic で、 [クエリやチャートの作成に使用することができます](/docs/using-new-relic/data/understand-data/query-new-relic-data) 。

<Callout variant="tip">
  このページで[は、 Kubernetesインテグレーション v3](/docs/kubernetes-pixie/kubernetes-integration/advanced-configuration/k8s-version2/changes-since-v3)について説明します。 v2 を実行している場合は、 [v2 のコントロール プレーン監視を構成する方法を](/docs/kubernetes-pixie/kubernetes-integration/advanced-configuration/k8s-version2/overview/##monitoring-control-plane)参照してください。
</Callout>

## 特徴 [#control-plane-integration-features]

当社は、以下のコントロール プレーン コンポーネントから[メトリックを](/docs/integrations/kubernetes-integration/understand-use-data/understand-use-data#metrics)監視し、収集します。

* <DNT>
    **etcd**
  </DNT>

  : リーダー情報、常駐メモリサイズ、OS スレッド数、コンセンサス提案データなど。 サポートされているメトリックのリストについては、 [etcd data](/docs/integrations/kubernetes-integration/understand-use-data/understand-use-data#etcd-data)を参照してください。

* <DNT>
    **API server**
  </DNT>

  : `apiserver`リクエストのレート、HTTP メソッドと応答コード別の`apiserver`リクエストの内訳など。 サポートされているメトリックの完全なリストについては、 [APIサーバー データ](/docs/integrations/kubernetes-integration/understand-use-data/understand-use-data#api-server-data)を参照してください。

* <DNT>
    **Scheduler**
  </DNT>

  : 要求された CPU/メモリとノード上で使用可能な CPU/メモリ、汚染に対する許容度、設定されたアフィニティまたは反アフィニティなど。 サポートされているメトリックの完全なリストについては、 [「スケジューラ データ」](/docs/integrations/kubernetes-integration/understand-use-data/understand-use-data#scheduler-data)を参照してください。

* <DNT>
    **Controller manager**
  </DNT>

  : 常駐メモリのサイズ、作成された OS スレッドの数、現在存在する goroutine など。 サポートされているメトリックの完全なリストについては、 [「コントローラー マネージャー データ」](/docs/integrations/kubernetes-integration/understand-use-data/understand-use-data#controller-manager-data)を参照してください。

## 互換性と要件 [#compatibility]

* AKS、EKS、GKEを含むほとんどのマネージドクラスターは、コントロールプレーンコンポーネントへの外部アクセスを許可していません。そのため、マネージドクラスターでは、New RelicはAPIサーバーのコントロールプレーンメトリックのみを取得でき、etcd、スケジューラー、またはコントローラーマネージャーのコントロールプレーンメトリックは取得できません。
* [非特権モード](/docs/integrations/kubernetes-integration/installation/kubernetes-installation-configuration#unprivileged) でソリューションを展開する場合、コントロールプレーンのセットアップには [追加の手順が必要となり、](#hostnetwork-privileged) いくつかの注意点が適用されます。
* [OpenShift](http://learn.openshift.com/?extIdCarryOver=true&sc_cid=701f2000001OH7iAAG) 4.xでは、コントロールプレーンコンポーネントのメトリックエンドポイントがデフォルトとは異なるものを使用しています。

## コントロールプレーンコンポーネント [#component]

Kubernetesコントロールプレーンを監視するタスクは、デフォルトでDaemonSetとしてデプロイされる`nrk8s-controlplane`コンポーネントの責任です。このコンポーネントは、 `node-role.kubernetes.io/control-plane`や`node-role.kubernetes.io/master`などのマスターノードを識別するために一般的に使用されるラベルを含む`nodeSelectorTerms`のデフォルトリストを使用して、マスターノードに自動的にデプロイされます。とにかく、このセレクターは`values.yml`ファイルで公開されているため、他の環境に合わせて再構成できます。

これらのセレクターに一致するノードがないクラスターでは、ポッドがスケジュールされないため、リソースを浪費することはなく、ヘルムチャートで`controlPlane.enabled`を`false`に設定することでコントロールプレーンの監視を完全に無効にすることと機能的に同等です。

コントロールプレーンの各コンポーネントには専用のセクションが設けられており、個別に対応することができます。

* そのコンポーネントのモニタリングを有効または無効にする
* そのコンポーネントを発見するための特定のセレクタと名前空間を定義します。
* そのコンポーネントのメトリクスを取得するために使用されるエンドポイントとパスの定義
* そのコンポーネントのメトリクスを取得するために使用する必要のある認証メカニズムの定義
* オートディスカバリーを完全にスキップするエンドポイントを手動で指定します。

<img
  title="Diagram showing a possible configuration scraping etcd with mTLS and API server with bearer Token."
  alt="Diagram showing a possible configuration scraping etcd with mTLS and API server with bearer Token. The monitoring is a DaemonSet deployed on master nodes only."
  src="/images/kubernetes_diagram_integration-cp.webp"
/>

`controlPlane`キーの下のnri-kubernetesチャートの[values.yaml](https://github.com/newrelic/nri-kubernetes/blob/main/charts/newrelic-infrastructure/values.yaml)で使用可能なすべての構成オプションを確認できます。

`nri-bundle` チャートを介して統合をインストールする場合は [、対応するサブチャートに値を渡す](https://helm.sh/docs/chart_template_guide/subcharts_and_globals/)必要があります。たとえば、 `controlPlane` コンポーネントで `etcd` 監視を無効にするには、次のようにします。

```yaml
newrelic-infrastructure:
  controlPlane:
    config:
      etcd:
        enabled: false
```

### オートディスカバリーとデフォルト設定 [#autodiscovery-default]

デフォルトでは、 [Helm Chart](/docs/kubernetes-pixie/kubernetes-integration/installation/install-kubernetes-integration-using-helm)は、 `Kubeadm`や`minikube`など、クラスター内でコントロールプレーンを実行するオンプレミスディストリビューションの一部のコントロールプレーンコンポーネントに対して、すぐに使用できる構成を提供します。

自動検出は検出メカニズムとしてポッドラベルに依存しているため、クラウド環境や、コントロールプレーンコンポーネントがクラスター内で実行されていない場合は機能しないことに注意してください。ただし、これらのシナリオでは、コントロールプレーンコンポーネントに到達できる場合、[静的エンドポイント](#static-endpoints)を利用できます。

#### `hostNetwork` および `privileged` [#hostnetwork-privileged]

3 以降のバージョンでは、 `privileged`フラグは`securityContext`オブジェクトにのみ影響します。つまり、コンテナがホスト メトリクスにアクセスできる root として実行されるかどうかに影響します。 ほとんどのディストリビューションでコントロール プレーンのエンドポイントに到達するために必要なため、コントロール プレーンからメトリクスを取得するポッドには`hostNetwork: true`が設定されていますが、すべての統合コンポーネントはデフォルトで`hostNetwork: false`になります。 すべてのコンポーネントの`hostNetwork`値は、 [`values.yaml`の`hostNetwork`トグルを](https://github.com/newrelic/helm-charts/tree/master/library/common-library#values-managed-globally)使用して、個別またはグローバルに変更できます。

<Callout variant="tip">
  バージョン 2 に関連する特定の設定については、 [自動検出とデフォルト構成: `hostNetwork`および`privileged`](/docs/kubernetes-pixie/kubernetes-integration/advanced-configuration/k8s-version2/overview/#hostnetwork-privileged)を参照してください。
</Callout>

クラスタまたはその他のポリシーのために、 `hostNetwork`でポッドを実行することがまったく受け入れられない場合、コントロールプレーンの監視は不可能であり、 `controlPlane.enabled`を`false`に設定して無効にする必要があります。

[カスタム自動検出](#autodiscovery-default)または[静的エンドポイント](#static-endpoints)を含む高度な構成がある場合は、 `hostNetwork`なしでコントロールプレーンを監視するために使用できます。 [プロジェクトのREADME](https://github.com/newrelic/nri-kubernetes/blob/main/charts/newrelic-infrastructure/README.md)を確認し、 [`values.yaml`](https://github.com/newrelic/nri-kubernetes/blob/main/charts/newrelic-infrastructure/values.yaml)で`controlPlane.hostNetwork`トゥーグルを探します。

#### カスタムオートディスカバリー [#autodiscovery-default]

自動検出に使用されるセレクターは、 `values.yaml`ファイルの構成エントリとして完全に公開されます。つまり、コントロールプレーンがクラスターの一部として実行されるほとんどすべての環境に合わせて、セレクターを微調整または置換できます。

オートディスカバリーのセクションは以下のようになります。

```yaml
autodiscover:
  - selector: "tier=control-plane,component=etcd"
    namespace: kube-system
    # Set to true to consider only pods sharing the node with the scraper pod.
    # This should be set to `true` if Kind is Daemonset, `false` otherwise.
    matchNode: true
    # Try to reach etcd using the following endpoints.
    endpoints:
      - url: https://localhost:4001
        insecureSkipVerify: true
        auth:
          type: bearer
      - url: http://localhost:2381
  - selector: "k8s-app=etcd-manager-main"
    namespace: kube-system
    matchNode: true
    endpoints:
      - url: https://localhost:4001
        insecureSkipVerify: true
        auth:
          type: bearer
```

`autodiscover`セクションには、自動検出エントリのリストが含まれています。各エントリには次のものがあります。

* `selector`：ポッドを検索するために使用される文字列でエンコードされたラベルセレクター。
* `matchNode`：trueに設定すると、検出を実行するDaemonSetの特定のインスタンスと同じノードで実行されているポッドに検出が追加で制限されます。
* `endpoints`：指定されたセレクターのポッドが見つかった場合に試行するエンドポイントのリスト。

さらに、各`endpoint`には次のものがあります。

* `url`：スキームを含む、ターゲットへのURL。 `http`または`https`にすることができます。
* `insecureSkipVerify`：trueに設定すると、証明書は`https` URLについてチェックされません。
* `auth.type`：リクエストの認証に使用するメカニズム。現在、次のメソッドがサポートされています。
* なし： `auth`が指定されていない場合、リクエストには認証がまったく含まれません。
* `bearer`：KubernetesAPIに対する認証に使用されたものと同じベアラトークンがこのリクエストに送信されます。
* `mtls`：mTLSはリクエストの実行に使用されます。

##### mTLS [#mtls]

`mtls`タイプの場合、以下を指定する必要があります。

```yaml
endpoints:
  - url: https://localhost:4001
    auth:
      type: mtls
      mtls:
        secretName: secret-name
        secretNamespace: secret-namespace
```

ここで、 `secret-name`は、 `secret-namespace`に存在するKubernetes TLSシークレットの名前であり、その特定のエンドポイントに接続するために必要な証明書、キー、およびCAが含まれています。

統合では、このシークレットをマウントするのではなく、実行時にフェッチします。つまり、このシークレットへのアクセスを許可するRBACロールが必要です。ヘルムチャートは、レンダリング時に`auth.mtls`エントリを自動的に検出し、 `rbac.create`がfalseに設定されていない限り、これらの特定のシークレットと名前空間のエントリを自動的に作成します。

私たちの統合は、以下のキーを持つ秘密を受け入れます。

* `cacert`: 署名に使用される PEM エンコードされた CA 証明書。 `cert`
* `cert`：etcdに提示されるPEMでエンコードされた証明書
* `key`：上記の証明書に対応するPEMでエンコードされた秘密鍵

これらの証明書は、etcdが動作に使用しているのと同じCAによって署名されている必要があります。

これらの証明書を生成する方法は、Kubernetesのディストリビューションによって大きく異なるため、このドキュメントの範囲外です。必要なetcdピア証明書を取得する方法については、ディストリビューションのドキュメントを参照してください。たとえば、Kubeadmでは、マスターノードの`/etc/kubernetes/pki/etcd/peer.{crt,key}`にあります。

etcdのピア証明書を見つけたり生成したりしたら、シークレットに含まれると思われるキーに合わせてファイル名を変更し、クラスタにシークレットを作成します。

```shell
mv peer.crt cert
mv peer.key key
mv ca.crt cacert

kubectl -n newrelic create secret tls newrelic-etcd-tls-secret --cert=./cert --key=./key --certificate-authority=./cacert
```

最後に、このセクションの冒頭に示されている構成スニペットにシークレット名（ `newrelic-etcd-tls-secret` ）と名前空間（ `newrelic` ）を入力できます。 Helm Chartはこの構成を自動的に解析し、RBACロールを作成して、 `nrk8s-controlplane`コンポーネントのこの特定のシークレットと名前空間へのアクセスを許可するため、その点で手動のアクションは必要ありません。

### 静的エンドポイント [#static-endpoints]

オートディスカバリーは、コントロールプレーンがKubernetesクラスター内に存在する場合をカバーする必要がありますが、ディストリビューションや洗練されたKubernetes環境の中には、可用性やリソースの分離など様々な理由から、コントロールプレーンを別の場所で実行するものもあります。

このような場合、コントロールプレーンラベルの付いたポッドがノードで見つかったかどうかに関係なく、任意の固定URLをスクレイプするように統合を構成できます。これは、 `staticEndpoint`エントリを指定することによって行われます。たとえば、外部etcdインスタンスの場合は次のようになります。

```yaml
controlPlane:
  etcd:
    staticEndpoint:
      url: https://url:port
      insecureSkipVerify: true
      auth: {}
```

<img
  title="Diagram showing a possible configuration scraping an external API server with bearer Token."
  alt="Diagram showing a possible configuration scraping an external API server with bearer Token. The monitoring is a Deployment with a single replica."
  src="/images/kubernetes_diagram_integration-cp-external.webp"
/>

`staticEndpoint` は`autodiscover`エントリの`endpoints`と同じタイプのエントリであり、そのフィールドは上記で説明されています。ここでは、認証メカニズムとスキーマがサポートされています。

`staticEndpoint`が設定されている場合、 `autodiscover`セクションは完全に無視されることに注意してください。

#### 制限 [#static-endpoints-limitations]

<Callout variant="important">
  ノード外のエンドポイント（たとえば、 `localhost`ではないエンドポイント）を指す`staticEndpoint`を使用している場合は、 `controlPlane.kind`を`DaemonSet`から`Deployment`に変更する必要があります。
</Callout>

`staticEndpoint`を使用すると、すべての `nrk8s-controlplane` Pod が上記のエンドポイントに到達してスクレイピングしようとします。これは、 `nrk8s-controlplane` が DaemonSet (デフォルト) の場合、DaemonSet のすべてのインスタンスがこのエンドポイントをスクレイピングすることを意味します。 `localhost`を指している場合はこれで問題ありませんが、エンドポイントがノードに対してローカルでない場合、メトリックが重複し、課金対象の使用量が増える可能性があります。 `staticEndpoint` 使用して非ローカル URL を指している場合は、必ず `controlPlane.kind` を Deployment に変更してください。

上記と同様の理由により、現在のところ、あるコントロールプレーンコンポーネントにオートディスカバリーを使用し、他のコンポーネントにスタティックエンドポイントを使用することはできません。これは既知の制限事項であり、将来のバージョンでの対応を検討しています。

最後に、 `staticEndpoint`ではコンポーネントごとに1つのエンドポイントのみを定義できます。これは、異なるホストに複数のコントロールプレーンシャードがある場合、現在、それらを個別にポイントすることはできないことを意味します。これは、将来のバージョンで対処するために取り組んでいる既知の制限でもあります。当面の回避策は、別の場所にあるさまざまなシャードのメトリックを集約し、集約された出力を`staticEndpoint` URLにポイントすることです。

#### マネージド環境やクラウド環境のためのコントロールプレーン監視 [#cloud-control-plane]

EKSやGKEのような一部のクラウド環境では、Kubernetes API Serverからメトリクスを取得することができます。これは静的なエンドポイントとして簡単に設定できます。

```yaml
controlPlane:
  affinity: false  # https://github.com/helm/helm/issues/9136
  kind: Deployment
  # `hostNetwork` is not required for monitoring API Server on AKS, EKS
  hostNetwork: false
  config:
    etcd:
      enabled: false
    scheduler:
      enabled: false
    controllerManager:
      enabled: false
    apiServer:
     staticEndpoint:
       url: "https://kubernetes.default:443"
       insecureSkipVerify: true
       auth:
         type: bearer
```

なお、この機能はAPIサーバーにのみ適用され、クラウド環境ではetcd、スケジューラー、コントローラーマネージャーにはアクセスできませんのでご注意ください。

さらに、特定のマネージド環境またはクラウド環境によっては、KubernetesサービスがAPIサーバーのさまざまなインスタンス間でトラフィックの負荷を分散する可能性があることに注意してください。この場合、ロードバランサーによって選択されている特定のインスタンスに依存するメトリックは信頼できません。

#### ランチャーRKE1のコントロールプレーンモニタリング [#rancher]

Rancher Kubernetes Engine（RKE1）を利用してデプロイされたクラスターは、コントロールプレーンコンポーネントを、Kubeletによって管理されるポッドとしてではなくDockerコンテナーとして実行します。そのため、統合ではこれらのコンテナーを自動検出できず、統合構成の`staticEndpoint`セクションですべてのエンドポイントを手動で指定する必要があります。

統合は、直接接続するか、使用可能な認証方法（サービスアカウントトークン、カスタムmTLS証明書、またはなし）を使用するか、プロキシを介して、さまざまなエンドポイントに到達できる必要があります。

たとえば、スケジューラとコントローラマネージャのメトリックを到達可能にするために、 `--authorization-always-allow-paths`[フラグ](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/)を追加して、 `/metrics`または`--authentication-kubeconfig`と`--authorization-kubeconfig`でトークン認証を有効にする必要がある場合があります。

すべてのコンポーネントが指定されたポートで到達可能であると仮定すると、次の構成はAPIサーバー、スケジューラー、およびコントローラーマネージャーを監視します。

```yaml
controlPlane:
  kind: DaemonSet
  config:
    scheduler:
      enabled: true
      staticEndpoint:
        url: https://localhost:10259
        insecureSkipVerify: true
        auth:
          type: bearer
    controllerManager:
      enabled: true
      staticEndpoint:
        url: https://localhost:10257
        insecureSkipVerify: true
        auth:
          type: bearer
    apiServer:
      enabled: true
      staticEndpoint:
        url: https://localhost:6443
        insecureSkipVerify: true
        auth:
          type: bearer
```

この例では、統合は各`staticEndpoint`にローカルに接続しているため、 `hostNetwork`オプションがtrueに設定されている各コントロールプレーンコンポーネントの同じノードで実行する必要があります。したがって、 `controlPlane.kind`は`DaemonSet`として保持する必要があります。また、DaemonSetには、監視するすべてのコントロールプレーンノードですべてのインスタンスが実行されるように、アフィニティルール、nodeSelector、および許容値を構成する必要があります。

`node-role.kubernetes.io/controlplane`ラベルを確認すると、コントロールプレーンノードを認識できます。このラベルは、統合のデフォルトのアフィニティルールによってすでに考慮されています。

インテグレーションのバージョン 2 を使用している場合は、「インテグレーション[バージョン 2 での監視コントロール プレーン」](/docs/kubernetes-pixie/kubernetes-integration/advanced-configuration/k8s-version2/overview/#monitoring-control-plane)を参照してください。

## OpenShiftの設定 [#openshift-4x-configuration]

Kubernetesインテグレーションのバージョン 3 には、OpenShift クラスタ内のコントロール プレーン コンポーネントを自動検出するデフォルト設定が含まれているため、etcd を除くすべてのコンポーネントですぐに機能するはずです。

OpenShift 環境ではメトリクスのエンドポイントが mTLS 認証を必要とするように構成されているため、Etcd はすぐにはサポートされません。当社の統合は、この構成でetcdのメトリクスを取得するためのmTLS認証をサポートしていますが、必要なmTLS証明書を手動で作成する必要があります。これは、ユーザーからの明示的な承認なしに当社の統合に広範な権限を与えることを避けるために必要です。

mTLSシークレットを作成するには、 [以下の本セクションの手順に従ってください。](#mtls-how-to-openshift) その後、 [mtlsセクション](#mtls) で説明されているように、新しく作成されたシークレットを使用するように統合を構成してください。

インテグレーションのバージョン 2 を使用している場合は、 [インテグレーション バージョン 2 で OpenShift を設定します](/docs/kubernetes-pixie/kubernetes-integration/advanced-configuration/k8s-version2/overview/#openshift-configuration)。

### OpenShift での etcd 用 mTLS の設定 [#mtls-how-to-openshift]

以下の手順で、 [OpenShift](http://learn.openshift.com/?extIdCarryOver=true&sc_cid=701f2000001OH7iAAG) 4.xでetcdの相互TLS認証を設定してください。

1. etcdクライアント証明書をクラスターから不透明なシークレットにエクスポートします。デフォルトのマネージドOpenShiftクラスターでは、シークレットの名前は`kube-etcd-client-certs`で、 `openshift-monitoring`名前空間に保存されます。

   ```shell
   kubectl get secret etcd-client -n openshift-etcd -o yaml > etcd-secret.yaml
   ```

   etcd-secret.yaml の内容は次のようになります。

   ```yaml
    apiVersion: v1
    data:
      tls.crt: <CERT VALUE>
      tls.key: <KEY VALUE>
    kind: Secret
    metadata:
      creationTimestamp: "2023-03-23T23:19:17Z"
      name: etcd-client
      namespace: openshift-etcd
      resourceVersion: 
      uid: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
    type: kubernetes.io/tls   
   ```

2. 必要に応じて、シークレットの名前と名前空間を意味のあるものに変更します。次の手順では、シークレットの名前と名前空間がそれぞれ `mysecret` と `newrelic` に変更されていることを前提としています。

3. メタデータ部のこれらの不要なキーを削除します。

   * `creationTimestamp`
   * `resourceVersion`
   * `uid`

4. マニフェストを新しい名前と名前空間でインストールします。

   ```shell
   kubectl apply -n newrelic -f etcd-secret.yaml
   ```

5. [mtls セクション](#mtls)の説明に従って、新しく作成されたシークレットを使用するように統合を構成します。これは、 `nri-bundle` チャートを介して統合をインストールする場合、 `values.yaml` に次の構成を追加することで実行できます。

   ```yaml
   newrelic-infrastructure:
    controlPlane:
      enabled: true
      config:
        etcd:
          enabled: true
          autodiscover:
            - selector: "app=etcd,etcd=true,k8s-app=etcd"
              namespace: openshift-etcd
              matchNode: true
              endpoints:
                - url: https://<ETCD_ENDPOINT>:<PORT>
                  insecureSkipVerify: true
                  auth:
                    type: mTLS
                    mtls:
                      secretName: mysecret
                      secretNamespace: newrelic
   ```

## 自分のデータを見る [#check-integration-works]

統合が正しく設定されていれば、 [Kubernetes cluster explorer](/docs/integrations/kubernetes-integration/cluster-explorer/kubernetes-cluster-explorer) には、以下のように、すべてのコントロールプレーンコンポーネントとそのステータスが専用のセクションに表示されます。

<img
  title="new-relic-one-k8s-cluster-explorer-control-plane-parameters.png"
  alt="New Relic - Kubernetes Cluster Explorer - Control Plane section"
  src="/images/kubernetes_screenshot-full_cluster-explorer-control-plane.webp"
/>

<figcaption>
  <DNT>**[one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > Kubernetes Cluster Explorer**</DNT>: Kubernetesクラスタ エクスプローラーを使用して、クラスタのコントロール プレーン コンポーネントからメトリックを監視および収集します。
</figcaption>

また、この [NRQL](/docs/query-data/nrql-new-relic-query-language/getting-started/introduction-nrql) クエリでコントロールプレーンデータを確認することもできます。

```SQL
SELECT latest(timestamp) FROM K8sApiServerSample, K8sEtcdSample, K8sSchedulerSample, K8sControllerManagerSample FACET entityName where clusterName = '_MY_CLUSTER_NAME_'
```

<Callout variant="tip">
  それでもコントロールプレーンのデータが表示されない場合は、 [Kubernetesインテグレーションのトラブルシューティングで説明している解決方法を試してみてください。データが見えない](/docs/integrations/kubernetes-integration/troubleshooting/kubernetes-integration-troubleshooting-not-seeing-data) 。
</Callout>
