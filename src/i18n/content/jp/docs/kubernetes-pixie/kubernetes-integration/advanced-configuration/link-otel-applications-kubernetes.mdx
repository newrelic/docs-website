---
title: OpenTelemetryをインストルメント化したアプリケーションをKubernetesにリンクする
tags:
  - Integrations
  - Kubernetes integration
  - OpenTelemetry
  - Link apps and services
metaDescription: Connect New Relic's Kubernetes monitoring with applications monitored with OpenTelemetry.
translationType: machine
---

import kubernetesOtelValidation from 'images/kubernetes_screenshot-crop_otel-validation.webp'

Kubernetesでアプリを実行している開発者の場合は、New Relicを使用して、KubernetesインフラストラクチャがOpenTelemetryが計測されたアプリケーションにどのように影響するかを理解できます。

以下の手順を完了すると、New Relic UIを使用して、OpenTelemetryのアプリケーションレベルの指標をKubernetesインフラストラクチャの指標と関連付けることができます。これにより、テレメトリデータの全体像を確認し、チーム間で作業して、Kubernetes環境の問題の平均解決時間（MTTR）を短縮できます。

## 概要：データをどのように関連付けるか [#correlate]

このガイドの手順により、アプリケーションはインフラストラクチャ固有のメタデータをテレメトリデータに挿入できます。その結果、NewRelicUIに実用的な情報が入力されます。これを実行するために実行する手順は次のとおりです。

* 各アプリケーションコンテナで、テレメトリデータをコレクタに送信するための環境変数を定義します
* OpenTelemetry Collector を、 `resourcedetection`、 `resource`、 `batch` 、および `k8sattributes` プロセッサを備えた [エージェント モード](https://opentelemetry.io/docs/collector/getting-started/#agent) の `DaemonSet` としてデプロイして、関連するメタデータ (クラスタ名、デプロイメント名、名前空間名) を挿入します。

## 前提条件 [#prereqs]

以下の手順を成功させるには、OpenTelemetryとKubernetesに精通しており、次のことを行っている必要があります。

* 次の環境変数を作成しました。

  * `OTEL_EXPORTER_OTLP_ENDPOINT` （ [お住まいの地域または目的に合わせたNew Relicエンドポイント](/docs/more-integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-setup/#review-settings)）

  * `NEW_RELIC_API_KEY` (

    <InlinePopover type="licenseKey"/>

    )

* クラスターに[NewRelicKubernetes統合](/docs/kubernetes-pixie/kubernetes-integration/installation/kubernetes-integration-install-configure)をインストールしました

* [OpenTelemetry](/docs/more-integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-setup/)を使用してアプリケーションをインストルメント化し、OpenTelemetry Protocol（OTLP）を介してNewRelicにデータを正常に送信しました

New Relicでのコレクターの使用に関する一般的な質問がある場合は[、NewRelicでのOpenTelemetryコレクターの概要を](/docs/more-integrations/open-source-telemetry-integrations/opentelemetry/collector/opentelemetry-collector-intro)参照してください。

## テレメトリ データを OpenTelemetry Collector に送信するようにアプリケーションを構成する [#instrument]

これを設定するには、カスタム スニペットを Kubernetes YAML ファイルの `env` スタンザに追加する必要があります。サンプル フロントエンド マイクロサービス (`Frontend.yaml`) のスニペットを示す例を以下に示します。このスニペットには、次のことを行う 2 つのセクションが含まれています。

* **セクション1：**テレメトリデータがコレクターに送信されていることを確認します。これにより、環境変数`OTEL_EXPORTER_OTLP_ENDPOINT`にホストIPが設定されます。これは、ダウンワードAPIを呼び出してホストIPをプルすることによって行われます。
* **セクション 2:** インフラストラクチャ固有のメタデータを添付します。これを行うには、下向き API を使用して `metadata.uid` をキャプチャし、それを OTEL_RESOURCE_ATTRIBUTES 環境変数に追加します。この環境変数は、OpenTelemetry Collector の `resourcedetection` および `k8sattributes` プロセッサによって使用され、追加のインフラストラクチャ固有のコンテキストをテレメトリ データに追加します。

OpenTelemetryを備えたマイクロサービスごとに、以下の強調表示された行をマニフェストの`env`スタンザに追加します。

```yaml
# Frontend.yaml
apiVersion: apps/v1
kind: Deployment
...
spec:
  containers:
  - name: yourfrontendservice
    image: yourfrontendservice-beta
  env:
  # Section 1: Ensure that telemetry data is sent to the collector
  - name: HOST_IP
        valueFrom:
          fieldRef:
            fieldPath: status.hostIP
      # This is picked up by the opentelemetry sdks
      - name: OTEL_EXPORTER_OTLP_ENDPOINT
        value: "http://$(HOST_IP):55680"
  # Section 2: Attach infrastructure-specific metadata
     # Get pod ip so that k8sattributes can tag resources
- name: POD_NAME
valueFrom:
fieldRef:
fieldPath: metadata.name
- name: POD_UID
valueFrom:
fieldRef:
fieldPath: metadata.uid
      # This is picked up by the resource detector
    - name: OTEL_RESOURCE_ATTRIBUTES
      value: service.instance.id=$(POD_NAME),k8s.pod.uid=$(POD_UID)”
```

## OpenTelemetry Collector をエージェントとして構成および展開する [#agent]

[コレクターをエージェントとして](https://opentelemetry.io/docs/collector/getting-started/#agent)Kubernetesクラスター内のすべてのノードにデプロイすることをお勧めします。エージェントは、テレメトリデータを受信できるだけでなく、メタデータでテレメトリデータを強化できます。たとえば、コレクターは、プロセッサーを介してカスタム属性またはインフラストラクチャ情報を追加したり、バッチ処理、再試行、圧縮、およびクライアントインストルメンテーションレベルであまり効率的に処理されないその他のより高度な機能を処理したりできます。

コレクターの構成については、以下のサンプルコレクター構成ファイルと、これらのオプションの設定に関するセクションを参照してください。

* [OTLPエクスポーター](#otlp-exporter)
* [バッチプロセッサ](#batch)
* [リソース検出プロセッサ](#resource-detection)
* [k8sattributesプロセッサ：一般](#attributes-general)
* [k8sattributesプロセッサ：RBAC](#rbac)
* [k8sattributesプロセッサ：フィルタ](#discovery-filter)

```yaml
receivers:
  otlp:
    protocols:
      grpc:

processors:
  batch:
  resource:
    attributes:
      - key: host.id
        from_attribute: host.name
        action: upsert
  resourcedetection:
    detectors: [ gke, gce ]
  k8sattributes:
    auth_type: "serviceAccount"
    passthrough: false
    filter:
      node_from_env_var: KUBE_NODE_NAME
    extract:
      metadata:
        - k8s.pod.name
        - k8s.pod.uid
        - k8s.deployment.name
        - k8s.cluster.name
        - k8s.namespace.name
        - k8s.node.name
        - k8s.pod.start_time
    pod_association:
      - from: resource_attribute
        name: k8s.pod.uid

exporters:
  otlp:
    endpoint: $OTEL_EXPORTER_OTLP_ENDPOINT
    headers:
      api-key: $NEW_RELIC_API_KEY
  logging:
    logLevel: DEBUG

service:
  pipelines:
    metrics:
      receivers: [ otlp ]
      processors: [ resourcedetection, k8sattributes, resource, cumulativetodelta, batch ]
      exporters: [ otlp ]
    traces:
      receivers: [ otlp ]
      processors: [ resourcedetection, k8sattributes, resource, batch ]
      exporters: [ otlp ]
    logs:
      receivers: [ otlp ]
      processors: [ resourcedetection, k8sattributes, resource, batch ]
      exporters: [ otlp ]
```

### ステップ1：OTLPエクスポーターを構成する [#otlp-exporter]

まず、New Relic とともに OTLP エクスポータを [OpenTelemetry Collector 設定 YAML ファイル](https://opentelemetry.io/docs/collector/configuration/) に追加して設定します。 <InlinePopover type="licenseKey"/>ヘッダーとして。

```yaml
exporters:
  otlp:
    endpoint: $OTEL_EXPORTER_OTLP_ENDPOINT
  headers: api-key: $NEW_RELIC_API_KEY
```

### ステップ2：バッチプロセッサを構成する [#batch]

バッチプロセッサは、スパン、メトリック、またはログを受け入れ、それらをバッチに配置して、データの圧縮を容易にし、コレクタからの送信要求の数を減らします。

```
processors:
  batch:
```

### 手順3：リソース検出プロセッサを構成する [#resource-detection]

`resourcedetection`プロセッサは、ホスト固有の情報を取得して、コレクタを介して処理されているテレメトリデータにコンテキストを追加します。この例では、Google Kubernetes Engine（GKE）とGoogle Compute Engine（GCE）を使用して、次のようなGoogleCloud固有のメタデータを取得します。

* cloud.provider（ "gcp"）
* cloud.platform（ "gcp_compute_engine"）
* cloud.account.id
* cloud.region
* cloud.availability_zone
* host.id
* host.image.id
* host.type

```yaml
processors:
  resourcedetection:
Detectors: [ gke, gce ]
```

### 手順4：Kubernetes属性プロセッサを設定する（一般） [#attributes-general]

エージェントとして実行される OpenTelemetry Collector の一部として `k8sattributes` プロセッサを実行すると、OpenTelemetry Collector エージェントにテレメトリ データを送信するポッドの IP アドレスが検出され、ポッドのメタデータの抽出に使用されます。以下は、プロセッサ セクションのみを含む基本的な Kubernetes マニフェストの例です。OpenTelemetry Collector を `DaemonSet`としてデプロイするには、この [包括的なマニフェストの例](https://github.com/newrelic-forks/microservices-demo/tree/main/src/otel-collector-agent)をお読みください。

```yaml
processors:
  k8sattributes:
    auth_type: "serviceAccount"
    passthrough: false
    filter:
      node_from_env_var: KUBE_NODE_NAME
    extract:
      metadata:
        - k8s.pod.name
        - k8s.pod.uid
        - k8s.deployment.name
        - k8s.cluster.name
        - k8s.namespace.name
        - k8s.node.name
        - k8s.pod.start_time
    pod_association:
      - from: resource_attribute
        name: k8s.pod.uid
```

### 手順5：Kubernetes属性プロセッサ（RBAC）を設定する [#rbac]

ロールベースのアクセス制御（RBAC）の構成を追加する必要があります。 `k8sattributes`プロセッサには、構成されたフィルターに含まれるポッドと名前空間リソースに対する`get` 、 `watch` 、および`list`のアクセス許可が必要です。クラスター内のすべてのポッドと名前空間に必要なアクセス許可を`ServiceAccount`に与えるために、 `ClusterRole`の役割ベースのアクセス制御（RBAC）を構成する方法のこの[例](https://github.com/newrelic-forks/microservices-demo/blob/main/otel-kubernetes-manifests/otel-collector-agent.yaml#L43-L69)を参照してください。

### 手順6：Kubernetes属性プロセッサを設定する（検出フィルタ） [#discovery-filter]

コレクターをエージェントとして実行する場合は、プロセッサーが実行しているのと同じホストからのみポッドを検出するように、検出フィルターを適用する必要があります。フィルタを使用しない場合、特に非常に大規模なクラスタでは、リソースの使用量が不必要に高くなる可能性があります。フィルタが適用されると、各プロセッサは、独自のノードで実行されているポッドについてのみKubernetesAPIにクエリを実行します。

フィルターを設定するには、下向き API を使用して、OpenTelemetry Collector エージェント構成 YAML ファイルのポッド `env` セクションに環境変数としてノード名を挿入します (例については、 [GitHub](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/examples/kubernetes/otel-collector-config.yml) を参照)。これにより、新しい環境変数が OpenTelemetry Collector エージェントのコンテナに挿入されます。値は、ポッドが実行されるようにスケジュールされたノードの名前になります。

```yaml
spec:
  containers:
  - env:
    - name: KUBE_NODE_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: spec.nodeName
```

次に、 `k8sattributes`のノードでフィルタリングできます。

```yaml
  k8sattributes:
    filter:
      node_from_env_var: KUBE_NODE_NAME
```

## 構成が機能していることを確認します [#validate]

OpenTelemetryデータをKubernetesデータに正常にリンクすると、分散トレースUI内のスパンに`k8s.cluster.name`や`k8s.deployment.name`などのKubernetes属性が表示されるはずです。

クリックして画像を拡大します。

<img
  src={kubernetesOtelValidation}
  title="Screenshot showing K8s metadata in the New Relic UI"
  alt="Screenshot showing K8s metadata in the New Relic UI"
/>

## 次は何ですか？ [#next]

OpenTelemetryをインストルメント化したアプリをKubernetesに接続したので、OpenTelemetryとNewRelicの使用を改善するためのヒントについての[ベストプラクティス](/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-concepts/)ガイドを確認してください。