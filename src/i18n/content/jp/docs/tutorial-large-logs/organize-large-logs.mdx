---
title: 大規模なログの取り込みを整理する
metaDescription: Organize your logs into managable partitions and tag their attributes with logs parsing
translationType: machine
---

import logsParsing from 'images/logs_screenshot_full-parsing.webp'
import logsPartition from 'images/logs_screenshot_full-partition.webp'

どのログを取り込んで保存するかを決めたら、ログを整理します。おそらく、依然として数百ギガバイトまたは数十テラバイトのログを取り込むことになります。当初よりはかなり減りましたが、効果的に使用しようとすると、やはり苦労が必要になります。

これを解決するために、これらの残りのログをテーマ別のパーティションにグループ化し、ログを解析して貴重な属性を取り出してタグ付けします。この方法でログを整理すると、次のことが可能になります。

* ログ内の任意の属性をクエリします
* フロントエンド ログとバックエンド ログなど、特定のパーティションを一度に管理
* クエリのロード時間を短縮する

## ログを分割する理由

ログを個別のパーティションに分割すると、パフォーマンスが向上し、ログの管理が容易になります。たとえば、フロントエンド ログにのみ存在する属性をクエリしたいとします。5 TB のログをクエリすると、フロントエンド ログのみの特定の 1 TB パーティションをクエリするよりも大幅に時間がかかります。

データを分割する目的:

* 環境や組織の中で、固定的であったり、変化の頻度が低い概念に合わせてデータパーティションを作成します（例：ビジネスユニット、チーム、環境、サービスなど）。
* 最適なパフォーマンスを得るためには、各パーティションが1日の取り込み量が1TB以下であることを確認してください。

パーティションの保持には 2 つのタイプがあります。保持力を最大限に高めるには、 **Standard** \[標準] を選択します。遠い将来にクエリを実行する可能性がある貴重なデータにこれを使用します。30 日間保持するには **Secondary** \[セカンダリを] 選択します。これは、価値の低いデータ、または近い将来にのみ必要となるデータに使用してください。

## ログを解析する理由

取り込み時にログを解析することは、あなたや組織内の他のユーザーがログ データをより使いやすくするための最良の方法です。たとえば、Grok 解析ルールを使用して、次の 2 つのログを解析前と解析後で比較します。

<SideBySide>
  <Side>
    事前解析:

    ```
    {
        "message": "32 4329 store_Portland"
    }


    ```
  </Side>

  <Side>
    事後解析:

    ```
    {
        "transaction_total": "32",
        "purchase_number": "4329",
        "store_location": "store_Portland",
    }
    ```
  </Side>
</SideBySide>

これにより、ダッシュボードやアラートで `transaction_total` などの新しく定義された属性を簡単にクエリできるようになります。

## ログを整理する

ACME Corp が今後数か月間で約 2 TB のログを取り込むことが分かっているとします。Java アプリとインフラストラクチャ エージェントからのログのパーティションを作成したいと考えています。彼らは、遠い将来に Java ログをクエリする必要があるかもしれないと考え、標準の保存期間を使用することにしました。一方、最近のインフラストラクチャ ログのみが必要なため、それらに対しては二次保持を使用します。

そのためには

<Steps>
  <Step>
    ### UIに移動します

    **[one.newrelic.com > Logs](https://one.newrelic.com/launcher/logger.log-launcher)**に移動します  

  </Step>

  <Step>
    ### ログを分割する

    <SideBySide>
      <Side>
        1. 左側のナビゲーションの **Manage Data** \[データの管理] で、 **Data partitions** \[データ パーティション]をクリックし、 **Create partition rule** \[パーティション ルールの作成]をクリックします。
        2. パーティション名を、 `Log_`で始まる英数字の文字列として定義します。この場合、 `Log_java`。
        3. 任意の説明を追加します。
        4. パーティションの標準の名前空間保持を選択します。
        5. ルールの一致基準を設定します。このパーティションに保存するログと一致する有効な NRQL `WHERE` 句を入力します。この場合、 `logtype=java`。
        6. **\[作成]** をクリックして、新しいパーティションを保存します。
      </Side>

      <Side>
        <img
          title="log-partition"
          alt="An image displaying New Relic's log partion UI"
          src={logsPartition}
        />
      </Side>
    </SideBySide>

    これにより、すべての Java ログに対して標準のデータ保持を持つデータ パーティションが作成されます。インフラストラクチャ ログを整理するには、上記と同じ手順に従い、保持をセカンダリに変更し、クエリを `logtype=infrastructure`に変更するだけです。
  </Step>

  <Step>
    ### ログを解析する

    ログが分割されたので、今度はログを解析します。それらを解析すると、ログ内の関連データを抽出して、クエリとアクセスを容易にすることができます。

    ログを解析するには:

    <SideBySide>
      <Side>
        1. ログ UI の左側のナビゲーションにある **Manage Data** \[データの管理] で、 **Parsing** \[解析]をクリックし、 **Create parsing rule** \[解析ルールの作成]をクリックします。
        2. 新しい解析規則の名前を入力します。
        3. 解析する既存のフィールドを選択するか (デフォルトは `message`)、または新しいフィールド名を入力します。
        4. 解析するログに一致する有効な NRQL `WHERE`句を入力してください。
        5. 一致するログが存在する場合はそれを選択するか、「ログの貼り付け」タブをクリックしてサンプル ログを貼り付けます。
        6. 解析ルールを入力し、 **Output** \[出力] セクションで結果を表示して、解析ルールが機能していることを検証します。(以下の例を参照してください)
        7. カスタム解析ルールを有効にして保存します。
      </Side>

      <Side>
        <img
          title="log-parsing"
          alt="An image displaying New Relic's log parsing UI"
          src={logsParsing}
        />
      </Side>
    </SideBySide>

    次の例は、解析ルールを作成する具体的な例を示しています。

    <CollapserGroup>
      <Collapser
        id="example"
        title="ログの解析例"
      >
        このドキュメントの前半で使用した例を使ってみましょう。次のパターンに従うログがあります。

        ```
        {
            "message": "32 4329 store_Portland"
        }
        ```

        取引金額、注文番号、店舗の場所を取得したいとします。解析ルールは Grok を使用して構築され、パターン `%{SYNTAX:SEMANTIC}`が使用されます。`SYNTAX` はテキストの検索に使用されるパターン、 `SEMANTIC` は一致した結果に与えられる識別子または属性です。

        この場合、解析ルールは次のようになります。

        ```
        %{INT:transaction_total} %{INT:purchase_number} store%{DATA:store_location}
        ```

        上記のパターンで解析ルールが作成されると、次の方法でログが返されます。

        ```
        {
            "transaction_total": "32",
            "purchase_number": "4329",
            "store_location": "store_Portland",
        }
        ```
      </Collapser>
    </CollapserGroup>

    ログを解析するための Grok パターンの作成の詳細については、 [ブログ投稿を参照してください](https://newrelic.com/blog/how-to-relic/how-to-use-grok-log-parsing)。
  </Step>
</Steps>

## 次のステップ

ログの真の価値を発見し、チームがログに対する何時間ものイライラを軽減できたこと、おめでとうございます。システムが成長して取り込むにつれて、解析ルールとパーティションを確実に維持する必要があります。New Relic のログ管理で何ができるかについて詳しく知りたい場合は、次のドキュメントを参照してください。

* [ログ データの解析](/docs/logs/ui-data/parsing): Grok を使用したログの解析を詳しく調べ、GraphQL API である NerdGraph を使用してログ解析ルールを作成、クエリ、管理する方法を学びます。
* [ログ パターン](/docs/logs/ui-data/find-unusual-logs-log-patterns/): ログ パターンは、検索せずにログ データの値を発見する最速の方法です。
* [ログの難読化](/docs/logs/ui-data/obfuscation-ui/): ログの難読化ルールを使用すると、特定の種類の情報が New Relic に保存されないようにすることができます。
* [長いログ (BLOB) でデータを検索する](/docs/logs/ui-data/long-logs-blobs/): 大量のログ データは、問題のトラブルシューティングに役立ちます。しかし、ログ内の属性に数千の文字が含まれている場合はどうなるでしょうか?New Relic はこのデータのうちどれくらいの量を保存できますか?そして、このすべてのデータから有用な情報を見つけるにはどうすればよいでしょうか?

<DocTiles numbered>
  <DocTile
    title="Get started"
    path="/docs/tutorial-large-logs/get-started-managing-large-logs"
  />

  <DocTile
    title="Filter and reduce your log ingest"
    path="/docs/tutorial-large-logs/filter-large-logs"
  />

  <DocTile
    title="Organize your logs"
    label={{text: 'You are here', color: '#FCD672'}}
    path="/docs/tutorial-large-logs/organize-large-logs"
  />
</DocTiles>