---
title: 모델 성능 모니터링(MLOps) 소개
metaDescription: Use New Relic ML model performance monitoring to monitor and observe the performance of your machine learning models.
translationType: human
---

머신 러닝 작업은 품질을 높이고 관리 프로세스를 간소화하며 대규모 운영 환경에서 머신 러닝 모델의 배포를 자동화하도록 설계된 일련의 관행으로 구성됩니다.

인공 지능과 머신 러닝에 투자하는 기업이 늘어나면서, 머신 러닝 모델을 개발하는 데이터 과학 팀과 이 모델을 지원하는 애플리케이션을 운영하는 데브옵스 팀 사이에 이해의 격차가 존재하게 되었습니다. 현재 기업의 15%만이 전체 활동에 AI를 구현합니다. 게다가 배포, 모니터링, 관리 및 거버넌스의 문제로 인해 운영에서 머신 러닝 모델의 75%가 전혀 사용되지 않고 있습니다. 궁극적으로 모델 작업을 하는 엔지니어와 데이터 과학자의 막대한 시간이 낭비되고, 투자에 대한 막대한 순손실이 발생하며, 머신 러닝 모델이 정량화 가능한 성장을 지원하는 경우 전반적인 신뢰 부족을 야기합니다.

모델 성능 모니터링은 운영 중인 모델의 행동과 효과를 모니터링하여 데이터 과학자와 MLOP 실무자에게 머신 러닝 애플리케이션에 대한 가시성을 제공합니다. 이를 통해 데이터 팀은 지속적인 개발, 테스트 및 운영 모니터링 프로세스를 생성하는 데브옵스 팀과 직접적으로 협업할 수 있습니다.

아직 계정이 없으시면, 아래에서 무료 뉴렐릭 계정을 생성해 지금 바로 데이터 모니터링을 시작하십시오.

<InlineSignup/>

## 머신 러닝 모델을 모니터링하는 방법 [#use-mlops]

뉴렐릭의 [응용 인텔리전스](/docs/alerts-applied-intelligence/new-relic-alerts/get-started/introduction-applied-intelligence/) 내에서 모델 성능 모니터링을 사용할 수 있는 몇 가지 옵션이 있습니다.

1. **자체 데이터 사용:** 뉴렐릭에서 권장하는 접근 방식입니다. 뉴렐릭의 ML 모델 성능 모니터링은 ML 모델이 운영에서 작동하는 방식에 대한 심층적인 옵저버빌리티를 제공합니다. 자체 데이터는 모든 환경(Python 스크립트, 컨테이너, Lambda 함수, SageMaker 등)에서 사용될 수 있으며, 모든 머신 러닝 프레임워크(Scikit-learn, Keras, Pytorch, Tensorflow, Jax 등)와 쉽게 통합될 수 있습니다. 자체 데이터를 사용하면 자체적인 ML 모델 텔레메트리를 뉴렐릭으로 가져와 ML 모델 데이터에서 가치를 실현할 수 있습니다. 단 몇 분 만에 모니터링하려는 다른 커스텀 메트릭과 함께 기능 분포, 통계 데이터 및 예측 분포를 확보할 수 있습니다. 뉴렐릭의 [문서](/docs/alerts-applied-intelligence/mlops/bring-your-own/mlops-byo)에서 자체 데이터를 사용하는 방법에 대해 자세히 알아보십시오.

2. **통합:** 뉴렐릭은 또한 Amazon SageMaker와 협력해, SageMaker에서 뉴렐릭으로의 성능 메트릭 뷰를 제공하며 ML 엔지니어 및 데이터 과학 팀의 옵저버빌리티에 대한 액세스를 확장해줍니다. [Amazon SageMaker 통합](/docs/mlops/integrations/aws-sagemaker-mlops-integration/)에 대해 자세히 알아보십시오.

3. **파트너십:** 뉴렐릭은 특정 사용 사례 및 모니터링 기능을 제공하는 7개의 MLOps 벤더와 파트너십을 맺었습니다. 파트너는 선별된 성능 <InlinePopover type="dashboards"/>과 다른 옵저버빌리티 툴에 액세스할 수 있는 좋은 방법으로, 즉시 사용 가능하며 모델에 대한 즉각적인 가시성을 제공하는 대시보드를 제공합니다.

   뉴렐릭는 현재 다음과 파트너 관계를 맺고 있습니다.

   * [Datarobot(알고리즘)](/docs/alerts-applied-intelligence/mlops/integrations/datarobot-mlops-integration/)
   * [Aporia](/docs/alerts-applied-intelligence/mlops/integrations/aporia-mlops-integration/)
   * [Comet](/docs/alerts-applied-intelligence/mlops/integrations/comet-mlops-integration/)
   * [DagsHub](/docs/alerts-applied-intelligence/mlops/integrations/dagshub-mlops-integration/)
   * [Mona](/docs/alerts-applied-intelligence/mlops/integrations/mona-mlops-integration/)
   * [Superwise](/docs/alerts-applied-intelligence/mlops/integrations/superwise-mlops-integration/)
   * [TruEra](/docs/alerts-applied-intelligence/mlops/integrations/truera-mlops-integration/)

이러한 옵션 중 하나를 사용해 몇 분 안에 머신 러닝 모델 성능 측정을 시작하려면 [모델 성능 모니터링 퀵스타트](https://newrelic.com/instant-observability/?category=machine-learning-ops)를 확인해 보십시오.

## OpenAI GPT 앱을 모니터링하는 방법 [#monitor-openai-gtp]

[GPT 시리즈 애플리케이션 통합](/docs/mlops/integrations/openai-integration/)을 사용하면 OpenAI 완료 쿼리를 모니터링하고 요청에 관해 맞춤화할 수 있는 뉴렐릭 대시보드에 유용한 통계를 로깅할 수 있습니다. 두 줄의 코드만 추가하면 비용, 응답 시간 및 샘플 입력/출력 같은 주요 성능 메트릭에 액세스할 수 있습니다. 완전하게 맞춤화 할 수 있는 대시보드를 통해 사용자는 총 요청, 평균 토큰/요청 및 모델 이름을 추적할 수 있습니다. [뉴렐릭 OpenAI 퀵스타트](https://newrelic.com/instant-observability/openai)를 방문하여 보다 자세히 알아보거나 통합을 설치하십시오.