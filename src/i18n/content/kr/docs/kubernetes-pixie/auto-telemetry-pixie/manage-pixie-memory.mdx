---
title: Pixie에서 사용하는 메모리 관리
tags:
  - Pixie Auto-telemetry
  - lowmemorymode
  - low memory mode
  - reduce memory
  - Kubernetes pods
  - Kubernetes
  - manage Pixie memory
  - oomkill Pixie
metaDescription: How to manage the memory used by Pixie
translationType: machine
---

Pixie가 사용하는 메모리 양을 구성할 수 있습니다. 설치하는 동안 Helm을 사용하여 메모리 요청 및 제한을 설정하거나 Pixie가 단기 데이터 저장에 사용하는 메모리 양을 줄이십시오.

## Pixie는 메모리를 어떻게 사용합니까? [#memory-usage]

[오픈 소스 Pixie 프로젝트](https://github.com/pixie-io/pixie) 의 주요 초점은 실시간 디버깅 플랫폼을 구축하는 것입니다. Pixie [는 장기 내구성 저장 솔루션을 위한 것이 아니며](https://docs.px.dev/about-pixie/faq/#data-collection-how-much-data-does-pixie-store) New Relic과 함께 사용하는 것이 가장 좋습니다. New Relic 통합은 몇 분마다 Pixie를 쿼리하고 Pixie의 원격 측정 데이터를 New Relic에 유지합니다.

New Relic Pixie 통합을 설치하면 DaemonSet를 통해 클러스터의 각 노드에 `vizier-pem` 에이전트가 배포됩니다. `vizier-pem` 에이전트는 두 가지 주요 목적으로 메모리를 사용합니다.

* **원격 측정 데이터 수집** : 무엇보다도 애플리케이션 트래픽 또는 CPU 프로필 추적. 이러한 값은 처리될 때 메모리 어딘가에 저장되어야 합니다.
* **원격 측정 데이터의 단기 저장** : New Relic에 저장되기 전.

기본적으로 `vizier-pem` 포드에는 `2Gi` 메모리 제한과 `2Gi` 메모리 요청이 있습니다. 할당된 메모리의 60%를 단기 데이터 저장에 할당하고 나머지 40%는 데이터 수집을 위해 남겨둡니다.

### 설치 후 Pixie의 메모리 사용량이 증가하는 이유는 무엇입니까? [#memory-increase]

Pixie를 설치한 후 원격 분석 데이터를 저장하기 시작하면 `vizier-pem` 포드의 메모리 사용량이 증가합니다. `vizier-pem` 의 메모리 제한에 도달하면 새 데이터를 위한 공간을 만들기 위해 이전 원격 분석 데이터가 만료되므로 메모리 사용률이 더 이상 증가하지 않아야 합니다.

## Pixie의 메모리 사용량 구성 [#configure-memory]

대부분의 클러스터에서는 기본 `2Gi` 메모리 구성을 사용하는 것이 좋습니다. 그러나 트래픽이 적은 특정 클러스터의 경우 Pixie는 `1Gi` 의 최소 메모리 제한을 지원할 수 있습니다. 애플리케이션 포드를 수용하려면 노드의 총 메모리 중 25% 이하를 Pixie에 할당하는 것이 좋습니다. 예를 들어, 노드의 총 메모리가 `4Gi` 인 경우 `1Gi` 메모리 제한을 사용하도록 Pixie를 구성할 수 있습니다.

### 특정 메모리 제한이 있는 Pixie 배포 [#set-memory-limit]

Pixie의 `vizier-pem` 에이전트에 대해 기본 `2Gi` 와 다른 메모리 제한을 지정하려는 경우 설치 중에 Helm 차트에 다음 구성 매개변수를 추가할 수 있습니다. 예를 들어 `1Gi` 메모리 제한의 경우 다음을 사용합니다.

```
-set pixie-chart.pemMemoryLimit=1Gi
```

### 특정 메모리 요청으로 Pixie 배포 [#set-memory-request]

기본적으로 `vizier-pem` 의 메모리 요청은 제한과 동일합니다. 다른 메모리 [요청](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits) 을 지정하려면 설치 중에 Helm 차트에 다음 구성 매개변수를 추가할 수 있습니다. 예를 들어 `1Gi` 메모리 요청의 경우 다음을 사용합니다.

```
--set pixie-chart.pemMemoryRequest=1Gi
```

<Callout variant="important">
  `vizier-pem` 메모리 요청은 `vizier-pem` 메모리 제한보다 작거나 같아야 합니다.
</Callout>

### 특정 단기 데이터 저장소로 Pixie 배포 [#reduce-data-storage]

기본적으로 `vizier-pem` 포드는 할당된 메모리의 60%를 단기 데이터 저장용으로 남겨두고 나머지 40%는 컬렉션용으로 남겨둡니다. 기본 `2Gi` 메모리 제한의 경우 이는 `vizier-pem` 포드가 데이터 저장을 위해 `1.2Gi` 메모리를 유지함을 의미합니다.

단기 저장을 위해 다른 양의 메모리를 지정하려는 경우 설치 중에 Helm 차트에 다음 구성 매개변수를 추가할 수 있습니다. 예를 들어, 단기 데이터 스토리지의 `750MiB` 에 대해 다음을 사용합니다.

```
--set pixie-chart.dataCollectorParams.customPEMFlags.PL_TABLE_STORE_DATA_LIMIT_MB=750
```

## 문제점 해결 [#troubleshooting]

### `vizier-pem` 포드가 예약에 실패한 이유는 무엇입니까? [#schedule-fail]

`vizier-pem` 포드가 예약에 실패하는 경우 이는 일반적으로 요청된 양의 메모리가 없는 노드가 있기 때문입니다.

[메모리 요청](#set-memory-request) 을 `1Gi` 까지 줄이십시오. 특히 크거나 높은 트래픽 클러스터의 경우 `1Gi` }로는 충분하지 않을 수 있으며 일부 `vizier-pem` OOMKill이 발생할 수 있습니다. 이러한 클러스터의 경우 제한에 대한 `2Gi` 과 같이 요청보다 높은 제한을 설정할 수 있습니다. 트래픽이 적은 소규모 클러스터의 경우 `1Gi` 배포를 제한하는 것이 좋습니다.

### `vizier-pem` 포드가 OOMKilled된 이유는 무엇입니까? [#oomkilled-pods]

Kubernetes에 배포할 때의 이점 중 일부는 Kubernetes가 클러스터의 다른 포드를 보호하기 위해 너무 많은 리소스를 사용하는 포드를 OOMKill한다는 것입니다.

Pixie의 `vizier-pem` 포드가 메모리 제한을 초과하면 OOMKilled됩니다. Kubernetes는 다른 애플리케이션을 보호하기 위해 이 작업을 수행하므로 다른 포드에서 문제를 일으키지 않습니다. 정기적으로 OOMKilled되는 `vizier-pem` 포드의 경우 [더 많은 메모리로 Pixie를 배포해](#set-memory-limit) 보십시오.