---
title: 제어 영역 데이터가 표시되지 않음
type: troubleshooting
tags:
  - Integrations
  - Kubernetes integration
  - Troubleshooting
metaDescription: Some troubleshooting tips if you are not seeing data control plane data for your New Relic's Kubernetes integration.
translationType: machine
---

## 문제

New Relic의 Kubernetes 통합을 위한 [설치 절차](/docs/kubernetes-monitoring-integration#install) 를 완료했습니다. New Relic 계정에 Kubernetes 데이터가 표시되지만 컨트롤 플레인 구성 요소의 데이터가 없습니다.

<CollapserGroup>
  <Collapser
    id="control-plane-sample-missing"
    title="ControlPlane 샘플이 누락되었습니다."
  >
    제어 영역 데이터가 누락된 경우(예: `K8sSchedulerSample` ) 가장 먼저 해야 할 일은 제어 영역 구성요소의 자세한 로그를 확인하는 것입니다.[자세한 로깅을 활성화](/docs/kubernetes-pixie/kubernetes-integration/troubleshooting/get-logs-version#verbose-logging) 하는 방법 읽기

    * 가능성은 자동 검색이 클러스터에서 가장 일반적인 레이블을 활용하는 컨트롤 플레인 포드를 찾으려고 시도할 수 있습니다. 단일 구성 요소에 대해 포드가 발견되지 않는 경우 더 많은 데이터가 누락되는 것을 방지하는 데 실패하지 않습니다.이 시나리오에서는 다음과 유사한 로그를 볼 수 있습니다.

      ```
      time="2022-06-21T12:21:25Z" level=debug msg="Autodiscovering pods for \"scheduler\""
      time="2022-06-21T12:21:25Z" level=debug msg="0 pods found with labels \"tier=control-plane,component=kube-scheduler\""
      time="2022-06-21T12:21:25Z" level=debug msg="No pod found for \"scheduler\" with labels \"tier=control-plane,component=kube-scheduler\""
      time="2022-06-21T12:21:25Z" level=debug msg="0 pods found with labels \"k8s-app=kube-scheduler\""
      time="2022-06-21T12:21:25Z" level=debug msg="No pod found for \"scheduler\" with labels \"k8s-app=kube-scheduler\""
      time="2022-06-21T12:21:25Z" level=debug msg="0 pods found with labels \"app=openshift-kube-scheduler,scheduler=true\""
      time="2022-06-21T12:21:25Z" level=debug msg="No pod found for \"scheduler\" with labels \"app=openshift-kube-scheduler,scheduler=true\""
      time="2022-06-21T12:21:25Z" level=debug msg="No \"scheduler\" pod has been discovered"
      ```

      이 경우 helm [차트 값](https://github.com/newrelic/nri-kubernetes/blob/main/charts/newrelic-infrastructure/values.yaml) 의 `controlplane.config.[component].autodiscover[].selector` 구성을 사용하여 검색 동작을 변경할 수 있습니다.[제어 평면 구성 요소](/docs/kubernetes-pixie/kubernetes-integration/get-started/changes-since-v3/#nrk8s-controlplane) 에 대해 자세히 알아보십시오.

    * 제어 평면 구성 요소를 찾았지만 끝점에 대한 인증이 실패할 수도 있습니다.이 시나리오에서는 다음과 유사한 로그를 볼 수 있습니다.

      ```
      time="2022-06-21T15:54:52Z" level=debug msg="Endpoint \"https://localhost:10257\" probe failed, skipping: http request failed with status: 403 Forbidden"
      ```

      이 경우 helm [차트 값](https://github.com/newrelic/nri-kubernetes/blob/main/charts/newrelic-infrastructure/values.yaml) 의 `controlplane.config.[component].autodiscover[].endpoints[].auth` 구성을 사용하여 각 엔드포인트에 대한 인증 동작을 변경할 수 있습니다.

    * 통합의 컨트롤플레인 구성 요소가 모든 마스터 노드에서 실행되고 있지 않을 수도 있습니다.

      다음이 실행 중인지 다시 확인할 수 있습니다.

      ```
      kubectl get pod -n <NEWRELIC_NAMESPACE> -l app.kubernetes.io/component=controlplane -o wide
      ```

      Newrelic 모니터링 인스턴스가 없는 노드에서 실행 중인 모니터링하려는 컨트롤플레인 포드가 있는 경우 필요에 따라 helm [차트 값](https://github.com/newrelic/nri-kubernetes/blob/main/charts/newrelic-infrastructure/values.yaml) 의 `controlplane.affinity` , `controlplane.nodeSelector` 및 `controlplane.tolerations` 를 변경할 수 있습니다.
  </Collapser>

  <Collapser
    id="control-plane-CrashLoopBackOff"
    title="ControlPlane 구성 요소가 CrashLoopBackOff에 있습니다."
  >
    컨트롤 플레인 구성 요소가 CrashLoopBackOff에 입력되는 컨트롤 플레인 포드를 자동으로 검색하거나 긁지 않는 경우.

    이전 섹션에서 설명한 대로 필요에 맞게 자동 검색 및 인증 방법의 동작을 변경할 수 있습니다.

    반면에 해당 데이터에 관심이 없으면 helm [차트 값](https://github.com/newrelic/nri-kubernetes/blob/main/charts/newrelic-infrastructure/values.yaml) 에서 `controlplane.enabled=false` 을 설정하여 제어 평면 구성 요소를 비활성화할 수 있습니다.
  </Collapser>
</CollapserGroup>

## 통합 버전 V2용 솔루션

<CollapserGroup>
  <Collapser
    id="invalid-license"
    title="마스터 노드에 올바른 레이블이 있는지 확인하십시오."
  >
    다음 명령을 실행하여 마스터 노드를 수동으로 찾습니다.

    ```
    kubectl get nodes -l node-role.kubernetes.io/master=""
    ```

    ```
    kubectl get nodes -l kubernetes.io/role="master"
    ```

    마스터 노드 [가 마스터 노드 및 제어 평면 구성 요소 문서 섹션의 검색에](/docs/integrations/kubernetes-integration/installation/configure-control-plane-monitoring#discover-nodes-components) 정의된 레이블 지정 규칙을 따르는 경우 다음과 같은 출력을 얻어야 합니다.

    ```
    NAME                         STATUS  ROLES   AGE   VERSION
    ip-10-42-24-4.ec2.internal   Ready   master  42d   v1.14.8
    ```

    노드를 찾을 수 없는 경우 두 가지 시나리오가 있습니다.

    마스터 노드에는 마스터 노드를 식별하는 필수 레이블이 없습니다. 이 경우 마스터 노드에 두 레이블을 모두 추가해야 합니다.

    당신은 관리 클러스터에 있고 당신의 공급자가 당신을 위해 마스터 노드를 처리하고 있습니다. 이 경우 공급자가 해당 노드에 대한 액세스를 제한하기 때문에 할 수 있는 일이 없습니다.
  </Collapser>

  <Collapser
    id="unable-connect"
    title="마스터 노드에서 통합이 실행 중인지 확인하십시오."
  >
    다음 명령의 자리 표시자를 이전 단계에서 반환된 노드 이름 중 하나로 교체하여 마스터 노드에서 실행 중인 통합 포드를 가져옵니다.

    ```
    kubectl get pods --field-selector spec.nodeName=NODE_NAME -l name=newrelic-infra --all-namespaces
    ```

    다음 명령은 노드를 선택한다는 점만 다를 뿐 동일합니다.

    ```
    kubectl get pods --field-selector spec.nodeName=$(kubectl get nodes -l node-role.kubernetes.io/master="" -o jsonpath="{.items[0].metadata.name}") -l name=newrelic-infra --all-namespaces
    ```

    모든 것이 정확하면 다음과 같은 출력이 표시되어야 합니다.

    ```
    NAME                   READY   STATUS    RESTARTS   AGE
    newrelic-infra-whvzt   1/1     Running   0          6d20h
    ```

    마스터 노드에서 통합이 실행되고 있지 않으면 데몬세트에 실행 중인 원하는 모든 인스턴스가 있고 준비가 되어 있는지 확인하십시오.

    ```
    kubectl get daemonsets -l app=newrelic-infra --all-namespaces
    ```
  </Collapser>

  <Collapser
    id="indicators"
    title="제어 평면 구성 요소에 필요한 레이블이 있는지 확인하십시오."
  >
    [마스터 노드 및 컨트롤 플레인 구성 요소 문서 섹션의 검색](/docs/integrations/kubernetes-integration/installation/configure-control-plane-monitoring#discover-nodes-components) 을 참조하고 통합에서 구성 요소를 검색하는 데 사용하는 레이블을 찾습니다. 그런 다음 다음 명령을 실행하여 이러한 레이블이 있는 포드와 실행 중인 노드가 있는지 확인합니다.

    ```
    kubectl get pods -l k8s-app=kube-apiserver --all-namespaces
    ```

    지정된 레이블이 있는 구성 요소가 있는 경우 다음과 같이 표시되어야 합니다.

    ```
    NAMESPACE    NAME                                        READY  STATUS   RESTARTS  AGE
    kube-system  kube-apiserver-ip-10-42-24-42.ec2.internal  1/1    Running  3         49d
    ```

    나머지 구성 요소에 대해서도 동일한 작업을 수행해야 합니다.

    ```
    kubectl get pods -l k8s-app=etcd-manager-main --all-namespaces
    ```

    ```
    kubectl get pods -l k8s-app=kube-scheduler --all-namespaces
    ```

    ```
    kubectl get pods -l k8s-app=kube-kube-controller-manager --all-namespaces
    ```
  </Collapser>

  <Collapser
    id="cannot-list-pods-for-cluster"
    title="마스터 노드에서 실행 중인 통합 중 하나의 자세한 로그를 검색하고 제어 평면 구성 요소 작업을 확인합니다."
  >
    로그를 검색하려면 [마스터 노드에서 실행 중인 팟에서 로그 가져오기](/docs/integrations/kubernetes-integration/troubleshooting/get-logs-version) 에 대한 지침을 따르십시오. 통합은 모든 구성요소에 대해 _"실행 중인 작업: COMPONENT_NAME"_ 메시지를 기록합니다. 전:

    ```
    Running job: scheduler
    ```

    ```
    Running job: etcd
    ```

    ```
    Running job: controller-manager
    ```

    ```
    Running job: api-server
    ```

    `ETCD_TLS_SECRET_NAME` 구성 옵션을 지정하지 않은 경우 로그에 다음 메시지가 표시됩니다.

    ```
    Skipping job creation for component etcd: etcd requires TLS configuration, none given
    ```

    구성 요소의 측정항목을 쿼리하는 동안 오류가 발생하면 `Running job` 메시지 다음에 기록됩니다.
  </Collapser>

  <Collapser
    id="cannot-list-pods-for-cluster"
    title="구성 요소의 메트릭을 수동으로 쿼리"
  >
    쿼리하려는 제어 평면 구성 요소의 끝점을 가져오려면 [마스터 노드 및 제어 평면 구성 요소 문서 섹션의 검색을](/docs/integrations/kubernetes-integration/installation/configure-control-plane-monitoring#discover-nodes-components) 참조하십시오. 엔드포인트를 사용하면 쿼리할 구성 요소와 동일한 노드에서 실행 중인 통합 포드를 사용할 수 있습니다. 다음은 Kubernetes 스케줄러를 쿼리하는 방법에 대한 예입니다.

    ```
    kubectl exec -ti POD_NAME -- wget -O - localhost:10251/metrics
    ```

    다음 명령은 동일한 작업을 수행하지만 포드도 선택합니다.

    ```
    kubectl exec -ti $(kubectl get pods --all-namespaces --field-selector spec.nodeName=$(kubectl get nodes -l node-role.kubernetes.io/master="" -o jsonpath="{.items[0].metadata.name}") -l name=newrelic-infra -o jsonpath="{.items[0].metadata.name}") -- wget -O - localhost:10251/metrics
    ```

    모든 것이 정확하면 다음과 같은 Prometheus 형식에 대한 몇 가지 메트릭을 가져와야 합니다.

    ```
    Connecting to localhost:10251 (127.0.0.1:10251)
    # HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.
    # TYPE apiserver_audit_event_total counter
    apiserver_audit_event_total 0
    # HELP apiserver_audit_requests_rejected_total Counter of apiserver requests rejected due to an error in audit logging backend.
    # TYPE apiserver_audit_requests_rejected_total counter
    apiserver_audit_requests_rejected_total 0
    # HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.
    # TYPE apiserver_client_certificate_expiration_seconds histogram
    apiserver_client_certificate_expiration_seconds_bucket{le="0"} 0
    apiserver_client_certificate_expiration_seconds_bucket{le="1800"} 0
    apiserver_client_certificate_expiration_seconds_bucket{le="3600"} 0
    ```
  </Collapser>
</CollapserGroup>