---
title: Kubernetes 통합 오류 v2
type: troubleshooting
tags:
  - Integrations
  - Kubernetes integration
  - Troubleshooting
metaDescription: Some of the more common error messages found in the infrastructure agent logs for New Relic Kubernetes integration v2.
freshnessValidatedDate: '2024-07-22T00:00:00.000Z'
translationType: machine
---

버전 2를 실행 중인 경우 일반적인 Kubernetes 통합 오류를 확인하세요. 이러한 오류는 상세하지 않은 표준 인프라 에이전트 로그에 표시됩니다. 더 자세한 로그가 필요한 경우 [Kubernetes 로그 를](/docs/kubernetes-pixie/kubernetes-integration/troubleshooting/get-logs-version/) 참조하세요.

<CollapserGroup>
  <Collapser
    id="unable-find-kube-state-metrics-v2"
    title="kube-state-metrics 검색 실패"
  >
    Kubernetes 통합에는 [`kube-state-metrics`](https://github.com/kubernetes/kube-state-metrics) 필요합니다. 이것이 누락된 경우 `newrelic-infra` 컨테이너 로그에 다음과 같은 오류가 표시됩니다.

    ```shell
    2018-04-11T08:02:41.765236022Z time="2018-04-11T08:02:41Z" level=error 
    msg="executing data source" data prefix="integration/com.newrelic.kubernetes" 
    error="exit status 1" plugin name=nri-kubernetes stderr="time=\"2018-04-11T08:02:41Z\" 
    level=fatal msg=\"failed to discover kube-state-metrics endpoint, 
    got error: no service found by label k8s-app=kube-state-metrics\"\n"
    ```

    이 오류의 일반적인 이유는 다음과 같습니다.

    * `kube-state-metrics` 클러스터에 배포되지 않았습니다.
    * `kube-state-metrics` 사용자 지정 배포를 사용하여 배포됩니다.
    * 실행 중인 `kube-state-metrics` 의 여러 버전이 있으며 Kubernetes 통합이 올바른 버전을 찾지 못합니다.

    Kubernetes 통합은 다음 로직을 사용하여 클러스터에서 `kube-state-metrics` 을 자동으로 검색합니다.

    1. `kube-system` 네임스페이스에서 실행 중인 `kube-state-metrics` 서비스를 찾습니다.
    2. 찾을 수 없는 경우 레이블이 `"k8s-app: kube-state-metrics"` 인 서비스를 찾습니다.

    또한 통합을 위해서는 kube-state-metrics 포드에 `k8s-app: kube-state-metrics` 또는 `app: kube-state-metrics` 레이블이 있어야 합니다. 둘 중 어느 것도 찾지 못하면 다음과 같은 로그 항목이 표시됩니다.

    ```shell
    2018-04-11T09:25:00.825532798Z time="2018-04-11T09:25:00Z" level=error 
    msg="executing data source" data prefix="integration/com.newrelic.kubernetes" 
    error="exit status 1" plugin name=nri-kubernetes stderr="time=\"2018-04-11T09:25:00Z\" 
    level=fatal msg=\"failed to discover nodeIP with kube-state-metrics, 
    got error: no pod found by label k8s-app=kube-state-metrics\"\n
    ```

    이 문제를 해결하려면 `k8s-app=kube-state-metrics` 레이블을 `kube-state-metrics` 포드에 추가하세요.
  </Collapser>

  <Collapser
    id="metrics-missing"
    title="네임스페이스, 배포 및 ReplicaSet에 대한 측정항목 누락"
  >
    Kubernetes 노드, 파드 및 컨테이너에 대한 지표가 표시되지만 지우스페이스, 배포 및 `ReplicaSets` 에 대한 지표가 누락된 경우 Kubernetes 통합은 `kube-state-metrics`에 연결할 수 없습니다.

    <CollapserGroup>
      <Collapser
        id="indicators"
        title="누락된 데이터 표시기"
      >
        누락된 네임스페이스, 배포 및 `ReplicaSet` 데이터 표시기:

        * <DNT>
            **# of K8s objects**
          </DNT>

          차트에서 해당 데이터가 누락되었습니다.

        * `K8sNamespaceSample` , `K8sDeploymentSample` 및 `K8sReplicasetSample` 에 대한 쿼리는 데이터를 표시하지 않습니다.
      </Collapser>
    </CollapserGroup>

    여기에는 몇 가지 가능한 이유가 있습니다.

    1. `kube-state-metrics` 서비스는 포트 80에서 수신 대기하도록 사용자 정의되었습니다. 다른 포트를 사용하는 경우 `verbose` 로그에 다음과 같은 오류가 표시될 수 있습니다.

       ```shell
       time="2018-04-04T09:35:47Z" level=error msg="executing data source" 
       data prefix="integration/com.newrelic.kubernetes" error="exit status 1" 
       plugin name=nri-kubernetes stderr="time=\"2018-04-04T09:35:47Z\" 
       level=fatal msg=\"Non-recoverable error group: error querying KSM. 
       Get http://kube-state-metrics.kube-system.svc.cluster.local:0/metrics: 
       net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"\n"
       ```

       이는 `kube-state-metrics` 에서 통합으로 보내기 전에 모든 클러스터 정보를 수집하는 데 너무 많은 시간이 걸리는 일부 클러스터에서 발생하는 알려진 문제입니다.

       해결 방법 [으로 kube-state-metrics 클라이언트 시간 초과를 늘리십시오](/docs/integrations/kubernetes-integration/installation/kubernetes-installation-configuration#kube-state-metrics-timeout-change) .

    2. `kube-state-metrics` 인스턴스가 `kube-rbac-proxy` 뒤에서 실행 중입니다. 뉴렐릭은 현재 이 설정을 지원하지 않습니다. `verbose` 로그에 다음과 같은 오류가 표시될 수 있습니다.

       ```shell
       time="2018-03-28T23:09:12Z" level=error msg="executing data source" 
       data prefix="integration/com.newrelic.kubernetes" error="exit status 1" 
       plugin name=nri-kubernetes stderr="time=\"2018-03-28T23:09:12Z\" 
       level=fatal msg=\"Non-recoverable error group: error querying KSM. 
       Get http://192.168.132.37:8443/metrics: net/http: HTTP/1.x 
       transport connection broken: malformed HTTP response \\\"\\\\x15\\\\x03\\\\x01\\\\x00\\\\x02\\\\x02\\\"\"\n"
       ```

    3. KSM 페이로드는 상당히 크고 날짜를 처리하는 Kubernetes 통합은 OOM이 종료됩니다. 통합은 컨테이너의 주요 프로세스가 아니므로 포드가 다시 시작되지 않습니다. 이 상황은 KSM의 동일한 노드에서 실행 중인 `newrelic-infra` 포드의 로그에서 확인할 수 있습니다.

       ```shell
       time="2020-12-10T17:40:44Z" level=error msg="Integration command failed" error="signal: killed" instance=nri-kubernetes integration=com.newrelic.kubernetes
       ```

       이 문제를 해결하려면 프로세스가 종료되지 않도록 DaemonSet 메모리 제한을 늘리십시오.
  </Collapser>

  <Collapser
    id="cannot-list-pods-for-cluster"
    title="클러스터 범위에서 포드를 나열할 수 없습니다."
  >
    Newrelic 포드 및 newrelic 서비스 계정은 동일한 네임스페이스에 배포되지 않습니다. 이는 일반적으로 현재 컨텍스트가 네임스페이스를 지정하기 때문입니다. 이 경우 다음과 같은 오류가 표시됩니다.

    ```shell
    time=\"2018-05-31T10:55:39Z\" level=panic msg=\"p
    ods is forbidden: User \\\"system:serviceaccount:kube-system:newrelic\\\" cannot list pods at the cluster scope\"
    ```

    이 경우인지 확인하려면 다음을 실행하십시오.

    ```shell
    kubectl describe serviceaccount newrelic | grep Namespace
    kubectl get pods -l name=newrelic-infra --all-namespaces
    kubectl config get-contexts
    ```

    이 문제를 해결하려면 New Relic `DaemonSet` YAML 파일에서 서비스 계정의 네임스페이스를 현재 컨텍스트의 네임스페이스와 동일하게 변경합니다.

    ```yaml
    - kind: ServiceAccount
      name: newrelic
      namespace: default
    ---
    ```
  </Collapser>
</CollapserGroup>

## 제어 영역 데이터가 표시되지 않음 [#control-plane-data]

<CollapserGroup>
  <Collapser
    id="invalid-license"
    title="마스터 노드에 올바른 레이블이 있는지 확인하십시오."
  >
    다음 명령을 실행하여 마스터 노드를 수동으로 찾습니다.

    ```shell
    kubectl get nodes -l node-role.kubernetes.io/master=""
    ```

    ```shell
    kubectl get nodes -l kubernetes.io/role="master"
    ```

    마스터 노드가 [제어 플레인 구성 요소](/docs/kubernetes-pixie/kubernetes-integration/advanced-configuration/configure-control-plane-monitoring/#component) 에 정의된 라벨 지정 규칙을 따르는 경우 다음과 같은 출력이 표시됩니다.

    ```shell
    NAME                         STATUS  ROLES   AGE   VERSION
    ip-10-42-24-4.ec2.internal   Ready   master  42d   v1.14.8
    ```

    노드를 찾을 수 없는 경우 두 가지 시나리오가 있습니다.

    마스터 노드에는 마스터로 식별하는 필수 라벨이 없습니다. 이 경우 마스터 노드에 두 레이블을 모두 추가해야 합니다.

    귀하는 관리형 클러스터에 있고 공급자가 귀하를 위해 마스터 노드를 처리하고 있습니다. 이 경우 공급자가 해당 노드에 대한 액세스를 제한하므로 귀하가 할 수 있는 일은 없습니다.
  </Collapser>

  <Collapser
    id="unable-connect"
    title="마스터 노드에서 통합이 실행 중인지 확인하십시오."
  >
    마스터 노드에서 실행 중인 통합 파드를 식별하려면 다음 명령의 `NODE_NAME` 이전 단계에 나열된 노드 이름 중 하나로 바꿉니다.

    ```shell
    kubectl get pods --field-selector spec.nodeName=NODE_NAME -l name=newrelic-infra --all-namespaces
    ```

    다음 명령은 이전 명령과 동일하지만 노드를 선택한다는 점만 다릅니다.

    ```shell
    kubectl get pods --field-selector spec.nodeName=$(kubectl get nodes -l node-role.kubernetes.io/master="" -o jsonpath="{.items[0].metadata.name}") -l name=newrelic-infra --all-namespaces
    ```

    모든 것이 정확하면 다음과 같은 출력이 표시되어야 합니다.

    ```shell
    NAME                   READY   STATUS    RESTARTS   AGE
    newrelic-infra-whvzt   1/1     Running   0          6d20h
    ```

    마스터 노드에서 통합이 실행되고 있지 않으면 데몬세트에 실행 중인 원하는 모든 인스턴스가 있고 준비가 되어 있는지 확인하십시오.

    ```shell
    kubectl get daemonsets -l app=newrelic-infra --all-namespaces
    ```
  </Collapser>

  <Collapser
    id="indicators"
    title="제어 평면 구성 요소에 필요한 레이블이 있는지 확인하십시오."
  >
    [마스터 노드 및 컨트롤 플레인 구성 요소 문서 섹션의 검색](/docs/integrations/kubernetes-integration/installation/configure-control-plane-monitoring#discover-nodes-components) 을 참조하고 통합에서 구성 요소를 검색하는 데 사용하는 레이블을 찾습니다. 그런 다음 다음 명령을 실행하여 이러한 레이블이 있는 포드와 실행 중인 노드가 있는지 확인합니다.

    ```shell
    kubectl get pods -l k8s-app=kube-apiserver --all-namespaces
    ```

    지정된 레이블이 있는 구성 요소가 있는 경우 다음과 같이 표시되어야 합니다.

    ```shell
    NAMESPACE    NAME                                        READY  STATUS   RESTARTS  AGE
    kube-system  kube-apiserver-ip-10-42-24-42.ec2.internal  1/1    Running  3         49d
    ```

    나머지 구성 요소에 대해서도 동일한 작업을 수행해야 합니다.

    ```shell
    kubectl get pods -l k8s-app=etcd-manager-main --all-namespaces
    ```

    ```shell
    kubectl get pods -l k8s-app=kube-scheduler --all-namespaces
    ```

    ```shell
    kubectl get pods -l k8s-app=kube-kube-controller-manager --all-namespaces
    ```
  </Collapser>

  <Collapser
    id="cannot-list-pods-for-cluster"
    title="마스터 노드에서 실행 중인 통합 중 하나의 자세한 로그를 검색하고 제어 평면 구성 요소 작업을 확인합니다."
  >
    로그인을 검색하려면 [마스터 노드에서 실행 중인 패드에서 로그인 가져오기](/docs/integrations/kubernetes-integration/troubleshooting/get-logs-version) 지침을 따르세요. 모든 구성 요소에 대한 통합 로그에는 다음 메시지 `Running job: COMPONENT_NAME` 가 기록됩니다. 예를 들어:

    ```shell
    Running job: scheduler
    ```

    ```shell
    Running job: etcd
    ```

    ```shell
    Running job: controller-manager
    ```

    ```shell
    Running job: api-server
    ```

    `ETCD_TLS_SECRET_NAME` 설정 옵션을 지정하지 않은 경우 로그에서 다음 메시지를 찾을 수 있습니다.

    ```shell
    Skipping job creation for component etcd: etcd requires TLS configuration, none given
    ```

    구성 요소의 지표를 쿼리하는 동안 오류가 발생하면 `Running job` 메시지 다음에 기록됩니다.
  </Collapser>

  <Collapser
    id="cannot-list-pods-for-cluster"
    title="구성 요소의 메트릭을 수동으로 쿼리"
  >
    쿼리하려는 제어 플레인 구성요소의 엔드포인트를 얻으려면 [제어 플레인 구성요소를](/docs/kubernetes-pixie/kubernetes-integration/advanced-configuration/configure-control-plane-monitoring/#component) 참조하세요. 엔드포인트를 사용하면 동일한 노드에서 실행되는 통합 파드를 컴포넌트와 함께 쿼리할 수 있습니다. Kubernetes 스케줄러를 쿼리하는 방법에 대한 다음 예를 참조하세요.

    ```shell
    kubectl exec -ti POD_NAME -- wget -O - localhost:10251/metrics
    ```

    다음 명령은 이전 명령과 동일하지만 파드도 선택합니다.

    ```shell
    kubectl exec -ti $(kubectl get pods --all-namespaces --field-selector spec.nodeName=$(kubectl get nodes -l node-role.kubernetes.io/master="" -o jsonpath="{.items[0].metadata.name}") -l name=newrelic-infra -o jsonpath="{.items[0].metadata.name}") -- wget -O - localhost:10251/metrics
    ```

    모든 것이 정확하다면 다음과 같은 Prometheus 형식의 지표를 얻어야 합니다.

    ```shell
    Connecting to localhost:10251 (127.0.0.1:10251)
    # HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.
    # TYPE apiserver_audit_event_total counter
    apiserver_audit_event_total 0
    # HELP apiserver_audit_requests_rejected_total Counter of apiserver requests rejected due to an error in audit logging backend.
    # TYPE apiserver_audit_requests_rejected_total counter
    apiserver_audit_requests_rejected_total 0
    # HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.
    # TYPE apiserver_client_certificate_expiration_seconds histogram
    apiserver_client_certificate_expiration_seconds_bucket{le="0"} 0
    apiserver_client_certificate_expiration_seconds_bucket{le="1800"} 0
    apiserver_client_certificate_expiration_seconds_bucket{le="3600"} 0
    ```
  </Collapser>
</CollapserGroup>
