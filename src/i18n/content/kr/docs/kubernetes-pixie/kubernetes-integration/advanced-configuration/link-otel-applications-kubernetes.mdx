---
title: OpenTelemetry 계측 애플리케이션을 Kubernetes에 연결
tags:
  - Integrations
  - Kubernetes integration
  - OpenTelemetry
  - Link apps and services
metaDescription: Connect New Relic's Kubernetes monitoring with applications monitored with OpenTelemetry.
freshnessValidatedDate: never
translationType: machine
---

Kubernetes 에서 개발자 기능을 실행하는 경우 뉴렐릭을 사용하여 Kubernetes OpenTelemetry 제공하는 데 어떤 영향을 미치는지 이해할 수 있습니다.

아래 단계를 완료한 후 뉴렐릭 UI 사용하여 OpenTelemetry 의 작곡 수준 지표를 Kubernetes 통합 지표와 상호 연관시킬 수 있습니다. 이를 통해 텔레메트리 데이터의 전체 환경을 확인하고 팀 간 협업을MTTR Kubernetes 통해 환경의 문제에 대한 평균 해결 시간(MTTR)( )을 더 빠르게 달성할 수 있습니다.

## 데이터를 상호 연관시키는 방법 [#correlate]

이 문서에서는 텔넷리 데이터에 고정 관련 데이터를 삽입할 수 있도록 설정하는 과정을 안내합니다. 그 결과 뉴렐릭 UI 실행 가능한 정보로 채워집니다. 시작하기 위해 취해야 할 단계는 다음과 같습니다.

* 각 컨테이너 컨테이너에서 텔레메트리 데이터를 수집기로 보내기 위한 환경 변수를 정의합니다.
* 구현하다, 배포하다 관련 데이터베이스(클러스터, 배포 및 지우스페이스 이름)를 OpenTelemetry 주입하기 `DaemonSet` [](https://opentelemetry.io/docs/collector/getting-started/#agent)위해 `resourcedetection`, `resource`, `batch` 및 프로세서가 포함된 에이전트 모드 에서 로 수집기를 배포합니다.`k8sattributes`

## 시작하기 전에 [#prereqs]

아래 단계를 성공적으로 완료하려면 OpenTelemetry 및 Kubernetes에 대해 이미 잘 알고 있어야 하며 다음을 수행해야 합니다.

* 다음 환경 변수를 만들었습니다.

  * `OTEL_EXPORTER_OTLP_ENDPOINT`: 자세한 내용은 [뉴렐릭 OTLP 엔드포인트를](/docs/opentelemetry/best-practices/opentelemetry-otlp/) 참조하세요.
  * `NEW_RELIC_API_KEY`: 자세한 내용은 [뉴렐릭 API 키를](/docs/apis/intro-apis/new-relic-api-keys/) 참조하세요.

* 클러스터에 [뉴렐릭 Kubernetes 통합을](/install/kubernetes) 설치했습니다.

* [OpenTelemetry](/docs/more-integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-setup/) 사용하여 로그를 작성했으며 OTLP( OpenTelemetry Protocol)를 통해 뉴렐릭에 데이터를 성공적으로 보냈습니다.

New Relic과 함께 수집기를 사용하는 방법에 대한 일반적인 질문이 있는 경우 New Relic [과 함께 OpenTelemetry Collector 소개를](/docs/more-integrations/open-source-telemetry-integrations/opentelemetry/collector/opentelemetry-collector-intro) 참조하세요.

## 원격 분석 데이터를 OpenTelemetry Collector로 보내도록 애플리케이션 구성 [#instrument]

이를 설정하려면 Kubernetes YAML 파일의 `env` 섹션에 사용자 정의 스니펫을 추가해야 합니다. 아래 예는 샘플 프런트엔드 마이크로서비스(`Frontend.yaml`)에 대한 스니펫을 보여줍니다. 스니펫에는 다음을 수행하는 2개의 섹션이 포함되어 있습니다.

* <DNT>**Section 1:**</DNT> 텔레메트리 데이터가 수집기로 전송되는지 확인하세요. 그러면 호스트 IP로 환경 변수 `OTEL_EXPORTER_OTLP_ENDPOINT` 가 설정됩니다. 호스트 IP를 가져오기 위해 하향 API를 호출하여 이를 수행합니다.
* <DNT>**Section 2:**</DNT> 인프라별 메타데이터를 첨부합니다. 이를 위해 하향 API를 사용하여 `metadata.uid` 캡처하고 이를 `OTEL_RESOURCE_ATTRIBUTES` 환경 변수에 추가합니다. 이 환경 변수는 OpenTelemetry 수집기의 `resourcedetection` 및 `k8sattributes` 프로세서에서 망원경 관련 컨텍스트를 추가로 추가하는 데 사용됩니다.

OpenTelemetry로 계측된 각 마이크로서비스에 대해 아래 강조표시된 줄을 매니페스트의 `env` 섹션에 추가하세요.

```yaml
# Frontend.yaml
apiVersion: apps/v1
kind: Deployment

# ...
spec:
  containers:
    - name: yourfrontendservice
      image: yourfrontendservice-beta
  env:
    # Section 1: Ensure that telemetry data is sent to the collector
    - name: HOST_IP
      valueFrom:
        fieldRef:
          fieldPath: status.hostIP
    
    # This is picked up by the opentelemetry sdks
    - name: OTEL_EXPORTER_OTLP_ENDPOINT
      value: "http://$(HOST_IP):55680"
    
    # Section 2: Attach infrastructure-specific metadata
    # Get pod ip so that k8sattributes can tag resources
    - name: POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    
    - name: POD_UID
      valueFrom:
        fieldRef:
          fieldPath: metadata.uid
    
    # This is picked up by the resource detector
    - name: OTEL_RESOURCE_ATTRIBUTES
      value: "service.instance.id=$(POD_NAME),k8s.pod.uid=$(POD_UID)"
```

## OpenTelemetry 수집기를 에이전트로 구성 및 배포 [#agent]

Kubernetes 클러스터 내의 모든 노드에 [에이전트로 수집기를](https://opentelemetry.io/docs/collector/getting-started/#agent) 배포하는 것이 좋습니다. 에이전트는 원격 측정 데이터를 수신하고 메타데이터로 원격 측정 데이터를 보강할 수 있습니다. 예를 들어 수집기는 프로세서를 통해 사용자 정의 특성이나 인프라 정보를 추가할 수 있을 뿐만 아니라 일괄 처리, 재시도, 압축 및 클라이언트 계측 수준에서 덜 효율적으로 처리되는 추가 고급 기능을 처리할 수 있습니다.

수집기 구성에 대한 도움말은 아래의 샘플 수집기 구성 파일과 이러한 옵션 설정에 대한 섹션을 참조하세요.

* [OTLP 수출업체](#otlp-exporter)
* [배치 프로세서](#batch)
* [리소스 감지 프로세서](#resource-detection)
* [k8속성 프로세서: 일반](#attributes-general)
* [k8속성 프로세서: RBAC](#rbac)
* [k8속성 프로세서: 필터](#discovery-filter)

```yaml
receivers:
  otlp:
    protocols:
      grpc:

processors:
  batch:
  resource:
    attributes:
      - key: host.id
        from_attribute: host.name
        action: upsert
  resourcedetection:
    detectors: [gke, gce]
  k8sattributes:
    auth_type: "serviceAccount"
    passthrough: false
    filter:
      node_from_env_var: KUBE_NODE_NAME
    extract:
      metadata:
        - k8s.pod.name
        - k8s.pod.uid
        - k8s.deployment.name
        - k8s.cluster.name
        - k8s.namespace.name
        - k8s.node.name
        - k8s.pod.start_time
    pod_association:
      - from: resource_attribute
        name: k8s.pod.uid

exporters:
  otlp:
    endpoint: $OTEL_EXPORTER_OTLP_ENDPOINT
    headers:
      api-key: $NEW_RELIC_API_KEY
  logging:
    logLevel: DEBUG

service:
  pipelines:
    metrics:
      receivers: [otlp]
      processors: [resourcedetection, k8sattributes, resource, cumulativetodelta, batch]
      exporters: [otlp]
    traces:
      receivers: [otlp]
      processors: [resourcedetection, k8sattributes, resource, batch]
      exporters: [otlp]
    logs:
      receivers: [otlp]
      processors: [resourcedetection, k8sattributes, resource, batch]
      exporters: [otlp]
```

OpenTelemetry 수집기를 에이전트로 구성하고 배포하려면 다음 단계를 따르세요.

<Steps>
  <Step>
    ### OTLP 내보내기 구성 [#otlp-exporter]

    먼저, 뉴렐릭 <InlinePopover type="licenseKey"/>헤더와 함께 OTLP 내보내기를 [OpenTelemetry 수집기 설정 YAML 파일](https://opentelemetry.io/docs/collector/configuration/) 에 추가하세요.

    ```yaml
    exporters:
    otlp:
        endpoint: $OTEL_EXPORTER_OTLP_ENDPOINT
        headers: api-key: $NEW_RELIC_API_KEY
    ```
  </Step>

  <Step>
    ### 배치 프로세서 구성 [#batch]

    배치 프로세서는 범위, 지표 또는 로그를 허용하고 이를 배치로 배치합니다. 이렇게 하면 데이터를 더 쉽게 압축하고 수집기의 아웃바운드 요청을 줄일 수 있습니다.

    ```yaml
    processors:
    batch:
    ```
  </Step>

  <Step>
    ### 리소스 감지 프로세서 구성 [#resource-detection]

    `resourcedetection` 프로세서는 수집기를 통해 처리되는 원격 분석 데이터에 컨텍스트를 추가하기 위해 호스트별 정보를 가져옵니다. 이 예시에서는 Google Kubernetes Engine(GKE) 및 Google Compute Engine(GCE)을 사용하여 다음을 포함한 Google Cloud 관련 메타데이터를 가져옵니다.

    * `cloud.provider` ("gcp")

    * `cloud.platform` ("`gcp_compute_engine`")

    * `cloud.account.id`

    * `cloud.region`

    * `cloud.availability_zone`

    * `host.id`

    * `host.image.id`

    * `host.type`

      ```yaml
      processors:
      resourcedetection:
          detectors: [gke, gce]
      ```
  </Step>

  <Step>
    ### Kubernetes 속성 프로세서 구성(일반) [#attributes-general]

    에이전트로 실행되는 OpenTelemetry Collector의 일부로 `k8sattributes` 프로세서를 실행하면 OpenTelemetry Collector 에이전트에 원격 측정 데이터를 보내는 Pod의 IP 주소를 감지하고 이를 사용하여 Pod 메타데이터를 추출합니다. 다음은 프로세서 섹션만 있는 기본 Kubernetes 매니페스트 예시입니다. OpenTelemetry Collector를 `DaemonSet` 로 배포하려면 이 [포괄적인 매니페스트 예제를](https://github.com/newrelic-forks/microservices-demo/tree/main/src/otel-collector-agent) 읽어보세요.

    ```yaml
    processors:
    k8sattributes:
        auth_type: "serviceAccount"
        passthrough: false
        filter:
        node_from_env_var: KUBE_NODE_NAME
        extract:
        metadata:
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.deployment.name
            - k8s.cluster.name
            - k8s.namespace.name
            - k8s.node.name
            - k8s.pod.start_time
        pod_association:
        - from: resource_attribute
            name: k8s.pod.uid
    ```
  </Step>

  <Step>
    ### Kubernetes 속성 프로세서(RBAC) 구성 [#rbac]

    역할 기반 액세스 제어(RBAC)에 대한 설정을 추가해야 합니다. `k8sattributes` 프로세서에는 구성된 필터에 포함된 파드 및 지우스페이스 리소스에 대한 `get`, `watch` 및 `list` 권한이 필요합니다. 이 [예에서는](https://github.com/newrelic-forks/microservices-demo/blob/main/otel-kubernetes-manifests/otel-collector-agent.yaml#L43-L69) 클러스터에 있는 모든 파드 및 지우스페이스에 대해 필요한 권한을 `ServiceAccount` 에 부여하기 위해 `ClusterRole` 에 대한 역할 기반 액세스 제어(RBAC)를 구성하는 방법을 보여줍니다.
  </Step>

  <Step>
    ### Kubernetes 속성 프로세서 구성(검색 필터) [#discovery-filter]

    에이전트로 수집기를 실행할 때 프로세서가 실행 중인 동일한 호스트에서만 파드를 검색하도록 검색 필터를 적용해야 합니다. 필터를 사용하지 않으면 특히 대규모 클러스터에서 리소스 사용량이 불필요하게 높아질 수 있습니다. 필터가 적용되면 각 프로세서는 자체 노드에서 실행되는 Pad용 Kubernetes API 만 쿼리합니다.

    필터를 설정하려면 하향 API 사용하여 OpenTelemetry 수집기 에이전트 설정 YAML 파일의 파드 `env` 섹션에 환경 변수로 노드 이름을 삽입합니다. 예제를 보려면 [GitHub를](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/examples/kubernetes/otel-collector-config.yml) 확인하세요. 그러면 OpenTelemetry 수집기 에이전트의 컨테이너에 새 환경 변수가 주입됩니다. 값은 파드가 실행되도록 예약된 노드의 이름이 됩니다.

    ```yaml
    spec:
    containers:
        - env:
            - name: KUBE_NODE_NAME
            valueFrom:
                fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
    ```

    그런 다음 `k8sattributes` 을 사용하여 노드별로 필터링할 수 있습니다.

    ```yaml
    k8sattributes:
    filter:
        node_from_env_var: KUBE_NODE_NAME
    ```
  </Step>
</Steps>

## 구성이 작동하는지 확인 [#validate]

OpenTelemetry 데이터를 Kubernetes 데이터와 성공적으로 연결한 후에는 설정이 작동하는지 확인할 수 있어야 합니다.

1. <DNT>**[one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > APM & Services**</DNT> 으로 이동하여 <DNT>**Services - OpenTelemetry**</DNT> 내에서 애플리케이션을 선택합니다.
2. **Monitor** \[모니터] 섹션에서 **Service map** \[서비스 맵을] 클릭합니다.
3. OpenTelemetry 서비스와 클러스터 간의 연결을 확인하세요. 오른쪽에는 `k8s.cluster.name`, `k8s.namespace.name` 및 `k8s.deployment.name` 와 같은 Kubernetes 속성이 태그로 표시됩니다.

<img
  src="/images/kubernetes_screenshot-crop_otel-validation.webp"
  title="Screenshot showing K8s metadata in the New Relic UI"
  alt="Screenshot showing K8s metadata in the New Relic UI"
/>

## 다음 단계 선택 [#next]

<DocTiles>
  <DocTile
    title="See our best practices guide"
    path="/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-concepts"
  >
    OpenTelemetry 및 뉴렐릭의 사용을 개선하는 방법을 알아보세요.
  </DocTile>

  <DocTile
    title="Check out this blog post"
    path="https://newrelic.com/blog/how-to-relic/k8s-with-otel"
  >
    OpenTelemetry 트레이스, 메트릭 및 복제를 Kubernetes 성능 데이터와 연관시키세요.
  </DocTile>
</DocTiles>
