---
title: Parsing log data
tags:
  - Logs
  - Log management
  - UI and data
metaDescription: How New Relic uses parsing and how to send customized log data.
freshnessValidatedDate: never
translationType: human
---

로그 <DNT>parsing</DNT>은 정의한 규칙에 따라 정형화되지 않은 로그 데이터를 속성(키:값 쌍)으로 변환하는 프로세스입니다. NRQL 쿼리에서 이 속성을 사용하면 편리하게 로그를 패싯하거나 필터링할 수 있습니다.

뉴렐릭은 특정 구문 분석 규칙에 따라 로그 데이터를 자동으로 구문 분석합니다. 이 문서에서는 로그 구문 분석이 작동하는 방식과 커스텀 구문 분석 규칙을 만드는 방법을 배웁니다.

GraphQL API인 NerdGraph를 사용하여 로그 구문 분석 규칙을 생성, 쿼리 및 관리할 수도 있습니다. 이를 위한 유용한 도구는 [Nerdgraph API 탐색기](https://api.newrelic.com/graphiql)입니다. 보다 자세한 내용은 [구문 분석에 대한 NerdGraph 튜토리얼](/docs/apis/nerdgraph/examples/nerdgraph-log-parsing-rules-tutorial/)을 참조하십시오.

로그 구문 분석에 대한 5분 길이 동영상을 확인해 보십시오.

<Video
  id="xPWM46yw3bQ"
  type="youtube"
/>

## 구문 분석 예시 [#parsing-defined]

좋은 예는 정형화되지 않은 텍스트를 포함하는 기본 NGINX 액세스 로그입니다. 검색할 때 유용하지만 그 외에는 그다지 유용하지 않습니다. 다음은 일반적인 라인의 예입니다.

```
127.180.71.3 - - [10/May/1997:08:05:32 +0000] "GET /downloads/product_1 HTTP/1.1" 304 0 "-" "Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)"
```

구문 분석되지 않은 포맷에서는 대부분의 질문에 답하기 위해 전체 텍스트 검색을 수행해야 합니다. 구문 분석 후, 로그는 `response code` 및 `request URL` 같은 속성으로 구성됩니다.

```json
{
  "remote_addr":"93.180.71.3",
  "time":"1586514731",
  "method":"GET",
  "path":"/downloads/product_1",
  "version":"HTTP/1.1",
  "response":"304",
  "bytesSent": 0,
  "user_agent": "Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)"
}
```

구문 분석을 사용하면 해당 값을 패싯하는 [커스텀 쿼리](/docs/using-new-relic/data/understand-data/query-new-relic-data)를 더 쉽게 만들 수 있습니다. 이를 통해 요청 URL당 응답 코드의 분포를 이해하고 문제가 있는 페이지를 빠르게 찾을 수 있습니다.

## 로그 구문 분석의 작동 방식 [#how-it-works]

다음은 뉴렐릭이 로그 구문 분석을 구현하는 방법에 대한 간단한 설명입니다.

<table>
  <thead>
    <tr>
      <th style={{ width: "100px" }}>
        로그 구문 분석
      </th>

      <th>
        작동 원리
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        대상
      </td>

      <td>
        * 구문 분석은 선택된 특정 필드에 적용됩니다. 기본적으로 `message` 필드가 사용됩니다. 그러나 현재 데이터에 존재하지 않는 필드/속성까지 포함해, 모든 필드/속성을 선택할 수 있습니다.
        * 각 구문 분석 규칙은 규칙이 구문 분석을 시도할 로그를 결정하는 NRQL `WHERE` 절을 사용하여 생성됩니다.
        * 매칭 프로세스를 간소화하려면 로그에 [`logtype`](#logtype) 속성을 추가하는 것이 좋습니다. 그러나 `logtype` 사용하는 것으로 제한되지 않습니다. NRQL `WHERE` 절에서 하나 이상의 속성을 매칭 기준으로 사용할 수 있습니다.
      </td>
    </tr>

    <tr>
      <td>
        시기
      </td>

      <td>
        * 구문 분석은 각 로그 메시지에 한 번만 적용됩니다. 여러 구문 분석 규칙이 로그와 매치하는 경우 성공한 첫 번째 규칙만 적용됩니다.

        * 구문 분석 규칙은 순서가 없습니다. 둘 이상의 구문 분석 규칙이 로그와 매치되는 경우 무작위로 하나가 선택됩니다. 동일한 로그와 매치되지 않도록 구문 분석 규칙을 작성해야 합니다.

        * 구문 분석은 데이터가 NRDB에 기록되기 전에 로그 수집 중에 실행됩니다. 데이터가 스토리지에 기록되면 더 이상 구문 분석을 할 수 없습니다.

        * 데이터 보강이 이뤄지기 전에(

          <DNT>
            **before**
          </DNT>

          ) 파이프라인에서 구문 분석이 실행됩니다. 구문 분석 규칙에 대한 매칭 기준을 정의할 때는 주의해야 합니다. 기준이 구문 분석 또는 보강이 수행될 때까지 존재하지 않는 속성을 기반으로 하는 경우 매칭이 발생할 때 해당 데이터가 로그에 표시되지 않습니다. 결과적으로, 구문 분석이 실행되지 않습니다.
      </td>
    </tr>

    <tr>
      <td>
        방법
      </td>

      <td>
        * 규칙은 [Grok](#grok), 정규식 또는 이 둘을 혼합하여 작성할 수 있습니다. Grok은 복잡한 정규식을 추상화하는 패턴 모음입니다.
      </td>
    </tr>
  </tbody>
</table>

## Grok을 사용한 속성 구문 분석 [#grok]

구문 분석 패턴은 로그 메시지 구문 분석을 위한 업계 표준인 Grok를 사용하여 지정됩니다.`logtype` 필드가 있는 수신 로그는 [내장된 구문 분석 ](#built-in-rules)과 비교 확인되며 가능한 경우 연결된 Grok 패턴이 로그에 적용됩니다.

Grok은 복잡한 리터럴 정규식 대신 사용될 내장 및 명명된 패턴을 추가하는 정규식의 상위 집합입니다. 예를 들어 정수가 정규식 `(?:[+-]?(?:[0-9]+))`과 매칭될 수 있다는 것을 기억할 필요 없이 동일한 정규식을 나타내는 Grok 패턴 `INT`를 사용하도록 `%{INT}`을 작성할 수 있습니다.

Grok 패턴의 구문은 다음과 같습니다.

```
%{PATTERN_NAME[:OPTIONAL_EXTRACTED_ATTRIBUTE_NAME[:OPTIONAL_TYPE[:OPTIONAL_PARAMETER]]]}
```

Where:

* `PATTERN_NAME` 지원 Grok 패턴 중 하나입니다. 패턴 이름은 정규식을 나타내는 사용자 친화적인 이름입니다. 해당 정규식과 정확히 동일합니다.
* `OPTIONAL_EXTRACTED_ATTRIBUTE_NAME`이 제공된 경우, 패턴 이름과 매치되는 값으로 로그 메시지에 추가될 속성의 이름입니다. 정규식을 사용하여 명명된 캡처 그룹을 사용하는 것과 같습니다. 이것이 제공되지 않으면 구문 분석 규칙은 문자열의 영역과 매치되지만 해당 값으로 속성을 추출하지는 않습니다.
* `OPTIONAL_TYPE` 은 추출할 속성 값의 유형을 지정합니다. 생략하면, 값이 문자열로 추출됩니다. 예를 들어, `"File Size: 123"`의 값 `123`을 숫자에서 `file_size` 속성으로 추출하려면 `value: %{INT:file_size:int}`을 사용합니다.
* `OPTIONAL_PARAMETER` 특정 유형에 대해 선택적으로 파라미터를 지정합니다. 현재는 `datetime` 유형만 파라미터를 지정할 수 있습니다. 자세한 내용은 아래를 참조하십시오.

매치되는 문자열에서 정규식과 Grok 패턴 이름을 혼합하여 사용할 수도 있습니다.

지원되는 [Grok 패턴](https://github.com/thekrakken/java-grok/tree/master/src/main/resources/patterns) 목록을 보려면 이 링크를 클릭하고, 지원되는 [Grok 유형](#grok-types) 목록을 보려면 여기를 클릭하십시오.

변수 이름은 명시적으로 설정해야 하며 `%{URI:uri}` 같이 소문자여야 한다는 데 유의하십시오. `%{URI}` 또는 `%{URI:URI}` 같은 표현식은 안됩니다.

<CollapserGroup>
  <Collapser
    id="grok-example"
    title="Grok 예: 로그에서 유용한 데이터 가져오기"
  >
    로그 레코드는 다음과 같은 형식을 띌 수 있습니다.

    ```json
    {
      "message": "54.3.120.2 2048 0"
    }
    ```

    이 정보는 정확하지만 그 의미가 직관적이지 않습니다. Grok 패턴은 원하는 텔레메트리 데이터를 추출하고 이해하는 데 도움이 됩니다. 예를 들어 다음과 같은 로그 레코드는 사용하기가 훨씬 쉽습니다.

    ```json
    {
      "host_ip": "43.3.120.2",
      "bytes_received": 2048,
      "bytes_sent": 0
    }
    ```

    이를 위해서는 이 세 필드를 추출하는 Grok 패턴을 만들어야 합니다. 예를 들어:

    ```
    %{IP:host_ip} %{INT:bytes_received} %{INT:bytes_sent}
    ```

    프로세싱 후, 로그 레코드에는 `host_ip`, `bytes_received` 및 `bytes_sent` 필드가 포함됩니다. 이제 뉴렐릭에서 이러한 필드를 사용하여 로그 데이터에 대한 통계 연산을 필터링, 패싯 및 수행할 수 있습니다. 뉴렐릭에서 Grok 패턴으로 로그를 구문 분석하는 방법에 대한 보다 자세한 내용은 [블로그 게시물](https://newrelic.com/blog/how-to-relic/how-to-use-grok-log-parsing)을 참조하십시오.
  </Collapser>

  <Collapser
    id="grok-ui"
    title="UI 예: Grok 구문 분석 규칙 만들기"
  >
    올바른 권한이 있는 경우 UI에서 구문 분석 규칙을 만들어 Grok 구문 분석을 생성, 테스트 및 활성화할 수 있습니다. 예를 들어 Inventory Services라는 마이크로 서비스에 대한 특정 유형의 오류 메시지를 가져오려면, 특정 오류 메시지와 제품을 찾는 Grok 구문 분석 규칙을 만듭니다. 이를 위해서는:

    1. 규칙에 이름을 지정합니다. 예: `Inventory Services error parsing`

    2. 구문 분석할 기존 필드를 선택(기본값 = `message`)하거나 새 필드 이름을 입력합니다.

    3. 들어오는 로그에 대한 사전 필터 역할을 하는 NRQL `WHERE`절을 식별합니다. 예: `entity.name='Inventory Service'`. 이 사전 필터는 규칙에서 처리해야 하는 로그 수를 줄여 불필요한 프로세싱을 제거합니다.

    4. 매치되는 로그가 있으면 선택하거나 Paste log 탭을 클릭하여 샘플 로그에 붙여넣습니다.

    5. Grok 구문 분석 규칙을 추가합니다. 예:

       ```
       Inventory error: %{DATA:error_message} for product %{INT:product_id}
       ```

       Where:

    * `Inventory error`: 파싱 규칙의 이름
    * `error_message`: 선택하려는 오류 메시지
    * `product_id`: Inventory Service의 제품 ID

    6. 구문 분석 규칙을 활성화하고 저장합니다.

       곧 Inventory Service 로그가 2개의 새로운 필드 `error_message`와 `product_id`로 강화된 것을 볼 수 있습니다. 여기에서 이러한 필드에 대해 쿼리하고, 차트 및 대시보드를 만들고, 알림을 설정하는 등의 작업을 수행할 수 있습니다.

       보다 자세한 내용은 [UI에 커스텀 구문 분석 규칙을 추가하는 방법](#custom-parsing)을 참조하십시오.
  </Collapser>

  <Collapser
    id="grok-types"
    title="지원되는 Grok 유형"
  >
    `OPTIONAL_TYPE` 필드는 추출할 속성 값의 유형을 지정합니다. 생략하면, 값이 문자열로 추출됩니다.

    지원되는 유형:

    <table>
      <thead>
        <tr>
          <th>
            Grok에 지정된 유형
          </th>

          <th>
            뉴렐릭 데이터베이스에 저장된 유형
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `boolean`
          </td>

          <td>
            `boolean`
          </td>
        </tr>

        <tr>
          <td>
            `byte` `short` `int` `integer`
          </td>

          <td>
            `integer`
          </td>
        </tr>

        <tr>
          <td>
            `long`
          </td>

          <td>
            `long`
          </td>
        </tr>

        <tr>
          <td>
            `float`
          </td>

          <td>
            `float`
          </td>
        </tr>

        <tr>
          <td>
            `double`
          </td>

          <td>
            `double`
          </td>
        </tr>

        <tr>
          <td>
            `string` (기본) `text`
          </td>

          <td>
            `string`
          </td>
        </tr>

        <tr>
          <td>
            `date` `datetime`
          </td>

          <td>
            시간 `long`

            기본적으로 ISO 8601로 해석됩니다. `OPTIONAL_PARAMETER`가 있는 경우 `datetime`을 해석하는 데 사용할 [날짜 및 시간 패턴 문자열](https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/text/SimpleDateFormat.html)을 지정합니다.

            이는 구문 분석 중에만 사용할 수 있습니다. 인제스트 파이프라인의 후반부에는 모든 로그에 대해 발생하는 [추가 타임스탬프 해석 단계](/docs/logs/ui-data/timestamp-support)가 별도로 존재합니다.
          </td>
        </tr>

        <tr>
          <td>
            `json`
          </td>

          <td>
            JSON 정형 데이터보다 자세한 내용은 [일반 텍스트와 혼합된 JSON 구문 분석](#parsing-json)을 참조하십시오.
          </td>
        </tr>

        <tr>
          <td>
            `csv`
          </td>

          <td>
            CSV 데이터. 자세한 내용은 [CSV 구문 분석](#parsing-csv)을 참조하세요.
          </td>
        </tr>

        <tr>
          <td>
            `geo`
          </td>

          <td>
            IP 주소의 지리적 위치. 보다 자세한 내용은 [IP 주소의 위치 파악(GeoIP)](#geo)을 참조하십시오.
          </td>
        </tr>
      </tbody>
    </table>
  </Collapser>

  <Collapser
    id="grok-multiline"
    title="Grok 멀티라인(여러 줄) 구문 분석"
  >
    멀티라인 로그의 경우 `GREEDYDATA` Grok 패턴은 새로운 줄을 매치하지 않습니다. ( `.*`와 동일)

    따라서 `%{GREEDYDATA:some_attribute}`를 직접 사용하는 것이 아니라 그 앞에 멀티라인 플래그를 추가해야 합니다. `(?s)%{GREEDYDATA:some_attribute}`
  </Collapser>

  <Collapser
    id="parsing-json"
    title="일반 텍스트와 혼합된 JSON 구문 분석"
  >
    뉴렐릭 로그 파이프라인은 기본적으로 로그 JSON 메시지를 구문 분석하지만, 때로 일반 텍스트와 혼합된 JSON 로그 메시지가 있는 경우가 있습니다. 이런 상황에서, 메시지를 구문 분석한 다음 JSON 속성을 사용하여 필터링하길 원할 수 있습니다. 이 경우 `json` [grok 유형](#grok-syntax)을 사용할 수 있습니다. 이 유형은 grok 패턴에서 캡처한 JSON을 구문 분석합니다. 이 포맷은 3가지 주요 부분, 즉 grok 구문, 구문 분석된 json 속성에 할당하려는 접두사 및 `json` [grok 유형](#grok-syntax)에 의존합니다. `json` [grok 유형](#grok-syntax)을 사용하면 포맷이 올바르지 않은 로그에서 JSON을 추출하고 구문 분석할 수 있습니다. 예를 들어, 로그에 날짜/시간 문자열이 접두사로 붙는 경우:

    ```json
    2015-05-13T23:39:43.945958Z {"event": "TestRequest", "status": 200, "response": {"headers": {"X-Custom": "foo"}}, "request": {"headers": {"X-Custom": "bar"}}}
    ```

    이 로그 형식에서 JSON 데이터를 추출하고 구문 분석하려면 다음 Grok 표현식을 작성합니다.

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json}
    ```

    결과 로그는 다음과 같습니다.

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    my_attribute_prefix.event: "TestRequest"
    my_attribute_prefix.status: 200
    my_attribute_prefix.response.headers.X-Custom: "foo"
    my_attribute_prefix.request.headers.X-Custom: "bar"
    ```

    `keepAttributes` 또는 `dropAttributes` 옵션을 사용하여 추출하거나 드롭할 속성 목록을 정의할 수 있습니다. 예를 들어, 다음 Grok 표현식을 사용하면:

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json({"keepAttributes": ["my_attribute_prefix.event", "my_attribute_prefix.response.headers.X-Custom"]})}
    ```

    결과 로그는 다음과 같습니다.

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    my_attribute_prefix.event: "TestRequest"
    my_attribute_prefix.request.headers.X-Custom: "bar"
    ```

    `my_attribute_prefix` 접두사를 생략하려면 설정에 `"noPrefix": true`를 포함할 수 있습니다.

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json({"noPrefix": true})}
    ```

    `my_attribute_prefix` 접두사를 생략하고 `status` 속성만 유지하려는 경우 설정에 `"noPrefix": true` 및 `"keepAttributes: ["status"]`를 포함할 수 있습니다.

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json({"noPrefix": true, "keepAttributes": ["status"]})}
    ```

    JSON이 이스케이프된 경우 `isEscaped` 옵션을 사용하여 구문 분석을 할 수 있습니다. JSON이 이스케이프되고 인용이 된 경우 아래와 같이 인용 부호도 일치시켜야 합니다. 예를 들어, 다음 Grok 표현식을 사용하면:

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} "%{GREEDYDATA:my_attribute_prefix:json({"isEscaped": true})}"
    ```

    이스케이프된 메시지를 구문 분석할 수 있습니다.

    ```
    2015-05-13T23:39:43.945958Z "{\"event\": \"TestRequest\", \"status\": 200, \"response\": {\"headers\": {\"X-Custom\": \"foo\"}}, \"request\": {\"headers\": {\"X-Custom\": \"bar\"}}}"
    ```

    결과 로그는 다음과 같습니다.

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    my_attribute_prefix.event: "TestRequest"
    my_attribute_prefix.status: 200
    my_attribute_prefix.response.headers.X-Custom: "foo"
    my_attribute_prefix.request.headers.X-Custom: "bar"
    ```

    `json` [Grok 유형](#grok-syntax)을 구성하려면 `:json(_CONFIG_)`를 사용합니다.

    * `json({"dropOriginal": true})`: 구문 분석에 사용된 JSON 스니펫을 삭제합니다. `true`(기본값)로 설정하면 구문 분석 규칙이 원본 JSON 스니펫을 삭제합니다. JSON 속성은 메시지 필드에 유지됩니다.
    * `json({"dropOriginal": false})`: 추출된 JSON 페이로드를 보여줍니다. `false`로 설정하면 전체 JSON 전용 페이로드가 위의 `my_attribute_prefix`에 이름이 지정된 속성 아래에 표시됩니다. JSON 속성은 여기 메시지 필드에 남아 있을 뿐만 아니라 사용자에게 JSON 데이터에 대한 3가지 다른 뷰를 제공합니다. 세 가지 버전 모두 저장하는 것이 우려되는 경우 여기에서 기본값인 `true`를 사용하는 것이 좋습니다.
    * `json({"depth": 62})`: JSON 값을 구문 분석하려는 깊이 수준(기본값은 62)입니다.
    * `json({"keepAttributes": ["attr1", "attr2", ..., "attrN"]})`: JSON에서 추출할 속성을 지정합니다. 제공된 목록은 공백으로 둘 수 없습니다. 이 구성 옵션을 설정하지 않으면 모든 속성이 추출됩니다.
    * `json({"dropAttributes": ["attr1", "attr2", ..., "attrN"]})`: JSON에서 드롭할 속성을 지정합니다. 이 구성 옵션이 설정되지 않으면 속성이 드롭되지 않습니다.
    * `json({"noPrefix": true})`: JSON에서 추출된 속성에서 접두사를 제거하려면 이 옵션을 `true`로 설정합니다.
    * `json({"isEscaped": true})`: 이스케이프된 JSON을 구문 분석하려면 이 옵션을 `true`로 설정합니다. (일반적으로 JSON이 문자열화될 때 볼 수 있습니다. 예: `{\"key\": \"value\"}`)
  </Collapser>

  <Collapser
    id="parsing-csv"
    title="CSV 구문 분석"
  >
    시스템이 쉼표로 구분된 값(CSV) 로그를 보내고 이를 뉴렐릭에서 구문 분석해야 하는 경우 Grok 패턴으로 캡처한 CSV를 구문 분석하는 `csv` [Grok 유형](#grok-syntax)을 사용할 수 있습니다. 이 형식은 Grok 구문, 구문 분석된 CSV 속성에 할당하려는 접두사 및 `csv` [Grok 유형](#grok-syntax)의 세 가지 주요 부분에 의존합니다. `csv` [Grok 유형](#grok-syntax)을 사용하여 로그에서 CSV를 추출하고 구문 분석할 수 있습니다.

    예를 들어 다음 CSV 로그 줄과

    ```
    "2015-05-13T23:39:43.945958Z,202,POST,/shopcart/checkout,142,10"
    ```

    다음 형태의 구문 분석 규칙의 경우:

    ```
    %{GREEDYDATA:log:csv({"columns": ["timestamp", "status", "method", "url", "time", "bytes"]})}
    ```

    다음과 같이 로그를 구문 분석합니다.

    ```
    log.timestamp: "2015-05-13T23:39:43.945958Z"
    log.status: "202"
    log.method: "POST"
    log.url: "/shopcart/checkout"
    log.time: "142"
    log.bytes: "10"
    ```

    "log" 접두사를 생략해야 하는 경우 구성에 `"noPrefix": true`를 포함할 수 있습니다.

    ```
    %{GREEDYDATA:log:csv({"columns": ["timestamp", "status", "method", "url", "time", "bytes"], "noPrefix": true})}
    ```

    ### 열 구성:

    * CSV Grok 유형 구성(유효한 JSON이어야 함)에서 필수적으로 열을 지시해야 합니다.
    * 열 이름으로 "\_"(밑줄)을 설정하여 결과 객체에서 열을 삭제하면 모든 열을 무시할 수 있습니다.

    ### 선택적 구성 옵션:

    "열" 구성은 필수이지만 다음 설정을 사용하여 CSV 구문 분석을 변경할 수 있습니다.

    * <DNT>
        **dropOriginal**
      </DNT>

      : (기본값은 `true`) 구문 분석에 사용된 CSV 스니펫을 삭제합니다. `true` (기본값)으로 설정하면 구문 분석 규칙이 원래 필드를 삭제합니다.

    * <DNT>
        **noPrefix**
      </DNT>

      : (기본값은 `false`) Grok 필드 이름을 결과 객체의 접두사로 포함하지 않습니다.

    * <DNT>
        **separator**
      </DNT>

      : (기본값은 `,`) 각 열을 분할하는 문자/문자열을 정의합니다.

      * 또 다른 일반적인 시나리오는 `\t`를 구분 기호로 지정해야 하는 TSV(탭으로 구분된 값)입니다. `%{GREEDYDATA:log:csv({"columns": ["timestamp", "status", "method", "url", "time", "bytes"], "separator": "\t"})`

    * <DNT>
        **quoteChar**
      </DNT>

      : (기본값은 `"`) 열 내용을 선택적으로 둘러싸는 문자를 정의합니다.
  </Collapser>

  <Collapser
    id="geo"
    title="IP 주소 위치 파악(GeoIP)"
  >
    시스템이 IPv4 주소가 포함된 로그를 보내는 경우 뉴렐릭은 지리적으로 주소를 찾고 지정된 속성으로 로그 이벤트를 보강할 수 있습니다. Grok 패턴이 캡처한 IP 주소의 위치를 찾는 `geo` [Grok type](#grok-syntax)을 사용할 수 있습니다. 이 포맷은 IP의 도시, 국가, 위도/경도 등 주소와 관련된 하나 이상의 필드를 반환하도록 구성할 수 있습니다.

    예를 들어 다음 로그 줄이 주어집니다.

    ```
    2015-05-13T23:39:43.945958Z 146.190.212.184
    ```

    다음 형태의 구문 분석 규칙의 경우:

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:ip:geo({"lookup":["city","region","countryCode", "latitude","longitude"]})}
    ```

    다음과 같이 로그를 구문 분석합니다.

    ```
    ip: 146.190.212.184
    ip.city: North Bergen
    ip.countryCode: US
    ip.countryName: United States
    ip.latitude: 40.793
    ip.longitude: -74.0247
    ip.postalCode: 07047
    ip.region: NJ
    ip.regionName: New Jersey
    containerTimestamp:2015-05-13T23:39:43.945958Z
    ISO8601_TIMEZONE:Z
    ```

    ### 룩업 구성:

    반드시 `geo` 작업에서 반환된 원하는 `lookup` 필드를 지정해야 합니다. 다음 옵션 중 적어도 1개 항목이 필요합니다.

    * <DNT>
        **city**
      </DNT>

      : 도시 이름

    * <DNT>
        **countryCode**
      </DNT>

      : 국가의 약자

    * <DNT>
        **countryName**
      </DNT>

      : 국가 이름

    * <DNT>
        **latitude**
      </DNT>

      : 위도

    * <DNT>
        **longitude**
      </DNT>

      : 경도

    * <DNT>
        **postalCode**
      </DNT>

      : 우편번호 또는 이와 유사한 것

    * <DNT>
        **region**
      </DNT>

      : 주, 도, 지역의 약자

    * <DNT>
        **regionName**
      </DNT>

      : 주, 도, 지역의 이름
  </Collapser>
</CollapserGroup>

## 로그 유형별 정리 [#type]

뉴렐릭의 로그 수집 파이프라인은 로그를 구문 분석하는 방법을 설명하는 규칙에 로그 이벤트를 매칭시켜 데이터를 구문 분석할 수 있습니다. 다음 두 가지 방법으로 로그 이벤트를 구문 분석할 수 있습니다.

* [내장된 규칙을](#built-in-rules) 사용합니다.
* [커스텀 규칙](#custom-parsing)을 정의합니다.

규칙은 매칭되는 로직과 구문 분석 로직의 조합입니다. 매칭은 로그의 속성에 대한 쿼리 매치를 정의함으로써 수행됩니다. 규칙은 소급 적용되지 않습니다. 규칙이 생성되기 전에 수집된 로그는 해당 규칙에 의해 구문 분석되지 않습니다.

로그를 구성하고 구문 분석하는 가장 간단한 방법은 로그 이벤트에 `logtype` 필드를 포함하는 것입니다. 이것은 로그에 적용할 내장된 규칙을 뉴렐릭에 알려줍니다.

<Callout variant="important">
  구문 분석 규칙이 활성화되면, 규칙에 의해 구문 분석된 데이터는 영구적으로 변경됩니다. 변경 사항은 되돌릴 수 없습니다.
</Callout>

## 제한

구문 분석은 계산 비용이 많이 들고 위험이 따릅니다. 계정에 정의된 커스텀 규칙과 패턴을 로그에 매칭시키기 위해 구문 분석이 수행됩니다. 많은 수의 패턴이나 잘못 정의된 커스텀 규칙은 엄청난 양의 메모리와 CPU 리소스를 소비할 뿐아니라 완료하는 데도 오랜 시간이 걸립니다.

문제를 방지하기 위해, 규칙당 메시지당, 계정당 2회의 구문 분석 제한을 적용합니다.

<table>
  <thead>
    <tr>
      <th style={{ width: "200px" }}>
        제한
      </th>

      <th>
        설명
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        Per-message-per-rule
      </td>

      <td>
        규칙당 메시지당 제한으로 단일 메시지를 구문 분석하는 데 소요되는 시간이 100ms를 초과하는 것을 방지합니다. 이 제한에 도달하면 시스템은 해당 규칙을 사용하여 로그 메시지를 구문 분석하려는 시도를 중지합니다.

        인제스트 파이프라인은 해당 메시지에 적용 가능한 다른 모든 항목을 실행하려고 시도하며, 메시지는 여전히 수집 파이프라인을 통해 전달되고 NRDB에 저장됩니다. 로그 메시지는 원래의 구문 분석되지 않은 포맷입니다.
      </td>
    </tr>

    <tr>
      <td>
        Per-account
      </td>

      <td>
        계정당 제한은 계정이 공정한 리소스 할당량 이상을 사용하는 것을 방지하기 위함입니다. 이 제한은 계정에 대한 분당 모든(<DNT>**all**</DNT>) 로그 메시지를 처리하는 데 소요된 총 시간을 고려합니다.
      </td>
    </tr>
  </tbody>
</table>

<Callout variant="tip">
  제한에 도달했는지 쉽게 확인하려면 뉴렐릭 UI의 [시스템 <DNT>**Limits**</DNT> 페이지](/docs/telemetry-data-platform/ingest-manage-data/manage-data/view-system-limits#limits-ui)로 이동합니다.
</Callout>

## 기본 구문 분석 규칙 [#built-in-rules]

공통 로그 포맷에는 잘 정립된 구문 분석 규칙이 이미 생성되어 있습니다. 내장된 구문 분석 규칙의 이점을 얻으려면, 로그를 전달할 때 `logtype` 속성을 추가합니다. 값을 다음 표에 나열된 값으로 설정하면, 해당 유형의 로그에 대한 규칙이 자동으로 적용됩니다.

### 내장된 규칙 목록 [#rulesets]

다음 `logtype` 속성 값은 사전 정의된 구문 분석 규칙에 매핑됩니다. 예를 들어, Application Load Balancer를 쿼리하려면:

* 뉴렐릭 UI에서 `logtype:"alb"` 포맷을 사용합니다.
* [NerdGraph](/docs/apis/nerdgraph/examples/nerdgraph-log-parsing-rules-tutorial/)에서 `logtype = 'alb'` 포맷을 사용합니다.

각 규칙에 대해 구문 분석되는 필드를 알아보려면 [내장된 구문 분석 규칙](/docs/logs/ui-data/built-log-parsing-rules)에 대한 문서를 참조하십시오.

<table>
  <thead>
    <tr>
      <th style={{ width: "200px" }}>
        `logtype`
      </th>

      <th>
        로그 소스
      </th>

      <th>
        매칭 쿼리의 예
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        [`apache`](/docs/logs/ui-data/built-log-parsing-rules#apache)
      </td>

      <td>
        Apache 액세스 로그
      </td>

      <td>
        `logtype:"apache"`
      </td>
    </tr>

    <tr>
      <td>
        [`apache_error`](/docs/logs/ui-data/built-log-parsing-rules#apache_error)
      </td>

      <td>
        Apache 오류 로그
      </td>

      <td>
        `logtype:"apache_error"`
      </td>
    </tr>

    <tr>
      <td>
        [`alb`](/docs/logs/ui-data/built-log-parsing-rules#application-load-balancer)
      </td>

      <td>
        애플리케이션 로드 밸런서 로그
      </td>

      <td>
        `logtype:"alb"`
      </td>
    </tr>

    <tr>
      <td>
        [`cassandra`](/docs/logs/ui-data/built-log-parsing-rules#cassandra)
      </td>

      <td>
        Cassandra 로그
      </td>

      <td>
        `logtype:"cassandra"`
      </td>
    </tr>

    <tr>
      <td>
        [`cloudfront-web`](/docs/logs/ui-data/built-log-parsing-rules#cloudfront)
      </td>

      <td>
        CloudFront (표준 웹 로그)
      </td>

      <td>
        `logtype:"cloudfront-web"`
      </td>
    </tr>

    <tr>
      <td>
        [`cloudfront-rtl`](/docs/logs/ui-data/built-log-parsing-rules#cloudfront-rtl)
      </td>

      <td>
        CloudFront (실시간 웹 로그)
      </td>

      <td>
        `logtype:"cloudfront-rtl"`
      </td>
    </tr>

    <tr>
      <td>
        [`elb`](/docs/logs/ui-data/built-log-parsing-rules#elastic-load-balancer)
      </td>

      <td>
        Elastic 로드 밸런서 로그
      </td>

      <td>
        `logtype:"elb"`
      </td>
    </tr>

    <tr>
      <td>
        [`haproxy_http`](/docs/logs/ui-data/built-log-parsing-rules#haproxy)
      </td>

      <td>
        HAProxy 로그
      </td>

      <td>
        `logtype:"haproxy_http"`
      </td>
    </tr>

    <tr>
      <td>
        [`ktranslate-health`](/docs/logs/ui-data/built-log-parsing-rules#ktranslate-health)
      </td>

      <td>
        KTranslate 컨테이너 상태 로그
      </td>

      <td>
        `logtype:"ktranslate-health"`
      </td>
    </tr>

    <tr>
      <td>
        [`linux_cron`](/docs/logs/ui-data/built-log-parsing-rules/#linux_cron)
      </td>

      <td>
        Linux cron
      </td>

      <td>
        `logtype:"linux_cron"`
      </td>
    </tr>

    <tr>
      <td>
        [`linux_messages`](/docs/logs/ui-data/built-log-parsing-rules/#linux_messages)
      </td>

      <td>
        Linux 메시지
      </td>

      <td>
        `logtype:"linux_messages"`
      </td>
    </tr>

    <tr>
      <td>
        [`iis_w3c`](/docs/logs/ui-data/built-log-parsing-rules/#iis)
      </td>

      <td>
        Microsoft IIS 서버 로그 - W3C 포맷
      </td>

      <td>
        `logtype:"iis_w3c"`
      </td>
    </tr>

    <tr>
      <td>
        [`mongodb`](/docs/logs/ui-data/built-log-parsing-rules#mongodb)
      </td>

      <td>
        MongoDB 로그
      </td>

      <td>
        `logtype:"mongodb"`
      </td>
    </tr>

    <tr>
      <td>
        [`monit`](/docs/logs/ui-data/built-log-parsing-rules#monit)
      </td>

      <td>
        Monit 로그
      </td>

      <td>
        `logtype:"monit"`
      </td>
    </tr>

    <tr>
      <td>
        [`mysql-error`](/docs/logs/ui-data/built-log-parsing-rules#mysql-error)
      </td>

      <td>
        MySQL 오류 로그
      </td>

      <td>
        `logtype:"mysql-error"`
      </td>
    </tr>

    <tr>
      <td>
        [`nginx`](/docs/logs/ui-data/built-log-parsing-rules#nginx)
      </td>

      <td>
        NGINX 액세스 로그
      </td>

      <td>
        `logtype:"nginx"`
      </td>
    </tr>

    <tr>
      <td>
        [`nginx-error`](/docs/logs/ui-data/built-log-parsing-rules#nginx-error)
      </td>

      <td>
        NGINX 오류 로그
      </td>

      <td>
        `logtype:"nginx-error"`
      </td>
    </tr>

    <tr>
      <td>
        [`postgresql`](/docs/logs/ui-data/built-log-parsing-rules#postgresql)
      </td>

      <td>
        PostgreSQL 로그
      </td>

      <td>
        `logtype:"postgresql"`
      </td>
    </tr>

    <tr>
      <td>
        [`rabbitmq`](/docs/logs/ui-data/built-log-parsing-rules#rabbitmq)
      </td>

      <td>
        Rabbitmq 로그
      </td>

      <td>
        `logtype:"rabbitmq"`
      </td>
    </tr>

    <tr>
      <td>
        [`redis`](/docs/logs/ui-data/built-log-parsing-rules#redis)
      </td>

      <td>
        Redis 로그
      </td>

      <td>
        `logtype:"redis"`
      </td>
    </tr>

    <tr>
      <td>
        [`route-53`](/docs/logs/ui-data/built-log-parsing-rules#route53)
      </td>

      <td>
        Route 53 로그
      </td>

      <td>
        `logtype:"route-53"`
      </td>
    </tr>

    <tr>
      <td>
        [`syslog-rfc5424`](/docs/logs/ui-data/built-log-parsing-rules/#syslog-rfc5424)
      </td>

      <td>
        RFC5424 포맷 Syslog
      </td>

      <td>
        `logtype:"syslog-rfc5424"`
      </td>
    </tr>
  </tbody>
</table>

### `logtype` 속성 추가 [#logattr]

로그를 집계할 때는 로그를 쉽게 구성, 검색 및 구문 분석할 수 있도록 메타데이터를 제공하는 것이 중요합니다. 이를 위한 한 가지 간단한 방법은 전송될 때 로그 메시지에 속성 `logtype`을 추가하는 것입니다. 내장된 [구문 분석 규칙](#built-in-rules)은 특정 `logtype` 값에 기본적으로 적용됩니다.

<Callout variant="tip">
  `logType`, `logtype` 및 `LOGTYPE` 필드는 모두 내장된 규칙을 지원합니다. 검색을 쉽게 하려면 조직이 단일 구문으로 정렬하는 것이 좋습니다.
</Callout>

다음은 [지원되는 전송 방법](/docs/logs/enable-new-relic-logs) 중 일부에서 보낸 로그에 `logtype`을 추가하는 몇 가지 방법의 예입니다.

<CollapserGroup>
  <Collapser
    className="freq-link"
    id="infrastructure-log-forwarder-example"
    title="뉴렐릭 인프라 에이전트 예시"
  >
    `logtype`을 [`attribute`](/docs/logs/forward-logs/forward-your-logs-using-infrastructure-agent#attributes)로 추가합니다. 명명된 각 소스에 대해 로그 유형을 설정해야 합니다.

    ```yml
    logs:
      - name: file-simple
        file: /path/to/file
        attributes:
          logtype: fileRaw
      - name: nginx-example
        file: /var/log/nginx.log
        attributes:
          logtype: nginx
    ```
  </Collapser>

  <Collapser
    className="freq-link"
    id="fluentd-example"
    title="Fluentd 예시"
  >
    `record_transformer`를 사용하여 새 필드를 추가하는 `.conf` 파일에 필터 블록을 추가합니다. 이 예에서는 `nginx`의 `logtype`을 사용하여 내장 NGINX 구문 분석 규칙을 트리거합니다. 다른 [Fluentd 예제](https://github.com/newrelic/fluentd-examples)들을 확인해 보십시오.

    ```apacheconf
    <filter containers>
      @type record_transformer
      enable_ruby true
      <record>
        #Add logtype to trigger a built-in parsing rule for nginx access logs
        logtype nginx
        #Set timestamp from the value contained in the field "time"
        timestamp record["time"]
        #Add hostname and tag fields to all records
        hostname "#{Socket.gethostname}"
        tag ${tag}
      </record>
    </filter>
    ```
  </Collapser>

  <Collapser
    className="freq-link"
    id="fluentbit-example"
    title="Fluent Bit 예시"
  >
    `record_modifier`를 사용하여 새 필드를 추가하는 필터 블록을 `.conf` 파일에 추가합니다.이 예에서는 `nginx`의 `logtype`을 사용하여 내장된 NGINX 구문 분석 규칙을 트리거합니다. 다른 [Fluent Bit 예시](https://github.com/newrelic/fluentbit-examples)를 확인해보십시오.

    ```ini
    [FILTER]
        Name   record_modifier
        Match  *
        Record logtype nginx
        Record hostname ${HOSTNAME}
        Record service_name Sample-App-Name
    ```
  </Collapser>

  <Collapser
    className="freq-link"
    id="logstash-example"
    title="Logstash 예시"
  >
    `add_field` mutate 필터를 사용해 새 필드를 추가하는 필터 블록을 Logstash 구성에 추가합니다. 이 예에서는 `nginx`의 `logtype`을 사용하여 내장된 NGINX 구문 분석 규칙을 트리거합니다. 다른 [Logstash 예시](https://github.com/newrelic/logstash-examples)를 확인해보십시오.

    ```ini
    filter {
      mutate {
        add_field => {
          "logtype" => "nginx"
          "service_name" => "myservicename"
          "hostname" => "%{host}"
        }
      }
    }
    ```
  </Collapser>

  <Collapser
    className="freq-link"
    id="api-example"
    title="로그 API 예시"
  >
    뉴렐릭으로 전송된 JSON 요청에 속성을 추가할 수 있습니다. 이 예에서는 값이 `nginx`인 `logtype` 속성을 추가하여 내장된 NGINX 구문 분석 규칙을 트리거합니다. [Logs API](/docs/logs/new-relic-logs/log-api/introduction-log-api)를 사용하는 방법을 보다 자세히 알아보십시오.

    ```
    POST /log/v1 HTTP/1.1
    Host: log-api.newrelic.com
    Content-Type: application/json
    X-License-Key: YOUR_LICENSE_KEY
    Accept: */*
    Content-Length: 133
    {
      "timestamp": TIMESTAMP_IN_UNIX_EPOCH,
      "message": "User 'xyz' logged in",
      "logtype": "nginx",
      "service": "login-service",
      "hostname": "login.example.com"
    }
    ```
  </Collapser>
</CollapserGroup>

## 커스텀 구문 분석 규칙 생성 및 보기 [#custom-parsing]

많은 로그가 고유한 방식으로 형식화되거나 구조화됩니다. 이를 구문 분석하려면 커스텀 로직을 빌드하고 적용해야 합니다.

<img
  title="Log parsing rules"
  alt="Screenshot of log parsing in UI"
  src="/images/logs_screenshot-full_parsing-ui.webp"
/>

<figcaption>
  로그 UI의 왼쪽 탐색 메뉴에서 <DNT>**Parsing**</DNT>을 선택한 다음 유효한 NRQL `WHERE` 절과 Grok 패턴을 사용하여 고유한 커스텀 구문 분석 규칙을 생성합니다.
</figcaption>

고유한 커스텀 구문 분석 규칙을 만들고 관리하려면 다음을 수행합니다.

1. <DNT>
     **[one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > Logs**
   </DNT>

   으로 이동합니다.

2. 로그 UI 왼쪽 탐색 메뉴의

   <DNT>
     **Manage data**
   </DNT>

   에서

   <DNT>
     **Parsing**
   </DNT>

   을 클릭한 다음

   <DNT>
     **Create parsing rule**
   </DNT>

   을 클릭합니다.

3. 새 구문 분석 규칙의 이름을 입력합니다.

4. 구문 분석할 기존 필드를 선택(기본값 = `message`)하거나 새 필드 이름을 입력합니다.

5. 구문 분석하려는 로그와 매치되는 유효한 NRQL `WHERE` 절을 입력합니다.

6. 매치하는 로그가 있으면 선택하거나

   <DNT>
     **Paste log**
   </DNT>

   탭을 클릭하여 샘플 로그를 붙여넣습니다. 로그 UI 또는 쿼리 빌더에서 텍스트를 복사하여 구문 분석 UI에 붙여넣는 경우 해당 버전이

   <DNT>
     _Unformatted_
   </DNT>

   버전인지 확인합니다.

7. 구문 분석 규칙을 입력하고

   <DNT>
     **Output**
   </DNT>

   섹션의 결과를 확인하여 작동하는지 확인합니다. Grok 및 커스텀 구문 분석 규칙에 대해 보다 자세히 알아보려면 [Grok 패턴으로 로그를 구문 분석하는 방법에 대한 블로그 게시물](https://blog.newrelic.com/product-news/how-to-use-grok-log-parsing)을 확인해보십시오.

8. 커스텀 구문 분석 규칙을 활성화하고 저장합니다.

기존 구문 분석 규칙을 보려면:

1. <DNT>
     **[one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) > Logs**
   </DNT>

   으로 이동합니다.

2. 로그 UI 왼쪽 탐색 메뉴의

   <DNT>
     **Manage data**
   </DNT>

   에서

   <DNT>
     **Parsing**
   </DNT>

   을 클릭합니다.

## 문제 해결 [#troubleshooting]

구문 분석이 의도한 대로 작동하지 않는 경우 다음과 같은 이유 때문일 수 있습니다.

* <DNT>
    **Logic:**
  </DNT>

  구문 분석 규칙 매칭 로직이 원하는 로그와 일치하지 않습니다.

* <DNT>
    **Timing:**
  </DNT>

  구문 분석 매칭 규칙이 아직 존재하지 않는 값을 대상으로 하는 경우, 구문 분석이 실패합니다. 보강 프로세스의 일부로 나중에 파이프라인에 값을 추가하면 이런 일이 발생할 수 있습니다.

* <DNT>
    **Limits:**
  </DNT>

  분당 구문 분석, 패턴, 드롭 필터 등을 통해 로그를 처리하는 데 사용할 수 있는 시간이 한정되어 있습니다. 최대 시간이 소요된 경우 추가 로그 이벤트 레코드에 대한 구문 분석을 건너뜁니다.

이러한 문제를 해결하려면 [커스텀 구문 분석 규칙](#custom-parsing)을 만들거나 조정해야 합니다.
