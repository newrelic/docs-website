---
title: 호스트 모니터링을 위한 노드 내보내기 통합
tags:
  - Integrations
  - Node Exporter
  - Prometheus
  - host monitoring
freshnessValidatedDate: '2024-04-10T00:00:00.000Z'
translationType: machine
---

Linux 서버의 하드웨어 및 커널 지표를 모니터링하시겠습니까? 블루렐릭 원격 쓰기 통합과 Prometheus 노드 내보내기를 사용하면 이 작업을 수행할 수 있습니다. 이 두 프로그램을 Prometheus 모니터링 시스템과 결합하면 뉴렐릭으로 데이터를 보내 문제 해결, 해결에 사용할 수 있습니다.

여기에 나와 있는 지침은 Prometheus 가이드 [노드 내보내기를 사용하여 Linux 호스트 메트릭 모니터링 을](https://prometheus.io/docs/guides/node-exporter/) 기반으로 합니다. 해당 정보 중 일부를 반복하고 귀하의 데이터를 뉴렐릭으로 보내는 데 도움이 되는 단계로 확장하겠습니다.

## 전제 조건 [#prerequisites]

시작하는 데 필요한 사항은 다음과 같습니다.

* 리뷰하고 싶은 Linux 호스트를 결정하세요. 아래에서는 EC2, GCP 및 Azure 인스턴스의 Linux 서버에 대한 예를 보여줍니다.
* Prometheus 모니터링 시스템을 설치했는지 확인하세요. 아직 다운로드하지 않았다면 [Prometheus 사이트](https://prometheus.io/download/) 에서 다운로드할 수 있습니다.

## 노드 내보내기 다운로드 및 시작 [#download-node-exporter]

다음을 완료:

1. 아래 명령을 사용하여 Node Importer를 다운로드하고 시작하세요. `wget` URL을 Prometheus [다운로드](https://prometheus.io/download#node_exporter) 페이지의 최신 URL로 바꾸세요.

   ```bash
   # Note that <VERSION>, <OS>, and <ARCH> are placeholders.
   wget https://github.com/prometheus/node_exporter/releases/download/v<VERSION>/node_exporter-<VERSION>.<OS>-<ARCH>.tar.gz
   tar xvfz node_exporter-*.*-amd64.tar.gz
   cd node_exporter-*.*-amd64
   ./node_exporter
   ```

2. 키보드 명령 `CONTROL + z` 및 `bg` 사용하여 노드 내보내기가 백그라운드에서 실행되도록 설정합니다. 운영 환경에서는 이를 서비스로 설정하려고 합니다(예: `systemd`).

## 구성 [#configs]

Prometheus를 시작하기 전에 기본 `prometheus.yml` 설정 파일을 일부 변경해야 합니다. 아래의 기본 `prometheus.yml` 예제로 시작하여 나머지 섹션에 설정을 추가하겠습니다. 이러한 예제를 복사하여 설정 파일에 붙여넣을 수 있습니다.

`job_name` 는 `node` 로 설정됩니다.

```yml lineHighlight=11
# my global config 
global:
  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
  # scrape_timeout is set to the global default (10s).

# A scrape configuration containing exactly one endpoint to scrape:
# Here it's Prometheus itself.
scrape_configs:
  # The job name is added as a label `job=<job_name>` to any time series scraped from this config.
  - job_name: node
```

### 프로메테우스를 뉴렐릭에 연결하세요 [#add-url-and-license]

`prometheus.yml` 에 아래 예의 `remote_write` 스니펫을 삽입합니다. 다음 사항에 유의하세요.

* 이는 Prometheus v2.26 이상에 대한 스니펫입니다. 이전 버전을 사용하는 경우 기본 [원격 쓰기 지침을](/docs/infrastructure/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/#setup) 참조하세요.
* `YOUR_LICENSE_KEY` 원하는 값으로 바꿔야 합니다.
* `global` 섹션과 동일한 들여쓰기 수준으로 설정 파일 하단에 이를 삽입할 수 있습니다.

```yml lineHighlight=12-15
# my global config 
global:
  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
  # scrape_timeout is set to the global default (10s).
# A scrape configuration containing exactly one endpoint to scrape:
# Here it's Prometheus itself.
scrape_configs:
  # The job name is added as a label `job=<job_name>` to any time series scraped from this config.
  - job_name: node

remote_write:
  - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=NodeExporter
    authorization:
      credentials: YOUR_LICENSE_KEY
```

### Set up 목표, 목표 [#set-up-targets]

`static_configs` 트리거를 통해 정적으로 목표를 구성하거나 지원되는 서비스 검색 메커니즘 중 하나를 사용하여 동적 검색을 사용할 수 있습니다.

#### 정적, 목표 [#static-targets]

새 주석 `# Target setup` 아래에서 정적 구성을 설정할 수 있습니다.

<CollapserGroup>
  <Collapser
    id="ec2-static"
    title="EC2"
  >
    `<INSTANCE_ID>`(를) 삽입하세요.

    ```yml lineHighlight=12-16
    # my global config
    global:
      scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
      evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
      # scrape_timeout is set to the global default (10s).
    # A scrape configuration containing exactly one endpoint to scrape:
    # Here it's Prometheus itself.
    scrape_configs:
      # The job name is added as a label `job=<job_name>` to any time series scraped from this config.
      - job_name: node

        # Target setup 
        static_configs:
        - targets: ['localhost:9100']
          labels:
            instanceid: <INSTANCE_ID> 

    remote_write:
      - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=NodeExporter
        authorization:
          credentials: YOUR_LICENSE_KEY
    ```
  </Collapser>

  <Collapser
    id="azure-static"
    title="하늘빛"
  >
    `<MACHINE_ID>`(를) 삽입하세요.

    ```yml lineHighlight=12-16
    # my global config
    global:
      scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
      evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
      # scrape_timeout is set to the global default (10s).
    # A scrape configuration containing exactly one endpoint to scrape:
    # Here it's Prometheus itself.
    scrape_configs:
      # The job name is added as a label `job=<job_name>` to any time series scraped from this config.
      - job_name: node

        # Target setup 
        static_configs:
        - targets: ['localhost:9100']
          labels:
            instanceid: <MACHINE_ID>

    remote_write:
      - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=NodeExporter
        authorization:
          credentials: YOUR_LICENSE_KEY
    ```
  </Collapser>

  <Collapser
    id="gcp-static"
    title="GCP"
  >
    `<INSTANCE_ID>`(를) 삽입하세요.

    ```yml lineHighlight=12-16
    # my global config
    global:
      scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
      evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
      # scrape_timeout is set to the global default (10s).
    # A scrape configuration containing exactly one endpoint to scrape:
    # Here it's Prometheus itself.
    scrape_configs:
      # The job name is added as a label `job=<job_name>` to any time series scraped from this config.
      - job_name: node

        # Target setup 
        static_configs:
        - targets: ['localhost:9100']
          labels:
            instanceid: <INSTANCE_ID>

    remote_write:
      - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=NodeExporter
        authorization:
          credentials: YOUR_LICENSE_KEY
    ```
  </Collapser>
</CollapserGroup>

#### 다이나믹한 목표, 목표 [#dynamic-targets]

정적, 목표를 구성하는 대신 서비스 검색을 구성할 수 있습니다.

<CollapserGroup>
  <Collapser
    id="ec2-dynamic"
    title="EC2"
  >
    `# Target setup` 아래에 `region`, `access_key`, `secret_key` 및 `port` 를 제공하여 AWS EC2 인스턴스에 대한 서비스 검색을 설정할 수 있습니다.

    ```yml lineHighlight=12-22
    # my global config
    global:
      scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
      evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
      # scrape_timeout is set to the global default (10s).
    # A scrape configuration containing exactly one endpoint to scrape:
    # Here it's Prometheus itself.
    scrape_configs:
      # The job name is added as a label `job=<job_name>` to any time series scraped from this config.
      - job_name: node

        # Target setup 
        ec2_sd_configs:
          - region: <EC2_REGION>
            # The AWS API keys. If blank, the environment variables `AWS_ACCESS_KEY_ID`
            # and `AWS_SECRET_ACCESS_KEY` are used.
            access_key: <ACCESS_KEY>
            secret_key: <SECRET_KEY>
            # The port to scrape metrics from. If using the public IP address, this must
            # instead be specified in the relabeling rule.
            # By default node_exporter will expose metrics on 9100
            port: <PORT>

    remote_write:
      - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=NodeExporter
        authorization:
          credentials: YOUR_LICENSE_KEY
    ```
  </Collapser>

  <Collapser
    id="azure-dynamic"
    title="하늘빛"
  >
    `# Target setup` 아래에 `<SUBSCRIPTION_ID>` 을 삽입해야 합니다.

    ```yml lineHighlight=12-15
    # my global config
    global:
      scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
      evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
      # scrape_timeout is set to the global default (10s).
    # A scrape configuration containing exactly one endpoint to scrape:
    # Here it's Prometheus itself.
    scrape_configs:
      # The job name is added as a label `job=<job_name>` to any time series scraped from this config.
      - job_name: node

        # Target setup 
        azure_sd_config:
          # The subscription ID. Always required.
          subscription_id: <SUBSCRIPTION_ID>

    remote_write:
      - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=NodeExporter
        authorization:
          credentials: YOUR_LICENSE_KEY
    ```
  </Collapser>

  <Collapser
    id="gcp-dynamic"
    title="GCP"
  >
    `# Target setup` 아래에 `<PROJECT>` 을 삽입해야 합니다.

    ```yml lineHighlight=12-15
    # my global config
    global:
      scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
      evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
      # scrape_timeout is set to the global default (10s).
    # A scrape configuration containing exactly one endpoint to scrape:
    # Here it's Prometheus itself.
    scrape_configs:
      # The job name is added as a label `job=<job_name>` to any time series scraped from this config.
      - job_name: node

        # Target setup 
        gce_sd_config:
          # The GCP Project
          project: <PROJECT>

    remote_write:
      - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=NodeExporter
        authorization:
          credentials: YOUR_LICENSE_KEY
    ```
  </Collapser>
</CollapserGroup>

### 호스트와 APM 관계 설정 [#host-to-apm]

이 Linux 서버에서 APM 에이전트를 사용하여 앱을 모니터링하는 경우 뉴렐릭에서 관계 기능을 활성화하기 위한 몇 가지 추가 설정을 해야 합니다. 이러한 기능은 호스트와 앱 간의 관계에 따라 달라집니다.

관계에는 Prometheus에서 기본적으로 삭제되는 속성이 필요합니다. 이 문제를 해결하려면 구성 파일의 `relabel_configs` 스탠자를 통해 이를 포함할 수 있습니다.

<Callout variant="tip">
  Prometheus [설정](https://prometheus.io/docs/prometheus/latest/configuration/configuration) 페이지의 해당 `sd_config` 아래에서 사용 가능한 모든 메타 속성을 볼 수 있습니다.
</Callout>

아래 예에서는 동적 검색과 라벨의 조합을 보여줍니다. 정적 타격, 목표를 사용하는 경우 위에 표시된 [정적 타격, 목표를](#static-targets) 삽입하면 됩니다.

<CollapserGroup>
  <Collapser
    id="ec2-labels"
    title="EC2"
  >
    자세한 내용은 Prometheus [EC2](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#ec2_sd_config) 설명서를 참조하세요.

    ```yml lineHighlight=23-26
    # my global config
    global:
      scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
      evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
      # scrape_timeout is set to the global default (10s).
    # A scrape configuration containing exactly one endpoint to scrape:
    # Here it's Prometheus itself.
    scrape_configs:
      # The job name is added as a label `job=<job_name>` to any time series scraped from this config.
      - job_name: node

        # Target setup 
        ec2_sd_configs:
          - region: <EC2_REGION>
            # The AWS API keys. If blank, the environment variables `AWS_ACCESS_KEY_ID`
            # and `AWS_SECRET_ACCESS_KEY` are used.
            access_key: <ACCESS_KEY>
            secret_key: <SECRET_KEY>
            # The port to scrape metrics from. If using the public IP address, this must
            # instead be specified in the relabeling rule.
            # By default node_exporter will expose metrics on 9100
            port: <PORT>
        # Relabel configs
        relabel_configs:
          - source_labels: [__meta_ec2_instance_id]
            target_label: instanceid

    remote_write:
      - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=NodeExporter
        authorization:
          credentials: YOUR_LICENSE_KEY
    ```
  </Collapser>

  <Collapser
    id="azure-dynamic"
    title="하늘빛"
  >
    자세한 내용은 Prometheus [EC2](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#azure_sd_config) 설명서를 참조하세요.

    ```yml lineHighlight=16-19
    # my global config
    global:
      scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
      evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
      # scrape_timeout is set to the global default (10s).
    # A scrape configuration containing exactly one endpoint to scrape:
    # Here it's Prometheus itself.
    scrape_configs:
      # The job name is added as a label `job=<job_name>` to any time series scraped from this config.
      - job_name: node

        # Target setup 
        azure_sd_config:
          # The subscription ID. Always required.
          subscription_id: <SUBSCRIPTION_ID>
        # Relabel configs
        relabel_configs:
          - source_labels: [__meta_azure_machine_id]
            target_label: instanceid

    remote_write:
      - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=NodeExporter
        authorization:
          credentials: YOUR_LICENSE_KEY
    ```
  </Collapser>

  <Collapser
    id="gcp-dynamic"
    title="GCP"
  >
    자세한 내용은 Prometheus [EC2](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#gce_sd_config) 설명서를 참조하세요.

    ```yml lineHighlight=16-19
    # my global config
    global:
      scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
      evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
      # scrape_timeout is set to the global default (10s).
    # A scrape configuration containing exactly one endpoint to scrape:
    # Here it's Prometheus itself.
    scrape_configs:
      # The job name is added as a label `job=<job_name>` to any time series scraped from this config.
      - job_name: node

        # Target setup 
        gce_sd_config:
          # The GCP Project
          project: <PROJECT>
        # Relabel configs
        relabel_configs:
          - source_labels: [__meta_gce_instance_id]
            target_label: instanceid

    remote_write:
      - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=NodeExporter
        authorization:
          credentials: YOUR_LICENSE_KEY
    ```
  </Collapser>
</CollapserGroup>

## 프로메테우스 시작 [#prometheus-startup]

이제 Prometheus 스크레이퍼를 시작할 수 있습니다.

1. 다음을 실행합니다.

   ```
   ./prometheus --config.file=./prometheus.yml
   ```

2. 키보드 명령 `CONTROL + z` 및 `bg` 사용하여 스크레이퍼가 백그라운드에서 실행되도록 설정합니다. 환경에서는 이를 서비스로 설정하려고 합니다(예: `systemd`).

3. **<DNT>[one.newrelic.com](https://one.newrelic.com/)</DNT> > Infrastructure > Hosts** 로으로 이동하여 뉴렐릭 UI에서 데이터를 확인하세요.
