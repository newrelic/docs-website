---
title: Amazon EFS 모니터링 통합
tags:
  - Integrations
  - Amazon integrations
  - AWS integrations list
metaDescription: 'New Relic''s Amazon EFS monitoring integration: what data it reports, and how to enable it.'
freshnessValidatedDate: never
translationType: machine
---

<Callout variant="important">
  [AWS CloudWatch Metric Streams 통합](/docs/infrastructure/amazon-integrations/aws-integrations-list/aws-metric-stream/) 을 활성화하여 사용자 지정 네임스페이스를 포함하여 AWS 서비스의 모든 CloudWatch 지표를 모니터링합니다. 개별 통합은 더 이상 권장되는 옵션이 아닙니다.
</Callout>

[뉴렐릭의 통합](/docs/infrastructure/introduction-infra-monitoring) 에는 EFS 데이터를 뉴렐릭에 보고하기 위한 Amazon EFS 통합이 포함되어 있습니다. 이 문서에서는 통합 기능, 통합을 활성화하는 방법, 보고할 수 있는 데이터 등을 설명합니다.

## 특징 [#features]

[AWS Elastic File System(EFS)](http://docs.aws.amazon.com/efs/latest/ug/whatisefs.html) 모니터링을 위한 New Relic의 통합으로 EFS 파일 시스템 크기, 읽기/쓰기 작업, I/O 용량, 처리량 등을 모니터링할 수 있습니다. AWS 통합 데이터는 [분석, 쿼리 및 차트 생성](/docs/infrastructure/infrastructure-integrations/getting-started/connect-aws-integrations-infrastructure#insights) 에도 사용할 수 있습니다.

[VPC](/docs/infrastructure/amazon-integrations/amazon-integrations/aws-vpc-monitoring-integration) 를 통해 연결된 경우 자체 온프레미스 서버와 함께 EFS 파일 시스템을 사용할 수도 있습니다. 이를 통해 하이브리드 솔루션에서 호스팅되는 다양한 애플리케이션 간에 파일 시스템을 공유할 수 있습니다.

## 통합 활성화 [#activate]

이 통합을 활성화하려면 [AWS 서비스를 New Relic에 연결](/docs/infrastructure/infrastructure-integrations/getting-started/connect-aws-integrations-infrastructure) 하기 위한 표준 절차를 따르십시오.

## 구성 및 폴링 [#polling]

[구성 옵션](/docs/integrations/new-relic-integrations/getting-started/configure-polling-frequency-data-collection-cloud-integrations) 을 사용하여 폴링 빈도를 변경하고 데이터를 필터링할 수 있습니다.

Amazon EFS 통합에 대한 [기본 폴링](/docs/infrastructure/amazon-integrations/aws-integrations-list/aws-polling-intervals-infrastructure-integrations) 정보:

* New Relic 폴링 간격: 5분
* Amazon CloudWatch 데이터 간격: 1분 또는 5분

## 데이터 찾기 및 사용 [#find-data]

이 통합의 데이터를 찾으려면 <DNT>**[one.newrelic.com &gt; All capabilities](https://one.newrelic.com/all-capabilities) &gt; Infrastructure &gt; AWS**</DNT> 으로 이동하여 Amazon EFS 통합 링크 중 하나를 선택하세요.

`provider` 값이 `EfsFileSystem` 인 `BlockDeviceSample` [이벤트 유형](/docs/data-apis/understand-data/new-relic-data-types/#event-data) 을 사용 [하여 데이터를 쿼리하고 탐색할](/docs/using-new-relic/data/understand-data/query-new-relic-data) 수 있습니다.

통합 데이터를 찾고 사용하는 방법에 대한 자세한 내용은 통합 데이터 [이해](/docs/infrastructure/integrations/find-use-infrastructure-integration-data) 를 참조하십시오.

## 측정항목 데이터 [#metrics]

이 통합은 다음 Amazon EFS 지표를 수집합니다.

<table>
  <thead>
    <tr>
      <th style={{ width: "250px" }}>
        미터법
      </th>

      <th>
        설명
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        `BurstCreditBalance`
      </td>

      <td>
        파일 시스템에 있는 버스트 크레딧 수입니다.

        버스트 크레딧을 사용하면 파일 시스템이 일정 기간 동안 파일 시스템의 기준 수준보다 높은 처리량 수준으로 버스트할 수 있습니다. 자세한 내용 [은 Amazon EFS의 처리량 조정 단원](http://docs.aws.amazon.com/efs/latest/ug/performance.html#bursting) 을 참조하십시오.

        `Minimum` 통계는 해당 기간 동안 1분 동안 가장 작은 버스트 크레딧 잔액입니다. `Maximum` 통계는 해당 기간 동안 1분 동안 가장 큰 버스트 크레딧 잔액입니다. `Average` 통계는 해당 기간 동안의 평균 버스트 크레딧 잔고입니다.

        단위: 바이트

        유효한 통계: `Minimum, Maximum, Average`
      </td>
    </tr>

    <tr>
      <td>
        `ClientConnections`
      </td>

      <td>
        파일 시스템에 대한 클라이언트 연결 수입니다. 표준 클라이언트를 사용하는 경우 탑재된 Amazon EC2 인스턴스당 하나의 연결이 있습니다.

        참고: 1분이 넘는 기간에 대한 평균 `ClientConnections` 을 계산하려면 `Sum` 통계를 기간의 분 수로 나눕니다.

        단위: 클라이언트 연결 수

        유효한 통계: `Sum`
      </td>
    </tr>

    <tr>
      <td>
        `DataReadIOBytes`
      </td>

      <td>
        각 파일 시스템 읽기 작업의 바이트 수입니다.

        `Sum` 통계는 읽기 작업과 관련된 총 바이트 수입니다. `Minimum` 통계는 기간 동안 가장 작은 읽기 작업의 크기입니다. `Maximum` 통계는 해당 기간 동안 가장 큰 읽기 작업의 크기입니다. `Average` 통계는 해당 기간 동안의 평균 읽기 작업 크기입니다. `SampleCount` 통계는 읽기 작업 수를 제공합니다.

        단위:

        * `Minimum, Maximum, Average,` 및 `Sum` 에 대한 바이트

        * `SampleCount` 을(를) 계산합니다.

          유효한 통계: `Minimum, Maximum, Average, Sum, SampleCount`
      </td>
    </tr>

    <tr>
      <td>
        `DataWriteIOBytes`
      </td>

      <td>
        각 파일 시스템 쓰기 작업의 바이트 수입니다.

        `Sum` 통계는 쓰기 작업과 관련된 총 바이트 수입니다. `Minimum` 통계는 기간 동안 가장 작은 쓰기 작업의 크기입니다. `Maximum` 통계는 기간 동안 가장 큰 쓰기 작업의 크기입니다. `Average` 통계는 해당 기간 동안의 평균 쓰기 작업 크기입니다. `SampleCount` 통계는 쓰기 작업 수를 제공합니다.

        단위:

        * 바이트는 `Minimum, Maximum, Average` 및 `Sum` 통계의 단위입니다.

        * `SampleCount` 을(를) 계산합니다.

          유효한 통계: `Minimum, Maximum, Average, Sum, SampleCount`
      </td>
    </tr>

    <tr>
      <td>
        `MetadataIOBytes`
      </td>

      <td>
        각 메타데이터 작업의 바이트 수입니다.

        `Sum` 통계는 메타데이터 작업과 연결된 총 바이트 수입니다. `Minimum` 통계는 기간 동안 가장 작은 메타데이터 작업의 크기입니다. `Maximum` 통계는 해당 기간 동안 가장 큰 메타데이터 작업의 크기입니다. `Average` 통계는 해당 기간 동안의 평균 메타데이터 작업의 크기입니다. `SampleCount` 통계는 메타데이터 작업 수를 제공합니다.

        단위:

        * 바이트는 `Minimum, Maximum, Average,` 및 `Sum` 통계의 단위입니다.

        * `SampleCount` 을(를) 계산합니다.

          유효한 통계: `Minimum, Maximum, Average, Sum, SampleCount`
      </td>
    </tr>

    <tr>
      <td>
        `PercentIOLimit`
      </td>

      <td>
        파일 시스템이 범용 성능 모드의 I/O 한계에 얼마나 근접했는지 보여줍니다. 이 메트릭이 100% 더 자주 발생하는 경우 최대 I/O 성능 모드를 사용하여 애플리케이션을 파일 시스템으로 이동하는 것을 고려하십시오.

        참고: 이 측정 단위는 범용 성능 모드를 사용하는 파일 시스템에 대해서만 제출됩니다.

        단위: 퍼센트
      </td>
    </tr>

    <tr>
      <td>
        `PermittedThroughput`
      </td>

      <td>
        파일 시스템 크기 및 `BurstCreditBalance` 이(가) 주어지면 파일 시스템이 허용하는 최대 처리량입니다. 자세한 내용은 [Amazon EFS 성능 단원](http://docs.aws.amazon.com/efs/latest/ug/performance.html) 을 참조하십시오.

        `Minimum` 통계는 해당 기간 동안 1분 동안 허용되는 가장 작은 처리량입니다. `Maximum` 통계는 해당 기간 동안 1분 동안 허용되는 최대 처리량입니다. `Average` 통계는 해당 기간 동안 허용되는 평균 처리량입니다.

        단위: 초당 바이트

        유효한 통계: `Minimum, Maximum, Average`
      </td>
    </tr>

    <tr>
      <td>
        `TotalIOBytes`
      </td>

      <td>
        데이터 읽기, 데이터 쓰기 및 메타데이터 작업을 포함한 각 파일 시스템 작업의 바이트 수입니다.

        `Sum` 통계는 모든 파일 시스템 작업과 관련된 총 바이트 수입니다. `Minimum` 통계는 기간 동안 가장 작은 작업의 크기입니다. `Maximum` 통계는 기간 동안 가장 큰 작업의 크기입니다. `Average` 통계는 기간 동안 작업의 평균 크기입니다. `SampleCount` 통계는 모든 작업의 수를 제공합니다.

        참고: 특정 기간의 초당 평균 작업을 계산하려면 `SampleCount` 통계를 해당 기간의 초 수로 나눕니다. 기간의 평균 처리량(초당 바이트 수)을 계산하려면 `Sum` 통계를 기간의 초 수로 나눕니다.

        단위:

        * `Minimum, Maximum, Average,` 및 `Sum` 통계의 바이트입니다.

        * `SampleCount` 을(를) 계산합니다.

          유효한 통계: `Minimum, Maximum, Average, Sum, SampleCount`
      </td>
    </tr>
  </tbody>
</table>