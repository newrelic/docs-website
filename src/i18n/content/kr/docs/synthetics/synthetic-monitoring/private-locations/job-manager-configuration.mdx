---
title: 합성 작업 관리자 구성
tags:
  - synthetics
  - Synthetic monitoring
  - Private locations
metaDescription: Customize your New Relic synthetics job manager.
freshnessValidatedDate: '2024-07-29T00:00:00.000Z'
translationType: machine
---

이 문서는 다음 방법을 보여줌으로써 [신세틱스 작업 관리자를](/docs/synthetics/synthetic-monitoring/private-locations/install-job-manager) 구성하는 과정을 안내합니다.

* [환경 변수를](#environment-variables) 사용하여 신세틱스 작업 관리자를 구성하세요.
* [스크립트 API](/docs/synthetics/synthetic-monitoring/scripting-monitors/write-synthetic-api-tests/) 또는 [스크립트](/docs/synthetics/new-relic-synthetics/scripting-monitors/write-scripted-browsers) 브라우저 모니터에 [대한 사용자 정의 모듈을 설정합니다.](#custom-modules)
* 설정에 [사용자 정의 변수를](#user-defined-vars) 제공하세요.

## 환경변수를 이용한 설정 [#environment-variables]

환경 변수를 사용하면 특정 환경 및 기능 요구 사항을 충족하도록 합성 작업 관리자 구성을 미세 조정할 수 있습니다.

<CollapserGroup>
  <Collapser id="docker-env-config" title="도커 환경 구성">
    변수는 시작 시 `-e, --env` 인수를 사용하여 제공됩니다.

    다음 표는 합성 작업 관리자가 지원하는 모든 환경 변수를 보여줍니다. `PRIVATE_LOCATION_KEY` 은 필수이고 다른 모든 변수는 선택 사항입니다.

    <table>
      <thead>
        <tr>
          <th>
            이름
          </th>

          <th>
            설명
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `PRIVATE_LOCATION_KEY`
          </td>

          <td>
            <DNT>**Required.**</DNT> 로케이션 로케이션 키는 사냥 로케이션 목록에서 찾을 수 있습니다.
          </td>
        </tr>

        <tr>
          <td>
            `DOCKER_API_VERSION`
          </td>

          <td>
            형식: `"vX.Y"` 지정된 Docker 서비스와 함께 사용할 API 버전입니다.

            기본: `v1.35.`
          </td>
        </tr>

        <tr>
          <td>
            `DOCKER_HOST`
          </td>

          <td>
            합성 작업 관리자가 지정된 `DOCKER_HOST` 을(를) 가리킵니다. 없는 경우 기본값은 `/var/run/docker.sock.`
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_ENDPOINT`
          </td>

          <td>
            미국 기반 계정의 경우 엔드포인트는 다음과 같습니다. `https://synthetics-horde.nr-data.net.`

            [EU 기반](/docs/using-new-relic/welcome-new-relic/get-started/introduction-eu-region-data-center#partner-hierarchy) 계정의 경우 엔드포인트는 다음과 같습니다. `https://synthetics-horde.eu01.nr-data.net/`

            모니터를 제공하기 위해 합성 작업 관리자가 적절한 엔드포인트에 연결할 수 있는지 확인하십시오.
          </td>
        </tr>

        <tr>
          <td>
            `DOCKER_REGISTRY`
          </td>

          <td>
            런타임 이미지가 호스팅되는 Docker 레지스트리 도메인입니다. 이를 사용하여 `docker.io` 을 기본값으로 재정의합니다.
          </td>
        </tr>

        <tr>
          <td>
            `DOCKER_REPOSITORY`
          </td>

          <td>
            런타임 이미지가 호스팅되는 도커 조직 또는 조직입니다. 이것을 사용하여 기본값인 `newrelic` 재정의합니다.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_HOST`
          </td>

          <td>
            Horde 통신에 사용되는 프록시 서버 호스트입니다. 형식: `"localhost"` .
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_PORT`
          </td>

          <td>
            Horde 통신에 사용되는 프록시 서버 포트입니다. 형식: `8888` .
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_USERNAME`
          </td>

          <td>
            Horde 통신에 사용되는 프록시 서버 사용자 이름입니다. 형식: `"username"` .
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_PW`
          </td>

          <td>
            Horde 통신에 사용되는 프록시 서버 비밀번호입니다. 형식: `"password"` .
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_ACCEPT_SELF_SIGNED_CERT`
          </td>

          <td>
            Horde 통신에 사용되는 프록시 서버 연결에 대해 자체 서명된 프록시 인증서를 수락하시겠습니까? 허용되는 값: `true`
          </td>
        </tr>

        <tr>
          <td>
            `CHECK_TIMEOUT`
          </td>

          <td>
            모니터 검사를 실행할 수 있는 최대 시간(초)입니다. 이 값은 0초(제외)에서 900초(포함) 사이의 정수여야 합니다(예: 1초에서 15분).

            기본값: 180초
          </td>
        </tr>

        <tr>
          <td>
            `LOG_LEVEL`
          </td>

          <td>
            기본: `INFO.`

            추가 옵션: `WARN`, `ERROR`, `DEBUG`
          </td>
        </tr>

        <tr>
          <td>
            `HEAVYWEIGHT_WORKERS`
          </td>

          <td>
            한 번에 실행할 수 있는 동시 중량 작업(브라우저/스크립트 브라우저 및 스크립트 API) 수입니다.

            기본값: 사용 가능한 CPU - 1.
          </td>
        </tr>

        <tr>
          <td>
            `DESIRED_RUNTIMES`
          </td>

          <td>
            특정 런타임 이미지를 실행하는 데 사용될 수 있는 기타입니다. 형식: \[&apos;newrelic/신세틱스-ping-runtime:latest&apos;,&apos;newrelic/신세틱스-node-API-runtime:latest&apos;,&apos;newrelic/신세틱스-node-브라우저-runtime:latest&apos;]

            기본값: 모든 최신 런타임.
          </td>
        </tr>

        <tr>
          <td>
            `VSE_PASSPHRASE`
          </td>

          <td>
            설정된 경우 <DNT>**verified script execution**</DNT> 활성화하고 이 값을 <DNT>**passphrase**</DNT> 로 사용합니다.
          </td>
        </tr>

        <tr>
          <td>
            `USER_DEFINED_VARIABLES`
          </td>

          <td>
            로컬에서 호스팅되는 사용자 정의 키 값 쌍 집합입니다.
          </td>
        </tr>

        <tr>
          <td>
            `ENABLE_WASM`
          </td>

          <td>
            설정된 경우 노드 브라우저 런타임을 위한 웹어셈블리를 활성화합니다. 웹어셈블리를 사용하려면 신세틱스 작업 관리자 최소 버전이 release-367 이상, 노드 브라우저 런타임 버전이 2.3.21 이상이어야 합니다.
          </td>
        </tr>
      </tbody>
    </table>
  </Collapser>

  <Collapser id="podman-env-config" title="Podman 환경 설정">
    변수는 시작 시 `-e, --env` 인수를 사용하여 제공됩니다.

    다음 표는 신세틱스 작업 관리자가 지원하는 모든 환경 변수를 표시합니다. `PRIVATE_LOCATION_KEY` 필수이고, 다른 모든 변수는 선택 사항입니다. Podman 환경에서 신세틱스 작업 관리자를 실행하려면 최소 버전이 release-418 이상이어야 합니다.

    <table>
      <thead>
        <tr>
          <th>
            이름
          </th>

          <th>
            설명
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `PRIVATE_LOCATION_KEY`
          </td>

          <td>
            <DNT>**Required.**</DNT> 로케이션 로케이션 키는 사냥 로케이션 목록에서 찾을 수 있습니다.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_ENDPOINT`
          </td>

          <td>
            미국 기반 계정의 경우 엔드포인트는 다음과 같습니다. `https://synthetics-horde.nr-data.net.`

            [EU 기반](/docs/using-new-relic/welcome-new-relic/get-started/introduction-eu-region-data-center#partner-hierarchy) 계정의 경우 엔드포인트는 다음과 같습니다. `https://synthetics-horde.eu01.nr-data.net/`

            모니터를 제공하기 위해 합성 작업 관리자가 적절한 엔드포인트에 연결할 수 있는지 확인하십시오.
          </td>
        </tr>

        <tr>
          <td>
            `PODMAN_API_SERVICE_HOST`
          </td>

          <td>
            파드에 추가된 호스트 항목은 SJM이 실행될 위치를 생성했습니다. 이것을 사용하여 기본값인 `podman.service` 재정의합니다.
          </td>
        </tr>

        <tr>
          <td>
            `PODMAN_API_SERVICE_PORT`
          </td>

          <td>
            인스턴스에서 Podman LibPod RESTful API 서비스가 실행되는 포트입니다. 이것을 사용하여 기본값인 `8000` 재정의합니다.
          </td>
        </tr>

        <tr>
          <td>
            `PODMAN_API_VERSION`
          </td>

          <td>
            사용되는 Podman LibPod RESTful API의 특정 버전입니다. 이것을 사용하여 기본값인 `v5.0.0` 재정의합니다.
          </td>
        </tr>

        <tr>
          <td>
            `PODMAN_POD_NAME`
          </td>

          <td>
            SJM컨버터가 실행되는 파드의 이름입니다. 이것을 사용하여 기본값인 `SYNTHETICS` 재정의합니다.
          </td>
        </tr>

        <tr>
          <td>
            `DOCKER_REGISTRY`
          </td>

          <td>
            런타임 이미지가 호스팅되는 Docker 레지스트리 도메인입니다. 이를 사용하여 `docker.io` 을 기본값으로 재정의합니다.
          </td>
        </tr>

        <tr>
          <td>
            `DOCKER_REPOSITORY`
          </td>

          <td>
            런타임 이미지가 호스팅되는 도커 조직 또는 조직입니다. 이것을 사용하여 기본값인 `newrelic` 재정의합니다.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_HOST`
          </td>

          <td>
            Horde 통신에 사용되는 프록시 서버 호스트입니다. 형식: `"localhost"` .
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_PORT`
          </td>

          <td>
            Horde 통신에 사용되는 프록시 서버 포트입니다. 형식: `8888` .
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_USERNAME`
          </td>

          <td>
            Horde 통신에 사용되는 프록시 서버 사용자 이름입니다. 형식: `"username"` .
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_PW`
          </td>

          <td>
            Horde 통신에 사용되는 프록시 서버 비밀번호입니다. 형식: `"password"` .
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_ACCEPT_SELF_SIGNED_CERT`
          </td>

          <td>
            Horde 통신에 사용되는 프록시 서버 연결에 대해 자체 서명된 프록시 인증서를 수락하시겠습니까? 허용되는 값: `true`
          </td>
        </tr>

        <tr>
          <td>
            `CHECK_TIMEOUT`
          </td>

          <td>
            모니터 검사를 실행할 수 있는 최대 시간(초)입니다. 이 값은 0초(제외)에서 900초(포함) 사이의 정수여야 합니다(예: 1초에서 15분).

            기본값: 180초
          </td>
        </tr>

        <tr>
          <td>
            `LOG_LEVEL`
          </td>

          <td>
            기본: `INFO.`

            추가 옵션: `WARN`, `ERROR`, `DEBUG`
          </td>
        </tr>

        <tr>
          <td>
            `HEAVYWEIGHT_WORKERS`
          </td>

          <td>
            한 번에 실행할 수 있는 동시 중량 작업(브라우저/스크립트 브라우저 및 스크립트 API) 수입니다.

            기본값: 사용 가능한 CPU - 1.
          </td>
        </tr>

        <tr>
          <td>
            `DESIRED_RUNTIMES`
          </td>

          <td>
            특정 런타임 이미지를 실행하는 데 사용될 수 있는 기타입니다. 형식: \[&apos;newrelic/신세틱스-ping-runtime:latest&apos;,&apos;newrelic/신세틱스-node-API-runtime:latest&apos;,&apos;newrelic/신세틱스-node-브라우저-runtime:latest&apos;]

            기본값: 모든 최신 런타임.
          </td>
        </tr>

        <tr>
          <td>
            `VSE_PASSPHRASE`
          </td>

          <td>
            설정된 경우 <DNT>**verified script execution**</DNT> 활성화하고 이 값을 <DNT>**passphrase**</DNT> 로 사용합니다.
          </td>
        </tr>

        <tr>
          <td>
            `USER_DEFINED_VARIABLES`
          </td>

          <td>
            로컬에서 호스팅되는 사용자 정의 키 값 쌍 집합입니다.
          </td>
        </tr>

        <tr>
          <td>
            `ENABLE_WASM`
          </td>

          <td>
            설정된 경우 노드 브라우저 런타임을 위한 웹어셈블리를 활성화합니다. 웹어셈블리를 사용하려면 신세틱스 작업 관리자 최소 버전이 release-367 이상, 노드 브라우저 런타임 버전이 2.3.21 이상이어야 합니다.
          </td>
        </tr>
      </tbody>
    </table>
  </Collapser>

  <Collapser id="kubernetes-env-config" title="Kubernetes 환경 구성">
    변수는 시작 시 `--set` 인수를 사용하여 제공됩니다.

    다음 목록은 합성 작업 관리자가 지원하는 모든 환경 변수를 보여줍니다. `synthetics.privateLocationKey` 은 필수이고 다른 모든 변수는 선택 사항입니다.

    많은 추가 고급 설정을 사용할 수 있으며 [Helm 차트 README](https://github.com/newrelic/helm-charts/blob/master/charts/synthetics-job-manager/README.md) 에 완전히 문서화되어 있습니다.

    <table>
      <thead>
        <tr>
          <th>
            이름
          </th>

          <th>
            설명
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `synthetics.privateLocationKey`
          </td>

          <td>
            <DNT>**Required if `synthetics.privateLocationKeySecretName` is not set**</DNT>. 레지스터 로케이션 웹페이지에서 찾을 수 있는 [레지스터 로케이션의 키입니다](/docs/synthetics/synthetic-monitoring/private-locations/install-job-manager/#private-location-key) .
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.privateLocationKeySecretName`
          </td>

          <td>
            <DNT>**Required if `synthetics.privateLocationKey` is not set**</DNT>. 신세틱스 파트 로케이션과 연결된 인증 키가 포함된 `privateLocationKey` 키가 포함된 Kubernetes 비밀의 이름입니다.
          </td>
        </tr>

        <tr>
          <td>
            `imagePullSecrets`
          </td>

          <td>
            지정된 컨테이너 레지스트리에서 이미지를 가져오는 데 사용되는 비밀 개체의 이름입니다.
          </td>
        </tr>

        <tr>
          <td>
            `fullnameOverride`
          </td>

          <td>
            배포에 사용되는 이름 재정의로 기본값을 대체합니다.
          </td>
        </tr>

        <tr>
          <td>
            `appVersionOverride`
          </td>

          <td>
            [chart.yml](https://github.com/newrelic/helm-charts/blob/master/charts/synthetics-job-manager/Chart.yaml) 에 지정된 버전 대신 사용할 합성 작업 관리자의 릴리스 버전입니다.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.logLevel`
          </td>

          <td>
            기본: `INFO.`

            추가 옵션: `WARN`, `ERROR`
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.hordeApiEndpoint`
          </td>

          <td>
            미국 기반 계정의 경우 엔드포인트는 다음과 같습니다. `https://synthetics-horde.nr-data.net.`

            [EU 기반](/docs/using-new-relic/welcome-new-relic/get-started/introduction-eu-region-data-center#partner-hierarchy) 계정의 경우 엔드포인트는 다음과 같습니다. `https://synthetics-horde.eu01.nr-data.net/`

            모니터를 제공하기 위해 합성 작업 관리자가 적절한 엔드포인트에 연결할 수 있는지 확인하십시오.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.minionDockerRunnerRegistryEndpoint`
          </td>

          <td>
            Minion Runner 이미지가 호스팅되는 Docker 레지스트리 및 조직입니다. 이를 사용하여 `quay.io/newrelic` 을 기본값으로 재정의합니다(예: `docker.io/newrelic` ).
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.vsePassphrase`
          </td>

          <td>
            설정된 경우 <DNT>**verified script execution**</DNT> 활성화하고 이 값을 <DNT>**passphrase**</DNT> 로 사용합니다.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.vsePassphraseSecretName`
          </td>

          <td>
            설정된 경우 확인된 스크립트 실행을 활성화하고 이 값을 사용하여 `vsePassphrase` 라는 키가 있는 Kubernetes 비밀에서 암호를 검색합니다.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.enableWasm`
          </td>

          <td>
            설정된 경우 노드 브라우저 런타임을 위한 웹어셈블리를 활성화합니다. 웹어셈블리를 사용하려면 신세틱스 작업 관리자 최소 버전이 release-367 이상, 노드 브라우저 런타임 버전이 2.3.21 이상이어야 합니다.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.apiProxyHost`
          </td>

          <td>
            호드 통신에 사용되는 프록시 서버입니다. 형식: `"host"` .
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.apiProxyPort`
          </td>

          <td>
            Horde 통신에 사용되는 프록시 서버 포트입니다. 형식: `port` .
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.hordeApiProxySelfSignedCert`
          </td>

          <td>
            Horde 통신에 프록시 서버를 사용할 때 자체 서명된 인증서를 수락합니다. 허용되는 값: `true` .
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.hordeApiProxyUsername`
          </td>

          <td>
            Horde 통신을 위한 프록시 서버 사용자 이름입니다. 체재: `"username"`
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.hordeApiProxyPw`
          </td>

          <td>
            Horde 통신을 위한 프록시 서버 비밀번호입니다. 형식: `"password"` .
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.userDefinedVariables.userDefinedJson`
          </td>

          <td>
            사용자 정의 변수의 JSON 문자열입니다. 사용자는 스크립트에서 이러한 변수에 액세스할 수 있습니다. 형식: `'{"key":"value","key2":"value2"}'`.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.userDefinedVariables.userDefinedFile`
          </td>

          <td>
            사용자 정의 변수가 포함된 JSON 파일에 대한 사용자 로컬 경로입니다. 이는 `--set-file` 통해 전달되며 값 파일에 설정할 수 없습니다.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.userDefinedVariables.userDefinedPath`
          </td>

          <td>
            user\_define\_variables.json 파일에 대한 사용자가 제공한 PertantVolume의 경로입니다.이 변수가 채워지면 사용자는 PertantVolume 또는 PertantVolumeClaim을 제공해야 합니다.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.persistence.existingClaimName`
          </td>

          <td>
            볼륨을 탑재하는 경우 사용자는 클러스터에 이미 존재하는 PertantVolumeClaim의 이름을 제공할 수 있습니다. 해당 PertantVolume이 존재한다고 가정합니다.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.persistence.existingVolumeName`
          </td>

          <td>
            볼륨을 마운트하고 PertantVolumeClaim을 제공하지 않는 경우 사용자는 최소한 PertantVolume 이름을 제공해야 합니다. Helm은 PertantVolumeClaim을 생성합니다.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.persistence.storageClass`
          </td>

          <td>
            생성된 PertantVolumeClaim에 대한 StorageClass의 이름입니다. 이는 기존 PV의 StorageClassName과 일치해야 합니다. 공급자가 아닌 경우 Kubernetes는 기본 스토리지 클래스(있는 경우)를 사용합니다.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.persistence.size`
          </td>

          <td>
            생성된 PertantVolumeClaim의 볼륨 크기입니다. 형식: `10Gi`. 기본 2Gi.
          </td>
        </tr>

        <tr>
          <td>
            `global.checkTimeout`
          </td>

          <td>
            모니터 검사를 실행할 수 있는 최대 시간(초)입니다. 이 값은 0초(제외)에서 900초(포함) 사이의 정수여야 합니다(예: 1초에서 15분).

            기본값: 180초
          </td>
        </tr>

        <tr>
          <td>
            `image.repository`
          </td>

          <td>
            가져올 컨테이너입니다.

            기본: `docker.io/newrelic/synthetics-job-runner`
          </td>
        </tr>

        <tr>
          <td>
            `image.pullPolicy`
          </td>

          <td>
            끌어오기 정책.

            기본: `IfNotPresent`
          </td>
        </tr>

        <tr>
          <td>
            `podSecurityContext`
          </td>

          <td>
            합성 작업 관리자 팟(Pod)에 대한 사용자 정의 보안 컨텍스트를 설정하십시오.
          </td>
        </tr>

        <tr>
          <td>
            `ping-runtime.enabled`
          </td>

          <td>
            영구 핑 런타임을 배포해야 하는지 여부입니다. ping 모니터를 사용하지 않는 경우 비활성화할 수 있습니다.

            기본: `true`
          </td>
        </tr>

        <tr>
          <td>
            `ping-runtime.replicaCount`
          </td>

          <td>
            배포할 ping 런타임 컨테이너의 수입니다. ping 모니터링 요구 사항에 따라 배포를 확장하려면 replicaCount를 늘립니다.

            기본: `1`
          </td>
        </tr>

        <tr>
          <td>
            `ping-runtime.image.repository`
          </td>

          <td>
            ping 런타임을 위해 가져올 컨테이너 이미지입니다.

            기본: `docker.io/newrelic/synthetics-ping-runtime`
          </td>
        </tr>

        <tr>
          <td>
            `ping-runtime.image.pullPolicy`
          </td>

          <td>
            ping-runtime 컨테이너에 대한 pull 정책입니다.

            기본: `IfNotPresent`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.enabled`
          </td>

          <td>
            Node.js API 런타임을 배포해야 하는지 여부입니다. 스크립팅된 API 모니터를 사용하지 않는 경우 비활성화할 수 있습니다.

            기본: `true`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.parallelism`
          </td>

          <td>
            배포할 Node.js API 런타임 `CronJobs` 의 수입니다. 언제든지 실행할 최대 동시 Node.js API 작업 수입니다. [추가 세부정보](#kubernetes-sizing) .

            기본: `1`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.completions`
          </td>

          <td>
            분당 완료할 Node.js API 런타임 `CronJobs` 의 수입니다. 처리량을 향상시키려면 병렬 처리와 함께 이 설정을 늘리십시오. 병렬 처리가 증가할 때마다 이 값을 늘려야 하며 완료는 항상 병렬 처리보다 크거나 같아야 합니다. . API 런타임 작업이 실행되지 않는 기간이 있는 경우 이 설정을 늘리십시오. [추가 세부정보](#kubernetes-sizing) .

            기본: `6`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.image.repository`
          </td>

          <td>
            Node.js API 런타임에 대해 가져올 컨테이너 이미지입니다.

            기본: `docker.io/newrelic/synthetics-node-api-runtime`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.image.pullPolicy`
          </td>

          <td>
            Node.js API 런타임 컨테이너에 대한 풀 정책입니다.

            기본: `IfNotPresent`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.enabled`
          </td>

          <td>
            Node.js 브라우저 런타임을 배포해야 하는지 여부입니다. 단순 또는 스크립팅된 브라우저 모니터를 사용하지 않는 경우 비활성화할 수 있습니다.

            기본: `true`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.parallelism`
          </td>

          <td>
            배포할 Chrome 브라우저 런타임 `CronJobs` 의 수입니다. 언제든지 실행할 최대 동시 Chrome 브라우저 작업 수입니다. [추가 세부정보](#kubernetes-sizing) .

            기본: `1`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.completions`
          </td>

          <td>
            분당 완료할 Chrome 브라우저 런타임 `CronJobs` 의 수입니다. 처리량을 향상시키려면 병렬 처리와 함께 이 설정을 늘리십시오. 병렬 처리가 증가할 때마다 이 값을 늘려야 하며 완료는 항상 병렬 처리보다 크거나 같아야 합니다. 브라우저 런타임 작업이 실행되지 않는 기간이 있는 경우 이 설정을 늘리십시오. [추가 세부정보](#kubernetes-sizing) .

            기본: `6`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.image.repository`
          </td>

          <td>
            Node.js 브라우저 런타임에 대해 가져올 컨테이너 이미지입니다.

            기본: `docker.io/newrelic/synthetics-node-browser-runtime`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.image.pullPolicy`
          </td>

          <td>
            Node.js 브라우저 런타임 컨테이너에 대한 풀 정책입니다.

            기본: `IfNotPresent`
          </td>
        </tr>
      </tbody>
    </table>
  </Collapser>

  <Collapser id="openshift-environment-config" title="OpenShift 환경 설정">
    변수는 시작 시 `--set` 인수를 사용하여 제공됩니다.

    다음 목록은 합성 작업 관리자가 지원하는 모든 환경 변수를 보여줍니다. `synthetics.privateLocationKey` 은 필수이고 다른 모든 변수는 선택 사항입니다.

    많은 추가 고급 설정을 사용할 수 있으며 [Helm 차트 README](https://github.com/newrelic/helm-charts/blob/master/charts/synthetics-job-manager/README.md) 에 완전히 문서화되어 있습니다.

    <table>
      <thead>
        <tr>
          <th>
            이름
          </th>

          <th>
            설명
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `synthetics.privateLocationKey`
          </td>

          <td>
            <DNT>**Required**</DNT>. [사냥로케이션 키](/docs/synthetics/synthetic-monitoring/private-locations/install-job-manager/#private-location-key), 사냥로케이션 목록에서 찾을 수 있습니다.
          </td>
        </tr>

        <tr>
          <td>
            `imagePullSecrets`
          </td>

          <td>
            지정된 컨테이너 레지스트리에서 이미지를 가져오는 데 사용되는 비밀 개체의 이름입니다.
          </td>
        </tr>

        <tr>
          <td>
            `fullnameOverride`
          </td>

          <td>
            배포에 사용되는 이름 재정의로 기본값을 대체합니다.
          </td>
        </tr>

        <tr>
          <td>
            `appVersionOverride`
          </td>

          <td>
            [chart.yml](https://github.com/newrelic/helm-charts/blob/master/charts/synthetics-job-manager/Chart.yaml) 에 지정된 버전 대신 사용할 합성 작업 관리자의 릴리스 버전입니다.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.logLevel`
          </td>

          <td>
            기본: `INFO.`

            추가 옵션: `WARN`, `ERROR`
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.hordeApiEndpoint`
          </td>

          <td>
            미국 기반 계정의 경우 엔드포인트는 다음과 같습니다. `https://synthetics-horde.nr-data.net.`

            [EU 기반](/docs/using-new-relic/welcome-new-relic/get-started/introduction-eu-region-data-center#partner-hierarchy) 계정의 경우 엔드포인트는 다음과 같습니다. `https://synthetics-horde.eu01.nr-data.net/`

            모니터를 제공하기 위해 합성 작업 관리자가 적절한 엔드포인트에 연결할 수 있는지 확인하십시오.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.vsePassphrase`
          </td>

          <td>
            설정된 경우 <DNT>**verified script execution**</DNT> 활성화하고 이 값을 <DNT>**passphrase**</DNT> 로 사용합니다.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.vsePassphraseSecretName`
          </td>

          <td>
            설정된 경우 확인된 스크립트 실행을 활성화하고 이 값을 사용하여 `vsePassphrase` 라는 키가 있는 Kubernetes 비밀에서 암호를 검색합니다.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.enableWasm`
          </td>

          <td>
            설정된 경우 노드 브라우저 런타임을 위한 웹어셈블리를 활성화합니다. 웹어셈블리를 사용하려면 신세틱스 작업 관리자 최소 버전이 release-367 이상, 노드 브라우저 런타임 버전이 2.3.21 이상이어야 합니다.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.apiProxyHost`
          </td>

          <td>
            호드 통신에 사용되는 프록시 서버입니다. 형식: `"host"` .
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.apiProxyPort`
          </td>

          <td>
            Horde 통신에 사용되는 프록시 서버 포트입니다. 형식: `port` .
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.hordeApiProxySelfSignedCert`
          </td>

          <td>
            Horde 통신에 프록시 서버를 사용할 때 자체 서명된 인증서를 수락합니다. 허용되는 값: `true` .
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.hordeApiProxyUsername`
          </td>

          <td>
            Horde 통신을 위한 프록시 서버 사용자 이름입니다. 체재: `"username"`
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.hordeApiProxyPw`
          </td>

          <td>
            Horde 통신을 위한 프록시 서버 비밀번호입니다. 형식: `"password"` .
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.userDefinedVariables.userDefinedJson`
          </td>

          <td>
            사용자 정의 변수의 JSON 문자열입니다. 사용자는 스크립트에서 이러한 변수에 액세스할 수 있습니다. 형식: `'{"key":"value","key2":"value2"}'`.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.userDefinedVariables.userDefinedFile`
          </td>

          <td>
            사용자 정의 변수가 포함된 JSON 파일에 대한 사용자 로컬 경로입니다. 이는 `--set-file` 통해 전달되며 값 파일에 설정할 수 없습니다.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.userDefinedVariables.userDefinedPath`
          </td>

          <td>
            사용자가 제공한 `PersistentVolume` 의` user_defined_variables.json` 파일에 대한 경로입니다. 이 변수가 채워지면 사용자는 `PersistentVolume` 또는 `PersistentVolumeClaim` 제공해야 합니다.
          </td>
        </tr>

        <tr>
          <td>
            `global.persistence.existingClaimName`
          </td>

          <td>
            볼륨을 마운트하는 경우 사용자는 클러스터에 이미 있는 `PersistentVolumeClaim` 에 대한 이름을 제공할 수 있습니다. 해당 `PersistentVolume` 의 존재를 가정합니다.
          </td>
        </tr>

        <tr>
          <td>
            `global.persistence.existingVolumeName`
          </td>

          <td>
            볼륨을 마운트하고 `PersistentVolumeClaim` 을 제공하지 않는 경우 사용자는 최소한 `PersistentVolume` 이름을 제공해야 합니다. Helm은 `PersistentVolumeClaim` 생성합니다.
          </td>
        </tr>

        <tr>
          <td>
            `global.persistence.storageClass`
          </td>

          <td>
            생성된 `PersistentVolumeClaim` 에 대한 `StorageClass` 의 이름입니다. 이는 기존 PV의 `StorageClassName` 와 일치해야 합니다. 공급자가 아닌 경우 **Kubernetes는** 기본 스토리지 클래스가 있는 경우 이를 사용합니다.
          </td>
        </tr>

        <tr>
          <td>
            `global.persistence.size`
          </td>

          <td>
            생성된 `PersistentVolumeClaim` 에 대한 볼륨의 크기입니다. 형식: `10Gi`. 기본값 `2Gi`.
          </td>
        </tr>

        <tr>
          <td>
            `global.checkTimeout`
          </td>

          <td>
            모니터 검사를 실행할 수 있는 최대 시간(초)입니다. 이 값은 0초(제외)에서 900초(포함) 사이의 정수여야 합니다(예: 1초에서 15분).

            기본값: 180초
          </td>
        </tr>

        <tr>
          <td>
            `image.repository`
          </td>

          <td>
            가져올 컨테이너입니다.

            기본: `docker.io/newrelic/synthetics-job-runner`
          </td>
        </tr>

        <tr>
          <td>
            `image.pullPolicy`
          </td>

          <td>
            끌어오기 정책.

            기본: `IfNotPresent`
          </td>
        </tr>

        <tr>
          <td>
            `podSecurityContext`
          </td>

          <td>
            `synthetics-job-manager` 파드에 대한 사용자 지정 보안 컨텍스트를 설정합니다.
          </td>
        </tr>

        <tr>
          <td>
            `ping-runtime.enabled`
          </td>

          <td>
            영구 핑 런타임을 배포해야 하는지 여부입니다. ping 모니터를 사용하지 않는 경우 비활성화할 수 있습니다.

            기본: `true`
          </td>
        </tr>

        <tr>
          <td>
            `ping-runtime.replicaCount`
          </td>

          <td>
            구현하다, 배포하다에 대한 ping 런타임 컨테이너의 수입니다. 핑 모니터링 요구 사항에 따라 구현, 배포를 확장하려면 `replicaCount` 를 늘립니다.

            기본: `1`
          </td>
        </tr>

        <tr>
          <td>
            `ping-runtime.image.repository`
          </td>

          <td>
            ping 런타임을 위해 가져올 컨테이너 이미지입니다.

            기본: `docker.io/newrelic/synthetics-ping-runtime`
          </td>
        </tr>

        <tr>
          <td>
            `ping-runtime.image.pullPolicy`
          </td>

          <td>
            ping-runtime 컨테이너에 대한 pull 정책입니다.

            기본: `IfNotPresent`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.enabled`
          </td>

          <td>
            Node.js API 런타임을 배포해야 하는지 여부입니다. 스크립팅된 API 모니터를 사용하지 않는 경우 비활성화할 수 있습니다.

            기본: `true`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.parallelism`
          </td>

          <td>
            배포할 Node.js API 런타임 `CronJobs` 의 수입니다. 언제든지 실행할 최대 동시 Node.js API 작업 수입니다. [추가 세부정보](#kubernetes-sizing) .

            기본: `1`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.completions`
          </td>

          <td>
            분당 완료할 Node.js API 런타임 `CronJobs` 의 수입니다. 처리량을 향상시키려면 병렬 처리와 함께 이 설정을 늘리십시오. 병렬 처리가 증가할 때마다 이 값을 늘려야 하며 완료는 항상 병렬 처리보다 크거나 같아야 합니다. . API 런타임 작업이 실행되지 않는 기간이 있는 경우 이 설정을 늘리십시오. [추가 세부정보](#kubernetes-sizing) .

            기본: `6`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.image.repository`
          </td>

          <td>
            Node.js API 런타임에 대해 가져올 컨테이너 이미지입니다.

            기본: `docker.io/newrelic/synthetics-node-api-runtime`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.image.pullPolicy`
          </td>

          <td>
            Node.js API 런타임 컨테이너에 대한 풀 정책입니다.

            기본: `IfNotPresent`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.enabled`
          </td>

          <td>
            Node.js 브라우저 런타임을 배포해야 하는지 여부입니다. 단순 또는 스크립팅된 브라우저 모니터를 사용하지 않는 경우 비활성화할 수 있습니다.

            기본: `true`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.parallelism`
          </td>

          <td>
            배포할 Chrome 브라우저 런타임 `CronJobs` 의 수입니다. 언제든지 실행할 최대 동시 Chrome 브라우저 작업 수입니다. [추가 세부정보](#kubernetes-sizing) .

            기본: `1`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.completions`
          </td>

          <td>
            분당 완료할 Chrome 브라우저 런타임 `CronJobs` 의 수입니다. 처리량을 향상시키려면 병렬 처리와 함께 이 설정을 늘리십시오. 병렬 처리가 증가할 때마다 이 값을 늘려야 하며 완료는 항상 병렬 처리보다 크거나 같아야 합니다. 브라우저 런타임 작업이 실행되지 않는 기간이 있는 경우 이 설정을 늘리십시오. [추가 세부정보](#kubernetes-sizing) .

            기본: `6`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.image.repository`
          </td>

          <td>
            Node.js 브라우저 런타임에 대해 가져올 컨테이너 이미지입니다.

            기본: `docker.io/newrelic/synthetics-node-browser-runtime`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.image.pullPolicy`
          </td>

          <td>
            Node.js 브라우저 런타임 컨테이너에 대한 풀 정책입니다.

            기본: `IfNotPresent`
          </td>
        </tr>
      </tbody>
    </table>
  </Collapser>
</CollapserGroup>

## 스크립트 모니터에 대한 사용자 정의 변수 [#user-defined-vars]

개인 신세틱스 작업 관리자를 사용하면 스크립트된 모니터에 대한 환경 변수를 구성할 수 있습니다. 이러한 변수는 SJM에서 로컬로 관리되며 `$env.USER_DEFINED_VARIABLES` 통해 액세스할 수 있습니다. 사용자 정의 변수는 두 가지 방법으로 설정할 수 있습니다. 등장에서 JSON 파일을 마운트하거나 SJM에 환경 변수를 제공할 수 있습니다. 둘 다 제공되는 경우 SJM은 환경에서 제공되는 값만 사용합니다.

<CollapserGroup>
  <Collapser id="user-file-example" title="JSON 파일 마운트">
    사용자는 JSON 형식의 파일을 생성하고 해당 파일이 있는 볼륨을 SJM 컨테이너의 지정된 바구니, 목표 경로에 마운트할 수 있습니다.

    파일에는 읽기 권한이 있어야 하며 JSON 형식의 맵이 포함되어 있어야 합니다. 사용자 정의 변수 파일의 예:

    ```json
    {
      "KEY": "VALUE",
      "user_name": "MINION",
      "my_password": "PASSW0RD123",
      "my_URL": "https://newrelic.com/",
      "ETC": "ETC"
    }
    ```

    파일을 호스트의 소스 디렉터리에 배치합니다. SJM은 파일 이름이 user\_define\_variables.json이 될 것으로 예상합니다.

    도커 예시:

    예상되는 목표 디렉토리는 다음과 같습니다: `/var/lib/newrelic/synthetics/variables/`

    ```sh
    docker run ... -v /variables:/var/lib/newrelic/synthetics/variables:rw ...
    ```

    포드만 예:

    SELinux의 경우 `:z` 또는 `:Z` 사용하여 볼륨을 추가로 마운트합니다. 자세한 내용은 [Podman 문서를 참조하세요.](https://docs.podman.io/en/latest/markdown/podman-run.1.html#volume-v-source-volume-host-dir-container-dir-options) 예상되는 목표 디렉토리는 다음과 같습니다: `/var/lib/newrelic/synthetics/variables/`

    ```sh
    podman run ... -v /variables:/var/lib/newrelic/synthetics/variables:rw,z ...
    ```

    쿠버네티스 예시:

    사용자는 Kubernetes 의 SJM 패드에 파일을 제공할 때 두 가지 옵션이 있습니다. 그들은 아마도:

    * 로컬 파일을 전달합니다.
    * `user_defined_variables.json` 포함하는 PersistentVolume을 제공합니다.

    ### 로컬 파일 전달

    이 옵션은 ConfigMap Kubernetes 리소스를 생성하고 이를 SJM 패드에 탑재합니다.

    ```sh
    helm install newrelic/synthetics-job-manager ... --set-file "synthetics.userDefinedVariables.userDefinedFile=[local-path]/user_defined_variables.json" ...
    ```

    ### 마운트하다 `PersistentVolume`

    이 옵션을 사용하려면 사용자가 `user_defined_variables.json` 파일을 포함하는 `PersistentVolume` 또는 동일한 파일에 대한 `PersistentVolumeClaim` 를 제공해야 합니다. `PersistentVolume` 사용하여 헬름 차트 설치에 대한 자세한 내용은 [영구 데이터 저장소](/docs/synthetics/synthetic-monitoring/private-locations/job-manager-configuration#permanent-data-storage) 의 지침을 따르세요.

    사용자가 아래 설명한 대로 `PersistentVolume` 을 준비하면 `user_defined_variables.json` 파일이 있는 경로를 설정하고 필요에 따라 다른 `synthetics.persistence` 변수를 설정하여 SJM을 릴리스합니다.

    ```sh
    helm install newrelic/synthetics-job-manger ... --set synthetics.userDefinedVariables.userDefinedPath="variables"
    ```
  </Collapser>

  <Collapser id="passing-env-var" title="환경 변수로 전달">
    변수는 환경 변수를 통해 해당 컨테이너 시스템으로 전달될 수 있습니다.

    도커 예시:

    `-e` 플래그를 사용하여 `USER_DEFINED_VARIABLES` 이라는 환경 변수를 설정하고 여기에 JSON 형식의 맵 문자열 값을 제공합니다.

    ```sh
    docker run ... -e USER_DEFINED_VARIABLES='{"key":"value","name":"sjm"}' ...
    ```

    포드만 예:

    `-e` 플래그를 사용하여 `USER_DEFINED_VARIABLES` 이라는 환경 변수를 설정하고 여기에 JSON 형식의 맵 문자열 값을 제공합니다.

    ```sh
    podman run ... -e USER_DEFINED_VARIABLES='{"key":"value","name":"sjm"}' ...
    ```

    쿠버네티스 예시:

    `--set-literal` 플래그를 사용하여 JSON 형식의 문자열을 전달합니다.

    ```sh
    helm install newrelic/synthetics-job-manager ... --set-literal synthetics.userDefinedVariables.userDefinedJson='{"key":"value","name":"sjm"}' ...
    ```
  </Collapser>
</CollapserGroup>

### 스크립트에서 사용자 정의 환경 변수에 액세스 [#env-vars-scripts]

구성된 사용자 정의 환경 변수를 참조하려면 예약된 `$env.USER_DEFINED_VARIABLES` 뒤에 점 표기법을 사용하여 지정된 변수 이름을 입력합니다(예: `$env.USER_DEFINED_VARIABLES.MY_VARIABLE`).

<Callout variant="caution">
  사용자 정의 환경 변수는 로그에서 삭제되지 않습니다. 민감한 정보에는 [보안 자격 증명](/docs/synthetics/new-relic-synthetics/using-monitors/secure-credentials-store-credentials-information-scripted-browsers) 기능을 사용하는 것이 좋습니다.
</Callout>

## 커스텀 노드 모듈 [#custom-modules]

맞춤형 노드 모듈은 분당호출수와 SJM 모두에서 제공됩니다. 이를 통해 사용자 정의된 [노드 모듈](https://docs.npmjs.com/about-packages-and-modules) 세트를 생성하고 이를 신세틱 모델링을 위한 스크립트 모니터(스크립트 API 및 스크립트 브라우저)에서 사용할 수 있습니다.

### 사용자 정의 모듈 디렉토리 설정

루트 폴더에 [npm 공식 지침에](https://docs.npmjs.com/files/package.json) 따라 `package.json` 파일이 포함된 디렉터리를 만듭니다. SJM은 package.json에 나열된 의존성/종속성을 설치합니다. `dependencies` 필드. 이러한 의존성/종속성은 개인 신세틱스 작업 관리자에서 모니터를 실행할 때 사용할 수 있습니다. 아래의 예를 참조하세요.

#### 예시

이 예에서 사용자 정의 모듈 디렉토리는 다음 구조로 사용됩니다.

```
/example-custom-modules-dir/
    ├── counter
    │   ├── index.js
    │   └── package.json
    └── package.json            ⇦ the only mandatory file
```

`package.json` 는 `dependencies` 로컬 모듈(예: `counter`)과 호스팅된 모듈(예: `smallest` 버전 `1.0.1`)로 정의합니다.

```json
{
    "name": "custom-modules",
    "version": "1.0.0",                                ⇦ optional
    "description": "example custom modules directory", ⇦ optional
    "dependencies": {
    "smallest": "1.0.1",                               ⇦ hosted module
    "counter": "file:./counter"                        ⇦ local module
    }
}
```

### docker, Podman 또는 Kubernetes용 SJM에 사용자 정의 모듈 디렉토리를 추가합니다.

<CollapserGroup>
  <Collapser id="docker" title="도커">
    docker 의 경우 SJM이 `/var/lib/newrelic/synthetics/modules`에 디렉터리를 마운트합니다. 예를 들어:

    ```sh
    docker run ... -v /example-custom-modules-dir:/var/lib/newrelic/synthetics/modules:rw ...
    ```
  </Collapser>

  <Collapser id="podman" title="포드만">
    Podman의 경우 SJM이 `/var/lib/newrelic/synthetics/modules` 에 디렉토리를 마운트합니다. SELinux의 경우 `:z` 또는 `:Z` 사용하여 볼륨을 추가로 마운트합니다. 자세한 내용은 [Podman 문서를](https://docs.podman.io/en/latest/markdown/podman-run.1.html#volume-v-source-volume-host-dir-container-dir-options) 참조하세요. 예를 들어:

    ```sh
    podman run ... -v /example-custom-modules-dir:/var/lib/newrelic/synthetics/modules:rw,z ...
    ```
  </Collapser>

  <Collapser id="kubernetes" title="Kubernetes">
    Kubernetes의 경우 사용자 정의 모듈이 활성화된 SJM을 시작하기 전에 `/var/lib/newrelic/synthetics/modules` 의 디렉토리가 PV에 존재해야 합니다.

    <Callout variant="tip">
      여러 개의 드라이브에서 저장소를 공유해야 하는 경우 PV 액세스 모드는 ReadWriteMany여야 합니다.
    </Callout>

    한 가지 방법은 사용자 정의 모듈 디렉토리를 PV에 복사하기 위해 PV를 마운트하는 파드를 만드는 것입니다. 다음 예제에서는 Amazon EKS와 함께 Amazon EFS를 사용합니다.

    #### 네임스페이스, 영구 볼륨 및 영구 볼륨 클레임을 생성합니다.

    1. 클러스터에 EFS 파일 시스템을 설정하고 [EFS CSI 드라이버를](https://github.com/kubernetes-sigs/aws-efs-csi-driver) 설치했는지 확인하세요. PV의 `spec.csi.volumeHandle` 에 대한 EFS 파일 시스템 ID도 필요합니다.

       ```sh
       kubectl apply -f - <<EOF
       apiVersion: v1
       kind: Namespace
       metadata:
         name: newrelic

       ---
       kind: StorageClass
       apiVersion: storage.k8s.io/v1
       metadata:
         name: efs-sc
       provisioner: efs.csi.aws.com

       ---
       apiVersion: v1
       kind: PersistentVolume
       metadata:
         name: custom-modules-pvc
       spec:
         capacity:
           storage: 5Gi
         volumeMode: Filesystem
         accessModes:
           - ReadWriteMany
         persistentVolumeReclaimPolicy: Retain
         storageClassName: efs-sc
         csi:
           driver: efs.csi.aws.com
           volumeHandle: <your-efs-filesystem-id>

       ---
       apiVersion: v1
       kind: PersistentVolumeClaim
       metadata:
         name: custom-modules-pvc
         namespace: newrelic
       spec:
         accessModes:
           - ReadWriteMany
         storageClassName: efs-sc
         resources:
           requests:
             storage: 5Gi
       EOF
       ```

    2. `~/.kube/config` 의 `newrelic` 네임스페이스로 전환합니다.

       ```sh
       kubectl config get-contexts
       kubectl config set-context YOUR_CONTEXT --namespace=newrelic
       kubectl config view --minify | grep namespace:
       ```

    3. 이 시점에서 PVC는 RWX 액세스 모드로 PV에 바인딩되어야 합니다.

       ```sh
       kubectl get pv,pvc
       [output] NAME                                  CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                         STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE
       [output] persistentvolume/custom-modules-pvc   5Gi        RWX            Retain           Bound    newrelic/custom-modules-pvc   efs-sc         <unset>                          4m46s
       [output]
       [output] NAME                                       STATUS   VOLUME               CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
       [output] persistentvolumeclaim/custom-modules-pvc   Bound    custom-modules-pvc   5Gi        RWX            efs-sc         <unset>                 4m10s
       ```

       #### 사용자 정의 모듈 디렉토리를 복사하려면 `mount-custom-mods-pod` 생성하세요.

       ```sh
       kubectl apply -f - <<EOF
       apiVersion: v1
       kind: Pod
       metadata:
         name: mount-custom-mods-pod
       spec:
         containers:
         - name: mount-custom-mods-pod
           image: nginx
           resources:
             requests:
               memory: "64Mi"
               cpu: "250m"
             limits:
               memory: "128Mi"
               cpu: "500m"
           volumeMounts:
             - mountPath: "/var/lib/newrelic/synthetics/modules"
               name: custom-modules-storage
         volumes:
         - name: custom-modules-storage
           persistentVolumeClaim:
             claimName: custom-modules-pvc
       EOF
       ```

       이 시점에서 `mount-custom-mods-pod` 을 생성하고 볼륨을 사용하도록 구성해야 합니다.

       ```sh
       kubectl describe po mount-custom-mods-pod | grep -A4 Volumes:
       [output] Volumes:
       [output]   custom-modules-storage:
       [output]     Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
       [output]     ClaimName:  custom-modules-pvc
       [output]     ReadOnly:   false
       ```

       PV, PVC 또는 `mount-custom-mods-pod` 와 관련된 경고가 있는지 이벤트를 확인하세요.

       ```sh
       kubectl get events --field-selector type=Warning --sort-by='.lastTimestamp'
       ```

       #### 사용자 정의 모듈 디렉토리를 PV에 복사하세요.

       `node_modules` 은 SJM에서 `npm install` 에 생성되므로 복사할 필요가 없습니다.

       ```sh
       cd custom-modules
       rm -rf node_modules && cd ..
       ```

    4. `mount-custom-mods-pod` 이 실행 중인지 확인하세요.

       ```sh
       kubectl get po
       [output] NAME                    READY   STATUS    RESTARTS   AGE
       [output] mount-custom-mods-pod   1/1     Running   0          5m43s
       ```

    5. PV에 복사합니다.

       ```sh
       kubectl cp custom-modules newrelic/mount-custom-mods-pod:/var/lib/newrelic/synthetics/modules
       ```

    6. PV에 `/var/lib/newrelic/synthetics/modules/custom-modules/package.json` 이 있는지 확인하세요.

       ```sh
       kubectl exec -it mount-custom-mods-pod -- bash
       [output] root@mount-custom-mods-pod:/# cd /var/lib/newrelic/synthetics/modules/
       [output] root@mount-custom-mods-pod:/var/lib/newrelic/synthetics/modules# ls -l
       [output] total 4
       [output] drwxr-xr-x 2 root root 6144 Jun 29 03:49 custom-modules
       [output] root@mount-custom-mods-pod:/var/lib/newrelic/synthetics/modules# ls -l custom-modules/
       [output] total 4
       [output] -rw-r--r-- 1 501 staff 299 Jun 29 03:49 package.json
       ```

       #### 사용자 정의 모듈 기능이 활성화된 SJM을 설치하십시오.

       설치 중에 명령줄이나 YAML 파일에서 `persistence.existingClaimName` 및 `customNodeModules.customNodeModulesPath` 에 대한 값을 설정합니다. `customNodeModules.customNodeModulesPath` 값은 사용자 정의 모듈 파일이 있는 영구 볼륨의 하위 경로를 지정해야 합니다. 예를 들어:

       ```sh
       helm upgrade --install synthetics-job-manager newrelic/synthetics-job-manager -n newrelic --set global.persistence.existingClaimName=custom-modules-pvc --set global.customNodeModules.customNodeModulesPath=custom-modules --set synthetics.privateLocationKey=YOUR_PRIVATE_LOCATION_KEY
       [output] Release "synthetics-job-manager" does not exist. Installing it now.
       [output] NAME: synthetics-job-manager
       [output] LAST DEPLOYED: Fri Jun 28 16:53:28 2024
       [output] NAMESPACE: newrelic
       [output] STATUS: deployed
       [output] REVISION: 1
       [output] TEST SUITE: None
       ```

       이제 `custom-modules` 디렉토리에는 `node_modules` 에 설치된 패키지가 포함되어야 합니다.

       ```sh
       kubectl exec -it mount-custom-mods-pod -- bash
       [output] root@mount-custom-mods-pod:/# cd /var/lib/newrelic/synthetics/modules/
       [output] root@mount-custom-mods-pod:/var/lib/newrelic/synthetics/modules# ls -l custom-modules/
       [output] total 16
       [output] -rw-r--r--  1 root root   836 Jun 29 03:51 README
       [output] drwxr-xr-x 18 root root  6144 Jun 29 03:51 node_modules
       [output] -rw-r--r--  1  501 staff  299 Jun 29 03:49 package.json
       [output] -rw-r--r--  1 root root   190 Jun 29 03:51 package.json.shasum
       ```

       사용자 정의 노드 모듈이 감지되지 않으면 `custom-modules` 디렉토리와 `package.json` 파일에 대한 권한을 조정합니다.

       ```sh
       kubectl exec -it mount-custom-mods-pod -- bash
       [output] root@mount-custom-mods-pod:/# cd /var/lib/newrelic/synthetics/modules/
       [output] root@mount-custom-mods-pod:/var/lib/newrelic/synthetics/modules# chmod -R 777 custom-modules
       [output] root@mount-custom-mods-pod:/var/lib/newrelic/synthetics/modules# chown -R 2000:2000 custom-modules
       ```
  </Collapser>
</CollapserGroup>

모듈이 올바르게 설치되었는지 또는 오류가 발생했는지 확인하려면 `synthetics-job-manager` [컨테이너](/docs/synthetics/new-relic-synthetics/private-locations/job-manager-maintenance-monitoring#monitor-docker-logs) 또는 [파드](/docs/synthetics/synthetic-monitoring/private-locations/job-manager-maintenance-monitoring/#review-kubernetes-logs) 로그에서 다음 줄을 찾으세요.

```log
2024-06-29 03:51:28,407{UTC} [main] INFO  c.n.s.j.p.options.CustomModules - Detected mounted path for custom node modules
2024-06-29 03:51:28,408{UTC} [main] INFO  c.n.s.j.p.options.CustomModules - Validating permission for custom node modules package.json file
2024-06-29 03:51:28,409{UTC} [main] INFO  c.n.s.j.p.options.CustomModules - Installing custom node modules...
2024-06-29 03:51:44,670{UTC} [main] INFO  c.n.s.j.p.options.CustomModules - Custom node modules installed successfully.
```

이제 이 개인 위치로 보내는 모니터의 [스크립트](/docs/synthetics/new-relic-synthetics/scripting-monitors/write-scripted-browsers) 에 `"require('smallest');"` 을(를) 추가할 수 있습니다.

### 변화 `package.json` [#change-package-json]

로컬 및 호스팅 모듈 외에도 [Node.js 모듈](/docs/synthetics/new-relic-synthetics/scripting-monitors/import-nodejs-modules) 도 활용할 수 있습니다. SJM에서 사용하는 사용자 정의 모듈을 업데이트하려면 `package.json` 파일을 변경하고 SJM을 다시 시작하세요. 재부팅 프로세스 동안 SJM은 설정 변경을 인식하고 자동으로 정리 및 재설치 작업을 수행하여 업데이트된 모듈이 적용되도록 합니다.

<Callout variant="caution">
  로컬 모듈: `package.json` 에는 모든 로컬 모듈이 포함될 수 있지만 이러한 모듈은 맞춤 모듈 디렉터리 아래의 트리 내부에 있어야 합니다. 트리 외부에 저장하면 초기화 프로세스가 실패하고 SJM을 시작한 후 [docker 로그](/docs/synthetics/new-relic-synthetics/private-locations/job-manager-maintenance-monitoring#monitor-docker-logs) 에 오류 메시지가 표시됩니다.
</Callout>

## 영구 데이터 저장 [#permanent-data-storage]

사용자는 `user_defined_variables.json` 파일을 제공하거나 사용자 정의 노드 모듈을 지원하기 위해 영구 데이터 저장소를 사용하고 싶어할 수 있습니다.

### 도커

Docker에서 영구 데이터 저장소를 설정하려면:

1. 작업 관리자를 실행하는 호스트에 디렉터리를 만듭니다. 이것이 소스 디렉터리입니다.

2. Job Manager에서 소스 디렉터리를 뻐, 목표 디렉터리 `/var/lib/newrelic/synthetics` 에 마운트합니다.

   예시:

   ```sh
   docker run ... -v /sjm-volume:/var/lib/newrelic/synthetics:rw ...
   ```

### 포드만

Podman에 영구 데이터 저장소를 설정하려면:

1. 작업 관리자를 실행하는 호스트에 디렉터리를 만듭니다. 이것이 소스 디렉터리입니다.
2. Job Manager에서 소스 디렉터리를 뻐, 목표 디렉터리 `/var/lib/newrelic/synthetics` 에 마운트합니다.

예시:

```sh
podman run ... -v /sjm-volume:/var/lib/newrelic/synthetics:rw,z ...
```

### Kubernetes

Kubernetes에 영구 데이터 저장소를 설정하기 위해 사용자에게는 두 가지 옵션이 있습니다.

1. 기존 PersistentVolume(PV)에 대한 기존 PersistentVolumeClaim(PVC)을 제공하고 `synthetics.persistence.existingClaimName` 구성 값을 설정합니다. 예:

   ```sh
   helm install ... --set synthetics.persistence.existingClaimName=sjm-claim ...
   ```

2. 기존 PersistentVolume(PV) 이름을 제공하고 `synthetics.persistence.existingVolumeName` 구성 값을 설정합니다. Helm은 사용자를 위해 PVC를 생성합니다. 사용자는 선택적으로 다음 값을 설정할 수도 있습니다.

* `synthetics.persistence.storageClass`: 기존 PV의 저장 클래스입니다. 제공되지 않으면 Kubernetes는 기본 스토리지 클래스를 사용합니다.

* `synthetics.persistence.size`: 청구의 크기. 설정하지 않으면 기본값은 현재 2Gi입니다.

  ```sh
  helm install ... --set synthetics.persistence.existingVolumeName=sjm-volume --set synthetics.persistence.storageClass=standard ...
  ```

## Sizing considerations for Docker and Podman [#vm-sizing]

방정식이 효율적으로 실행되도록 하려면 호스트에서 모니터링 워크로드를 처리할 만큼 충분한 CPU 리소스를 프로비저닝해야 합니다. 사이즈에 영향을 미치는 요소는 많지만, 귀하의 필요 사항을 빠르게 예측할 수 있습니다. **중량급 모니터(예: 간단한 브라우저, 스크립트 브라우저 또는 스크립트 API 모니터) 하나당 CPU 코어 1개가** 필요합니다. 현재 설정을 진단하거나 향후 설정을 계획하는 경우 필요한 코어 수를 계산하는 데 도움이 되는 두 가지 공식은 다음과 같습니다.

### 공식 1: 기존 위치 진단

현재 사용하고 있는 가상화 로케이션이 따라잡기 힘들고 작업이 대기하고 있다고 의심되는 경우, 이 공식을 사용하여 실제로 필요한 코어 수를 확인하세요. 이는 시스템의 관찰 가능한 성능에 따라 결정됩니다.

$$ C\_req = (R\_proc + R\_growth) \cdot D\_avg,m $$

* $C\_req$ = **필요한 CPU 코어**.
* $R\_proc$ = 분당 **처리** 되는 중량 작업의 **비율** 입니다.
* $R\_growth$ = `jobManagerHeavyweightJobs` 대기열이 분당 **증가하는** **속도** 입니다.
* $D\_avg,m$ = 중량 작업의 **평균 소요 시간** **(분)** 입니다.

This formula calculates your true job arrival rate by adding the jobs your system *is processing* to the jobs that are *piling up* in the queue. Multiplying this total load by the average job duration tells you exactly how many cores you need to clear all the work without queuing.

### 공식 2: 새 위치 또는 미래 위치 예측

새로운 위치 위치를 설정하거나 더 많은 모니터를 추가할 계획이라면 이 공식을 사용하여 요구 사항을 미리 예측하세요.

$$ C\_req = N\_mon \cdot D\_avg,m \cdot \frac1P\_avg,m $$

* $C\_req$ = **필요한 CPU 코어**.
* $N\_mon$ = 실행하려는 중량 **모니터** 의 총 **수** 입니다.
* $D\_avg,m$ = 중량 작업의 **평균 소요 시간** **(분)** 입니다.
* $P\_avg,m$ = 중량 모니터의 **평균 기간** **(분)** (예: 5분마다 실행되는 모니터의 경우 $P\_avg,m = 5$).

This calculates your expected workload from first principles: how many monitors you have, how often they run, and how long they take.

**중요한 크기 요소**

이러한 공식을 사용할 때는 다음 요소를 고려해야 합니다.

* **작업 기간($D\_avg,m$):** 평균에는 **시간 초과** 된 작업(대개 \~3분)도 포함되어야 합니다. 이러한 작업은 전체 기간 동안 핵심을 유지합니다.
* **작업 실패 및 재시도:** 모니터가 실패하면 자동으로 재시도됩니다. 이러한 재시도는 전체 부하를 증가시키는 추가 작업입니다. 지속적으로 실패하고 재시도하는 모니터는 **주기를 늘려** 처리량에 상당한 영향을 미칩니다.
* **스케일 아웃:** 호스트에 더 많은 코어를 추가하는 것(스케일 업) 외에도 동일한 독립 로케이션 키를 사용하여 추가 신세틱스 작업 관리자를 구현하고 배치하여 여러 환경에 걸쳐 작업의 로드 밸런싱을 수행할 수 있습니다(스케일 아웃).

단일 신세틱스 Job Manager(SJM)는 **분당 약 15개의 중량 작업을** 처리할 수 있는 처리량 제한이 있다는 점에 유의하는 것이 중요합니다. 이는 SJM당 처리되는 작업의 순수한 개수보다 여러 SJM 간의 작업의 효율적인 경쟁을 선호하는 내부 스레딩 전략 때문입니다. 계산 결과 더 높은 처리량이 필요하다는 것을 나타내는 경우 추가 SJM을 구현, 배포하여 **확장** 해야 합니다. [작업 대기열이 늘어나는지 확인하여](/docs/synthetics/synthetic-monitoring/private-locations/job-manager-maintenance-monitoring/) 더 많은 SJM이 필요한지 확인할 수 있습니다.

동일한 위치 위치 키를 사용하여 더 많은 SJM을 추가하면 다음과 같은 이점이 있습니다.

* **로드 밸런싱**: 고립로케이션에 대한 작업은 사용 가능한 모든 SJM에 분산됩니다.
* **장애 조치 보호**: 하나의 SJM 인스턴스가 다운되더라도 다른 인스턴스가 작업을 계속 처리할 수 있습니다.
* **더 높은 총 처리량**: 상대 로케이션에 대한 총 처리량은 각 SJM의 처리량의 합이 됩니다(예: 2개의 SJM은 분당 최대 30개의 작업을 제공합니다).

### 진단을 위한 NRQL 쿼리

진단 공식에 대한 입력을 얻으려면 [쿼리 빌더](/docs/query-your-data/explore-query-data/get-started/introduction-querying-new-relic-data/) 에서 이러한 쿼리를 실행할 수 있습니다. 안정적인 평균을 얻으려면 시간 범위를 충분히 길게 설정하세요.

**1. Find the rate of jobs processed per minute ($R\_proc$)**: This query counts the number of non-ping (heavyweight) jobs completed over the last day and shows the average rate per minute.

```sql
FROM SyntheticCheck
SELECT rate(uniqueCount(id), 1 minute) AS 'job rate per minute'
WHERE location = 'YOUR_PRIVATE_LOCATION' AND typeLabel != 'Ping'
SINCE 1 day ago
```

**2. Find the rate of queue growth per minute ($R\_growth$)**: This query calculates the average per-minute growth of the `jobManagerHeavyweightJobs` queue on a time series chart. A line above zero indicates the queue is growing, while a line below zero means it&apos;s shrinking.

```sql
FROM SyntheticsPrivateLocationStatus
SELECT derivative(jobManagerHeavyweightJobs, 1 minute) AS 'queue growth rate per minute'
WHERE name = 'YOUR_PRIVATE_LOCATION'
TIMESERIES SINCE 1 day ago
```

<Callout variant="tip">
  해당 로그인이 존재하는 계정을 선택하세요. 파생 함수가 크게 달라질 수 있으므로 이 쿼리를 시계열로 보는 것이 가장 좋습니다. 목표는 분당 대기열 증가율을 추정하는 것입니다. 다양한 시간 범위를 적용해 보고 어떤 것이 가장 효과적인지 확인 Play .
</Callout>

**3. Find total number of heavyweight monitors ($N\_mon$)**: This query finds the unique count of heavyweight monitors.

```sql
FROM SyntheticCheck
SELECT uniqueCount(monitorId) AS 'monitor count'
WHERE location = 'YOUR_PRIVATE_LOCATION' AND typeLabel != 'Ping'
SINCE 1 day ago
```

**4. Find average job duration in minutes ($D\_avg,m$)**: This query finds the average execution duration of completed non-ping jobs and converts the result from milliseconds to minutes. `executionDuration` represents the time the job took to execute on the host.

```sql
FROM SyntheticCheck
SELECT average(executionDuration)/60e3 AS 'avg job duration (m)'
WHERE location = 'YOUR_PRIVATE_LOCATION' AND typeLabel != 'Ping'
SINCE 1 day ago
```

**5. 평균 헤비급 감시 기간($P\_avg,m$) 찾기:** 처리로케이션의 `jobManagerHeavyweightJobs` 큐가 증가하는 경우 기존 결과에서 평균 감시 기간을 계산하는 것은 정확하지 않습니다. 이는 [Synthetic Monitor](https://one.newrelic.com/synthetics) 페이지의 모니터 목록에서 추산해야 합니다. 올바른 뉴렐릭 계정을 선택했는지 확인하고 `privateLocation` 로 필터링해야 할 수도 있습니다.

<Callout variant="tip">
  합성 모니터는 여러 하위 계정에 존재할 수 있습니다. 쿼리 빌더에서 선택할 수 있는 하위 계정보다 많은 하위 계정이 있는 경우, 모니터가 가장 많은 계정을 선택하세요.
</Callout>

### ping 모니터 및 `pingJobs` 대기열에 대한 참고 사항

**Ping 모니터는 다릅니다.** 각각 전체 CPU 코어를 소모하지 않는 가벼운 작업입니다. 대신 별도의 대기열(`pingJobs`)을 사용하고 작업자 스레드 풀에서 실행됩니다.

ping 작업은 리소스를 덜 사용하지만, 특히 실패하는 작업의 경우 ping 작업량이 많으면 여전히 성능 문제가 발생할 수 있습니다. 다음 사항을 명심하세요.

* **리소스 모델:** Ping 작업은 전용 CPU 코어가 아닌 작업자 스레드를 활용합니다. 이러한 경우에는 작업당 코어 계산이 적용되지 않습니다.
* **시간 초과 및 재시도:** 실패한 ping 작업은 최대 **60초** 동안 작업자 스레드를 차지할 수 있습니다. 먼저 HTTP HEAD 요청을 시도합니다(제한 시간 30초). 실패하면 즉시 HTTP GET 요청으로 재시도합니다(30초의 시간 초과).
* **크기 조정:** 크기 조정 공식은 다르지만 동일한 원칙이 적용됩니다. 대량의 ping 작업을 처리하고 `pingJobs` 대기열이 커지는 것을 방지하려면 확장 및/또는 확장이 필요할 수 있습니다. 확장은 호스트나 네임스페이스당 CPU와 메모리 리소스를 늘리는 것을 의미합니다. 확장은 ping 런타임 인스턴스를 더 추가하는 것을 의미합니다. 이는 더 많은 호스트, 더 많은 라벨스페이스 또는 [동일한 라벨스페이스 내에서](/docs/synthetics/synthetic-monitoring/private-locations/job-manager-configuration#scaling-out-with-multiple-sjm-instances) 구현하다, 배포하다 더 많은 작업 관리자를 통해 수행될 수 있습니다. 또는 Kubernetes 의 `ping-runtime` 을 사용하면 구현, 배포당 [더 많은 수의 복제본을](https://github.com/newrelic/helm-charts/blob/41c03e287dafd41b9c914e5a6c720d5aa5c01ace/charts/synthetics-job-manager/values.yaml#L173) 설정할 수 있습니다.

## Sizing considerations for Kubernetes and OpenShift [#kubernetes-sizing]

Kubernetes와 OpenShift 합성 작업 관리자가 사용하는 각 런타임은 [helm 차트](https://github.com/newrelic/helm-charts/tree/master/charts/synthetics-job-manager) 에서 값을 설정하여 독립적으로 크기를 조정할 수 있습니다. [node-api-runtime](https://github.com/newrelic/helm-charts/tree/master/charts/synthetics-job-manager/charts/node-api-runtime) 과 [node-browser-runtime은](https://github.com/newrelic/helm-charts/tree/master/charts/synthetics-job-manager/charts/node-browser-runtime) `parallelism` 및 `completions` 설정의 조합을 사용하여 독립적으로 크기가 조정됩니다.

* The `parallelism` setting controls how many pods of a particular runtime run concurrently.
* The `completions` setting controls how many pods must complete before the `CronJob` starts another Kubernetes Job for that runtime.

### How to Size Your Deployment: A Step-by-Step Guide

Your goal is to configure enough parallelism to handle your job load without exceeding the throughput limit of your SJM instances.

### Step 1: Estimate Your Required Workload

**Completions:** This determines how many runtime pods should complete before a new Kubernetes Job is started.

First, determine your private location&apos;s average job execution duration and job rate. Use `executionDuration` as it most accurately reflects the pod&apos;s active runtime.

```sql
-- Get average job execution duration (in seconds)
FROM SyntheticCheck
SELECT average(executionDuration / 60e3) AS 'D_avg_m'
WHERE typeLabel != 'Ping' AND location = 'YOUR_PRIVATE_LOCATION'
FACET typeLabel SINCE 1 hour ago
```

$$ Completions = \frac5D\_avg,m $$

Where $D\_avg,m$ is your **average job execution duration in seconds**.

**Required Parallelism:** This determines how many workers (pods) you need running concurrently to handle your 5-minute job load.

```sql
-- Get jobs per 5 minutes
FROM SyntheticCheck
SELECT rate(uniqueCount(id), 5 minutes) AS 'N_m'
WHERE typeLabel != 'Ping' AND location = 'YOUR_PRIVATE_LOCATION'
FACET typeLabel SINCE 1 hour ago
```

$$ P\_req = \fracN\_mCompletions $$

Where $N\_m$ is your **number of jobs per 5 minutes**. This $P\_req$ value is your **target total parallelism**.

### Step 2: Check Against the Single-SJM Throughput Limit

**Max Parallelism:** This determines how many workers (pods) your SJM can effectively utilize.

$$ P\_max \approx 15 \cdot D\_avg,m $$

This $P\_max$ value is your **system limit for one SJM Helm deployment**.

<Callout variant="tip">
  The above queries are based on current results. If your private location does not have any results or the job manager is not performing at its best, query results may not be accurate. In that case, start with the examples in the table below and adjust until your queue is stable.
</Callout>

<Callout variant="tip">
  A key consideration is that a **single SJM instance has a maximum throughput of approximately 15 heavyweight jobs per minute**. You can calculate the maximum effective parallelism ($P\_max$) a single SJM can support before hitting this ceiling.
</Callout>

### Step 3: Compare, Configure, and Scale

Compare your **required** parallelism ($P\_req$) from Step 1 to the **maximum** parallelism ($P\_max$) from Step 2.

<DNT>
  **Scenario A:** $P\_req \le P\_max$
</DNT>

* **Diagnosis:** Your job load is within the limit of a single SJM instance.

* **Action:**

  1. You will deploy **one** SJM Helm release.
  2. In your Helm chart `values.yaml`, set `parallelism` to your calculated $P\_req$.
  3. Set `completions` to your calculated **Completions**. For improved efficiency, this value should typically be 6-10x your `parallelism` setting.

<DNT>
  **Scenario B:** $P\_req &gt; P\_max$
</DNT>

* **Diagnosis:** Your job load **exceeds the \~15 jobs/minute limit** of a single SJM.

* **Action:**

  1. You must **scale out by deploying multiple, separate SJM Helm releases**.
  2. See the **[Scaling Out with Multiple SJM Deployments](#scaling-out-with-multiple-sjm-deployments)** section below for the correct procedure.
  3. **Do not** increase the `replicaCount` in your Helm chart.

### Step 4: Monitor Your Queue

After applying your changes, you must verify that your job queue is stable and not growing. A consistently growing queue means your location is still under-provisioned.

Run this query to check the queue&apos;s growth rate:

```sql
-- Check for queue growth (a positive value means the queue is growing)
SELECT derivative(jobManagerHeavyweightJobs, 1 minute) AS 'Heavyweight Queue Growth Rate (per min)'
FROM SyntheticsPrivateLocationStatus
WHERE name = 'YOUR_PRIVATE_LOCATION'
SINCE 1 hour ago TIMESERIES
```

If the &quot;Queue Growth Rate&quot; is consistently positive, you need to install more SJM Helm deployments (Scenario B) or re-check your `parallelism` settings (Scenario A).

### Configuration Examples and Tuning

The `parallelism` setting directly affects how many synthetics jobs per minute can be run. Too small a value and the queue may grow. Too large a value and nodes may become resource constrained.

<table>
  <thead>
    <tr>
      <th style={{ width: "300px" }}>
        예시
      </th>

      <th>
        설명
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        `parallelism=1` `completions=1`
      </td>

      <td>
        런타임은 분당 1개의 신세틱스 작업을 실행합니다. 1개의 작업이 완료된 후 `CronJob` 설정은 다음 순간에 새 작업을 시작합니다. <DNT>**Throughput will be extremely limited with this configuration.**</DNT>
      </td>
    </tr>

    <tr>
      <td>
        `parallelism=1` `completions=6`
      </td>

      <td>
        The runtime will execute 1 synthetics job at a time. After the job completes, a new job will start immediately. After 6 jobs complete, the `CronJob` configuration will start a new Kubernetes Job. <DNT>**Throughput will be limited.**</DNT> A single long-running synthetics job will block the processing of any other synthetics jobs of this type.
      </td>
    </tr>

    <tr>
      <td>
        `parallelism=3` `completions=24`
      </td>

      <td>
        The runtime will execute 3 synthetics jobs at once. After any of these jobs complete, a new job will start immediately. After 24 jobs complete, the `CronJob` configuration will start a new Kubernetes Job. <DNT>**Throughput is much better with this or similar configurations.**</DNT>
      </td>
    </tr>
  </tbody>
</table>

If your `parallelism` setting is working well (keeping the queue at zero), setting a higher `completions` value (e.g., 6-10x `parallelism`) can improve efficiency by:

* Accommodating variability in job durations.
* Reducing the number of completion cycles to minimize the &quot;nearing the end of completions&quot; inefficiency where the next batch can&apos;t start until the final job from the current batch completes.

`completions` 값이 너무 커서는 안 됩니다. 그렇지 않으면 CronJob에서 다음과 같은 경고 이벤트가 발생합니다.

```sh
8m40s     Warning   TooManyMissedTimes   cronjob/synthetics-node-browser-runtime   too many missed start times: 101. Set or decrease .spec.startingDeadlineSeconds or check clock skew
```

<Callout variant="tip">
  뉴렐릭은 신세틱스 작업 관리자 파일에 대한 수정 사항에 대해 책임을 지지 않습니다.
</Callout>

### Scaling out with multiple SJM deployments

To scale beyond the \~15 jobs/minute throughput of a single SJM, you must install **multiple, separate SJM Helm releases**.

<Callout variant="important">
  **Do not use `replicaCount` to scale the job manager pod.** You **cannot** scale by increasing the `replicaCount` for a single Helm release. The SJM architecture requires a 1:1 relationship between a runtime pod and its parent SJM pod. If runtime pods send results back to the wrong SJM replica (e.g., through a Kubernetes service), those results will be lost.
</Callout>

The correct strategy is to deploy multiple SJM instances, each as its own Helm release. Each SJM will compete for jobs from the same private location, providing load balancing, failover protection, and an increased total job throughput.

#### Simplified Scaling Strategy

Assuming $P\_req &gt; P\_max$ and you need to scale out, you can simplify maintenance by treating each SJM deployment as a fixed-capacity unit.

1. **Set Max Parallelism:** For *each* SJM, set `parallelism` to the same $P\_max$ value. This maximizes the potential throughput of each SJM.

2. **Set Completions:** For *each* SJM, set `completions` to a fixed value as well. The $P\_req$ formula from [Step 1](#step-1-estimate-your-required-workload) can be modified to estimate completions by substituting in the $P\_max$ value:

   $$ Completions = \fracN\_mP\_max $$

   Where $N\_m$ is your **number of jobs per 5 minutes**. Adjust as needed after deploying to target a 5 minute Kubernetes job age per runtime, i.e., node-browser-runtime and node-api-runtime.

3. **Install Releases:** Install as many separate Helm releases as you need to handle your total $P\_req$. For example, if your total $P\_req$ is 60 and you&apos;ve fixed each SJM&apos;s `parallelism` at 20 ($P\_max$ from [Step 2](#step-2-check-against-the-single-sjm-throughput-limit)), you would need **three** separate Helm deployments to meet the required job demand.

4. **Monitor and Add:** Monitor your job queue (see [Step 4](#step-4-monitor-your-queue)). If it starts to grow, simply install another Helm release (e.g., `sjm-delta`) using the same fixed configuration.

By fixing parallelism and completions to static values based on $P\_max$, increasing or decreasing capacity becomes a simpler process of **adding or removing Helm releases**. This helps to avoid wasting cluster resources on a parallelism value that is higher than the SJM can effectively utilize.

#### Installation Example

When installing multiple SJM releases, you must provide a **unique name for each release**. All instances must be configured with the **same private location key**.

Setting the `fullnameOverride` is highly recommended to create shorter, more manageable resource names. For example, to install two SJMs named `sjm-alpha` and `sjm-beta` into the `newrelic` namespace (both using the same `values.yaml` with your fixed parallelism and completions):

```sh
# Install the first SJM deployment
helm upgrade --install sjm-alpha newrelic/synthetics-job-manager \
  -n newrelic \
  -f values.yaml \
  --set fullnameOverride=sjm-alpha \
  --set ping-runtime.fullnameOverride=sjm-alpha-ping \
  --set node-api-runtime.fullnameOverride=sjm-alpha-api \
  --set node-browser-runtime.fullnameOverride=sjm-alpha-browser
```

```sh
# Install the second SJM deployment to add capacity
helm upgrade --install sjm-beta newrelic/synthetics-job-manager \
  -n newrelic \
  -f values.yaml \
  --set fullnameOverride=sjm-beta
  --set ping-runtime.fullnameOverride=sjm-beta-ping \
  --set node-api-runtime.fullnameOverride=sjm-beta-api \
  --set node-browser-runtime.fullnameOverride=sjm-beta-browser
```

You can continue this pattern (`sjm-charlie`, `sjm-delta`, etc.) for as many SJMs as needed to keep the job queue from growing.