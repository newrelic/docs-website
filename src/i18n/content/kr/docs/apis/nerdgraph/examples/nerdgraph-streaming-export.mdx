---
title: 'NerdGraph 튜토리얼: AWS Kinesis Firehose 또는 Azure 이벤트 허브로 데이터 스트리밍'
tags:
  - APIs
  - NerdGraph
metaDescription: 'With the New Relic streaming export feature, you can send your data as it''s ingested to New Relic to an AWS Kinesis Firehose or Azure Event Hub.'
freshnessValidatedDate: never
translationType: machine
---

[Data Plus](/docs/accounts/accounts-billing/new-relic-one-pricing-billing/data-ingest-billing/#data-plus) 에서 사용할 수 있는 스트리밍 내보내기 기능을 사용하면 New Relic에서 데이터를 수집할 때 AWS Kinesis Firehose 또는 Azure Event Hub로 데이터를 보낼 수 있습니다. [NerdGraph](/docs/apis/nerdgraph/get-started/introduction-new-relic-nerdgraph) 를 사용하여 스트리밍 규칙을 만들고 업데이트하는 방법과 기존 규칙을 보는 방법을 설명합니다. [NerdGraph 탐색기](/docs/apis/nerdgraph/get-started/nerdgraph-explorer) 를 사용하여 이러한 호출을 할 수 있습니다.

## 스트리밍 내보내기란 무엇입니까? [#definition]

New Relic 조직에서 데이터를 수집하면 스트리밍 내보내기 기능이 해당 데이터를 AWS Kinesis Firehose 또는 Azure Event Hub로 보냅니다. [NRQL을](/docs/query-your-data/nrql-new-relic-query-language/get-started/introduction-nrql-new-relics-query-language) 사용하여 정의된 사용자 정의 규칙을 설정하여 내보낼 New Relic 데이터의 종류를 제어할 수 있습니다. 새로운 [압축 내보내기](#compression) 기능을 사용하여 데이터를 내보내기 전에 압축하도록 선택할 수도 있습니다.

스트리밍 내보내기를 사용할 수 있는 몇 가지 예:

* 데이터 레이크를 채우려면
* AI/ML 교육 강화
* 규정 준수, 법적 또는 보안상의 이유로 장기 보존

원할 때마다 스트리밍 내보내기 규칙을 비활성화하거나 활성화할 수 있습니다. 그러나 스트리밍 내보내기는 현재 수집된 데이터만 전송합니다. 즉, 비활성화했다가 다시 활성화하면 꺼져 있을 때 수집된 데이터가 이 기능과 함께 전송되지 않습니다. 과거 데이터를 내보내려면 [기록 데이터 내보내기](/docs/apis/nerdgraph/examples/nerdgraph-historical-data-export) 를 사용할 수 있습니다.

## 요구 사항 및 제한 사항 [#requirements]

스트리밍 데이터 제한: 한 달에 스트리밍할 수 있는 데이터의 양은 한 달에 [수집된 총 데이터](/docs/accounts/accounts-billing/new-relic-one-pricing-billing/data-ingest-billing/#usage-calculation) 에 의해 제한됩니다. 스트리밍 데이터 양이 수집된 데이터 양을 초과하는 경우 스트리밍 내보내기에 대한 액세스 및 사용을 일시 중지할 수 있습니다.

권한 관련 요구 사항:

* [Data Plus](/docs/accounts/accounts-billing/new-relic-one-pricing-billing/data-ingest-billing/#data-plus) 옵션이 포함된 Pro 또는 Enterprise 에디션
* 사용자 유형: [핵심 사용자 또는 전체 플랫폼 사용자](/docs/accounts/accounts-billing/new-relic-one-user-management/user-type)
* [스트리밍 데이터 권한](/docs/accounts/accounts-billing/new-relic-one-user-management/user-permissions#data-platform)

New Relic 데이터를 수신하도록 설정된 AWS Kinesis Firehose 또는 Azure Event Hub가 있어야 합니다. 아직 이 작업을 수행하지 않은 경우 [AWS](#firehose-setup) 또는 [Azure](#event-hub-setup) 에 대한 아래 단계를 따를 수 있습니다.

NRQL 요구 사항:

* 집계가 없는 단순 쿼리여야 합니다. 예를 들어 `SELECT *` 또는 `SELECT column1, column2` 형식이 지원됩니다.
* 하위 쿼리를 제외한 `WHERE` 절의 모든 항목에 적용할 수 있습니다.
* 쿼리에는 `FACET` 절, `COMPARE WITH` 또는 `LOOKUP` 가 있을 수 없습니다.
* 중첩 쿼리는 지원되지 않습니다.
* [메트릭 타임슬라이스 데이터](/docs/data-apis/understand-data/new-relic-data-types/#timeslice-data) 가 아닌 [NRDB에 저장된 데이터 유형을](/docs/data-apis/understand-data/new-relic-data-types/#timeslice-data) 지원합니다.

## AWS Kinesis Firehose 설정 [#firehose-setup]

AWS로 스트리밍 데이터 내보내기를 설정하려면 먼저 Amazon Kinesis Firehose를 설정해야 합니다. 다음 세 단계에서 해당 절차를 안내합니다.

<Steps>
  <Step>
    ### 스트리밍 내보내기 위한 Firehose 만들기 [#create-firehose]

    New Relic 데이터를 스트리밍할 전용 Firehose를 생성:

    1. Amazon Kinesis Data Firehose로 이동합니다.
    2. 배달 스트림을 만듭니다.
    3. 스트림 이름을 지정합니다(이 이름은 나중에 규칙 등록에서 사용합니다).
    4. <DNT>**Direct PUT or other sources**</DNT> 사용하고 뉴렐릭의 JSON 이벤트 형식(예: S3, Redshift 또는 OpenSearch)과 호환되는 대상을 지정합니다.
  </Step>

  <Step>
    ### IAM Firehose 쓰기 액세스 정책 생성 [#create-policy]

    1. IAM 콘솔로 이동하여 사용자로 로그인합니다.
    2. 왼쪽 탐색에서 **Policies** \[정책을] 클릭한 다음 **Create policy** \[정책 만들기를] 클릭합니다.
    3. Firehose 서비스를 선택한 다음 `PutRecord` 및 `PutRecordBatch` 을 선택합니다.
    4. `Resources` 의 경우 전송 스트림을 선택하고 ARN을 추가한 다음 스트림의 지역을 선택합니다.
    5. AWS 계정 번호를 입력한 다음 이름 상자에 원하는 전송 스트림 이름을 입력합니다.
    6. 정책을 만듭니다.
  </Step>

  <Step>
    ### 뉴렐릭 쓰기 액세스 권한을 부여하기 위한 IAM 역할 생성 [#iam-role]

    IAM 역할을 설정하려면:

    1. IAM으로 이동하여 <DNT>**Roles**</DNT> 클릭합니다.
    2. AWS 계정에 대한 역할을 생성한 후 <DNT>**for another AWS account**</DNT> 선택합니다.
    3. New Relic 내보내기 계정 ID 입력: `888632727556` .
    4. <DNT>**Require external ID**</DNT> 선택하고 내보내려는 뉴렐릭 계정의 [계정 ID를](/docs/accounts/accounts-billing/account-structure/account-id) 입력하세요.
    5. <DNT>**Permissions**</DNT> 클릭한 다음 위에서 생성한 정책을 선택합니다.
    6. 역할 이름(내보내기 등록에 사용됨) 및 설명을 추가합니다.
    7. 역할을 만듭니다.
  </Step>
</Steps>

이러한 단계를 모두 마치면 NerdGraph를 사용하여 내보내기 규칙을 설정할 수 있습니다. 자세한 내용은 [NerdGraph 호출에 대한 중요 필드](#fields) 로 이동하세요.

## Azure 이벤트 허브 설정 [#event-hub-setup]

Azure로 스트리밍 데이터 내보내기를 설정하려면 먼저 Event Hub를 설정해야 합니다. 다음 세 단계에서 해당 절차를 안내합니다.

또는 [여기](https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-create) 에서 Azure 가이드를 따를 수 있습니다.

<Steps>
  <Step>
    ### 이벤트 허브 네임스페이스 만들기 [#create-event-hubs-namespace]

    1. Microsoft Azure 계정 내에서 Event Hubs로 이동합니다.
    2. 단계에 따라 Event Hubs 네임스페이스를 만듭니다. 모든 데이터를 받을 수 있도록 자동 팽창을 활성화하는 것이 좋습니다.
    3. 공개 액세스가 활성화되어 있는지 확인하십시오. 공유 액세스 정책을 사용하여 Event Hub에서 안전하게 인증합니다.
    4. Event Hubs 네임스페이스가 구현하다, 배포되면, <DNT>**Go to resource**</DNT> 클릭합니다.
  </Step>

  <Step>
    ### 이벤트 허브 만들기 [#create-event-hub]

    1. 왼쪽 열에서 <DNT>**Event Hubs**</DNT> 클릭합니다.
    2. 그런 다음 <DNT>**+Event Hub**</DNT> 클릭하여 이벤트 허브를 만듭니다.
    3. 원하는 Event Hub 이름을 입력합니다. 나중에 스트리밍 내보내기 규칙을 만드는 데 필요하므로 저장합니다.
    4. <DNT>**Retention**</DNT> 의 경우, <DNT>**Delete**</DNT> `Cleanup policy` 및 원하는 `Retention time (hrs)` 선택하세요.

    <Callout variant="important">
      현재 <DNT>**Compact**</DNT> 보존 정책이 있는 이벤트 허브에서는 스트리밍 내보내기가 지원되지 않습니다.
    </Callout>

    5. Event Hub가 생성되면 Event Hub를 클릭합니다.
  </Step>

  <Step>
    ### 공유 액세스 정책을 만들고 첨부합니다. [#event-hub-policy]

    1. 왼쪽 열에서 <DNT>**Shared access policies**</DNT> 으로 이동합니다.
    2. 페이지 상단 근처에 있는 <DNT>**+Add**</DNT> 클릭합니다.
    3. 공유 액세스 정책의 이름을 선택하십시오.
    4. <DNT>**Send**</DNT> 선택하고 <DNT>**Create**</DNT> 클릭합니다.
    5. 생성된 정책을 클릭하고 <DNT>**Connection string–primary key**</DNT> 을 복사합니다. 이를 저장하여 이벤트 허브에 데이터를 인증하고 전송하는 데 사용합니다.
  </Step>
</Steps>

이러한 단계를 마치면 Nerdgraph 호출에 필요한 중요 필드에 대한 다음 섹션을 참조하세요.

## NerdGraph 호출을 위한 중요 필드 [#fields]

우리가 다룰 대부분의 스트리밍 데이터 내보내기 NerdGraph 호출은 계정과 관련된 몇 가지 필드를 사용합니다.

AWS Kinesis Firehose의 경우:

* `awsAccountId`: [AWS 계정 ID](https://docs.aws.amazon.com/IAM/latest/UserGuide/console_account-alias.html) . 예를 들어: `10000000000`
* `deliveryStreamName`: [Kinesis 스트림 이름](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_CreateStream.html) 입니다. 예: `firehose-test-stream` .
* `region`:[AWS 리전](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html) . 예: `us-east-1` .
* `role`: Kinesis Firehose에 대한 [AWS IAM 역할](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html) 입니다. 이것은 항상 `firehose-role` 입니다.

Azure 이벤트 허브의 경우:

* `eventHubConnectionString`: [Azure 이벤트 허브 연결 문자열](https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-get-connection-string) 입니다. 다음과 유사합니다. `Endpoint=sb://<NamespaceName>.servicebus.windows.net/;SharedAccessKeyName=<KeyName>;SharedAccessKey=<KeyValue>;EntityPath=<EventHubName>`
* `eventHubName`: 이벤트 허브 이름입니다. 예: `my-event-hub` .

## 스트리밍 내보내기 규칙을 만드는 방법 [#create-stream]

먼저 내보낼 데이터를 결정합니다. 그런 다음 NerdGraph 호출로 NRQL을 사용하여 원하는 스트리밍 규칙을 생성합니다. 몇 가지 예를 들어보겠습니다.

### 스트림 생성 [#create-stream]

새 스트리밍 규칙을 만들 때 다음 필드가 모두 필요합니다. 다음은 AWS Kinesis Firehose로 내보내는 스트리밍 규칙을 생성하는 예입니다.

```graphql
mutation {
  streamingExportCreateRule(
    accountId: YOUR_NR_ACCOUNT_ID,
    ruleParameters: {
      description: "ADD_RULE_DESCRIPTION",
      name: "PROVIDE_RULE_NAME",
      nrql: "SELECT * FROM NodeStatus",
      payloadCompression: DISABLED
    },
    awsParameters: {
      awsAccountId: "YOUR_AWS_ACCOUNT_ID",
      deliveryStreamName: "FIREHOSE_STREAM_NAME",
      region: "SPECIFY_AWS_REGION",
      role: "firehose-role"
    }
  ) {
    id
    status
  }
}
```

다음은 Azure Event Hub로 내보내는 스트리밍 규칙을 만드는 예입니다.

```graphql
mutation {
  streamingExportCreateRule(
    accountId: YOUR_NR_ACCOUNT_ID,
    ruleParameters: {
      description: "ADD_RULE_DESCRIPTION",
      name: "PROVIDE_RULE_NAME",
      nrql: "SELECT * FROM NodeStatus",
      payloadCompression: DISABLED
    },
    azureParameters: {
      eventHubConnectionString: "YOUR_EVENT_HUB_SAS_CONNECTION_STRING",
      eventHubName: "YOUR_EVENT_HUB_NAME"
    }
  ) {
    id
    status
  }
}
```

규칙 ID 및 상태와 함께 즉시 결과를 얻을 수 있습니다. 상태는 `CREATION_IN_PROGRESS` 으로 표시됩니다. 규칙 ID를 사용하여 규칙이 성공적으로 생성되었는지 확인할 수 있습니다.

정책 유효성 검사에 시간이 걸리므로 규칙 생성을 완료하는 데 최대 6분이 걸릴 수 있습니다.

규칙 등록이 완료되기 전에는 규칙이 생성 프로세스에 대해 잠겨 있기 때문에 다른 변형 작업(예: `Enable` , `Disable` 또는 `Update` )을 시작할 수 없습니다. 규칙이 등록 프로세스를 완료하기 전에 다른 변형 작업을 시도하면 &quot;현재 내보내기 규칙이 다른 요청에 의해 업데이트되고 있습니다. 잠시 기다렸다가 다시 시도하십시오.&quot;와 같은 메시지가 표시됩니다.

언제든지 `Delete` 을(를) 사용할 수 있습니다.

규칙 생성에 필요한 대략 6분 이내에 생성을 완료하고 언제든지 상태를 변경할 수 있습니다. 상태가 `ENABLED` , `DISABLED` 또는 `CREATION_FAILED` 로 변경됩니다.

값에 대한 다음 세부정보를 참조하세요.

* `ENABLED` 규칙이 성공적으로 생성되고 데이터 스트리밍이 시작되었음을 의미합니다.
* `CREATION_FAILED` 생성 시 규칙이 실패했음을 의미합니다. 이는 여러 가지 이유로 발생할 수 있지만 종종 AWS 정책 또는 Azure SAS 유효성 검사 실패로 인해 발생합니다.
* `DISABLED` 규칙이 생성되었지만 필터 스트림 제한에 도달했거나 필터 스트림 규칙 생성 실패와 같은 이유로 아직 활성화되지 않았음을 의미합니다. 6분 후에도 상태가 계속 `CREATION_IN_PROGRESS` 으로 유지되면 서비스의 시스템 오류로 인해 규칙 생성이 실패했음을 의미합니다. 규칙을 삭제하고 새 규칙을 다시 만들 수 있습니다.

스트리밍 규칙이 생성되면 [볼 수 있습니다](#view-stream) .

## 스트림 업데이트 [#update-stream]

새 스트리밍 규칙을 업데이트할 때 다음 필드가 모두 필요합니다. 다음은 스트리밍 규칙을 업데이트하는 예입니다.

AWS 키네시스 파이어호스:

```graphql
mutation {
  streamingExportUpdateRule(
    id: RULE_ID,
    ruleParameters: {
      description: "ADD_RULE_DESCRIPTION",
      name: "PROVIDE_RULE_NAME",
      nrql: "YOUR_NRQL_QUERY",
      payloadCompression: DISABLED
    },
    awsParameters: {
      awsAccountId: "YOUR_AWS_ACCOUNT_ID",
      deliveryStreamName: "FIREHOSE_STREAM_NAME",
      region: "SPECIFY_AWS_REGION",
      role: "firehose-role"
    }
  ) {
    id
    status
  }
}
```

Azure 이벤트 허브:

```graphql
mutation {
  streamingExportUpdateRule(
    id: RULE_ID,
    ruleParameters: {
      description: "ADD_RULE_DESCRIPTION",
      name: "PROVIDE_RULE_NAME",
      nrql: "YOUR_NRQL_QUERY",
      payloadCompression: DISABLED
    },
    azureParameters: {
      eventHubConnectionString: "YOUR_EVENT_HUB_SAS_CONNECTION_STRING",
      eventHubName: "YOUR_EVENT_HUB_NAME"
    }
  ) {
    id
    status
  }
}
```

업데이트할 때 메시지 필드에 &quot;내보내기 규칙이 업데이트되고 있으며 프로세스를 완료하는 데 몇 분 정도 걸릴 수 있습니다. 나중에 다시 확인해 주세요.” 완전히 업데이트되는 데 최대 6분이 소요될 수 있습니다.

규칙을 검색하기 위해 `streamingRule` 를 호출하여 규칙이 업데이트되었는지 확인할 수 있습니다. 규칙이 업데이트되는 기간 동안 규칙이 잠기고 규칙에 대해 다른 변형 작업을 수행할 수 없습니다. 동일한 규칙에 대해 다른 변형 작업을 수행하려고 하면 &quot;현재 내보내기 규칙이 다른 요청에 의해 업데이트되고 있습니다. 잠시 기다렸다가 나중에 다시 시도하십시오.&quot;라는 메시지가 표시됩니다. 사용자는 삭제된 규칙을 제외한 모든 상태의 규칙을 업데이트할 수 있습니다.

## 스트림 비활성화 [#disable-stream]

규칙을 비활성화하려면 규칙 ID만 제공하면 됩니다. 다음은 스트림을 비활성화하는 예입니다.

```graphql
mutation {
  streamingExportDisableRule(id: RULE_ID) {
    id
    status
    message
  }
}
```

규칙 상태가 `ENABLED` 인 경우에만 규칙을 비활성화할 수 있습니다. 다른 상태에 있는 규칙을 비활성화하려고 하면 &quot;허용되지 않는 상태로 인해 내보내기 규칙을 활성화하거나 비활성화할 수 없습니다.&quot;라는 오류 메시지가 반환됩니다. 다른 변형이 수행되어 규칙이 잠긴 경우 규칙을 비활성화할 수 없습니다.

## 스트림 활성화 [#enable-stream]

규칙을 활성화하려면 규칙 ID만 제공하면 됩니다. 다음은 스트림을 활성화하는 예입니다.

```graphql
mutation {
  streamingExportEnableRule(id: RULE_ID) {
    id
    status
    message
  }
}
```

상태가 `DISABLED` 인 경우에만 규칙을 활성화할 수 있습니다. 다른 상태의 규칙을 활성화하려고 하면 &quot;허용되지 않는 상태로 인해 내보내기 규칙을 활성화하거나 비활성화할 수 없습니다.&quot;와 같은 오류 메시지가 반환됩니다. 수행 중인 다른 변형으로 인해 규칙이 잠긴 경우 규칙을 활성화할 수 없습니다.

## 스트림 삭제 [#delete-stream]

스트림을 삭제하려면 규칙 ID를 제공해야 합니다. 다음은 예입니다.

```graphql
mutation {
  streamingExportDeleteRule(id: RULE_ID) {
    id
    ...
  }
}
```

삭제는 이미 삭제되지 않은 상태의 규칙에서 수행할 수 있습니다. 규칙을 삭제하면 다시 활성화할 수 없습니다. 규칙 ID로 `steamingRule` API를 호출하면 삭제 후 처음 24시간 이내에 규칙을 계속 볼 수 있습니다. 24시간이 지나면 NerdGraph를 통해 더 이상 규칙을 검색할 수 없습니다.

## 스트림 보기 [#view-stream]

계정 ID 및 규칙 ID를 쿼리하여 특정 스트림 규칙에 대한 정보를 쿼리할 수 있습니다. 다음은 예입니다.

AWS 키네시스 파이어호스:

```graphql
{
  actor {
    account(id: YOUR_NR_ACCOUNT_ID) {
      streamingExport {
        streamingRule(id: "RULE_ID") {
          aws {
            awsAccountId
            deliveryStreamName
            region
            role
          }
          createdAt
          description
          id
          message
          name
          nrql
          status
          updatedAt
          payloadCompression
        }
      }
    }
  }
}
```

Azure 이벤트 허브:

```graphql
{
  actor {
    account(id: YOUR_NR_ACCOUNT_ID) {
      streamingExport {
        streamingRule(id: "RULE_ID") {
          azure {
            eventHubConnectionString
            eventHubName
          }
          createdAt
          description
          id
          message
          name
          nrql
          status
          updatedAt
          payloadCompression
        }
      }
    }
  }
}
```

모든 기존 스트림을 쿼리할 수도 있습니다. 다음은 예입니다.

```graphql
{
  actor {
    account(id: YOUR_NR_ACCOUNT_ID) {
      streamingExport {
        streamingRules {
          aws {
            awsAccountId
            region
            deliveryStreamName
            role
          }
          azure {
            eventHubConnectionString
            eventHubName
          }
          createdAt
          description
          id
          message
          name
          nrql
          status
          updatedAt
          payloadCompression
        }
      }
    }
  }
}
```

## 수출 압축 이해 [#compression]

선택적으로, 페이로드를 내보내기 전에 압축할 수 있지만 이 기능은 기본적으로 비활성화되어 있습니다. 이를 통해 수집된 데이터 한도에 도달하는 것을 방지하고 유출 비용을 절감할 수 있습니다.

`ruleParameters` 아래의 `payloadCompression` 필드를 사용하여 압축을 활성화할 수 있습니다. 이 필드는 다음 값 중 하나일 수 있습니다.

* `DISABLED`: 페이로드는 내보내기 전에 압축되지 않습니다. 지정하지 않으면 `payloadCompression` 이 값을 기본값으로 사용합니다.
* `GZIP`: 내보내기 전에 GZIP 형식으로 페이로드를 압축합니다.

GZIP은 현재 사용할 수 있는 유일한 압축 형식이지만 앞으로 더 많은 형식을 사용할 수 있도록 선택할 수도 있습니다.

기존 AWS 내보내기 규칙에서 압축이 활성화되면 Kinesis Firehose의 다음 메시지에 압축된 데이터와 압축되지 않은 데이터가 모두 포함될 수 있습니다. 이는 Kinesis Firehose 내의 버퍼링 때문입니다. 이를 방지하려면 압축을 활성화하기 전에 내보내기 규칙을 일시적으로 비활성화하거나 압축된 데이터만 흐르도록 새 Kinesis Firehose 스트림을 생성할 수 있습니다.

이 문제가 발생하고 S3 또는 다른 파일 스토리지 시스템으로 내보내는 경우 다음 단계에 따라 데이터의 압축된 부분을 볼 수 있습니다.

1. 개체를 수동으로 다운로드합니다.
2. 압축된 데이터를 새 파일에 복사하여 개체를 두 개의 개별 파일로 분리합니다.
3. 새로운 압축 전용 데이터 파일의 압축을 풉니다.

압축된 데이터가 있으면 이를 S3(또는 사용 중인 다른 서비스)에 다시 업로드하고 이전 파일을 삭제할 수 있습니다.

S3 또는 다른 파일 스토리지 시스템에서는 객체가 연속적으로 추가되는 여러 GZIP 인코딩 페이로드로 구성될 수 있다는 점에 유의하세요. 따라서 압축 해제 라이브러리에는 이러한 연결된 GZIP 페이로드를 처리할 수 있는 기능이 있어야 합니다.

### AWS의 자동 압축 풀기

데이터가 AWS에 도착하면 자동으로 압축을 풀 수 있는 옵션이 필요할 수 있습니다. 해당 데이터를 S3 버킷으로 스트리밍하는 경우 자동 압축 풀기를 활성화하는 두 가지 방법이 있습니다.

<CollapserGroup>
  <Collapser id="collapser-1" title="객체 Lambda 액세스 포인트">
    액세스 포인트는 S3 버킷의 객체에 액세스하고 다운로드할 수 있는 별도의 방법으로 작동합니다. AWS는 액세스 포인트를 통해 액세스되는 각 S3 객체에 대해 Lambda 기능을 수행하는 [객체 Lambda 액세스 포인트](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingMetadata.html) 라는 기능을 제공합니다. 이러한 액세스 포인트를 활성화하려면 다음 단계를 따르십시오.

    1. [이 페이지](https://docs.aws.amazon.com/AmazonS3/latest/userguide/olap-examples.html#olap-examples-3) 로 이동하여 서버리스 저장소 링크를 클릭하세요.

    2. 페이지 하단의 <DNT>**Deploy**</DNT> 버튼을 클릭하세요.

    3. [S3 버킷에 액세스 포인트를 설정합니다](https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-access-points.html).

    4. [객체 Lambda 액세스 포인트를 생성합니다](https://docs.aws.amazon.com/AmazonS3/latest/userguide/olap-create.html). 이 액세스 포인트에는 다음 설정이 있어야 합니다.

       * 이 Lambda 액세스 포인트의 <DNT>**Supporting Access Point**</DNT> 는 S3 버킷에 설정한 액세스 포인트로 설정되어야 합니다.
       * <DNT>**Transformation Configuration**</DNT> 아래:
       * <DNT>**GetObject**</DNT> 상자를 선택해야 합니다.
       * DecompressGZFunction Lambda 함수(또는 다른 압축 형식이 사용되는 경우 필요한 다른 함수)를 지정해야 합니다.
  </Collapser>

  <Collapser id="collapser-2" title="메타데이터 설정 Lambda 함수">
    AWS는 S3에서 다운로드한 객체에 올바른 메타데이터 세트가 있는 경우 자동으로 객체의 압축을 풉니다. 우리는 설정된 S3 객체에 다운로드된 모든 새 객체에 이 메타데이터를 자동으로 적용하는 함수를 작성했습니다. 설정 방법은 다음과 같습니다.

    1. [여기로](https://github.com/newrelic/metadata-setting-lambda-function) 이동하여 리포지토리를 로컬로 복제하고 README 파일에 제공된 단계에 따라 람다 함수가 포함된 ZIP 파일을 생성하세요.
    2. 함수에 대한 [IAM 역할을](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html) 생성합니다.

    * [역할을 생성할](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-service.html#roles-creatingrole-service-console) 때 신뢰할 수 있는 엔터티 유형을 &quot;AWS 서비스&quot;로 설정하고 사용 사례로 &quot;Lambda&quot;를 설정해야 합니다.
    * 이 역할에는 `s3:PutObject` 및 `s3:GetObject` 권한이 있는 정책이 있어야 합니다.

    3. AWS에서 [Lambda 함수 페이지](https://console.aws.amazon.com/lambda/home#/functions) 로 이동합니다.

    4. <DNT>**Create function**</DNT> 을(를) 클릭합니다.

    5. Java 11 런타임 환경을 선택합니다.

    6. <DNT>**Change default execution role &gt; Use an existing role**</DNT> 클릭합니다. 여기에 2단계에서 생성한 역할을 입력합니다.

    7. 아래로 스크롤하여 <DNT>**Create function**</DNT> 클릭합니다.

    8. 함수가 생성되면 <DNT>**Upload from**</DNT> 클릭하고 드롭다운 메뉴에서 <DNT>**.zip or .jar file**</DNT> 선택하세요.

    9. 팝업 상자에서 <DNT>**Upload**</DNT> 클릭하고 1단계에서 생성된 ZIP 파일을 선택합니다.

    10. 업로드가 완료되면 <DNT>**Save**</DNT> 클릭하여 팝업 상자를 종료하세요.

    11. 핸들러를 추가하여 <DNT>**Runtime settings**</DNT> 편집합니다. 제공된 함수에서 핸들러는 <DNT>`metadatasetter.App::handleRequest`</DNT> 입니다.

    12. 이제 남은 일은 이 Lambda 함수가 S3 객체 생성 시 트리거되도록 활성화하는 것입니다. 설정을 시작하려면 <DNT>**Add trigger**</DNT> 클릭하세요.

    13. 다운 드롭 메뉴에서 소스로 <DNT>**S3**</DNT> 선택하세요.

    14. <DNT>**Bucket**</DNT> 필드에 메타데이터를 적용하려는 S3 버킷의 이름을 입력합니다.

    15. 이벤트 유형에서 기본 <DNT>**All object create events**</DNT> 를 제거합니다. 이벤트 유형 드롭다운 메뉴에서 <DNT>**PUT**</DNT> 선택하세요.

    16. <DNT>**Recursive invocation**</DNT> 상자를 선택한 다음 오른쪽 하단에서 <DNT>**Add**</DNT> 클릭합니다.

        이제 Lambda 함수는 새로 추가된 모든 S3 객체에 압축 메타데이터를 자동으로 추가하기 시작합니다.
  </Collapser>
</CollapserGroup>

### Azure의 자동 압축 풀기

데이터를 Azure로 내보내는 경우 [Stream Analytics 작업을](https://learn.microsoft.com/en-us/azure/event-hubs/process-data-azure-stream-analytics) 사용하여 이벤트 허브에 저장된 개체의 압축이 풀린 버전을 볼 수 있습니다. 이렇게 하려면 다음 단계를 따르세요.

1. [이 가이드를](https://learn.microsoft.com/en-us/azure/event-hubs/process-data-azure-stream-analytics) 16단계까지 따르세요.

* 13단계에서는 아무것도 중단하지 않고 출력과 동일한 이벤트 허브를 사용하도록 선택할 수 있지만, 17단계로 진행하여 작업을 시작하려는 경우에는 테스트되지 않았으므로 권장하지 않습니다.

2. 스트리밍 분석 작업의 왼쪽 창에서 <DNT>**Inputs**</DNT> 클릭한 다음 설정한 입력을 클릭합니다.
3. 오른쪽에 나타나는 창 아래쪽으로 스크롤하고 다음 설정으로 입력을 구성합니다.

* 이벤트 직렬화 형식: JSON
* 인코딩: UTF-8
* 이벤트 압축 유형: GZip

4. 창 하단에서 <DNT>**Save**</DNT> 클릭합니다.
5. 화면 측면에서 <DNT>**Query**</DNT> 클릭합니다. 이제 <DNT>**Input preview**</DNT> 탭을 사용하여 이 화면에서 이벤트 허브를 쿼리할 수 있습니다.