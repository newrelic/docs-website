---
title: '스트리밍 데이터 내보내기: 데이터를 AWS Kinesis Firehose 또는 Azure Event Hub로 전송'
tags:
  - APIs
  - NerdGraph
metaDescription: 'With the New Relic streaming export feature, you can send your data as it''s ingested to New Relic to an AWS Kinesis Firehose or Azure Event Hub.'
freshnessValidatedDate: never
translationType: machine
---

import {Collapser} from "@newrelic/gatsby-theme-newrelic";

[Data Plus](/docs/accounts/accounts-billing/new-relic-one-pricing-billing/data-ingest-billing/#data-plus) 에서 사용할 수 있는 스트리밍 내보내기 기능을 사용하면 New Relic에서 데이터를 수집할 때 AWS Kinesis Firehose 또는 Azure Event Hub로 데이터를 보낼 수 있습니다. [NerdGraph](/docs/apis/nerdgraph/get-started/introduction-new-relic-nerdgraph) 를 사용하여 스트리밍 규칙을 만들고 업데이트하는 방법과 기존 규칙을 보는 방법을 설명합니다. [NerdGraph 탐색기](/docs/apis/nerdgraph/get-started/nerdgraph-explorer) 를 사용하여 이러한 호출을 할 수 있습니다.

## 스트리밍 내보내기란 무엇입니까? [#definition]

New Relic 조직에서 데이터를 수집하면 스트리밍 내보내기 기능이 해당 데이터를 AWS Kinesis Firehose 또는 Azure Event Hub로 보냅니다. [NRQL을](/docs/query-your-data/nrql-new-relic-query-language/get-started/introduction-nrql-new-relics-query-language) 사용하여 정의된 사용자 정의 규칙을 설정하여 내보낼 New Relic 데이터의 종류를 제어할 수 있습니다. 새로운 [압축 내보내기](#compression) 기능을 사용하여 데이터를 내보내기 전에 압축하도록 선택할 수도 있습니다.

스트리밍 내보내기를 사용할 수 있는 몇 가지 예:

* 데이터 레이크를 채우려면
* AI/ML 교육 강화
* 규정 준수, 법적 또는 보안상의 이유로 장기 보존

원할 때마다 스트리밍 내보내기 규칙을 비활성화하거나 활성화할 수 있습니다. 그러나 스트리밍 내보내기는 현재 수집된 데이터만 전송합니다. 즉, 비활성화했다가 다시 활성화하면 꺼져 있을 때 수집된 데이터가 이 기능과 함께 전송되지 않습니다. 과거 데이터를 내보내려면 [기록 데이터 내보내기](/docs/apis/nerdgraph/examples/nerdgraph-historical-data-export) 를 사용할 수 있습니다.

## 요구 사항 및 제한 사항 [#requirements]

스트리밍 데이터 제한: 한 달에 스트리밍할 수 있는 데이터의 양은 한 달에 [수집된 총 데이터](/docs/accounts/accounts-billing/new-relic-one-pricing-billing/data-ingest-billing/#usage-calculation) 에 의해 제한됩니다. 스트리밍 데이터 양이 수집된 데이터 양을 초과하는 경우 스트리밍 내보내기에 대한 액세스 및 사용을 일시 중지할 수 있습니다.

권한 관련 요구 사항:

* [Data Plus](/docs/accounts/accounts-billing/new-relic-one-pricing-billing/data-ingest-billing/#data-plus) 옵션이 포함된 Pro 또는 Enterprise 에디션
* 사용자 유형: [핵심 사용자 또는 전체 플랫폼 사용자](/docs/accounts/accounts-billing/new-relic-one-user-management/user-type)
* [스트리밍 데이터 권한](/docs/accounts/accounts-billing/new-relic-one-user-management/user-permissions#data-platform)

New Relic 데이터를 수신하도록 설정된 AWS Kinesis Firehose 또는 Azure Event Hub가 있어야 합니다. 아직 이 작업을 수행하지 않은 경우 [AWS](#firehose-setup) 또는 [Azure](#event-hub-setup) 에 대한 아래 단계를 따를 수 있습니다.

NRQL 요구 사항:

* 집계가 없는 단순 쿼리여야 합니다. 예를 들어 `SELECT *` 또는 `SELECT column1, column2` 형식이 지원됩니다.
* 하위 쿼리를 제외한 `WHERE` 절의 모든 항목에 적용할 수 있습니다.
* 쿼리에는 `FACET` 절, `COMPARE WITH` 또는 `LOOKUP` 가 있을 수 없습니다.
* 중첩 쿼리는 지원되지 않습니다.
* [메트릭 타임슬라이스 데이터](/docs/data-apis/understand-data/new-relic-data-types/#timeslice-data) 가 아닌 [NRDB에 저장된 데이터 유형을](/docs/data-apis/understand-data/new-relic-data-types/#timeslice-data) 지원합니다.

## AWS Kinesis Firehose 설정 [#firehose-setup]

AWS로 스트리밍 데이터 내보내기를 설정하려면 먼저 Amazon Kinesis Firehose를 설정해야 합니다. 다음 세 단계에서 해당 절차를 안내합니다.

### 1단계. 스트리밍 내보내기를 위한 Firehose 생성 [#create-firehose]

New Relic 데이터를 스트리밍할 전용 Firehose를 생성:

1. Amazon Kinesis Data Firehose로 이동합니다.
2. 배달 스트림을 만듭니다.
3. 스트림 이름을 지정합니다(이 이름은 나중에 규칙 등록에서 사용합니다).
4. **Direct PUT 또는 기타 소스** 를 사용하고 New Relic의 JSON 이벤트 형식(예: S3, Redshift 또는 OpenSearch)과 호환되는 대상을 지정합니다.

### 2단계. IAM Firehose 쓰기 액세스 정책 생성 [#create-policy]

1. IAM으로 이동하여 **정책** 을 선택합니다.
2. 정책을 만듭니다.
3. Firehose 서비스를 선택한 다음 `PutRecord` 및 `PutRecordBatch` 을 선택합니다.
4. `Resources` 의 경우 전송 스트림을 선택하고 ARN을 추가한 다음 스트림의 지역을 선택합니다.
5. AWS 계정 번호를 입력한 다음 이름 상자에 원하는 전송 스트림 이름을 입력합니다.
6. 정책을 만듭니다.

### 3단계. New Relic 쓰기 액세스 권한을 부여하기 위한 IAM 역할 생성 [#iam-role]

IAM 역할을 설정하려면:

1. IAM으로 이동하여 **역할** 을 클릭합니다.
2. AWS 계정에 대한 역할을 생성한 다음 **다른 AWS 계정에 대해** 선택합니다.
3. New Relic 내보내기 계정 ID 입력: `888632727556` .
4. **외부 ID 필요** 를 선택하고 내보낼 New Relic 계정의 [계정 ID](/docs/accounts/accounts-billing/account-structure/account-id) 를 입력합니다.
5. **권한** 을 클릭한 다음 위에서 생성한 정책을 선택합니다.
6. 역할 이름(내보내기 등록에 사용됨) 및 설명을 추가합니다.
7. 역할을 만듭니다.

완료되면 NerdGraph를 사용하여 내보내기 규칙을 설정하는 작업을 할 수 있습니다. 자세한 내용은 계속 읽으십시오.

## Azure 이벤트 허브 설정 [#event-hub-setup]

Azure로 스트리밍 데이터 내보내기를 설정하려면 먼저 Event Hub를 설정해야 합니다. 다음 세 단계에서 해당 절차를 안내합니다.

또는 [여기](https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-create) 에서 Azure 가이드를 따를 수 있습니다.

### 1단계. Event Hubs 네임스페이스 만들기 [#create-event-hubs-namespace]

1. Microsoft Azure 계정 내에서 Event Hubs로 이동합니다.
2. 단계에 따라 Event Hubs 네임스페이스를 만듭니다. 모든 데이터를 받을 수 있도록 자동 팽창을 활성화하는 것이 좋습니다.
3. 공개 액세스가 활성화되어 있는지 확인하십시오. 공유 액세스 정책을 사용하여 Event Hub에서 안전하게 인증합니다.
4. Event Hubs 네임스페이스가 배포되면 **리소스로 이동 을** 클릭합니다.

### 2단계. 이벤트 허브 만들기 [#create-event-hub]

1. 왼쪽 열에서 **Event Hubs** 를 클릭합니다.
2. 그런 다음 **+이벤트 허브** 를 클릭하여 이벤트 허브를 만듭니다.
3. 원하는 Event Hub 이름을 입력합니다. 나중에 스트리밍 내보내기 규칙을 만드는 데 필요하므로 저장합니다.
4. Event Hub가 생성되면 Event Hub를 클릭합니다.

### 3단계. 공유 액세스 정책 생성 및 연결 [#event-hub-policy]

1. 왼쪽 열에서 **공유 액세스 정책** 으로 이동합니다.
2. 페이지 상단에서 **+추가** 를 클릭합니다.
3. 공유 액세스 정책의 이름을 선택하십시오.
4. **보내기** 를 선택하고 **만들기** 를 클릭합니다.
5. 생성된 정책을 클릭하고 **연결 문자열–기본 키** 를 복사합니다. 이를 저장하여 인증하고 데이터를 Event Hub로 전송합니다.

완료되면 NerdGraph를 사용하여 내보내기 규칙을 설정하는 작업을 할 수 있습니다. 자세한 내용은 계속 읽으십시오.

## 중요 필드 [#fields]

우리가 다룰 대부분의 스트리밍 데이터 내보내기 NerdGraph 호출은 계정과 관련된 몇 가지 필드를 사용합니다.

AWS Kinesis Firehose의 경우:

* `awsAccountId`: [AWS 계정 ID](https://docs.aws.amazon.com/IAM/latest/UserGuide/console_account-alias.html) . 예를 들어: `10000000000`
* `deliveryStreamName`: [Kinesis 스트림 이름](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_CreateStream.html) 입니다. 예: `firehose-test-stream` .
* `region`:[AWS 리전](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html) . 예: `us-east-1` .
* `role`: Kinesis Firehose에 대한 [AWS IAM 역할](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html) 입니다. 이것은 항상 `firehose-role` 입니다.

Azure 이벤트 허브의 경우:

* `eventHubConnectionString`: [Azure 이벤트 허브 연결 문자열](https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-get-connection-string) 입니다. 다음과 유사합니다. `Endpoint=sb://<NamespaceName>.servicebus.windows.net/;SharedAccessKeyName=<KeyName>;SharedAccessKey=<KeyValue>;EntityPath=<EventHubName>`
* `eventHubName`: 이벤트 허브 이름입니다. 예: `my-event-hub` .

## 스트리밍 내보내기 규칙을 만드는 방법 [#create-stream]

먼저 내보낼 데이터를 결정합니다. 그런 다음 NerdGraph 호출로 NRQL을 사용하여 원하는 스트리밍 규칙을 생성합니다. 몇 가지 예를 들어보겠습니다.

### 스트림 생성 [#create-stream]

새 스트리밍 규칙을 만들 때 다음 필드가 모두 필요합니다. 다음은 AWS Kinesis Firehose로 내보내는 스트리밍 규칙을 생성하는 예입니다.

```graphql
mutation {
  streamingExportCreateRule(
    accountId: YOUR_NR_ACCOUNT_ID,
    ruleParameters: {
      description: "ADD_RULE_DESCRIPTION",
      name: "PROVIDE_RULE_NAME",
      nrql: "SELECT * FROM NodeStatus",
      payloadCompression: DISABLED
    },
    awsParameters: {
      awsAccountId: "YOUR_AWS_ACCOUNT_ID",
      deliveryStreamName: "FIREHOSE_STREAM_NAME",
      region: "SPECIFY_AWS_REGION",
      role: "firehose-role"
    }
  ) {
    id
    status
  }
}
```

다음은 Azure Event Hub로 내보내는 스트리밍 규칙을 만드는 예입니다.

```graphql
mutation {
  streamingExportCreateRule(
    accountId: YOUR_NR_ACCOUNT_ID,
    ruleParameters: {
      description: "ADD_RULE_DESCRIPTION",
      name: "PROVIDE_RULE_NAME",
      nrql: "SELECT * FROM NodeStatus",
      payloadCompression: DISABLED
    },
    azureParameters: {
      eventHubConnectionString: "YOUR_EVENT_HUB_SAS_CONNECTION_STRING",
      eventHubName: "YOUR_EVENT_HUB_NAME"
    }
  ) {
    id
    status
  }
}
```

규칙 ID 및 상태와 함께 즉시 결과를 얻을 수 있습니다. 상태는 `CREATION_IN_PROGRESS` 으로 표시됩니다. 규칙 ID를 사용하여 규칙이 성공적으로 생성되었는지 확인할 수 있습니다.

정책 유효성 검사에 시간이 걸리므로 규칙 생성을 완료하는 데 최대 6분이 걸릴 수 있습니다.

규칙 등록이 완료되기 전에는 규칙이 생성 프로세스에 대해 잠겨 있기 때문에 다른 변형 작업(예: `Enable` , `Disable` 또는 `Update` )을 시작할 수 없습니다. 규칙이 등록 프로세스를 완료하기 전에 다른 변형 작업을 시도하면 "현재 내보내기 규칙이 다른 요청에 의해 업데이트되고 있습니다. 잠시 기다렸다가 다시 시도하십시오."와 같은 메시지가 표시됩니다.

언제든지 `Delete` 을(를) 사용할 수 있습니다.

규칙 생성에 필요한 대략 6분 이내에 생성을 완료하고 언제든지 상태를 변경할 수 있습니다. 상태가 `ENABLED` , `DISABLED` 또는 `CREATION_FAILED` 로 변경됩니다.

값에 대한 다음 세부정보를 참조하세요.

* `ENABLED` 규칙이 성공적으로 생성되고 데이터 스트리밍이 시작되었음을 의미합니다.
* `CREATION_FAILED` 생성 시 규칙이 실패했음을 의미합니다. 이는 여러 가지 이유로 발생할 수 있지만 종종 AWS 정책 또는 Azure SAS 유효성 검사 실패로 인해 발생합니다.
* `DISABLED` 규칙이 생성되었지만 필터 스트림 제한에 도달했거나 필터 스트림 규칙 생성 실패와 같은 이유로 아직 활성화되지 않았음을 의미합니다. 6분 후에도 상태가 계속 `CREATION_IN_PROGRESS` 으로 유지되면 서비스의 시스템 오류로 인해 규칙 생성이 실패했음을 의미합니다. 규칙을 삭제하고 새 규칙을 다시 만들 수 있습니다.

스트리밍 규칙이 생성되면 [볼 수 있습니다](#view-stream) .

### 스트림 업데이트 [#update-stream]

새 스트리밍 규칙을 업데이트할 때 다음 필드가 모두 필요합니다. 다음은 스트리밍 규칙을 업데이트하는 예입니다.

AWS 키네시스 파이어호스:

```graphql
mutation {
  streamingExportUpdateRule(
    id: RULE_ID,
    ruleParameters: {
      description: "ADD_RULE_DESCRIPTION",
      name: "PROVIDE_RULE_NAME",
      nrql: "YOUR_NRQL_QUERY",
      payloadCompression: DISABLED
    },
    awsParameters: {
      awsAccountId: "YOUR_AWS_ACCOUNT_ID",
      deliveryStreamName: "FIREHOSE_STREAM_NAME",
      region: "SPECIFY_AWS_REGION",
      role: "firehose-role"
    }
  ) {
    id
    status
  }
}
```

Azure 이벤트 허브:

```graphql
mutation {
  streamingExportUpdateRule(
    id: RULE_ID,
    ruleParameters: {
      description: "ADD_RULE_DESCRIPTION",
      name: "PROVIDE_RULE_NAME",
      nrql: "YOUR_NRQL_QUERY",
      payloadCompression: DISABLED
    },
    azureParameters: {
      eventHubConnectionString: "YOUR_EVENT_HUB_SAS_CONNECTION_STRING",
      eventHubName: "YOUR_EVENT_HUB_NAME"
    }
  ) {
    id
    status
  }
}
```

업데이트할 때 메시지 필드에 "내보내기 규칙이 업데이트되고 있으며 프로세스를 완료하는 데 몇 분 정도 걸릴 수 있습니다. 나중에 다시 확인해 주세요.” 완전히 업데이트되는 데 최대 6분이 소요될 수 있습니다.

규칙을 검색하기 위해 `streamingRule` 를 호출하여 규칙이 업데이트되었는지 확인할 수 있습니다. 규칙이 업데이트되는 기간 동안 규칙이 잠기고 규칙에 대해 다른 변형 작업을 수행할 수 없습니다. 동일한 규칙에 대해 다른 변형 작업을 수행하려고 하면 "현재 내보내기 규칙이 다른 요청에 의해 업데이트되고 있습니다. 잠시 기다렸다가 나중에 다시 시도하십시오."라는 메시지가 표시됩니다. 사용자는 삭제된 규칙을 제외한 모든 상태의 규칙을 업데이트할 수 있습니다.

### 스트림 비활성화 [#disable-stream]

규칙을 비활성화하려면 규칙 ID만 제공하면 됩니다. 다음은 스트림을 비활성화하는 예입니다.

```graphql
mutation {
  streamingExportDisableRule(id: RULE_ID) {
    id
    status
    message
  }
}
```

규칙 상태가 `ENABLED` 인 경우에만 규칙을 비활성화할 수 있습니다. 다른 상태에 있는 규칙을 비활성화하려고 하면 "허용되지 않는 상태로 인해 내보내기 규칙을 활성화하거나 비활성화할 수 없습니다."라는 오류 메시지가 반환됩니다. 다른 변형이 수행되어 규칙이 잠긴 경우 규칙을 비활성화할 수 없습니다.

### 스트림 활성화 [#enable-stream]

규칙을 활성화하려면 규칙 ID만 제공하면 됩니다. 다음은 스트림을 활성화하는 예입니다.

```graphql
mutation {
  streamingExportEnableRule(id: RULE_ID) {
    id
    status
    message
  }
}
```

상태가 `DISABLED` 인 경우에만 규칙을 활성화할 수 있습니다. 다른 상태의 규칙을 활성화하려고 하면 "허용되지 않는 상태로 인해 내보내기 규칙을 활성화하거나 비활성화할 수 없습니다."와 같은 오류 메시지가 반환됩니다. 수행 중인 다른 변형으로 인해 규칙이 잠긴 경우 규칙을 활성화할 수 없습니다.

### 스트림 삭제 [#delete-stream]

스트림을 삭제하려면 규칙 ID를 제공해야 합니다. 다음은 예입니다.

```graphql
mutation {
  streamingExportDeleteRule(id: RULE_ID) {
    id
    ...
  }
}
```

삭제는 이미 삭제되지 않은 상태의 규칙에서 수행할 수 있습니다. 규칙을 삭제하면 다시 활성화할 수 없습니다. 규칙 ID로 `steamingRule` API를 호출하면 삭제 후 처음 24시간 이내에 규칙을 계속 볼 수 있습니다. 24시간이 지나면 NerdGraph를 통해 더 이상 규칙을 검색할 수 없습니다.

## 스트림 보기 [#view-stream]

계정 ID 및 규칙 ID를 쿼리하여 특정 스트림 규칙에 대한 정보를 쿼리할 수 있습니다. 다음은 예입니다.

AWS 키네시스 파이어호스:

```graphql
{
  actor {
    account(id: YOUR_NR_ACCOUNT_ID) {
      streamingExport {
        streamingRule(id: "RULE_ID") {
          aws {
            awsAccountId
            deliveryStreamName
            region
            role
          }
          createdAt
          description
          id
          message
          name
          nrql
          status
          updatedAt
          payloadCompression
        }
      }
    }
  }
}
```

Azure 이벤트 허브:

```graphql
{
  actor {
    account(id: YOUR_NR_ACCOUNT_ID) {
      streamingExport {
        streamingRule(id: "RULE_ID") {
          azure {
            eventHubConnectionString
            eventHubName
          }
          createdAt
          description
          id
          message
          name
          nrql
          status
          updatedAt
          payloadCompression
        }
      }
    }
  }
}
```

모든 기존 스트림을 쿼리할 수도 있습니다. 다음은 예입니다.

```graphql
{
  actor {
    account(id: YOUR_NR_ACCOUNT_ID) {
      streamingExport {
        streamingRules {
          aws {
            awsAccountId
            region
            deliveryStreamName
            role
          }
          azure {
            eventHubConnectionString
            eventHubName
          }
          createdAt
          description
          id
          message
          name
          nrql
          status
          updatedAt
          payloadCompression
        }
      }
    }
  }
}
```

## 압축 내보내기 [#compression]

선택적으로 페이로드를 내보내기 전에 압축할 수 있지만 기본적으로는 비활성화되어 있습니다. 이렇게 하면 수집된 데이터 한도에 도달하는 것을 방지하고 수신 비용을 절약할 수 있습니다.

`ruleParameters` 아래의 `payloadCompression` 필드를 사용하여 압축을 활성화할 수 있습니다. 이 필드는 다음 값 중 하나일 수 있습니다.

* `DISABLED`: 페이로드는 내보내기 전에 압축되지 않습니다. 지정하지 않으면 `payloadCompression` 이 값을 기본값으로 사용합니다.
* `GZIP`: 내보내기 전에 GZIP 형식으로 페이로드를 압축합니다.

GZIP은 현재 사용할 수 있는 유일한 압축 형식이지만 앞으로 더 많은 형식을 사용할 수 있도록 선택할 수도 있습니다.

기존 AWS 내보내기 규칙에서 압축이 활성화되면 Kinesis Firehose의 다음 메시지에 압축된 데이터와 압축되지 않은 데이터가 모두 포함될 수 있습니다. 이는 Kinesis Firehose 내의 버퍼링 때문입니다. 이를 방지하려면 압축을 활성화하기 전에 내보내기 규칙을 일시적으로 비활성화하거나 압축된 데이터만 흐르도록 새 Kinesis Firehose 스트림을 생성할 수 있습니다.

이 문제가 발생하고 S3 또는 다른 파일 스토리지 시스템으로 내보내는 경우 다음 단계에 따라 데이터의 압축된 부분을 볼 수 있습니다.

1. 개체를 수동으로 다운로드합니다.
2. 압축된 데이터를 새 파일에 복사하여 개체를 두 개의 개별 파일로 분리합니다.
3. 새로운 압축 전용 데이터 파일의 압축을 풉니다.

압축된 데이터가 있으면 이를 S3(또는 사용 중인 다른 서비스)에 다시 업로드하고 이전 파일을 삭제할 수 있습니다.

### AWS의 자동 압축 풀기

데이터가 AWS에 도착하면 자동으로 압축을 풀 수 있는 옵션이 필요할 수 있습니다. 해당 데이터를 S3 버킷으로 스트리밍하는 경우 자동 압축 풀기를 활성화하는 두 가지 방법이 있습니다.

<CollapserGroup>
  <Collapser
    id="collapser-1"
    title="객체 Lambda 액세스 포인트"
  >
    액세스 포인트는 S3 버킷의 객체에 액세스하고 다운로드할 수 있는 별도의 방법으로 작동합니다. AWS는 액세스 포인트를 통해 액세스되는 각 S3 객체에 대해 Lambda 기능을 수행하는 [객체 Lambda 액세스 포인트](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingMetadata.html) 라는 기능을 제공합니다. 이러한 액세스 포인트를 활성화하려면 다음 단계를 따르십시오.

    1. [이 페이지](https://docs.aws.amazon.com/AmazonS3/latest/userguide/olap-examples.html#olap-examples-3) 로 이동하여 서버리스 저장소 링크를 클릭하세요.

    2. 페이지 하단의 **Deploy** \[배포] 버튼을 클릭합니다.

    3. [S3 버킷에 액세스 포인트를 설정합니다](https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-access-points.html).

    4. [객체 Lambda 액세스 포인트를 생성합니다](https://docs.aws.amazon.com/AmazonS3/latest/userguide/olap-create.html). 이 액세스 포인트에는 다음 설정이 있어야 합니다.

       * 이 Lambda 액세스 포인트의 **Supporting Access Point** \[지원 액세스 포인트는] S3 버킷에 설정한 액세스 포인트로 설정되어야 합니다.
       * **Transformation Configuration** \[변환 구성] 에서 다음을 수행합니다.
       * **GetObject** 상자를 선택해야 합니다.
       * DecompressGZFunction Lambda 함수(또는 다른 압축 형식이 사용되는 경우 필요한 다른 함수)를 지정해야 합니다.
  </Collapser>

  <Collapser
    id="collapser-2"
    title="메타데이터 설정 Lambda 함수"
  >
    AWS는 S3에서 다운로드한 객체에 올바른 메타데이터 세트가 있는 경우 자동으로 객체의 압축을 풉니다. 우리는 설정된 S3 객체에 다운로드된 모든 새 객체에 이 메타데이터를 자동으로 적용하는 함수를 작성했습니다. 설정 방법은 다음과 같습니다.

    1. [여기로](https://github.com/newrelic/metadata-setting-lambda-function) 이동하여 리포지토리를 로컬로 복제하고 README 파일에 제공된 단계에 따라 람다 함수가 포함된 ZIP 파일을 생성하세요.
    2. 함수에 대한 [IAM 역할을](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html) 생성합니다.

    * [역할을 생성할](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-service.html#roles-creatingrole-service-console) 때 신뢰할 수 있는 엔터티 유형을 "AWS 서비스"로 설정하고 사용 사례로 "Lambda"를 설정해야 합니다.
    * 이 역할에는 `s3:PutObject` 및 `s3:GetObject` 권한이 있는 정책이 있어야 합니다.

    3. AWS에서 [Lambda 함수 페이지](https://console.aws.amazon.com/lambda/home#/functions) 로 이동합니다.

    4. **Create function** \[함수 생성 을] 클릭합니다.

    5. Java 11 런타임 환경을 선택합니다.

    6. **Change default execution role > Use an existing role** 클릭합니다. 여기에 2단계에서 생성한 역할을 입력합니다.

    7. 아래로 스크롤하여 **Create function** \[함수 생성 을] 클릭합니다.

    8. 함수가 생성되면 다음에서 **Upload from** \[업로드를] 클릭하고 드롭다운에서 **.zip or .jar file** \[.zip 또는 .jar 파일을] 선택합니다.

    9. 팝업 상자에서 **Upload** \[업로드를] 클릭하고 1단계에서 생성한 ZIP 파일을 선택합니다.

    10. 업로드가 완료되면 **Save** \[저장을] 클릭하여 팝업 상자를 종료하세요.

    11. 이제 남은 일은 이 Lambda 함수가 S3 객체 생성 시 트리거되도록 활성화하는 것입니다. **Add trigger** \[트리거 추가를] 클릭하여 설정을 시작하세요.

    12. 드롭다운에서 **S3**를 소스로 선택합니다.

    13. **Bucket** \[버킷] 필드에 메타데이터를 적용하려는 S3 버킷의 이름을 입력합니다.

    14. 이벤트 유형에서 기본 **All object create event** \[모든 개체 생성 이벤트를] 제거합니다. 이벤트 유형 드롭다운에서 **PUT**을 선택합니다.

    15. **Recursive invocation** \[재귀 호출] 상자를 선택한 다음 오른쪽 하단에서 **Add** \[추가를] 클릭합니다.

        이제 Lambda 함수는 새로 추가된 모든 S3 객체에 압축 메타데이터를 자동으로 추가하기 시작합니다.
  </Collapser>
</CollapserGroup>

### Azure의 자동 압축 풀기

데이터를 Azure로 내보내는 경우 [Stream Analytics 작업을](https://learn.microsoft.com/en-us/azure/event-hubs/process-data-azure-stream-analytics) 사용하여 이벤트 허브에 저장된 개체의 압축이 풀린 버전을 볼 수 있습니다. 이렇게 하려면 다음 단계를 따르세요.

1. [이 가이드를](https://learn.microsoft.com/en-us/azure/event-hubs/process-data-azure-stream-analytics) 16단계까지 따르세요.

* 13단계에서는 아무것도 중단하지 않고 출력과 동일한 이벤트 허브를 사용하도록 선택할 수 있지만, 17단계로 진행하여 작업을 시작하려는 경우에는 테스트되지 않았으므로 권장하지 않습니다.

2. 스트리밍 분석 작업의 왼쪽 창에서 **Inputs** \[입력 을] 클릭한 다음 설정한 입력을 클릭합니다.
3. 오른쪽에 나타나는 창 아래쪽으로 스크롤하고 다음 설정으로 입력을 구성합니다.

* 이벤트 직렬화 형식: JSON
* 인코딩: UTF-8
* 이벤트 압축 유형: GZip

4. 창 하단에서 **Save** \[저장을] 클릭합니다.
5. 화면 옆에 있는 **Query** \[쿼리를] 클릭하세요. 이제 **Input preview** \[입력 미리 보기] 탭을 사용하여 이 화면에서 이벤트 허브를 쿼리할 수 있습니다.