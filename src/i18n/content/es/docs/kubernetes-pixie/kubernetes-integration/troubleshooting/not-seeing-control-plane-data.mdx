---
title: No ver datos del plano de control
type: troubleshooting
tags:
  - Integrations
  - Kubernetes integration
  - Troubleshooting
metaDescription: Some troubleshooting tips if you are not seeing data control plane data for your New Relic's Kubernetes integration.
freshnessValidatedDate: never
translationType: machine
---

## Problemas

Ha completado el [procedimiento de instalación](/docs/kubernetes-monitoring-integration#install) para la integración de Kubernetes de New Relic, está viendo datos de Kubernetes en su cuenta de New Relic pero no hay datos de ninguno de los componentes del plano de control.

<CollapserGroup>
  <Collapser
    id="control-plane-sample-missing"
    title="Falta una muestra de ControlPlane"
  >
    En caso de que falten datos del plano de control, por ejemplo `K8sSchedulerSample`, lo primero que debe hacer es verificar el registro detallado de los componentes del plano de control. Lea cómo [habilitar el registro detallado](/docs/kubernetes-pixie/kubernetes-integration/troubleshooting/get-logs-version#verbose-logging)

    * Una posibilidad es que el autodiscovery intente encontrar en el clúster el pod del plano de control aprovechando las etiquetas más comunes, en caso de que no se encuentre ningún pod para un solo componente no deja de evitar perder más datos. En este escenario verá un registro similar al siguiente:

      ```
      time="2022-06-21T12:21:25Z" level=debug msg="Autodiscovering pods for \"scheduler\""
      time="2022-06-21T12:21:25Z" level=debug msg="0 pods found with labels \"tier=control-plane,component=kube-scheduler\""
      time="2022-06-21T12:21:25Z" level=debug msg="No pod found for \"scheduler\" with labels \"tier=control-plane,component=kube-scheduler\""
      time="2022-06-21T12:21:25Z" level=debug msg="0 pods found with labels \"k8s-app=kube-scheduler\""
      time="2022-06-21T12:21:25Z" level=debug msg="No pod found for \"scheduler\" with labels \"k8s-app=kube-scheduler\""
      time="2022-06-21T12:21:25Z" level=debug msg="0 pods found with labels \"app=openshift-kube-scheduler,scheduler=true\""
      time="2022-06-21T12:21:25Z" level=debug msg="No pod found for \"scheduler\" with labels \"app=openshift-kube-scheduler,scheduler=true\""
      time="2022-06-21T12:21:25Z" level=debug msg="No \"scheduler\" pod has been discovered"
      ```

      En este caso, puede cambiar el comportamiento de descubrimiento con la configuración `controlplane.config.[component].autodiscover[].selector` de los [valores del gráfico de](https://github.com/newrelic/nri-kubernetes/blob/main/charts/newrelic-infrastructure/values.yaml) timón. Lea más sobre [los componentes del plano de control](/docs/kubernetes-pixie/kubernetes-integration/advanced-configuration/k8s-version2/changes-since-v3/#nrk8s-controlplane).

    * También es posible que se encuentre el componente del plano de control, pero falle la autenticación con el extremo. En este escenario verá un registro similar al siguiente:

      ```
      time="2022-06-21T15:54:52Z" level=debug msg="Endpoint \"https://localhost:10257\" probe failed, skipping: http request failed with status: 403 Forbidden"
      ```

      En este caso, puede cambiar el comportamiento de autenticación para cada extremo con la configuración `controlplane.config.[component].autodiscover[].endpoints[].auth` de [los valores del gráfico de](https://github.com/newrelic/nri-kubernetes/blob/main/charts/newrelic-infrastructure/values.yaml) timón.

    * También es posible que el componente del plano de control de la integración no se esté ejecutando en todos los nodos maestros.

      Puedes volver a verificar que se esté ejecutando:

      ```
      kubectl get pod -n <NEWRELIC_NAMESPACE> -l app.kubernetes.io/component=controlplane -o wide
      ```

      Si hay algún pod de plano de control que desea monitor ejecutándose en un nodo sin una instancia de monitoreo de Newrelic, puede cambiar según sea necesario `controlplane.affinity`, `controlplane.nodeSelector` y `controlplane.tolerations` de los [valores del gráfico de](https://github.com/newrelic/nri-kubernetes/blob/main/charts/newrelic-infrastructure/values.yaml) timón.
  </Collapser>

  <Collapser
    id="control-plane-CrashLoopBackOff"
    title="El componente ControlPlane está en CrashLoopBackOff"
  >
    En caso de que los componentes del plano de control no detecten automáticamente ni eliminen con éxito ningún módulo pod plano de control que ingrese en CrashLoopBackOff.

    Como se describe en la sección anterior, puede cambiar el comportamiento del descubrimiento automático y los métodos de autenticación para satisfacer sus necesidades.

    Por otro lado, si no está interesado en esos datos, simplemente puede desactivar el componente del plano de control configurando `controlplane.enabled=false` en los [valores del gráfico de](https://github.com/newrelic/nri-kubernetes/blob/main/charts/newrelic-infrastructure/values.yaml) timón.
  </Collapser>
</CollapserGroup>

## Solución para integración versión V2

<CollapserGroup>
  <Collapser
    id="invalid-license"
    title="Verifique que los nodos maestros tengan las etiquetas correctas"
  >
    Ejecute los siguientes comandos para buscar manualmente los nodos maestros:

    ```
    kubectl get nodes -l node-role.kubernetes.io/master=""
    ```

    ```
    kubectl get nodes -l kubernetes.io/role="master"
    ```

    Si los nodos maestros siguen la convención de etiquetado definida en la [sección de documentación sobre descubrimiento de nodos maestros y componentes del plano de control](/docs/integrations/kubernetes-integration/installation/configure-control-plane-monitoring#discover-nodes-components), debería obtener un resultado como:

    ```
    NAME                         STATUS  ROLES   AGE   VERSION
    ip-10-42-24-4.ec2.internal   Ready   master  42d   v1.14.8
    ```

    Si no se encuentran nodos, existen dos escenarios:

    Sus nodos maestros no tienen las etiquetas requeridas que los identifiquen como maestros; en este caso, debe agregar ambas etiquetas a sus nodos maestros.

    Estás en un clúster administrado y tu proveedor maneja los nodos maestros por ti. En este caso no hay nada que puedas hacer, ya que tu proveedor está limitando el acceso a esos nodos.
  </Collapser>

  <Collapser
    id="unable-connect"
    title="Verifique que la integración se esté ejecutando en los nodos maestros"
  >
    Reemplace el marcador de posición en el siguiente comando con uno de los nombres de nodo devueltos en el paso anterior para ejecutar un pod de integración en un nodo maestro:

    ```
    kubectl get pods --field-selector spec.nodeName=NODE_NAME -l name=newrelic-infra --all-namespaces
    ```

    El siguiente comando es el mismo, solo que selecciona el nodo por usted:

    ```
    kubectl get pods --field-selector spec.nodeName=$(kubectl get nodes -l node-role.kubernetes.io/master="" -o jsonpath="{.items[0].metadata.name}") -l name=newrelic-infra --all-namespaces
    ```

    Si todo es correcto deberías obtener algún resultado como:

    ```
    NAME                   READY   STATUS    RESTARTS   AGE
    newrelic-infra-whvzt   1/1     Running   0          6d20h
    ```

    Si la integración no se está ejecutando en sus nodos maestros, verifique que el daemonset tenga todas las instancias deseadas ejecutándose y listas.

    ```
    kubectl get daemonsets -l app=newrelic-infra --all-namespaces
    ```
  </Collapser>

  <Collapser
    id="indicators"
    title="Verifique que los componentes del plano de control tengan las etiquetas requeridas"
  >
    Consulte la [sección de documentación sobre descubrimiento de nodos maestros y componentes del plano de control](/docs/integrations/kubernetes-integration/installation/configure-control-plane-monitoring#discover-nodes-components) y busque las etiquetas que utiliza la integración para descubrir los componentes. Luego ejecute los siguientes comandos para ver si hay algún pod con dichas etiquetas y los nodos donde se están ejecutando:

    ```
    kubectl get pods -l k8s-app=kube-apiserver --all-namespaces
    ```

    Si hay un componente con la etiqueta dada, deberías ver algo como:

    ```
    NAMESPACE    NAME                                        READY  STATUS   RESTARTS  AGE
    kube-system  kube-apiserver-ip-10-42-24-42.ec2.internal  1/1    Running  3         49d
    ```

    Lo mismo se debe hacer con el resto de componentes:

    ```
    kubectl get pods -l k8s-app=etcd-manager-main --all-namespaces
    ```

    ```
    kubectl get pods -l k8s-app=kube-scheduler --all-namespaces
    ```

    ```
    kubectl get pods -l k8s-app=kube-kube-controller-manager --all-namespaces
    ```
  </Collapser>

  <Collapser
    id="cannot-list-pods-for-cluster"
    title="Recupere el registro detallado de una de las integraciones que se ejecutan en un nodo maestro y verifique los trabajos de los componentes del plano de control."
  >
    Para recuperar el registro, siga las instrucciones sobre [cómo obtener el registro del pod que se ejecuta en un nodo maestro](/docs/integrations/kubernetes-integration/troubleshooting/get-logs-version). El registro de integración para cada componente muestra el siguiente mensaje _"Running job: COMPONENT_NAME"_. Ex:

    ```
    Running job: scheduler
    ```

    ```
    Running job: etcd
    ```

    ```
    Running job: controller-manager
    ```

    ```
    Running job: api-server
    ```

    Si no especificaste la opción de configuración `ETCD_TLS_SECRET_NAME` , encontrarás el siguiente mensaje en el registro:

    ```
    Skipping job creation for component etcd: etcd requires TLS configuration, none given
    ```

    Si ocurre algún error al consultar la métrica de algún componente, se registrará después del mensaje `Running job` .
  </Collapser>

  <Collapser
    id="cannot-list-pods-for-cluster"
    title="Consultar manualmente la métrica de los componentes."
  >
    Consulte la [sección de documentación sobre descubrimiento de nodos maestros y componentes del plano de control](/docs/integrations/kubernetes-integration/installation/configure-control-plane-monitoring#discover-nodes-components) para obtener el extremo del componente del plano de control que desea consultar. Con el extremo podemos usar el pod de integración que se ejecuta en el mismo nodo que el componente a consultar. Los siguientes son ejemplos de cómo consultar el programador Kubernetes :

    ```
    kubectl exec -ti POD_NAME -- wget -O - localhost:10251/metrics
    ```

    El siguiente comando hace lo mismo, pero también elige el pod por usted:

    ```
    kubectl exec -ti $(kubectl get pods --all-namespaces --field-selector spec.nodeName=$(kubectl get nodes -l node-role.kubernetes.io/master="" -o jsonpath="{.items[0].metadata.name}") -l name=newrelic-infra -o jsonpath="{.items[0].metadata.name}") -- wget -O - localhost:10251/metrics
    ```

    Si todo está correcto deberías obtener alguna métrica en el formato Prometheus, algo como:

    ```
    Connecting to localhost:10251 (127.0.0.1:10251)
    # HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.
    # TYPE apiserver_audit_event_total counter
    apiserver_audit_event_total 0
    # HELP apiserver_audit_requests_rejected_total Counter of apiserver requests rejected due to an error in audit logging backend.
    # TYPE apiserver_audit_requests_rejected_total counter
    apiserver_audit_requests_rejected_total 0
    # HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.
    # TYPE apiserver_client_certificate_expiration_seconds histogram
    apiserver_client_certificate_expiration_seconds_bucket{le="0"} 0
    apiserver_client_certificate_expiration_seconds_bucket{le="1800"} 0
    apiserver_client_certificate_expiration_seconds_bucket{le="3600"} 0
    ```
  </Collapser>
</CollapserGroup>