---
title: 'Diagnóstico de ingeniería de confiabilidad: una guía para principiantes sobre la resolución de problemas de rendimiento de la aplicación'
tags:
  - Observability maturity
  - 'Uptime, performance, and reliability'
  - Site reliability engineering
  - SRE
metaDescription: 'New Relic observability maturity series: A beginner''s guide on identifying common application performance issues.'
freshnessValidatedDate: never
translationType: machine
---

import solutionsOmaUprPatternNormal from 'images/solutions_screenshot-full_oma-upr-pattern-normal.webp'

import solutionsNormalPercentilePattern from 'images/solutions_screenshot-full_normal-percentile-pattern.webp'

import solutionsPatternAbnormal from 'images/solutions_screenshot-full_pattern-abnormal.webp'

import solutionsPatternAbnormalCompare from 'images/solutions_screenshot-full_pattern-abnormal-compare.webp'

Esta guía es una introducción para mejorar su habilidad para diagnosticar problemas que afectan a los clientes. Podrá recuperarse de los problemas de rendimiento de la aplicación más rápidamente siguiendo los procedimientos de esta guía.

Esta guía es parte de nuestra [serie sobre madurez de observabilidad](/docs/new-relic-solutions/observability-maturity/introduction).

## Requisitos previos

A continuación se detallan algunos requisitos y algunas recomendaciones para utilizar esta guía:

* Cobertura de observabilidad de New Relic:

  * <DoNotTranslate>**Required**</DoNotTranslate>

    : [APM con rastreo distribuido](/docs/apm/apm-ui-pages/monitoring/apm-summary-page-view-transaction-apdex-usage-data), [contexto de inicio de sesión de APM](/docs/apm/new-relic-apm/getting-started/get-started-logs-context) y [agente de infraestructura](/docs/infrastructure/infrastructure-monitoring/get-started/get-started-infrastructure-monitoring)

  * <DoNotTranslate>**Recommended**</DoNotTranslate>

    : [Registros](/docs/logs/get-started/get-started-log-management) y [Monitoreo de red](/docs/network-performance-monitoring/get-started/npm-introduction) (NPM)

* <DoNotTranslate>**Required**</DoNotTranslate>

  : [administración a nivel de servicio](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide)

* <DoNotTranslate>**Recommended**</DoNotTranslate>

  : Alguna experiencia con el uso de New Relic APM, rastreo distribuido, consultas NRQL y

  <InlinePopover type="logs"/>

  UI

* <DoNotTranslate>**Recommended**</DoNotTranslate>

  : has leído estas guías:

  * [Gestión de calidad alerta](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/alert-quality-management-guide)
  * [Administración a nivel de servicio](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide)

## Descripción general [#overview]

Antes de comenzar a utilizar esta guía, le ayudará a comprender lo que aprenderá. Esta guía le ayudará a comprender:

* Cómo se ve impactado su negocio al mejorar sus habilidades de diagnóstico.

* La clave operativa de indicadores de rendimiento utilizada para medir el éxito.

* Cómo percibe finalmente su usuario los diferentes tipos de problemas de confiabilidad.

* La diferencia entre la _causa directa_ y la _causa raíz subyacente_ de un problema.

* Los pasos básicos de diagnóstico para encontrar y resolver problemas, que incluyen:

  * Definir el problema: crear un planteamiento del problema
  * Encontrar la fuente del problema
  * Encontrar la causa directa de ese problema.

* Algunas categorías de problemas de desempeño (rendimiento de salida, desempeño de entrada y desempeño de cliente) y la característica New Relic utilizada para diagnosticar esos problemas (APM, Sintético,

  <InlinePopover type="browser"/>

  y monitoreo de móviles).

* Cómo utilizar una matriz de problemas, que es una hoja de referencia para comprender los problemas comunes y sus posibles fuentes.

Hacia el final, revisaremos algunos problemas de rendimiento de ejemplo, que deberían ayudarle a comprender mejor estos conceptos.

## Resultados deseados [#desired-outcomes]

### Resumen

El valor para el negocio es:

* Reducir el número de incidentes que perturban el negocio
* Reducir el tiempo necesario para resolver problemas (MTTR)
* Reducir el coste operativo del incidente

El valor para operaciones de TI e ingeniería de confiabilidad del sitio (SRE) es:

* Mejorar el tiempo para comprender y resolver.

### Resultado comercial [#business-outcome]

En 2014, [Gartner estimó](https://blogs.gartner.com/andrew-lerner/2014/07/16/the-cost-of-downtime) que el coste medio del tiempo de inactividad de TI era de 5.600 dólares por minuto. El costo acumulativo de un incidente que afecta el negocio está determinado por factores como el tiempo de conocimiento, la frecuencia, el tiempo de reparación, el impacto en los ingresos y la cantidad de ingenieros que clasifican y resuelven el incidente. En pocas palabras, desea menos incidentes que afecten al negocio, una duración más corta de los incidentes y diagnósticos más rápidos, con menos personas necesarias para resolver los impactos en el rendimiento.

En última instancia, el objetivo empresarial es maximizar el tiempo de actividad y minimizar el tiempo de inactividad, donde el costo del tiempo de inactividad es:

**`Downtime minutes x Cost per minute = Downtime cost`**

El tiempo de inactividad está determinado por la cantidad de incidentes que interrumpen el negocio y su duración. El costo del tiempo de inactividad incluye muchos factores, pero los que se pueden medir más directamente son el costo operativo y la pérdida de ingresos.

La empresa debe impulsar una reducción en lo siguiente:

* número de incidentes que perturban el negocio
* costo operativo del incidente

### Resultado operativo [#operational-outcome]

El resultado operativo requerido es mantener el cumplimiento de los objetivos de nivel de servicio a nivel de producto. Para ello, diagnostique el nivel de servicio degradado, comunique su diagnóstico y realice una resolución rápida. Pero siempre ocurren degradaciones e incidentes inesperados y es necesario responder con rapidez y eficacia.

En otras guías de esta serie, nos centramos en mejorar <DoNotTranslate>**time to know**</DoNotTranslate>. En nuestra [guía de gestión de calidad alerta](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/alert-quality-management-guide), nos centramos en <DoNotTranslate>**reactive**</DoNotTranslate> formas de mejorar el tiempo para saber, y en nuestra [guía de administración a nivel de servicio](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide) nos centramos en <DoNotTranslate>**proactive**</DoNotTranslate> métodos.

En la guía que estás leyendo ahora, nos centramos en mejorar <DoNotTranslate>**time to understand**</DoNotTranslate> y <DoNotTranslate>**time to resolve**</DoNotTranslate>.

## Indicadores de rendimiento clave - operacional [#operational-kpis]

Hay muchas métricas discutidas y debatidas en el mundo de la “gestión de incidentes” y la teoría de la ingeniería de confiabilidad del sitio (SRE); sin embargo, la mayoría está de acuerdo en que es importante centrarse en un pequeño conjunto de indicadores de rendimiento clave.

Los KPI a continuación son los indicadores más comunes utilizados por las prácticas exitosas de ingeniería de confiabilidad del sitio (SRE) y gestión de incidentes.

<CollapserGroup>
  <Collapser
    id="slo-compliance"
    title="Cumplimiento de los objetivos de nivel de servicio (SLO)"
  >
    Este es su indicador principal. El nivel de servicio mide el inicio de la degradación del rendimiento, la tendencia del rendimiento, el alcance del impacto y cuándo se ha resuelto el problema.

    Para conocer más sobre este proceso, consulta nuestra [guía de administración a nivel de servicio](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide).
  </Collapser>

  <Collapser
    id="time-to-know"
    title="Hora de saber"
  >
    Este es el momento en que un humano registró el incidente por primera vez. Medidas de tiempo de conocimiento entre el inicio de la degradación del nivel de servicio y el momento en que se creó un registro del problema de rendimiento. La [guía de gestión de calidad alerta](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/alert-quality-management-guide) demuestra cómo medir y mejorar esta métrica operativa.
  </Collapser>

  <Collapser
    id="time-to-understand"
    title="Hora de entender"
  >
    Es el tiempo entre el registro del incidente (tiempo para conocer) y la resolución del impacto (tiempo para resolver).
  </Collapser>

  <Collapser
    id="time-to-resolve"
    title="Es hora de resolver"
  >
    El tiempo de resolución a menudo se denomina MTTR (tiempo medio para restaurar/reparar/resolver). Se mide desde el inicio de la degradación del rendimiento (según lo determinado por los niveles de servicio) hasta el momento en que el nivel de servicio vuelve al nivel de rendimiento esperado.

    <DoNotTranslate>**Note**</DoNotTranslate>: El tiempo de resolución no significa que la causa raíz haya sido identificada y solucionada permanentemente. Las soluciones permanentes son parte del proceso de "gestión de problemas", una vez resuelto el incidente. Investigue sobre la causa raíz versus la causa directa y los "síntomas de las causas raíz".
  </Collapser>
</CollapserGroup>

## Percepción de confiabilidad del usuario final [#end-user-perception]

La forma en que los clientes perciben el rendimiento de su producto es fundamental para comprender cómo medir la urgencia y la prioridad. Además, comprender la perspectiva de los clientes ayuda a comprender cómo la empresa ve el problema, así como a comprender el flujo de trabajo requerido para respaldar las capacidades impactadas. Una vez que comprenda la percepción de los clientes y del negocio, podrá comprender mejor qué podría estar afectando la confiabilidad de dichas capacidades.

En última instancia, la observabilidad desde la perspectiva de los clientes es el primer paso para volverse proactivo y competente en ingeniería de confiabilidad.

Hay dos experiencias principales que impactan la percepción del usuario final sobre el rendimiento de su producto digital y sus capacidades. Los términos a continuación son desde la perspectiva de los clientes utilizando el lenguaje común de los clientes.

<CollapserGroup>
  <Collapser
    id="availability"
    title="Disponibilidad, es decir, no funciona."
  >
    La disponibilidad también se conoce como: conectividad, tiempo de actividad, accesibilidad. Pero también se combina con el éxito (sin errores).

    Un usuario final puede indicar que no puede acceder a una capacidad requerida, como iniciar sesión, explorar, buscar o ver inventario. O simplemente pueden indicar que todo el servicio no está disponible. Este es un síntoma de la imposibilidad de conectarse a un servicio o de que un servicio devuelve un error.

    Tradicionalmente, la "disponibilidad" o el "tiempo de actividad" se medían mediante una metodología binaria "ARRIBA/ABAJO" midiendo la capacidad de conectarse a un servicio. El método tradicional tiene una brecha crítica: solo mide cuando un servicio completo deja de estar disponible por completo. Esta medida clásica de confiabilidad da como resultado importantes lagunas de observabilidad, diagnósticos difíciles y un impacto significativo en el usuario final antes de que pueda reaccionar.

    La disponibilidad se mide tanto por "la capacidad de llegar a un servicio", también conocida como "tiempo de actividad", como por "la capacidad del servicio de devolver la respuesta esperada" (en otras palabras, "sin errores"). framework de madurez de observabilidad de New Relic distingue los dos por <DoNotTranslate>**input performance**</DoNotTranslate> (conectividad) y <DoNotTranslate>**output performance**</DoNotTranslate> (éxito y latencia de la respuesta).
  </Collapser>

  <Collapser
    id="performance"
    title="El rendimiento, también conocido como, es demasiado lento."
  >
    El rendimiento también se conoce como latencia y tiempo de respuesta.

    Un usuario final puede afirmar que el servicio es demasiado lento.

    Tanto para los líderes empresariales como de TI, el término "rendimiento" puede abarcar una variedad de cuestiones. En la administración a nivel de servicio de New Relic, la "lentitud" se mide tanto en la categoría de "producción" como en la de "cliente". Sin embargo, la mayoría de los problemas de lentitud se deben a un problema de salida, derivado de lo que tradicionalmente se denomina "servicios backend".
  </Collapser>
</CollapserGroup>

## Causa raíz versus causa directa [#root-cause-vs-direct-cause]

La causa raíz de un problema **no** es lo mismo que la causa directa de ese problema. Del mismo modo, solucionar la causa directa (a corto plazo) no suele significar que se haya solucionado la causa raíz (a largo plazo) del problema. <DoNotTranslate>**It's very important to make this distinction.**</DoNotTranslate>

Cuando busque un problema de rendimiento, primero debe intentar encontrar la causa directa del problema haciendo la pregunta "¿Qué cambió?". El componente o comportamiento que cambió no suele ser la causa raíz, pero de hecho es la causa directa que debe resolverse primero. Resolver la causa raíz es importante, pero generalmente requiere una discusión retroactiva posterior al incidente y una gestión del problema a largo plazo.

Por ejemplo: el nivel de servicio para su capacidad de inicio de sesión cae repentinamente. Inmediatamente se descubre que los patrones de tráfico son mucho más altos de lo habitual. Usted traza el problema de rendimiento a una configuración de límite de conexión TCP abierta que está provocando una cola de conexión TCP mucho más grande. Inmediatamente resuelve el problema implementando un aumento del límite de TCP y alguna instancia de servidor adicional. Resolvió la causa directa del problema a corto plazo, pero la causa raíz podría ser cualquier cosa, desde una planificación de capacidad inadecuada, una comunicación perdida por parte de marketing o un despliegue relacionado con consecuencias no deseadas en las cargas ascendentes.

Esta distinción también se hace en ITIL/ITSM <DoNotTranslate>**Incident management**</DoNotTranslate> frente a <DoNotTranslate>**Problem management**</DoNotTranslate>. Las causas fundamentales se discuten en conversaciones posteriores al incidente y luego se resuelven en procesos de gestión de problemas a más largo plazo.

## Pasos de diagnóstico (descripción general) [#diagnostic-steps]

### Paso 1: definir el problema [#create-problem-statement]

La primera regla es establecer rápidamente el planteamiento del problema. Hay muchas guías sobre cómo crear planteamientos de problemas, pero lo mejor es lo simple y efectivo. Un planteamiento del problema bien formado hará lo siguiente:

1. Describa lo que está experimentando el usuario final. ¿Cuál es el problema que está experimentando el usuario final?
2. Describa el comportamiento esperado de la capacidad del producto. ¿Qué es lo que debería experimentar el usuario final?
3. Describa el comportamiento actual de la capacidad del producto. ¿Cuál es la valoración técnica de lo que está experimentando el usuario?

Evite cualquier suposición en el planteamiento de su problema. Cíñete a los hechos.

### Paso 2: Encuentra la fuente [#find-source]

La "fuente" es el componente o código más cercano a la causa directa del problema.

Piense en muchas tuberías de agua conectadas a través de muchas uniones, divisores y válvulas. Se le alerta de que su nivel de servicio de salida de agua está degradado. Trazas el problema desde la salida del agua por las tuberías hasta determinar qué unión, split, válvula o tubería está causando el problema. Descubres que una de las válvulas eléctricas está en cortocircuito. Esa válvula es la fuente de tu problema. El corto es la causa directa de su problema. Resuelve fácilmente la causa directa reemplazando el valor. Tenga en cuenta que la causa raíz puede ser algo más complejo, como las condiciones climáticas, los productos químicos en el agua o la fabricación.

Este es el mismo concepto para diagnosticar una pila de tecnología compleja. Si su capacidad de inicio de sesión es limitada (salida), debe rastrear el problema hasta el componente (fuente) que está causando ese límite y solucionarlo. Podría ser el software API (límite de servicio), los servicios de middleware, la base de datos, las limitaciones de recursos, un servicio de terceros o cualquier otra cosa.

En TI existen tres categorías principales de puntos de interrupción para mejorar su tiempo de respuesta:

1. <DoNotTranslate>
     **Output**
   </DoNotTranslate>
2. <DoNotTranslate>
     **Input**
   </DoNotTranslate>
3. <DoNotTranslate>
     **Client**
   </DoNotTranslate>

Definir su desempeño métrico dentro de estas categorías, también conocido como [nivel de servicio](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide), mejorará significativamente su tiempo de respuesta para determinar dónde está la fuente del problema. La medición de estas categorías está cubierta en [nuestra guía de administración a nivel de servicio](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide). Para comprender cómo utilizarlos en el diagnóstico, siga leyendo.

### Paso 3: encuentre la causa directa [#find-direct-cause]

Una vez que esté cerca de la fuente del problema, determine qué cambió. Esto le ayudará a determinar rápidamente cómo resolver el problema de forma inmediata y a corto plazo. En el ejemplo del [Paso 2](#find-source), el cambio fue que la válvula ya no funcionaba debido a un hardware degradado que provocaba un cortocircuito.

Ejemplos de cambios comunes en TI son:

1. Rendimiento (tráfico)
2. Código (despliegue)
3. Recursos (asignación de hardware)
4. Cambios de dependencia ascendentes o descendentes
5. Volumen de datos

Para ver otros ejemplos comunes de problemas que afectan el rendimiento, consulte la [matriz de problemas](#problem-matrix) a continuación.

## Utilice puntos de datos de salud [#health-data-points]

Como se mencionó anteriormente, existen tres categorías de rendimiento principales que impulsan su viaje de diagnóstico. Comprender estos puntos de datos de salud reducirá significativamente el tiempo necesario para comprender dónde está el origen del problema.

<CollapserGroup>
  <Collapser
    id="output-perf"
    title="Rendimiento de salida"
  >
    <DoNotTranslate>**This requires**</DoNotTranslate>: APM

    El rendimiento de salida es la capacidad de su pila interna de tecnología para entregar las respuestas esperadas (salida) al usuario final. Esto se conoce tradicionalmente como servicios "backend".

    En la gran mayoría de escenarios, el rendimiento de la salida se mide simplemente por la velocidad de la respuesta y la calidad de la respuesta (en otras palabras, está libre de errores). Recuerde la perspectiva del usuario descrita anteriormente. El usuario final indicará que el servicio es lento, no funciona o es inaccesible.

    El problema más común es la capacidad de responder a las solicitudes finales de los usuarios de manera oportuna <DoNotTranslate>**and**</DoNotTranslate> exitosa.

    Esto se identifica fácilmente por una anomalía de latencia o una anomalía de error en los servicios que respaldan la capacidad del producto problemático.
  </Collapser>

  <Collapser
    id="input-perf"
    title="Rendimiento de entrada"
  >
    <DoNotTranslate>**This requires**</DoNotTranslate>: Sintético

    El rendimiento de entrada es simplemente la capacidad de sus servicios para recibir una solicitud del cliente. Esto no es lo mismo que la capacidad del cliente para enviar una solicitud.

    Su rendimiento de salida (servicios backend) podría estar excediendo los niveles de rendimiento esperados. Sin embargo, algo entre el cliente y sus servicios está rompiendo el ciclo de vida solicitud-respuesta. Esto podría ser cualquier cosa entre el cliente y sus servicios.
  </Collapser>

  <Collapser
    id="client-perf"
    title="Rendimiento del cliente"
  >
    <DoNotTranslate>**This requires**</DoNotTranslate>: monitoreo de navegador y/o monitoreo de móviles

    El rendimiento del cliente es la capacidad de un browser y/o aplicación móvil para realizar solicitudes y generar respuestas. browser y/o el móvil se identifican fácilmente como la fuente del problema una vez que se han descartado tanto el rendimiento de la salida (backend) como el de la entrada (Sintético).

    El rendimiento de los insumos y los productos es relativamente fácil de descartar (o descartar). Debido a la profundidad de los diagnósticos de entrada y salida, browser y los dispositivos móviles se cubrirán en una guía de diagnóstico avanzado en el futuro.
  </Collapser>
</CollapserGroup>

## Matriz de problemas [#problem-matrix]

La matriz de problemas es una hoja de referencia de problemas comunes categorizados según los tres puntos de datos de salud.

Las fuentes de los problemas están ordenadas según su frecuencia, siendo las más comunes en la fila superior y a la izquierda. A continuación se enumera un desglose más detallado. Una administración a nivel de servicio bien hecha le ayudará a descartar dos de cada tres de estos puntos de datos rápidamente.

Esta tabla es una matriz de problemas ordenada por puntos de datos de salud:

| Punto de datos | New Relic                           | Fuentes de problemas comunes                                                                                                     |
| -------------- | ----------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- |
| Producción     | APM, infraestructura, registro, NPM | Aplicación, fuentes de datos, cambio de configuración de hardware, infraestructura, redes internas, proveedor externo (AWS, GCP) |
| Aporte         | Sintético, registro                 | Enrutamiento externo (CDN, gateways, etc.), enrutamiento interno, cosas en Internet (ISP, etc.)                                  |
| Cliente        | Browser, móvil                      | Código browser o móvil                                                                                                           |

Los problemas tienden a agravarse, pero el objetivo es "encontrar la fuente" y luego determinar "qué cambió" para restaurar rápidamente el nivel de servicio.

### Problemas de ejemplo [#example-problem]

Veamos un problema de ejemplo. Digamos que su empresa lanza un nuevo producto y el aumento significativo de solicitudes provoca un tiempo de respuesta inaceptable. La fuente se descubre en el servicio de middleware de inicio de sesión. El problema es un salto en los tiempos de cola TCP.

Aquí hay un desglose de esta situación:

* <DoNotTranslate>**Category**</DoNotTranslate>

  : rendimiento de salida

* <DoNotTranslate>**Source**</DoNotTranslate>

  : iniciar sesión en el middleware

* <DoNotTranslate>**Direct cause**</DoNotTranslate>

  : Tiempos de cola TCP debido a la carga de solicitudes adicionales

* <DoNotTranslate>**Solution**</DoNotTranslate>

  : mayor límite de conexión TCP y recursos escalados

* <DoNotTranslate>**Root-cause**</DoNotTranslate>

  : planificación de capacidad insuficiente y pruebas de control de calidad en el servicio descendente que afectan el middleware de inicio de sesión

### Otro problema de ejemplo [#example-problem-2]

Aquí hay otro problema de ejemplo:

* Hubo un aumento repentino en 500 errores de puerta de enlace al iniciar sesión...
* El tiempo de respuesta de la API de inicio de sesión aumentó hasta el punto en que comenzaron los tiempos de espera...
* Los tiempos de espera se trazaron a las conexiones de la base de datos en la capa de middleware...
* La traza de la transacción reveló un aumento significativo en el número de consultas de la base de datos por solicitud de inicio de sesión...
* Se encontró un marcador de despliegue para un despliegue que ocurrió justo antes del problema.

Aquí hay un desglose de esta situación:

* <DoNotTranslate>**Category**</DoNotTranslate>

  : degradación del rendimiento de salida que conduce a una falla en el rendimiento de entrada

* <DoNotTranslate>**Source**</DoNotTranslate>

  : base de datos de llamada de servicio de middleware

* <DoNotTranslate>**Direct cause**</DoNotTranslate>

  : 10 veces mayor consulta de la base de datos después de la implementación del código

* <DoNotTranslate>**Solution**</DoNotTranslate>

  : despliegue de retroceso

* <DoNotTranslate>**Root-cause**</DoNotTranslate>

  : pruebas de garantía de calidad insuficientes

### Matriz de problemas por fuente [#problem-matrix-sources]

A continuación se muestra una tabla con la matriz de problemas ordenada por fuentes.

<table>
  <thead>
    <tr>
      <th>
        <DoNotTranslate>
          **Source**
        </DoNotTranslate>
      </th>

      <th>
        <DoNotTranslate>
          **Common direct causes**
        </DoNotTranslate>
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        Aplicación
      </td>

      <td>
        1. Despliegue reciente (código)
        2. Restricciones de recursos de hardware
        3. Restricciones de la base de datos
        4. Cambio de configuración (hardware/enrutamiento/redes)
        5. Dependencia de terceros
      </td>
    </tr>

    <tr>
      <td>
        Fuente de datos
      </td>

      <td>
        1. Restricciones de la base de datos
        2. Cambio en la lógica de consulta (n+1)
        3. Cola de mensajes (generalmente resulta en un mal desempeño del productor o del consumidor)
      </td>
    </tr>

    <tr>
      <td>
        Redes internas y enrutamiento
      </td>

      <td>
        1. Balanceadores de carga
        2. Apoderados
        3. Puertas de enlace API
        4. Enrutadores (raros)
        5. ISP/CDN (raro)
      </td>
    </tr>
  </tbody>
</table>

## Identificación de anomalías en el patrón de rendimiento [#pattern-anomalies]

<Callout variant="tip">
  Tener [un nivel de servicio](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide) bien formado en los límites de su servicio relacionados con la clave de transacción (capacidades) le ayudará a identificar más rápidamente en qué flujo de trabajo de extremo a extremo reside el problema.
</Callout>

Identificar un patrón de anomalía mejorará su capacidad para identificar cuál y dónde puede estar la causa directa de los problemas.

Hay mucha información excelente y clases gratuitas en línea sobre cómo identificar patrones, pero el concepto general es bastante simple y puede desbloquear poderosas capacidades de diagnóstico.

La clave para identificar patrones y anomalías en los datos de rendimiento es que no es necesario saber cómo debería funcionar el servicio: <DoNotTranslate>**You only need to determine if the recent behavior has changed.**</DoNotTranslate>

Los ejemplos proporcionados en esta sección utilizan el tiempo de respuesta o la latencia como métrica, pero puede aplicar el mismo análisis a casi cualquier conjunto de datos, como errores, rendimiento, métricas de recursos de hardware, profundidad de cola, etc.

### Normal [#normal]

A continuación se muestra un ejemplo de un gráfico de tiempo de respuesta aparentemente volátil (7 días) en APM. Mirando más de cerca, puedes ver que el comportamiento del tiempo de respuesta es repetitivo. En otras palabras, no hay ningún cambio radical en el comportamiento durante un período de 7 días. Los picos son repetitivos y no inusuales en comparación con el resto de la línea de tiempo.

<img
  alt="normal pattern"
  title="Normal pattern"
  src={solutionsOmaUprPatternNormal}
/>

De hecho, si cambia la vista de los datos de <DoNotTranslate>**average over time**</DoNotTranslate> a <DoNotTranslate>**percentiles over time**</DoNotTranslate>, queda aún más claro cuán "regulares" son los cambios en el tiempo de respuesta.

<img
  alt="normal pattern with percentile"
  title="Normal pattern with percentile"
  src={solutionsNormalPercentilePattern}
/>

### Anormal [#abnormal]

Este gráfico muestra un tiempo de respuesta de la aplicación que parece haber aumentado de manera inusual en comparación con el comportamiento reciente.

<img
  alt="abnormal pattern"
  title="Abnormal pattern"
  src={solutionsPatternAbnormal}
/>

Esto se puede confirmar utilizando la comparación semana tras semana.

<img
  alt="abnormal pattern week-over-week"
  title="Abnormal pattern week-over-week comparison"
  src={solutionsPatternAbnormalCompare}
/>

Vemos que el patrón ha cambiado y que parece estar empeorando con respecto a la comparación de la semana pasada.

## Encontrar la fuente [#finding-source]

A continuación, veremos cómo encontrar la fuente en New Relic. Tenga en cuenta que este flujo de trabajo se basa en rastreo distribuido.

Primero, encontrarás una aplicación relacionada con la latencia o errores que experimenta el usuario final. Esto no significa que la aplicación o el código sean el problema, pero encontrar cualquier aplicación dentro del flujo (_primero_) lo acercará más rápidamente a la fuente. Una vez encontrada esta aplicación, puede descartar rápidamente componentes como código, host, base de datos, configuración y red.

Una vez identificada la aplicación, la pregunta es qué transacciones dentro de esa aplicación son parte del problema. Utilice la aplicación que identificó como que experimenta un problema de rendimiento e identifique qué transacción o transacciones se ven afectadas. Aquí puede repetir la habilidad de anomalía del patrón de desempeño descrita anteriormente en [Identificar anomalías del patrón de desempeño](#pattern-anomalies), pero esta vez en la transacción misma.

Los siguientes documentos le ayudarán a utilizar New Relic para identificar transacciones problemáticas:

1. [Página de transacciones: Encuentre problemas de rendimiento específicos](/docs/apm/apm-ui-pages/monitoring/transactions-page-find-specific-performance-problems/)
2. [Transacción lenta en la página de resumen del servicio.](/whats-new/2021/03/slow-transactions)

Una vez que se identifican las transacciones problemáticas, puede utilizar rastreo distribuido para revisar los componentes de extremo a extremo que respaldan esa transacción. rastreo distribuido lo ayuda a identificar rápidamente dónde está la latencia y/o dónde ocurren los errores en toda su stack, todo desde una sola vista.

Los siguientes recursos le ayudarán a aprender cómo utilizar rastreo distribuido para identificar el componente fuente del problema:

1. [Introducción al rastreo distribuido](/docs/distributed-tracing/concepts/introduction-distributed-tracing)
2. [Cómo utilizar la UIde rastreo distribuido](/docs/distributed-tracing/ui-data/understand-use-distributed-tracing-ui/)
3. [Seminarios web gratuitos en línea sobre rastreo distribuido](https://learn.newrelic.com/new-relic-distributed-tracing-tracking-across-your-application-stacks)
4. [Un vídeo sobre el uso del rastreo distribuido para el análisis de causas directas](https://www.youtube.com/watch?v=r9ImAQ5J5h4)

Aquí hay un breve resumen de los pasos para encontrar la fuente:

1. Examinar una aplicación relacionada con el rendimiento afectado.
2. Identifique qué transacciones están contribuyendo al problema.
3. Utilice rastreo distribuido para identificar el componente problemático dentro del flujo de un extremo a otro.

Ahora podemos pasar al paso final, identificar la causa directa.

## Encontrar la causa directa [#finding-direct-cause]

Una vez que se encuentra el componente fuente, puede comenzar a determinar la causa directa.

Utilizando el conocimiento de los pasos anteriores, sabrá si el problema es la latencia, el éxito o ambos.

Los problemas de latencia se pueden encontrar usando la traza de la transacción y/o "span en proceso" dentro de rastreo distribuido.

El mensaje de error de problemas de éxito también se puede ver en la traza, pero los mejores detalles para los problemas de éxito generalmente se encuentran en el registro de aplicación.

De cualquier manera, si usted es el respondedor de incidentes de primer nivel o una ingeniería de confiabilidad del sitio (SRE), encontrar la causa directa recaería en los expertos en la materia (PYME), que generalmente son los desarrolladores y el ingeniero responsable del componente fuente. encontró.

El siguiente paso más eficaz después de descubrir el componente fuente es ponerse en contacto con los expertos en la materia de dicho componente. Muéstreles los datos descubiertos en la clasificación y los diagnósticos que haya completado para tener ventaja en la resolución de problemas.

<Callout variant="tip">
  Tenga en cuenta que tanto el contexto de inicio de sesión como el rastreo distribuido están habilitados de forma predeterminada con nuestro agente <InlinePopover type="apm"/>más nuevo. (Si no ha actualizado su agente por un tiempo, le recomendamos [actualizarlo](/docs/new-relic-solutions/new-relic-one/install-configure/update-new-relic-agent) periódicamente).

  Log-in-context y rastreo distribuido son capacidades críticas necesarias para reducir el tiempo de clasificación, diagnóstico y resolución de problemas a largo plazo.
</Callout>

¡Ahora, adelante y conviértete en un excelente ingeniero de confiabilidad del sitio con New Relic!

## Próximos pasos [#next-steps]

Si aún no lo ha hecho, es posible que desee leer algunas de nuestras [guías de madurez de observabilidad](/docs/new-relic-solutions/observability-maturity/introduction) relacionadas, que incluyen:

* [Gestión de calidad alerta](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/alert-quality-management-guide)
* [Administración a nivel de servicio](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide)