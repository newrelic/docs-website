---
title: Condición de alerta
tags:
  - Alerts and applied intelligence
  - Alerts
  - Alert conditions
metaDescription: 'Use the conditions page to identify what triggers an alert policy''s notification, starting with the product and type of metric or service.'
freshnessValidatedDate: never
translationType: machine
---

import alertsSetaQuery from 'images/alerts_screenshot-crop_set-a-query.webp'

import alertsFineTuneAlertSignals from 'images/alerts_screenshot-crop_fine-tune-alert-signals.webp'

import alertsSetaThresholdforanAlertCondition from 'images/alerts_screenshot-crop_set-a-threshold-for-an-alert-condition.webp'

import alertsNameYourAlertCondition from 'images/alerts_screenshot-crop_name-your-alert-condition.webp'

import alertsAlertConditionPage from 'images/alerts_screenshot-crop_alert-condition-page.webp'

Una condición de alerta es el elemento central que define cuándo se crea un [incidente](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/understand-technical-concepts/incident-event-attributes/#definition) . Actúa como punto de partida esencial para crear cualquier alerta significativa. La condición de alerta contiene el parámetro o umbral cumplido antes de ser informado. Pueden mitigar las alertas excesivas o informar a su equipo cuando aparece un comportamiento nuevo o inusual.

<img
  title="alert condition homepage"
  alt="A screenshot showing the alert condition homepage."
  src={alertsAlertConditionPage}
/>

<figcaption>
  La página <DoNotTranslate>**Alert conditions list**</DoNotTranslate> es el centro universal para todas sus condiciones de alerta.
</figcaption>

## Crear una nueva condición de alerta [#create-alert-condition]

Una condición de alerta es una consulta que se ejecuta continuamente que mide un conjunto determinado de eventos frente a un umbral definido y abre un [incidente](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/specify-when-alerts-create-incidents/) cuando se alcanza el umbral durante un período de tiempo específico.

Este ejemplo demuestra la creación manual de una nueva condición de alerta utilizando la página <DoNotTranslate>**Alert condition details**</DoNotTranslate> . Pero hay muchas maneras de crear una condición de alerta. Puede crear una condición de alerta desde:

* [Una tabla](/docs/tutorial-create-alerts/create-an-alert/)

* [Una página de política](https://one.newrelic.com/nr1-core/condition-builder/policy-entity)

* [La página ](https://one.newrelic.com/alerts-ai/detection-gaps/)

  <DoNotTranslate>[**Alert coverage gaps**](https://one.newrelic.com/alerts-ai/detection-gaps/)</DoNotTranslate>

También puedes utilizar uno de nuestros creadores de alertas:

* Utilice

  <DoNotTranslate>**Write your own query**</DoNotTranslate>

  para crear alerta desde cero

* <DoNotTranslate>
    **Use guided mode**
  </DoNotTranslate>

Para todos los métodos, excepto nuestro modo guiado, el proceso para crear una condición de alerta será _exactamente_ el mismo que se describe en los pasos a continuación.

<Steps>
  <Step>
    ### Establece el comportamiento de tu señal

    En este ejemplo, imagine que su equipo administra la aplicación `WebPortal` para un sitio de comercio electrónico. Quiere estar alerta ante cualquier problema de latencia.

    Para crear una nueva condición de alerta:

    * Vaya a [one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) >

      <DoNotTranslate>**Alerts & AI**</DoNotTranslate>

      .

    * Seleccione

      <DoNotTranslate>**Alert Conditions**</DoNotTranslate>

      en la navegación izquierda.

    * Luego seleccione

      <DoNotTranslate>**New alert condition**</DoNotTranslate>

      .

    * Seleccione

      <DoNotTranslate>**Write your own query**</DoNotTranslate>

      .

    <CollapserGroup>
      <Collapser
        id="set-your-query"
        title="Establece el comportamiento de tu señal"
      >
        Puede utilizar una consulta NRQL para definir las señales que desea que utilice una condición de alerta como base para su alerta. Para este ejemplo, utilizará esta consulta:

        ```
        SELECT average(duration)
        FROM PageView
        WHERE appName = 'WebPortal'
        ```

        El uso de esta consulta para su condición de alerta le indica a New Relic que desea conocer el `pageviews` promedio de su aplicación `WebPortal` . El monitoreo `pageviews` puede ayudarle a detectar cualquier problema de latencia en su aplicación.

        Puede obtener más información sobre cómo utilizar NRQL, el lenguaje de consulta de New Relic, consulte nuestra [documentación de NRQL](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-nrql-alert-conditions/).
      </Collapser>
    </CollapserGroup>
  </Step>

  <Step>
    ### Ajuste la configuración de señal avanzada [#advanced-signal-settings]

    Una vez que haya definido su señal, haga clic en <DoNotTranslate>**Run**</DoNotTranslate>. Aparecerá un gráfico que mostrará el parámetro que has configurado.

    <img
      title="set a query"
      alt="A screenshot showing a user how to set the signal behavior for an alert condition."
      src={alertsSetaQuery}
    />

    Para este ejemplo, el gráfico mostrará el promedio `pageviews` para su aplicación `WebPortal` . Haga clic en <DoNotTranslate>**Next**</DoNotTranslate> y comience a configurar su condición de alerta.

    Para este ejemplo, personalizará esta configuración de señal avanzada para la condición que creó para monitor una actividad inusual para `pageviews` en su aplicación `WebPortal`.

    <CollapserGroup>
      <Collapser
        id="window-duration"
        title="Duración de la ventana"
      >
        <img
          title="fine tune alert settings"
          alt="A screenshot depicting advanced signal settings."
          src={alertsFineTuneAlertSignals}
        />

        La duración de la ventana define cómo New Relic agrupa sus datos para su análisis en una condición de alerta. Elegir la configuración correcta depende de la frecuencia de sus datos y del nivel de detalle deseado:

        * <DoNotTranslate>**High-frequency data (for example, pageviews every minute)**</DoNotTranslate>

          : establezca la duración de la ventana para que coincida con la frecuencia de los datos (1 minuto en este caso) para obtener información valiosa en tiempo real sobre fluctuaciones y tendencias.

        * <DoNotTranslate>**Low-frequency data (for example, hourly signals)**</DoNotTranslate>

          : elija una duración de ventana que capture suficientes datos para revelar patrones y anomalías (por ejemplo, 60 minutos para señales horarias).

        Recuerde, puede personalizar la duración de la ventana según sus necesidades y experiencia. Recomendamos utilizar los valores predeterminados al iniciar y experimentar a medida que se sienta más cómodo creando condiciones de alerta.
      </Collapser>

      <Collapser
        id="sliding-window"
        title="Usar agregación de ventanas deslizantes"
      >
        Los métodos de agregación tradicionales pueden resultar insuficientes cuando se trata de datos escasamente poblados o que muestran fluctuaciones significativas entre intervalos. A continuación se explica cómo utilizar la agregación de ventanas deslizantes para analizar dichos datos y activar alertas oportunas de manera efectiva:

        1. <DoNotTranslate>**Smooth out the noise**</DoNotTranslate>: Comience creando una ventana de agregación grande. Esta ventana (por ejemplo, 5 minutos) actúa como un búfer, suavizando el "ruido" inherente o la variabilidad de sus datos. Esto ayuda a prevenir alertas espurias provocadas por picos o caídas aisladas.

        2. <DoNotTranslate>**Avoid lag with a sliding window**</DoNotTranslate>: Si bien una ventana grande ayuda en el análisis de datos, si espera a que transcurra todo el intervalo antes de verificar el umbral, puede experimentar retrasos significativos en la notificación de alerta. Recomendamos ventanas correderas más pequeñas (por ejemplo, un minuto). Imagine esta ventana deslizante como un marco en movimiento que escanea sus datos dentro de la ventana de agregación más grande. Cada vez que el cuadro avanza en su intervalo menor, calcula un valor agregado (por ejemplo, promedio).

        3. <DoNotTranslate>**Set your threshold duration**</DoNotTranslate>: Ahora puede definir su umbral de alerta dentro del contexto de la ventana deslizante más pequeña. Esto le permite activar alerta rápidamente cuando el valor agregado en el cuadro actual se desvía significativamente del rango deseado sin sacrificar el efecto de suavizado de la ventana más grande.

           Puede obtener más información sobre la agregación de ventanas deslizantes en [este tutorial de NRQL](/docs/query-your-data/nrql-new-relic-query-language/nrql-query-tutorials/create-smoother-charts-with-sliding-windows/).
      </Collapser>

      <Collapser
        id="streaming-method"
        title="Método de transmisión"
      >
        En general, recomendamos utilizar el método de transmisión <DoNotTranslate>**event flow**</DoNotTranslate> . Esto es mejor para los datos que ingresan a su sistema de manera frecuente y constante. Hay casos específicos en los que <DoNotTranslate>**event timer**</DoNotTranslate> podría ser un mejor método a elegir, pero para su primera alerta, recomendamos nuestro método predeterminado, <DoNotTranslate>**event flow**</DoNotTranslate>. Recomendamos ver este breve vídeo (aprox. 5:31 minutos) para comprender mejor qué método de transmisión elegir.

        <Video
          type="wistia"
          id="n6nei987ln"
        />
      </Collapser>

      <Collapser
        id="delay"
        title="Demora"
      >
        La característica de demora en la condición de alerta protege contra posibles problemas que surjan de una recopilación de datos inconsistente. Actúa como un buffer, permitiendo tiempo adicional para que los datos lleguen y se procesen antes de activar una alerta. Esto ayuda a prevenir el falso positivo y garantiza una creación de incidentes más precisa.

        Cómo funciona:

        La configuración de retraso adecuada se determina evaluando la coherencia de los datos entrantes:

        * Datos consistentes: una configuración de retraso más baja es suficiente si los puntos de datos llegan consistentemente con una marca de tiempo dentro de un solo minuto.
        * Datos inconsistentes: si los puntos de datos llegan con una marca de tiempo que abarca varios minutos en el pasado o en el futuro, es necesaria una configuración de retraso más alta para dar cabida a la inconsistencia.

        Creando un búfer:

        * La configuración de retraso seleccionada introduce un período de espera antes de que la condición de alerta evalúe los datos con respecto al umbral definido.
        * Este búfer da tiempo para que se resuelvan las discrepancias de datos, lo que reduce la probabilidad de alertas engañosas.
      </Collapser>

      <Collapser
        id="gap-filling-strategy"
        title="Estrategia para llenar vacíos"
      >
        Está creando una condición de alerta para notificar a su equipo sobre cualquier problema de latencia con la aplicación `WebPortal` . En este ejemplo, su aplicación envía constantemente datos de New Relic. Hay un flujo constante de señales que se envían desde su aplicación a New Relic y no se espera que haya una brecha en la señal, por lo que no necesitará seleccionar una estrategia para llenar las brechas.

        Las estrategias para llenar vacíos abordan escenarios en los que la recopilación de datos puede ser intermitente o incompleta. Proporcionan un método para sustituir puntos de datos faltantes con valores estimados, asegurando que la condición de alerta pueda seguir funcionando eficazmente incluso con lagunas en el flujo de datos.

        Cuándo dejar de rellenar huecos:

        * <DoNotTranslate>**Consistent data flow**</DoNotTranslate>

          : Si su aplicación envía datos constantemente a New Relic sin los espacios esperados, como en el caso de la aplicación WebPortal, generalmente no es necesario completar los espacios. En tales casos, dejar la estrategia para llenar vacíos en ninguna suele ser el enfoque más apropiado.

        Consideraciones clave:

        * <DoNotTranslate>**Popular use case**</DoNotTranslate>: Un uso común para rellenar espacios es insertar un valor de 0 para ventanas sin datos recibidos.

        * <DoNotTranslate>**Anomaly thresholds**</DoNotTranslate>: El valor de llenado de espacios se interpreta como el número de desviación estándar desde el último valor observado cuando se utiliza el umbral de anomalía. Por ejemplo, llenar los vacíos con un valor de 0 replicaría el último valor visto, asumiendo efectivamente que no hay cambios.

          Obtenga más información sobre las estrategias para llenar espacios en nuestros [documentos sobre señales perdidas](/docs/apis/nerdgraph/examples/nerdgraph-api-loss-signal-gap-filling/).
      </Collapser>
    </CollapserGroup>
  </Step>

  <Step>
    ### Establecer umbral para condición de alerta [#thresholds]

    Si una condición de alerta es un contenedor, entonces el umbral son las reglas que debe seguir cada condición de alerta. A medida que los datos ingresan a su sistema, la condición de alerta busca cualquier incidente de estas reglas. Si la condición de alerta ve datos de su sistema que han cumplido todas las condiciones que ha establecido, creará un incidente. Un incidente indica que algo anda mal en su sistema y usted debe mirar.

    <img
      title="set a threshold"
      alt="A screenshot depicting how to set the threshold for an alert condition."
      src={alertsSetaThresholdforanAlertCondition}
    />

    <CollapserGroup>
      <Collapser
        id="anomaly-threshold"
        title="Umbral de anomalía"
      >
        Los umbrales de anomalía son ideales cuando le preocupa más la desviación de los patrones esperados que los valores numéricos específicos. Le permiten monitor actividades inusuales sin necesidad de establecer límites predefinidos. La detección de anomalías de New Relic analiza dinámicamente sus datos a lo largo del tiempo, adaptando el umbral para reflejar el comportamiento en evolución del sistema.

        Configurar la detección de anomalías:

        1. Elija superior o inferior:

           * Seleccione superior e inferior para estar alerta sobre cualquier desviación mayor o menor de lo esperado.
           * Seleccione solo superior para centrarse únicamente en valores inusualmente altos.

        2. Asignar prioridad:

           * Establezca el nivel de prioridad en crítico para su alerta inicial a fin de garantizar la atención a problemas potenciales.
           * Consulte los documentos de condición de alerta para obtener más detalles sobre los niveles de prioridad.

        3. Definir criterios de incumplimiento:

           * Comience con la configuración predeterminada: abra un incidente cuando una consulta devuelva un valor que se desvíe del valor previsto durante al menos cinco minutos en tres desviación estándar.
           * Personalice estas configuraciones según sea necesario para alinearlas con su aplicación específica y sus requisitos de alerta.

        Puede obtener más información sobre los niveles de prioridad en nuestros [documentos de condición de alerta](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/set-thresholds-alert-condition#threshold-levels).

        Obtenga más información sobre el umbral de anomalía y los comportamientos del modelo en nuestra [documentación](/docs/alerts-applied-intelligence/applied-intelligence/anomaly-detection/anomaly-detection-applied-intelligence/) sobre anomalías.
      </Collapser>

      <Collapser
        id="static-threshold"
        title="Umbral estático"
      >
        A diferencia de los umbrales de anomalía, un umbral estático no analiza su conjunto de datos como un todo y determina qué comportamiento es inusual en función del historial de su sistema. En cambio, un umbral estático abrirá un incidente cada vez que su sistema se comporte de manera diferente a los criterios que <DoNotTranslate>**you set**</DoNotTranslate>.

        Debe establecer el nivel de prioridad tanto para la anomalía como para el umbral estático. Consulte la sección anterior para obtener más detalles.
      </Collapser>

      <Collapser
        id="lost-signal"
        title="Umbral de señal perdida (opcional)"
      >
        El umbral de pérdida de señal determina cuánto tiempo esperar antes de considerar perdida una señal faltante. Si en ese tiempo no vuelve la señal, podrás optar por abrir un nuevo incidente o cerrar alguno relacionado. Establezca el umbral según el comportamiento esperado de su sistema y la frecuencia de recopilación de datos. Por ejemplo, si una pérdida de señal para `pageviews` podría indicar un problema de latencia, establezca un umbral con el que se sienta cómodo y marque la casilla para abrir un nuevo incidente de pérdida de señal.
      </Collapser>
    </CollapserGroup>
  </Step>

  <Step>
    ### Agregar detalles de condición de alerta [#add-details]

    En este punto del proceso, ahora tiene una condición completamente definida y establece todas las reglas para garantizar que un incidente se abra cuando usted lo desea. Según la configuración anterior, si su condición de alerta reconoce este comportamiento en su sistema que infringe el umbral que ha establecido, creará un incidente. Ahora, todo lo que necesita hacer es nombrar esta condición y adjuntarla a una póliza.

    La póliza es el sistema de clasificación del incidente. Cuando creas una política, creas la herramienta que organiza todos tus incidentes entrantes. Puede conectar políticas a <DoNotTranslate>**[workflows](/docs/alerts-applied-intelligence/applied-intelligence/incident-workflows/incident-workflows/)**</DoNotTranslate> que le indiquen a New Relic dónde desea que vaya toda esta información entrante, con qué frecuencia desea que se envíe y dónde.

    <img
      title="name an alert condition "
      alt="A screenshot demonstrating how you can new alert condition."
      src={alertsNameYourAlertCondition}
    />

    <CollapserGroup>
      <Collapser
        id="name-your-condition"
        title="Nombra tu condición"
      >
        Es esencial darle un nombre descriptivo a su condición de alerta. Digamos que nombra esta condición <DoNotTranslate>**pageviews**</DoNotTranslate> y luego crea otra condición para una aplicación completamente diferente y etiqueta esa condición también como <DoNotTranslate>**pageviews**</DoNotTranslate> . Si esto ocurre, no podrá distinguir qué condición corresponde a cada aplicación. Por lo tanto, asegúrese de darle a su afección un nombre específico y único. En este caso, llamarías a esta condición <DoNotTranslate>**pageviews: WebPortal App**</DoNotTranslate>.
      </Collapser>

      <Collapser
        id="select-an-existing-policy"
        title="Seleccione una política existente"
      >
        Si ya tiene una política que desea conectar a una condición de alerta, seleccione la política existente.

        Obtenga más información sobre cómo crear políticas [aquí](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/specify-when-alerts-create-incidents/).
      </Collapser>

      <Collapser
        id="create-a-new-policy"
        title="Crear una nueva política"
      >
        Equilibrar la capacidad de respuesta y la fatiga en su estrategia de alertas es crucial y usted ha establecido las consideraciones clave con respecto al `pageview` monitoreo de su aplicación `WebPortal` . Exploremos las opciones de políticas:

        1. Un problema por política (predeterminado):

           * Ventajas: Reduce el ruido y garantiza una acción inmediata.
           * Desventajas: agrupa todos los incidentes bajo un mismo problema, incluso si se desencadenan por condiciones diferentes. No es ideal para problemas de visualización de páginas múltiples.

        2. Un problema por condición:

           * Ventajas: Crea problemas separados para cada condición, ideal para aislar y abordar problemas de latencia específicos.
           * Contras: Puede generar más alerta, lo que podría provocar fatiga.

        3. Un problema para cada incidente:

           * Ventajas: Proporciona detalles granulares para sistemas externos, pero no es óptimo para el consumo interno debido a una posible sobrecarga.
           * Desventajas: Es la opción más ruidosa y resulta complicado seguir tendencias más amplias y establecer prioridades de forma eficaz.

           Obtenga más información sobre cómo crear políticas [aquí](/docs/alerts-applied-intelligence/new-relic-alerts/alert-policies/specify-when-alerts-create-incidents/).
      </Collapser>

      <Collapser
        id="close-open-incidents"
        title="Cerrar incidente abierto"
      >
        Un incidente se cierra automáticamente cuando la señal objetivo vuelve a un estado de no infracción durante el período indicado en el umbral de la condición. Este tiempo de espera se llama período de recuperación.

        Por ejemplo, si el comportamiento de infracción es "`pageviews` es inferior a 300 al menos una vez cada 5 minutos", el incidente se cerrará automáticamente cuando `pageviews` sea igual o superior a 300 durante cinco minutos consecutivos.

        Cuando un incidente se cierra automáticamente:

        1. La timestamp de cierre tiene una fecha retroactiva al inicio del período de recuperación.

        2. La evaluación se reinicia y se reinicia desde que finalizó el incidente anterior.

           Todas las condiciones tienen una configuración de límite de tiempo de incidente que fuerza el cierre automático de un incidente de larga duración.

           New Relic tiene un valor predeterminado automático de 3 días y recomienda que utilice nuestra configuración predeterminada para su primera alerta.

           Otra forma de cerrar un incidente abierto cuando la señal no devuelve datos es configurando un umbral `loss of signal` . Consulte la sección anterior sobre umbral de señal perdida para obtener más detalles.
      </Collapser>

      <Collapser
        id="custom-incident-description"
        title="Enviar una descripción personalizada del incidente"
      >
        Dado que está creando una condición de alerta que le permite saber si hay algún problema de latencia con su aplicación `WebPortal` , desea asegurarse de que sus desarrolladores tengan toda la información que necesitan cuando se les notifique sobre este incidente. Utilizará flujo de trabajo para notificar a un canal de Slack del equipo cuando se cree un incidente.

        Obtenga más información sobre descripciones de incidentes personalizadas en nuestros [documentos](/docs/alerts-applied-intelligence/new-relic-alerts/advanced-alerts/advanced-techniques/alert-custom-incident-descriptions).
      </Collapser>

      <Collapser
        id="runbook"
        title="Agregar URL del runbook"
      >
        Si desea vincular a un runbook, puede colocar la URL en el campo URL del runbook.
      </Collapser>
    </CollapserGroup>
  </Step>
</Steps>

## Editar o mejorar una condición de alerta existente [#edit-existing-alert-condition]

Si desea editar una condición de alerta que ya creó, puede:

1. Vaya a [one.newrelic.com > All capabilities](https://one.newrelic.com/all-capabilities) >

   <DoNotTranslate>**Alerts & AI**</DoNotTranslate>

   .

2. Seleccione

   <DoNotTranslate>**Alert Conditions**</DoNotTranslate>

   en la navegación izquierda.

3. Haga clic en la condición de alerta que desea editar.

Desde allí, verá la página <DoNotTranslate>**Alert conditions details**</DoNotTranslate> . Esta página contiene todos los elementos que configuró cuando creó su condición. Puede editar aspectos específicos de la condición de alerta haciendo clic en el lápiz en la parte superior derecha de cada sección.

### Historial de señales [#signal-coverage]

En <DoNotTranslate>**Signal history**</DoNotTranslate>, puede ver los resultados más recientes de la consulta NRQL que utilizó para crear su condición de alerta. En este ejemplo, verá el `pageviews` promedio en la aplicación `WebPortal` durante el período de tiempo específico que ha establecido.

Para todas las condiciones de alerta creadas con NRQL consulta, el <DoNotTranslate>**Signal history**</DoNotTranslate> se presentará con un gráfico de líneas.

Cualquier condición de alerta creada con un monitor Sintético será un poco diferente. Esto se debe a que los monitores Sintético le permiten hacer ping a su aplicación desde múltiples ubicaciones, lo que puede producir resultados positivos o negativos cada vez que se ejecuta el monitor . Estos datos sólo se pueden presentar mediante una tabla.

## Solucionar problemas [#troubleshoot]

Si ve una señal vacía en el gráfico histórico, considere una de las siguientes opciones:

* <DoNotTranslate>**Review the condition's settings**</DoNotTranslate>

  : Vuelva a verificar que todos los elementos estén configurados correctamente.

* <DoNotTranslate>**Inspect NRQL queries**</DoNotTranslate>

  : Asegúrese de que tengan un objetivo métrico o evento válido y devuelvan datos.

* <DoNotTranslate>**Examine entity configuration**</DoNotTranslate>

  : Confirme que la entidad esté configurada correctamente para enviar señales.

* <DoNotTranslate>**Consult New Relic documentation**</DoNotTranslate>

  : Consulte las guías pertinentes para obtener ayuda con problemas específicos.

## ¿Que sigue? [#whats-next]

<DocTiles>
  <DocTile
    title="Create your first New Relic alert"
    path="/docs/tutorial-create-alerts/create-new-relic-alerts/"
    label={{text: "Start here", color: "orange"}}
  >
    Un curso intensivo de alerta para principiantes
  </DocTile>

  <DocTile
    title="Advanced alert conditions"
    path="/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-alert-conditions/"
  >
    Si ya configuró su condición de alerta, profundice con la configuración avanzada
  </DocTile>

  <DocTile
    title="Get notified"
    path="/docs/alerts-applied-intelligence/applied-intelligence/incident-workflows/incident-workflows/"
  >
    Utilice flujo de trabajo para recibir notificaciones sobre cualquier comportamiento inusual en su stack
  </DocTile>
</DocTiles>