---
title: Configuración del gestor de trabajos sintético
tags:
  - synthetics
  - Synthetic monitoring
  - Private locations
metaDescription: Customize your New Relic synthetics job manager.
freshnessValidatedDate: '2024-07-29T00:00:00.000Z'
translationType: machine
---

Este documento lo guiará a través de la configuración de su [administrador de trabajos Sintético](/docs/synthetics/synthetic-monitoring/private-locations/install-job-manager) mostrándole cómo:

* Emplee [variables de entorno](#environment-variables) para configurar su administrador de trabajos Sintéticos.
* Configure [módulos personalizados](#custom-modules) para [API con script](/docs/synthetics/synthetic-monitoring/scripting-monitors/write-synthetic-api-tests/) o monitor [browser con script](/docs/synthetics/new-relic-synthetics/scripting-monitors/write-scripted-browsers) .
* Proporcione [variables definidas por el usuario](#user-defined-vars) en su configuración.

## configuración usando variables de entorno [#environment-variables]

Las variables ambientales le permiten ajustar la configuración del administrador de trabajos de Sintético para satisfacer sus necesidades ambientales y funcionales específicas.

<CollapserGroup>
  <Collapser id="docker-env-config" title="Configuración del entorno Docker">
    Las variables se proporcionan al inicio utilizando el argumento `-e, --env` .

    La siguiente tabla muestra todas las variables de entorno que soporta Sintético job manager. `PRIVATE_LOCATION_KEY` es obligatorio y todas las demás variables son opcionales.

    <table>
      <thead>
        <tr>
          <th>
            Nombre
          </th>

          <th>
            Descripción
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `PRIVATE_LOCATION_KEY`
          </td>

          <td>
            <DNT>**Required.**</DNT> Clave de ubicación privada, como se encuentra en la lista de entidades de Ubicación Privada.
          </td>
        </tr>

        <tr>
          <td>
            `DOCKER_API_VERSION`
          </td>

          <td>
            Formato: `"vX.Y"` versión de API que se utilizará con el servicio Docker determinado.

            Por defecto: `v1.35.`
          </td>
        </tr>

        <tr>
          <td>
            `DOCKER_HOST`
          </td>

          <td>
            Apunta al administrador de trabajos de Sintético a un `DOCKER_HOST` determinado. Si está ausente, el valor predeterminado es `/var/run/docker.sock.`
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_ENDPOINT`
          </td>

          <td>
            Para cuentas con sede en EE. UU., el extremo es: `https://synthetics-horde.nr-data.net.`

            Para cuentas [basadas en la UE](/docs/using-new-relic/welcome-new-relic/get-started/introduction-eu-region-data-center#partner-hierarchy) , el extremo es: `https://synthetics-horde.eu01.nr-data.net/`

            Asegúrese de que su administrador de trabajo de Sintético pueda conectarse al extremo apropiado para atender su monitor.
          </td>
        </tr>

        <tr>
          <td>
            `DOCKER_REGISTRY`
          </td>

          <td>
            El dominio del Registro Docker donde se alojan las imágenes en tiempo de ejecución. Utilice esto para anular `docker.io` como valor predeterminado.
          </td>
        </tr>

        <tr>
          <td>
            `DOCKER_REPOSITORY`
          </td>

          <td>
            El repositorio u organización Docker donde se alojan las imágenes en tiempo de ejecución. Emplee esto para anular `newrelic` como valor predeterminado.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_HOST`
          </td>

          <td>
            Host de servidor proxy utilizado para la comunicación de la Horda. Formato: `"localhost"`.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_PORT`
          </td>

          <td>
            Puerto del servidor proxy utilizado para la comunicación de la Horda. Formato: `8888`.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_USERNAME`
          </td>

          <td>
            Nombre de usuario del servidor proxy utilizado para la comunicación de la Horda. Formato: `"username"`.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_PW`
          </td>

          <td>
            Contraseña del servidor proxy utilizada para la comunicación de la Horda. Formato: `"password"`.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_ACCEPT_SELF_SIGNED_CERT`
          </td>

          <td>
            ¿Aceptar certificados de proxy autofirmados para la conexión del servidor proxy utilizado para la comunicación de la Horda? Valores aceptables: `true`
          </td>
        </tr>

        <tr>
          <td>
            `CHECK_TIMEOUT`
          </td>

          <td>
            La cantidad máxima de segundos que se permite que se ejecuten las comprobaciones de su monitor. Este valor debe ser un número entero entre 0 segundos (excluido) y 900 segundos (incluido) (por ejemplo, de 1 segundo a 15 minutos).

            Predeterminado: 180 segundos
          </td>
        </tr>

        <tr>
          <td>
            `LOG_LEVEL`
          </td>

          <td>
            Por defecto: `INFO.`

            Opciones adicionales: `WARN`, `ERROR`, `DEBUG`
          </td>
        </tr>

        <tr>
          <td>
            `HEAVYWEIGHT_WORKERS`
          </td>

          <td>
            El número de trabajos pesados simultáneos (browser/ browser con secuencias de comandos y API con secuencias de comandos) que se pueden ejecutar al mismo tiempo.

            Valor predeterminado: CPU disponibles - 1.
          </td>
        </tr>

        <tr>
          <td>
            `DESIRED_RUNTIMES`
          </td>

          <td>
            Una matriz que se puede utilizar para ejecutar imágenes en tiempo de ejecución específicas. Formato: \[&apos;newrelic/Sintético-ping-runtime:latest&apos;,&apos;newrelic/Sintético-node-API-runtime:latest&apos;,&apos;newrelic/Sintético-node-browser-runtime:latest&apos;]

            Valor predeterminado: todos los tiempos de ejecución más recientes.
          </td>
        </tr>

        <tr>
          <td>
            `VSE_PASSPHRASE`
          </td>

          <td>
            Si se establece, habilita <DNT>**verified script execution**</DNT> y utiliza este valor como <DNT>**passphrase**</DNT>.
          </td>
        </tr>

        <tr>
          <td>
            `USER_DEFINED_VARIABLES`
          </td>

          <td>
            Un conjunto alojado localmente de pares de valores principales definidos por el usuario.
          </td>
        </tr>

        <tr>
          <td>
            `ENABLE_WASM`
          </td>

          <td>
            Si se configura, habilita webassembly para el tiempo de ejecución del navegador de nodos. Para emplear webassembly, la versión mínima de su administrador de trabajos Sintéticos debe ser release-367 o superior y la versión de ejecución del navegador de nodo debe ser 2.3.21 o superior.
          </td>
        </tr>
      </tbody>
    </table>
  </Collapser>

  <Collapser id="podman-env-config" title="Configuración del entorno Podman">
    Las variables se proporcionan al inicio utilizando el argumento `-e, --env` .

    La siguiente tabla muestra todas las variables de entorno que admite el administrador de trabajos de Sintéticos. Se requiere `PRIVATE_LOCATION_KEY` y todas las demás variables son opcionales. Para ejecutar el administrador de trabajos Sintéticos en un entorno Podman, la versión mínima debe ser release-418 o superior.

    <table>
      <thead>
        <tr>
          <th>
            Nombre
          </th>

          <th>
            Descripción
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `PRIVATE_LOCATION_KEY`
          </td>

          <td>
            <DNT>**Required.**</DNT> Clave de ubicación privada, como se encuentra en la lista de entidades de Ubicación Privada.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_ENDPOINT`
          </td>

          <td>
            Para cuentas con sede en EE. UU., el extremo es: `https://synthetics-horde.nr-data.net.`

            Para cuentas [basadas en la UE](/docs/using-new-relic/welcome-new-relic/get-started/introduction-eu-region-data-center#partner-hierarchy) , el extremo es: `https://synthetics-horde.eu01.nr-data.net/`

            Asegúrese de que su administrador de trabajo de Sintético pueda conectarse al extremo apropiado para atender su monitor.
          </td>
        </tr>

        <tr>
          <td>
            `PODMAN_API_SERVICE_HOST`
          </td>

          <td>
            La entrada de host agregada al pod creado donde se ejecutará el SJM. Emplee esto para anular `podman.service` como valor predeterminado.
          </td>
        </tr>

        <tr>
          <td>
            `PODMAN_API_SERVICE_PORT`
          </td>

          <td>
            El puerto en el que se ejecuta el servicio API RESTful de Podman LibPod en la instancia. Emplee esto para anular `8000` como valor predeterminado.
          </td>
        </tr>

        <tr>
          <td>
            `PODMAN_API_VERSION`
          </td>

          <td>
            La versión específica de la API RESTful de Podman LibPod que se está empleando. Emplee esto para anular `v5.0.0` como valor predeterminado.
          </td>
        </tr>

        <tr>
          <td>
            `PODMAN_POD_NAME`
          </td>

          <td>
            El nombre del pod en el que se ejecuta el contenedor SJM. Emplee esto para anular `SYNTHETICS` como valor predeterminado.
          </td>
        </tr>

        <tr>
          <td>
            `DOCKER_REGISTRY`
          </td>

          <td>
            El dominio del Registro Docker donde se alojan las imágenes en tiempo de ejecución. Utilice esto para anular `docker.io` como valor predeterminado.
          </td>
        </tr>

        <tr>
          <td>
            `DOCKER_REPOSITORY`
          </td>

          <td>
            El repositorio u organización Docker donde se alojan las imágenes en tiempo de ejecución. Emplee esto para anular `newrelic` como valor predeterminado.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_HOST`
          </td>

          <td>
            Host de servidor proxy utilizado para la comunicación de la Horda. Formato: `"localhost"`.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_PORT`
          </td>

          <td>
            Puerto del servidor proxy utilizado para la comunicación de la Horda. Formato: `8888`.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_USERNAME`
          </td>

          <td>
            Nombre de usuario del servidor proxy utilizado para la comunicación de la Horda. Formato: `"username"`.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_PW`
          </td>

          <td>
            Contraseña del servidor proxy utilizada para la comunicación de la Horda. Formato: `"password"`.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_ACCEPT_SELF_SIGNED_CERT`
          </td>

          <td>
            ¿Aceptar certificados de proxy autofirmados para la conexión del servidor proxy utilizado para la comunicación de la Horda? Valores aceptables: `true`
          </td>
        </tr>

        <tr>
          <td>
            `CHECK_TIMEOUT`
          </td>

          <td>
            La cantidad máxima de segundos que se permite que se ejecuten las comprobaciones de su monitor. Este valor debe ser un número entero entre 0 segundos (excluido) y 900 segundos (incluido) (por ejemplo, de 1 segundo a 15 minutos).

            Predeterminado: 180 segundos
          </td>
        </tr>

        <tr>
          <td>
            `LOG_LEVEL`
          </td>

          <td>
            Por defecto: `INFO.`

            Opciones adicionales: `WARN`, `ERROR`, `DEBUG`
          </td>
        </tr>

        <tr>
          <td>
            `HEAVYWEIGHT_WORKERS`
          </td>

          <td>
            El número de trabajos pesados simultáneos (browser/ browser con secuencias de comandos y API con secuencias de comandos) que se pueden ejecutar al mismo tiempo.

            Valor predeterminado: CPU disponibles - 1.
          </td>
        </tr>

        <tr>
          <td>
            `DESIRED_RUNTIMES`
          </td>

          <td>
            Una matriz que se puede utilizar para ejecutar imágenes en tiempo de ejecución específicas. Formato: \[&apos;newrelic/Sintético-ping-runtime:latest&apos;,&apos;newrelic/Sintético-node-API-runtime:latest&apos;,&apos;newrelic/Sintético-node-browser-runtime:latest&apos;]

            Valor predeterminado: todos los tiempos de ejecución más recientes.
          </td>
        </tr>

        <tr>
          <td>
            `VSE_PASSPHRASE`
          </td>

          <td>
            Si se establece, habilita <DNT>**verified script execution**</DNT> y utiliza este valor como <DNT>**passphrase**</DNT>.
          </td>
        </tr>

        <tr>
          <td>
            `USER_DEFINED_VARIABLES`
          </td>

          <td>
            Un conjunto alojado localmente de pares de valores principales definidos por el usuario.
          </td>
        </tr>

        <tr>
          <td>
            `ENABLE_WASM`
          </td>

          <td>
            Si se configura, habilita webassembly para el tiempo de ejecución del navegador de nodos. Para emplear webassembly, la versión mínima de su administrador de trabajos Sintéticos debe ser release-367 o superior y la versión de ejecución del navegador de nodo debe ser 2.3.21 o superior.
          </td>
        </tr>
      </tbody>
    </table>
  </Collapser>

  <Collapser id="kubernetes-env-config" title="Configuración del entorno de Kubernetes">
    Las variables se proporcionan al inicio utilizando el argumento `--set` .

    La siguiente lista muestra todas las variables de entorno que admite Sintético job manager. `synthetics.privateLocationKey` es obligatorio y todas las demás variables son opcionales.

    Una serie de configuraciones avanzadas adicionales están disponibles y completamente documentadas en [nuestro archivo README del gráfico Helm.](https://github.com/newrelic/helm-charts/blob/master/charts/synthetics-job-manager/README.md)

    <table>
      <thead>
        <tr>
          <th>
            Nombre
          </th>

          <th>
            Descripción
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `synthetics.privateLocationKey`
          </td>

          <td>
            <DNT>**Required if `synthetics.privateLocationKeySecretName` is not set**</DNT>. [ubicación privada clave](/docs/synthetics/synthetic-monitoring/private-locations/install-job-manager/#private-location-key) de la ubicación privada, tal como se encuentra en la página web de la ubicación privada.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.privateLocationKeySecretName`
          </td>

          <td>
            <DNT>**Required if `synthetics.privateLocationKey` is not set**</DNT>. Nombre del secreto de Kubernetes que contiene la clave `privateLocationKey`, que contiene la clave de autenticación asociada a tu ubicación privada de Sintético.
          </td>
        </tr>

        <tr>
          <td>
            `imagePullSecrets`
          </td>

          <td>
            El nombre del objeto secreto utilizado para extraer una imagen de un registro de contenedor específico.
          </td>
        </tr>

        <tr>
          <td>
            `fullnameOverride`
          </td>

          <td>
            Anulación del nombre utilizado para su implementación, reemplazando el predeterminado.
          </td>
        </tr>

        <tr>
          <td>
            `appVersionOverride`
          </td>

          <td>
            Versión de lanzamiento de Sintético-job-manager para usar en lugar de la versión especificada en [chart.yml](https://github.com/newrelic/helm-charts/blob/master/charts/synthetics-job-manager/Chart.yaml).
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.logLevel`
          </td>

          <td>
            Por defecto: `INFO.`

            Opciones adicionales: `WARN`, `ERROR`
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.hordeApiEndpoint`
          </td>

          <td>
            Para cuentas con sede en EE. UU., el extremo es: `https://synthetics-horde.nr-data.net.`

            Para cuentas [basadas en la UE](/docs/using-new-relic/welcome-new-relic/get-started/introduction-eu-region-data-center#partner-hierarchy) , el extremo es: `https://synthetics-horde.eu01.nr-data.net/`

            Asegúrese de que su administrador de trabajo de Sintético pueda conectarse al extremo apropiado para atender su monitor.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.minionDockerRunnerRegistryEndpoint`
          </td>

          <td>
            El registro y la organización Docker donde se aloja la imagen minion Runner. Utilice esto para anular `quay.io/newrelic` como valor predeterminado (por ejemplo, `docker.io/newrelic`).
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.vsePassphrase`
          </td>

          <td>
            Si se establece, habilita <DNT>**verified script execution**</DNT> y utiliza este valor como <DNT>**passphrase**</DNT>.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.vsePassphraseSecretName`
          </td>

          <td>
            Si se establece, habilita la ejecución de script verificada y utiliza este valor para recuperar la frase de contraseña de un secreto de Kubernetes con una clave llamada `vsePassphrase`.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.enableWasm`
          </td>

          <td>
            Si se configura, habilita webassembly para el tiempo de ejecución del navegador de nodos. Para emplear webassembly, la versión mínima de su administrador de trabajos Sintéticos debe ser release-367 o superior y la versión de ejecución del navegador de nodo debe ser 2.3.21 o superior.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.apiProxyHost`
          </td>

          <td>
            Servidor proxy utilizado para la comunicación de la Horda. Formato: `"host"`.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.apiProxyPort`
          </td>

          <td>
            Puerto del servidor proxy utilizado para la comunicación de la Horda. Formato: `port`.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.hordeApiProxySelfSignedCert`
          </td>

          <td>
            Acepte certificados autofirmados cuando utilice un servidor proxy para la comunicación de la Horda. Valores aceptables: `true`.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.hordeApiProxyUsername`
          </td>

          <td>
            Nombre de usuario del servidor proxy para la comunicación de la Horda. Formato: `"username"`
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.hordeApiProxyPw`
          </td>

          <td>
            Contraseña del servidor proxy para la comunicación de la Horda. Formato: `"password"`.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.userDefinedVariables.userDefinedJson`
          </td>

          <td>
            Una cadena JSON de variables definidas por el usuario. El usuario puede acceder a estas variables en su script. Formato: `'{"key":"value","key2":"value2"}'`.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.userDefinedVariables.userDefinedFile`
          </td>

          <td>
            Una ruta local para el usuario a un archivo JSON que contiene variables definidas por el usuario. Esto se pasa a través de `--set-file` y no se puede configurar en el archivo de valores.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.userDefinedVariables.userDefinedPath`
          </td>

          <td>
            Una ruta en el PersistentVolume proporcionado por el usuario al archivo user\_defined\_variables.json. El usuario debe proporcionar un PersistentVolume o PersistentVolumeClaim si esta variable está completa.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.persistence.existingClaimName`
          </td>

          <td>
            Si monta un volumen, el usuario puede proporcionar un nombre para un PersistentVolumeClaim que ya existe en el clúster. Presume la existencia de un PersistentVolume correspondiente.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.persistence.existingVolumeName`
          </td>

          <td>
            Si monta un volumen y no proporciona un PersistentVolumeClaim, el usuario debe, como mínimo, proporcionar un nombre de PersistentVolume. Helm generará un PersistentVolumeClaim.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.persistence.storageClass`
          </td>

          <td>
            El nombre de StorageClass para el PersistentVolumeClaim generado. Esto debe coincidir con StorageClassName en el PV existente. Si no son proveedores, Kubernetes utilizará la clase de almacenamiento predeterminada, si está presente.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.persistence.size`
          </td>

          <td>
            El tamaño del volumen para el PersistentVolumeClaim generado. Formato: `10Gi`. 2Gi predeterminado.
          </td>
        </tr>

        <tr>
          <td>
            `global.checkTimeout`
          </td>

          <td>
            La cantidad máxima de segundos que se permite que se ejecuten las comprobaciones de su monitor. Este valor debe ser un número entero entre 0 segundos (excluido) y 900 segundos (incluido) (por ejemplo, de 1 segundo a 15 minutos).

            Predeterminado: 180 segundos
          </td>
        </tr>

        <tr>
          <td>
            `image.repository`
          </td>

          <td>
            El contenedor para tirar.

            Por defecto: `docker.io/newrelic/synthetics-job-runner`
          </td>
        </tr>

        <tr>
          <td>
            `image.pullPolicy`
          </td>

          <td>
            La política de atracción.

            Por defecto: `IfNotPresent`
          </td>
        </tr>

        <tr>
          <td>
            `podSecurityContext`
          </td>

          <td>
            Establezca un contexto de seguridad personalizado para el pod Sintético-job-manager.
          </td>
        </tr>

        <tr>
          <td>
            `ping-runtime.enabled`
          </td>

          <td>
            Si se debe implementar o no el tiempo de ejecución del ping persistente. Esto se puede desactivar si no utiliza el monitor de ping.

            Por defecto: `true`
          </td>
        </tr>

        <tr>
          <td>
            `ping-runtime.replicaCount`
          </td>

          <td>
            El número de contenedores de tiempo de ejecución de ping a desplegar. Aumente el replicaCount para escalar el despliegue según sus necesidades de monitoreo de ping.

            Por defecto: `1`
          </td>
        </tr>

        <tr>
          <td>
            `ping-runtime.image.repository`
          </td>

          <td>
            La imagen del contenedor que se extraerá para el tiempo de ejecución de ping.

            Por defecto: `docker.io/newrelic/synthetics-ping-runtime`
          </td>
        </tr>

        <tr>
          <td>
            `ping-runtime.image.pullPolicy`
          </td>

          <td>
            La política de extracción para el contenedor de tiempo de ejecución de ping.

            Por defecto: `IfNotPresent`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.enabled`
          </td>

          <td>
            Si se debe implementar o no el tiempo de ejecución de la API de Node.js. Esto se puede desactivar si no utiliza el monitor API con script.

            Por defecto: `true`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.parallelism`
          </td>

          <td>
            La cantidad de tiempo de ejecución de API de Node.js `CronJobs` para implementar. La cantidad máxima de trabajos simultáneos de la API de Node.js que se ejecutarán en cualquier momento. [Detalles adicionales](#kubernetes-sizing).

            Por defecto: `1`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.completions`
          </td>

          <td>
            La cantidad de tiempos de ejecución de API de Node.js `CronJobs` que se deben completar por minuto. Aumente esta configuración junto con el paralelismo para mejorar el rendimiento. Esto debe aumentarse cada vez que se aumenta el paralelismo y las terminaciones siempre deben ser al menos mayores o iguales que el paralelismo. . Aumente esta configuración si observa períodos de tiempo sin trabajos de ejecución de API en ejecución. [Detalles adicionales](#kubernetes-sizing).

            Por defecto: `6`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.image.repository`
          </td>

          <td>
            La imagen del contenedor que se extraerá para el tiempo de ejecución de la API de Node.js.

            Por defecto: `docker.io/newrelic/synthetics-node-api-runtime`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.image.pullPolicy`
          </td>

          <td>
            La política de extracción para el contenedor de tiempo de ejecución de la API de Node.js.

            Por defecto: `IfNotPresent`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.enabled`
          </td>

          <td>
            Si se debe desplegar o no el tiempo de ejecución browser Node.js. Esto se puede desactivar si no utiliza el script simple o monitor del browser.

            Por defecto: `true`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.parallelism`
          </td>

          <td>
            El número de tiempo de ejecución browser Chrome `CronJobs` para desplegar. La cantidad máxima de trabajos simultáneos browser Chrome que se ejecutarán en cualquier momento. [Detalles adicionales](#kubernetes-sizing).

            Por defecto: `1`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.completions`
          </td>

          <td>
            El número de tiempos de ejecución browser Chrome `CronJobs` que se deben completar por minuto. Aumente esta configuración junto con el paralelismo para mejorar el rendimiento. Esto debe aumentarse cada vez que se aumenta el paralelismo y las terminaciones siempre deben ser al menos mayores o iguales que el paralelismo. Aumente esta configuración si observa períodos de tiempo sin que se ejecuten trabajos de tiempo de ejecución browser . [Detalles adicionales](#kubernetes-sizing).

            Por defecto: `6`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.image.repository`
          </td>

          <td>
            La imagen del contenedor que se extraerá para el tiempo de ejecución browser Node.js.

            Por defecto: `docker.io/newrelic/synthetics-node-browser-runtime`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.image.pullPolicy`
          </td>

          <td>
            La política de extracción para el contenedor de tiempo de ejecución browser Node.js.

            Por defecto: `IfNotPresent`
          </td>
        </tr>
      </tbody>
    </table>
  </Collapser>

  <Collapser id="openshift-environment-config" title="Configuración del entorno OpenShift">
    Las variables se proporcionan al inicio utilizando el argumento `--set` .

    La siguiente lista muestra todas las variables de entorno que admite Sintético job manager. `synthetics.privateLocationKey` es obligatorio y todas las demás variables son opcionales.

    Una serie de configuraciones avanzadas adicionales están disponibles y completamente documentadas en [nuestro archivo README del gráfico Helm.](https://github.com/newrelic/helm-charts/blob/master/charts/synthetics-job-manager/README.md)

    <table>
      <thead>
        <tr>
          <th>
            Nombre
          </th>

          <th>
            Descripción
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `synthetics.privateLocationKey`
          </td>

          <td>
            <DNT>**Required**</DNT>. [Clave de ubicación privada](/docs/synthetics/synthetic-monitoring/private-locations/install-job-manager/#private-location-key), como se encuentra en la lista de entidades de ubicación privada.
          </td>
        </tr>

        <tr>
          <td>
            `imagePullSecrets`
          </td>

          <td>
            El nombre del objeto secreto utilizado para extraer una imagen de un registro de contenedor específico.
          </td>
        </tr>

        <tr>
          <td>
            `fullnameOverride`
          </td>

          <td>
            Anulación del nombre utilizado para su implementación, reemplazando el predeterminado.
          </td>
        </tr>

        <tr>
          <td>
            `appVersionOverride`
          </td>

          <td>
            Versión de lanzamiento de Sintético-job-manager para usar en lugar de la versión especificada en [chart.yml](https://github.com/newrelic/helm-charts/blob/master/charts/synthetics-job-manager/Chart.yaml).
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.logLevel`
          </td>

          <td>
            Por defecto: `INFO.`

            Opciones adicionales: `WARN`, `ERROR`
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.hordeApiEndpoint`
          </td>

          <td>
            Para cuentas con sede en EE. UU., el extremo es: `https://synthetics-horde.nr-data.net.`

            Para cuentas [basadas en la UE](/docs/using-new-relic/welcome-new-relic/get-started/introduction-eu-region-data-center#partner-hierarchy) , el extremo es: `https://synthetics-horde.eu01.nr-data.net/`

            Asegúrese de que su administrador de trabajo de Sintético pueda conectarse al extremo apropiado para atender su monitor.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.vsePassphrase`
          </td>

          <td>
            Si se establece, habilita <DNT>**verified script execution**</DNT> y utiliza este valor como <DNT>**passphrase**</DNT>.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.vsePassphraseSecretName`
          </td>

          <td>
            Si se establece, habilita la ejecución de script verificada y utiliza este valor para recuperar la frase de contraseña de un secreto de Kubernetes con una clave llamada `vsePassphrase`.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.enableWasm`
          </td>

          <td>
            Si se configura, habilita webassembly para el tiempo de ejecución del navegador de nodos. Para emplear webassembly, la versión mínima de su administrador de trabajos Sintéticos debe ser release-367 o superior y la versión de ejecución del navegador de nodo debe ser 2.3.21 o superior.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.apiProxyHost`
          </td>

          <td>
            Servidor proxy utilizado para la comunicación de la Horda. Formato: `"host"`.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.apiProxyPort`
          </td>

          <td>
            Puerto del servidor proxy utilizado para la comunicación de la Horda. Formato: `port`.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.hordeApiProxySelfSignedCert`
          </td>

          <td>
            Acepte certificados autofirmados cuando utilice un servidor proxy para la comunicación de la Horda. Valores aceptables: `true`.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.hordeApiProxyUsername`
          </td>

          <td>
            Nombre de usuario del servidor proxy para la comunicación de la Horda. Formato: `"username"`
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.hordeApiProxyPw`
          </td>

          <td>
            Contraseña del servidor proxy para la comunicación de la Horda. Formato: `"password"`.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.userDefinedVariables.userDefinedJson`
          </td>

          <td>
            Una cadena JSON de variables definidas por el usuario. El usuario puede acceder a estas variables en su script. Formato: `'{"key":"value","key2":"value2"}'`.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.userDefinedVariables.userDefinedFile`
          </td>

          <td>
            Una ruta local para el usuario a un archivo JSON que contiene variables definidas por el usuario. Esto se pasa a través de `--set-file` y no se puede configurar en el archivo de valores.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.userDefinedVariables.userDefinedPath`
          </td>

          <td>
            Una ruta en el `PersistentVolume` proporcionado por el usuario al archivo` user_defined_variables.json` . El usuario debe proporcionar `PersistentVolume` o `PersistentVolumeClaim` si esta variable está completa.
          </td>
        </tr>

        <tr>
          <td>
            `global.persistence.existingClaimName`
          </td>

          <td>
            Si se monta un volumen, el usuario puede proporcionar un nombre para un `PersistentVolumeClaim` que ya existe en el clúster. Supone la existencia de un `PersistentVolume` correspondiente.
          </td>
        </tr>

        <tr>
          <td>
            `global.persistence.existingVolumeName`
          </td>

          <td>
            Si se monta un volumen y no se proporciona un `PersistentVolumeClaim`, el usuario debe proporcionar como mínimo un nombre `PersistentVolume` . Helm generará un `PersistentVolumeClaim`.
          </td>
        </tr>

        <tr>
          <td>
            `global.persistence.storageClass`
          </td>

          <td>
            El nombre del `StorageClass` para el `PersistentVolumeClaim` generado. Esto debe coincidir con el `StorageClassName` en el PV existente. Si no hay proveedores, **Kubernetes** empleará la clase de almacenamiento predeterminada si está presente.
          </td>
        </tr>

        <tr>
          <td>
            `global.persistence.size`
          </td>

          <td>
            El tamaño del volumen para el `PersistentVolumeClaim` generado. Formato: `10Gi`. Valor predeterminado `2Gi`.
          </td>
        </tr>

        <tr>
          <td>
            `global.checkTimeout`
          </td>

          <td>
            La cantidad máxima de segundos que se permite que se ejecuten las comprobaciones de su monitor. Este valor debe ser un número entero entre 0 segundos (excluido) y 900 segundos (incluido) (por ejemplo, de 1 segundo a 15 minutos).

            Predeterminado: 180 segundos
          </td>
        </tr>

        <tr>
          <td>
            `image.repository`
          </td>

          <td>
            El contenedor para tirar.

            Por defecto: `docker.io/newrelic/synthetics-job-runner`
          </td>
        </tr>

        <tr>
          <td>
            `image.pullPolicy`
          </td>

          <td>
            La política de atracción.

            Por defecto: `IfNotPresent`
          </td>
        </tr>

        <tr>
          <td>
            `podSecurityContext`
          </td>

          <td>
            Establezca un contexto de seguridad personalizado para el pod `synthetics-job-manager` .
          </td>
        </tr>

        <tr>
          <td>
            `ping-runtime.enabled`
          </td>

          <td>
            Si se debe implementar o no el tiempo de ejecución del ping persistente. Esto se puede desactivar si no utiliza el monitor de ping.

            Por defecto: `true`
          </td>
        </tr>

        <tr>
          <td>
            `ping-runtime.replicaCount`
          </td>

          <td>
            El número de ping que el contenedor va a desplegar. Aumente `replicaCount` para escalar la implementación según sus necesidades de monitoreo de ping.

            Por defecto: `1`
          </td>
        </tr>

        <tr>
          <td>
            `ping-runtime.image.repository`
          </td>

          <td>
            La imagen del contenedor que se extraerá para el tiempo de ejecución de ping.

            Por defecto: `docker.io/newrelic/synthetics-ping-runtime`
          </td>
        </tr>

        <tr>
          <td>
            `ping-runtime.image.pullPolicy`
          </td>

          <td>
            La política de extracción para el contenedor de tiempo de ejecución de ping.

            Por defecto: `IfNotPresent`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.enabled`
          </td>

          <td>
            Si se debe implementar o no el tiempo de ejecución de la API de Node.js. Esto se puede desactivar si no utiliza el monitor API con script.

            Por defecto: `true`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.parallelism`
          </td>

          <td>
            La cantidad de tiempo de ejecución de API de Node.js `CronJobs` para implementar. La cantidad máxima de trabajos simultáneos de la API de Node.js que se ejecutarán en cualquier momento. [Detalles adicionales](#kubernetes-sizing).

            Por defecto: `1`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.completions`
          </td>

          <td>
            La cantidad de tiempos de ejecución de API de Node.js `CronJobs` que se deben completar por minuto. Aumente esta configuración junto con el paralelismo para mejorar el rendimiento. Esto debe aumentarse cada vez que se aumenta el paralelismo y las terminaciones siempre deben ser al menos mayores o iguales que el paralelismo. . Aumente esta configuración si observa períodos de tiempo sin trabajos de ejecución de API en ejecución. [Detalles adicionales](#kubernetes-sizing).

            Por defecto: `6`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.image.repository`
          </td>

          <td>
            La imagen del contenedor que se extraerá para el tiempo de ejecución de la API de Node.js.

            Por defecto: `docker.io/newrelic/synthetics-node-api-runtime`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.image.pullPolicy`
          </td>

          <td>
            La política de extracción para el contenedor de tiempo de ejecución de la API de Node.js.

            Por defecto: `IfNotPresent`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.enabled`
          </td>

          <td>
            Si se debe desplegar o no el tiempo de ejecución browser Node.js. Esto se puede desactivar si no utiliza el script simple o monitor del browser.

            Por defecto: `true`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.parallelism`
          </td>

          <td>
            El número de tiempo de ejecución browser Chrome `CronJobs` para desplegar. La cantidad máxima de trabajos simultáneos browser Chrome que se ejecutarán en cualquier momento. [Detalles adicionales](#kubernetes-sizing).

            Por defecto: `1`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.completions`
          </td>

          <td>
            El número de tiempos de ejecución browser Chrome `CronJobs` que se deben completar por minuto. Aumente esta configuración junto con el paralelismo para mejorar el rendimiento. Esto debe aumentarse cada vez que se aumenta el paralelismo y las terminaciones siempre deben ser al menos mayores o iguales que el paralelismo. Aumente esta configuración si observa períodos de tiempo sin que se ejecuten trabajos de tiempo de ejecución browser . [Detalles adicionales](#kubernetes-sizing).

            Por defecto: `6`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.image.repository`
          </td>

          <td>
            La imagen del contenedor que se extraerá para el tiempo de ejecución browser Node.js.

            Por defecto: `docker.io/newrelic/synthetics-node-browser-runtime`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.image.pullPolicy`
          </td>

          <td>
            La política de extracción para el contenedor de tiempo de ejecución browser Node.js.

            Por defecto: `IfNotPresent`
          </td>
        </tr>
      </tbody>
    </table>
  </Collapser>
</CollapserGroup>

## Variables definidas por el usuario para monitor con script [#user-defined-vars]

Los administradores de trabajos de Private Sintético le permiten configurar variables de entorno para el monitor con script. Estas variables se administran localmente en SJM y se puede acceder a ellas a través de `$env.USER_DEFINED_VARIABLES`. Puede configurar variables definidas por el usuario de dos maneras. Puede montar un archivo JSON o puede proporcionar una variable de entorno al SJM en el lanzamiento. Si se proporcionan ambos, el SJM solo utilizará valores proporcionados por el entorno.

<CollapserGroup>
  <Collapser id="user-file-example" title="Montaje del archivo JSON">
    El usuario puede crear un archivo con formato JSON y montar el volumen donde se encuentra el archivo en una ruta de destino especificada en el contenedor SJM.

    El archivo debe tener permisos de lectura y contener un mapa con formato JSON. Ejemplo de archivo de variables definidas por el usuario:

    ```json
    {
      "KEY": "VALUE",
      "user_name": "MINION",
      "my_password": "PASSW0RD123",
      "my_URL": "https://newrelic.com/",
      "ETC": "ETC"
    }
    ```

    Coloque el archivo en el directorio de origen del host. El SJM espera que el nombre del archivo sea user\_defined\_variables.json

    Ejemplo Docker:

    El directorio objetivo esperado es: `/var/lib/newrelic/synthetics/variables/`

    ```sh
    docker run ... -v /variables:/var/lib/newrelic/synthetics/variables:rw ...
    ```

    Ejemplo de Podman:

    En el caso de SELinux, monte el volumen adicionalmente con `:z` o `:Z`. Para obtener más información, consulte [la documentación de Podman.](https://docs.podman.io/en/latest/markdown/podman-run.1.html#volume-v-source-volume-host-dir-container-dir-options) El directorio objetivo esperado es: `/var/lib/newrelic/synthetics/variables/`

    ```sh
    podman run ... -v /variables:/var/lib/newrelic/synthetics/variables:rw,z ...
    ```

    Ejemplo de Kubernetes:

    El usuario tiene dos opciones al proporcionar un archivo al pod SJM en Kubernetes. Que puede:

    * Pasar un archivo local.
    * Proporcione un PersistentVolume que incluya `user_defined_variables.json`.

    ### Pasar en un archivo local

    Esta opción crea un recurso ConfigMap Kubernetes y lo monta en el pod SJM.

    ```sh
    helm install newrelic/synthetics-job-manager ... --set-file "synthetics.userDefinedVariables.userDefinedFile=[local-path]/user_defined_variables.json" ...
    ```

    ### Montar una `PersistentVolume`

    Esta opción requiere que el usuario proporcione un `PersistentVolume` que incluya el archivo `user_defined_variables.json` o un `PersistentVolumeClaim` al mismo. Para obtener más detalles sobre la instalación del gráfico de Helm usando un `PersistentVolume`, siga las instrucciones en [almacenamiento de datos permanente](/docs/synthetics/synthetic-monitoring/private-locations/job-manager-configuration#permanent-data-storage).

    Una vez que el usuario preparó un `PersistentVolume` como se describe a continuación, inicie el SJM, configure la ruta donde se encuentra el archivo `user_defined_variables.json` y configure cualquier otra variable `synthetics.persistence` según sea necesario.

    ```sh
    helm install newrelic/synthetics-job-manger ... --set synthetics.userDefinedVariables.userDefinedPath="variables"
    ```
  </Collapser>

  <Collapser id="passing-env-var" title="Pasando como una variable de entorno">
    Las variables pueden pasar a su respectivo sistema contenedor a través de la variable de entorno.

    Ejemplo Docker:

    Utilice la bandera `-e` para configurar una variable de entorno denominada `USER_DEFINED_VARIABLES` y asígnele el valor de una cadena de mapa con formato JSON.

    ```sh
    docker run ... -e USER_DEFINED_VARIABLES='{"key":"value","name":"sjm"}' ...
    ```

    Ejemplo de Podman:

    Utilice la bandera `-e` para configurar una variable de entorno denominada `USER_DEFINED_VARIABLES` y asígnele el valor de una cadena de mapa con formato JSON.

    ```sh
    podman run ... -e USER_DEFINED_VARIABLES='{"key":"value","name":"sjm"}' ...
    ```

    Ejemplo de Kubernetes:

    Emplee la bandera `--set-literal` para pasar la cadena con formato JSON.

    ```sh
    helm install newrelic/synthetics-job-manager ... --set-literal synthetics.userDefinedVariables.userDefinedJson='{"key":"value","name":"sjm"}' ...
    ```
  </Collapser>
</CollapserGroup>

### Acceder a variables de entorno definidas por el usuario desde un script [#env-vars-scripts]

Para hacer referencia a una variable de entorno definida por el usuario configurada, emplee el `$env.USER_DEFINED_VARIABLES` reservado seguido del nombre de una variable dada con notación de punto (por ejemplo, `$env.USER_DEFINED_VARIABLES.MY_VARIABLE`).

<Callout variant="caution">
  Las variables de entorno definidas por el usuario no se desinfectan del log. Considere utilizar la característica [de credenciales seguras](/docs/synthetics/new-relic-synthetics/using-monitors/secure-credentials-store-credentials-information-scripted-browsers) para información confidencial.
</Callout>

## Módulos de nodo personalizados [#custom-modules]

Se proporcionan módulos de nodo personalizados tanto en llamadas por minuto como en SJM. Le permiten crear un conjunto personalizado de [módulos de nodo](https://docs.npmjs.com/about-packages-and-modules) y usarlos en un monitor con script ( API con script y browser con script) para monitoreo sintético.

### Configurar su directorio de módulos personalizados

Cree un directorio con un archivo `package.json` siguiendo [las pautas oficiales de npm](https://docs.npmjs.com/files/package.json) en la carpeta raíz. El SJM instalará cualquier dependencia enumerada en el paquete.json. campo `dependencies` . Estas dependencias estarán disponibles cuando se ejecute el monitor en el administrador de trabajos privado de Sintético. Vea un ejemplo de esto a continuación.

#### Ejemplo

En este ejemplo, se utiliza un directorio de módulo personalizado con la siguiente estructura:

```
/example-custom-modules-dir/
    ├── counter
    │   ├── index.js
    │   └── package.json
    └── package.json            ⇦ the only mandatory file
```

`package.json` define `dependencies` como un módulo local (por ejemplo, `counter`) y cualquier módulo alojado (por ejemplo, `smallest` versión `1.0.1`):

```json
{
    "name": "custom-modules",
    "version": "1.0.0",                                ⇦ optional
    "description": "example custom modules directory", ⇦ optional
    "dependencies": {
    "smallest": "1.0.1",                               ⇦ hosted module
    "counter": "file:./counter"                        ⇦ local module
    }
}
```

### Agregue su directorio de módulos personalizados al SJM para Docker, Podman o Kubernetes

<CollapserGroup>
  <Collapser id="docker" title="Docker">
    Para docker, lanza SJM montando el directorio en `/var/lib/newrelic/synthetics/modules`. Por ejemplo:

    ```sh
    docker run ... -v /example-custom-modules-dir:/var/lib/newrelic/synthetics/modules:rw ...
    ```
  </Collapser>

  <Collapser id="podman" title="Podman">
    Para podman, inicie SJM montando el directorio en `/var/lib/newrelic/synthetics/modules`. En el caso de SELinux, monte el volumen adicionalmente con `:z` o `:Z`. Para obtener más información, consulte [la documentación de Podman](https://docs.podman.io/en/latest/markdown/podman-run.1.html#volume-v-source-volume-host-dir-container-dir-options). Por ejemplo:

    ```sh
    podman run ... -v /example-custom-modules-dir:/var/lib/newrelic/synthetics/modules:rw,z ...
    ```
  </Collapser>

  <Collapser id="kubernetes" title="Kubernetes">
    Para Kubernetes, el directorio en `/var/lib/newrelic/synthetics/modules` debe existir en un PV antes de iniciar SJM con módulos personalizados habilitados.

    <Callout variant="tip">
      El modo de acceso a PV debe ser ReadWriteMany si necesita compartir almacenamiento entre varios pods.
    </Callout>

    Un método es crear un pod que monte el PV solo con el propósito de copiar el directorio de módulos personalizados al PV. El siguiente ejemplo emplea Amazon EFS con Amazon EKS:

    #### Cree el namespace, el volumen persistente y la reclamación del volumen persistente

    1. Cerciorar de que ya configuró su sistema de archivos EFS e instaló el [controlador CSI de EFS](https://github.com/kubernetes-sigs/aws-efs-csi-driver) en su clúster. También necesitará su ID de sistema de archivos EFS para los PV `spec.csi.volumeHandle`.

       ```sh
       kubectl apply -f - <<EOF
       apiVersion: v1
       kind: Namespace
       metadata:
         name: newrelic

       ---
       kind: StorageClass
       apiVersion: storage.k8s.io/v1
       metadata:
         name: efs-sc
       provisioner: efs.csi.aws.com

       ---
       apiVersion: v1
       kind: PersistentVolume
       metadata:
         name: custom-modules-pvc
       spec:
         capacity:
           storage: 5Gi
         volumeMode: Filesystem
         accessModes:
           - ReadWriteMany
         persistentVolumeReclaimPolicy: Retain
         storageClassName: efs-sc
         csi:
           driver: efs.csi.aws.com
           volumeHandle: <your-efs-filesystem-id>

       ---
       apiVersion: v1
       kind: PersistentVolumeClaim
       metadata:
         name: custom-modules-pvc
         namespace: newrelic
       spec:
         accessModes:
           - ReadWriteMany
         storageClassName: efs-sc
         resources:
           requests:
             storage: 5Gi
       EOF
       ```

    2. Cambie al namespace `newrelic` en su `~/.kube/config`.

       ```sh
       kubectl config get-contexts
       kubectl config set-context YOUR_CONTEXT --namespace=newrelic
       kubectl config view --minify | grep namespace:
       ```

    3. En este punto, el PVC debe estar vinculado al PV con el modo de acceso RWX.

       ```sh
       kubectl get pv,pvc
       [output] NAME                                  CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                         STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE
       [output] persistentvolume/custom-modules-pvc   5Gi        RWX            Retain           Bound    newrelic/custom-modules-pvc   efs-sc         <unset>                          4m46s
       [output]
       [output] NAME                                       STATUS   VOLUME               CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
       [output] persistentvolumeclaim/custom-modules-pvc   Bound    custom-modules-pvc   5Gi        RWX            efs-sc         <unset>                 4m10s
       ```

       #### Crea `mount-custom-mods-pod` para copiar tu directorio de módulos personalizados

       ```sh
       kubectl apply -f - <<EOF
       apiVersion: v1
       kind: Pod
       metadata:
         name: mount-custom-mods-pod
       spec:
         containers:
         - name: mount-custom-mods-pod
           image: nginx
           resources:
             requests:
               memory: "64Mi"
               cpu: "250m"
             limits:
               memory: "128Mi"
               cpu: "500m"
           volumeMounts:
             - mountPath: "/var/lib/newrelic/synthetics/modules"
               name: custom-modules-storage
         volumes:
         - name: custom-modules-storage
           persistentVolumeClaim:
             claimName: custom-modules-pvc
       EOF
       ```

       En este punto, se debe crear y configurar el `mount-custom-mods-pod` para usar el volumen.

       ```sh
       kubectl describe po mount-custom-mods-pod | grep -A4 Volumes:
       [output] Volumes:
       [output]   custom-modules-storage:
       [output]     Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
       [output]     ClaimName:  custom-modules-pvc
       [output]     ReadOnly:   false
       ```

       Consulte el evento para ver si hay advertencias relacionadas con PV, PVC o `mount-custom-mods-pod`.

       ```sh
       kubectl get events --field-selector type=Warning --sort-by='.lastTimestamp'
       ```

       #### Copie su directorio de módulos personalizados al PV

       No es necesario copiar `node_modules` ya que lo generará el SJM en `npm install`.

       ```sh
       cd custom-modules
       rm -rf node_modules && cd ..
       ```

    4. Compruebe que `mount-custom-mods-pod` se esté ejecutando.

       ```sh
       kubectl get po
       [output] NAME                    READY   STATUS    RESTARTS   AGE
       [output] mount-custom-mods-pod   1/1     Running   0          5m43s
       ```

    5. Copiar al PV.

       ```sh
       kubectl cp custom-modules newrelic/mount-custom-mods-pod:/var/lib/newrelic/synthetics/modules
       ```

    6. Compruebe que `/var/lib/newrelic/synthetics/modules/custom-modules/package.json` exista en el PV.

       ```sh
       kubectl exec -it mount-custom-mods-pod -- bash
       [output] root@mount-custom-mods-pod:/# cd /var/lib/newrelic/synthetics/modules/
       [output] root@mount-custom-mods-pod:/var/lib/newrelic/synthetics/modules# ls -l
       [output] total 4
       [output] drwxr-xr-x 2 root root 6144 Jun 29 03:49 custom-modules
       [output] root@mount-custom-mods-pod:/var/lib/newrelic/synthetics/modules# ls -l custom-modules/
       [output] total 4
       [output] -rw-r--r-- 1 501 staff 299 Jun 29 03:49 package.json
       ```

       #### lanzar el SJM con la característica de módulos personalizados habilitada

       Establezca valores para `persistence.existingClaimName` y `customNodeModules.customNodeModulesPath` en la línea de comando o en un archivo YAML durante la instalación. El valor `customNodeModules.customNodeModulesPath` debe especificar la subruta en el volumen persistente donde existen sus archivos de módulos personalizados. Por ejemplo:

       ```sh
       helm upgrade --install synthetics-job-manager newrelic/synthetics-job-manager -n newrelic --set global.persistence.existingClaimName=custom-modules-pvc --set global.customNodeModules.customNodeModulesPath=custom-modules --set synthetics.privateLocationKey=YOUR_PRIVATE_LOCATION_KEY
       [output] Release "synthetics-job-manager" does not exist. Installing it now.
       [output] NAME: synthetics-job-manager
       [output] LAST DEPLOYED: Fri Jun 28 16:53:28 2024
       [output] NAMESPACE: newrelic
       [output] STATUS: deployed
       [output] REVISION: 1
       [output] TEST SUITE: None
       ```

       El directorio `custom-modules` ahora debería contener los paquetes instalados en `node_modules`.

       ```sh
       kubectl exec -it mount-custom-mods-pod -- bash
       [output] root@mount-custom-mods-pod:/# cd /var/lib/newrelic/synthetics/modules/
       [output] root@mount-custom-mods-pod:/var/lib/newrelic/synthetics/modules# ls -l custom-modules/
       [output] total 16
       [output] -rw-r--r--  1 root root   836 Jun 29 03:51 README
       [output] drwxr-xr-x 18 root root  6144 Jun 29 03:51 node_modules
       [output] -rw-r--r--  1  501 staff  299 Jun 29 03:49 package.json
       [output] -rw-r--r--  1 root root   190 Jun 29 03:51 package.json.shasum
       ```

       Si no se detectan módulos de nodo personalizados, ajuste las licencias en el directorio `custom-modules` y el archivo `package.json` .

       ```sh
       kubectl exec -it mount-custom-mods-pod -- bash
       [output] root@mount-custom-mods-pod:/# cd /var/lib/newrelic/synthetics/modules/
       [output] root@mount-custom-mods-pod:/var/lib/newrelic/synthetics/modules# chmod -R 777 custom-modules
       [output] root@mount-custom-mods-pod:/var/lib/newrelic/synthetics/modules# chown -R 2000:2000 custom-modules
       ```
  </Collapser>
</CollapserGroup>

Para comprobar si los módulos se instalaron correctamente o si se produjo algún error, busque las siguientes líneas en los registros [del contenedor](/docs/synthetics/new-relic-synthetics/private-locations/job-manager-maintenance-monitoring#monitor-docker-logs) o [pod](/docs/synthetics/synthetic-monitoring/private-locations/job-manager-maintenance-monitoring/#review-kubernetes-logs) `synthetics-job-manager` :

```log
2024-06-29 03:51:28,407{UTC} [main] INFO  c.n.s.j.p.options.CustomModules - Detected mounted path for custom node modules
2024-06-29 03:51:28,408{UTC} [main] INFO  c.n.s.j.p.options.CustomModules - Validating permission for custom node modules package.json file
2024-06-29 03:51:28,409{UTC} [main] INFO  c.n.s.j.p.options.CustomModules - Installing custom node modules...
2024-06-29 03:51:44,670{UTC} [main] INFO  c.n.s.j.p.options.CustomModules - Custom node modules installed successfully.
```

Ahora puede agregar `"require('smallest');"` al [script](/docs/synthetics/new-relic-synthetics/scripting-monitors/write-scripted-browsers) del monitor que envía a esta ubicación privada.

### Cambiar `package.json` [#change-package-json]

Además de los módulos locales y alojados, también puede utilizar [módulos de Node.js.](/docs/synthetics/new-relic-synthetics/scripting-monitors/import-nodejs-modules) Para actualizar los módulos personalizados utilizados por su SJM, realice cambios en el archivo `package.json` y reinicie SJM. Durante el proceso de reinicio, el SJM reconocerá el cambio de configuración y realizará automáticamente operaciones de limpieza y reinstalación para garantizar que se apliquen los módulos actualizados.

<Callout variant="caution">
  Módulos locales: si bien su `package.json` puede incluir cualquier módulo local, estos módulos deben residir dentro del árbol debajo de su directorio de módulos personalizados. Si se almacena fuera del árbol, el proceso de inicialización fallará y verá un mensaje de error en el [log docker ](/docs/synthetics/new-relic-synthetics/private-locations/job-manager-maintenance-monitoring#monitor-docker-logs)después de iniciar SJM.
</Callout>

## Almacenamiento permanente de datos [#permanent-data-storage]

Es posible que el usuario desee emplear almacenamiento de datos permanente para proporcionar el archivo `user_defined_variables.json` o admitir módulos de nodo personalizados.

### Docker

Para configurar el almacenamiento permanente de datos en Docker:

1. Cree un directorio en el host donde está iniciando Job Manager. Este es su directorio de origen.

2. Inicie el Administrador de trabajos, montando el directorio de origen en el directorio de destino `/var/lib/newrelic/synthetics`.

   Ejemplo:

   ```sh
   docker run ... -v /sjm-volume:/var/lib/newrelic/synthetics:rw ...
   ```

### Podman

Para configurar el almacenamiento de datos permanente en Podman:

1. Cree un directorio en el host donde está iniciando Job Manager. Este es su directorio de origen.
2. Inicie el Administrador de trabajos, montando el directorio de origen en el directorio de destino `/var/lib/newrelic/synthetics`.

Ejemplo:

```sh
podman run ... -v /sjm-volume:/var/lib/newrelic/synthetics:rw,z ...
```

### Kubernetes

Para configurar el almacenamiento permanente de datos en Kubernetes, el usuario tiene dos opciones:

1. Proporcione un PersistentVolumeClaim (PVC) existente para un PersistentVolume (PV) existente y establezca el valor de configuración `synthetics.persistence.existingClaimName` . Ejemplo:

   ```sh
   helm install ... --set synthetics.persistence.existingClaimName=sjm-claim ...
   ```

2. Proporcione un nombre de PersistentVolume (PV) existente y establezca el valor de configuración `synthetics.persistence.existingVolumeName` . Helm generará una PVC para el usuario. El usuario también puede configurar opcionalmente los siguientes valores:

* `synthetics.persistence.storageClass`:La clase de almacenamiento del PV existente. Si no se proporciona, Kubernetes empleará la clase de almacenamiento predeterminada.

* `synthetics.persistence.size`:El tamaño de la reclamación. Si no se configura, el valor predeterminado actualmente es 2Gi.

  ```sh
  helm install ... --set synthetics.persistence.existingVolumeName=sjm-volume --set synthetics.persistence.storageClass=standard ...
  ```

## Consideraciones de dimensionamiento para Docker y Podman [#vm-sizing]

Para garantizar que su ubicación privada funcione de manera eficiente, debe aprovisionar suficientes recursos de CPU en su host para manejar su carga de trabajo de monitoreo. Muchos factores influyen en el tamaño, pero puedes estimar rápidamente tus necesidades. Necesitará **1 núcleo de CPU para cada monitor pesado** (es decir, navegador simple, navegador con script o monitor de API con script). A continuación se muestran dos fórmulas que lo ayudarán a calcular la cantidad de núcleos que necesita, ya sea que esté diagnosticando una configuración actual o planeando una futura.

### Fórmula 1: Diagnóstico de una ubicación existente

Si su ubicación privada actual tiene dificultades para mantener el ritmo y sospecha que hay cola de trabajos, emplee esta fórmula para descubrir cuántos núcleos necesita realmente. Se basa en el rendimiento observable de su sistema.

$$ C\_est = (R\_proc + R\_growth) \cdot D\_avg,m $$

* $C\_est$ = **Núcleos de CPU estimados**.
* $R\_proc$ = La **tasa** de trabajos pesados que se **procesan** por minuto.
* $R\_growth$ = La **velocidad a** la que su cola `jobManagerHeavyweightJobs` **crece** por minuto.
* $D\_avg,m$ = La **duración promedio** de trabajos pesados en **minutos**.

Esta fórmula calcula la tasa real de llegada de trabajos sumando los trabajos que su sistema *está procesando* a los trabajos que se están *acumulando* en la cola. Multiplicar esta carga total por la duración media del trabajo le indicará exactamente cuántos núcleos necesita para completar todo el trabajo sin colas.

### Fórmula 2: Pronosticar una ubicación nueva o futura

Si está configurando una nueva ubicación privada o planea agregar más monitores, use esta fórmula para pronosticar sus necesidades con anticipación.

$$ C\_est = N\_mon \cdot D\_avg,m \cdot \frac1P\_avg,m $$

* $C\_est$ = **Núcleos de CPU estimados**.
* $N\_mon$ = La **cantidad** total de **monitores** pesados que planea ejecutar.
* $D\_avg,m$ = La **duración promedio** de un trabajo pesado en **minutos**.
* $P\_avg,m$ = El **periodo promedio** para monitores pesados en **minutos** (por ejemplo, un monitor que se ejecuta cada 5 minutos tiene $P\_avg,m = 5$).

Esto calcula tu carga de trabajo prevista a partir de principios básicos: cuántos monitores tienes, con qué frecuencia se ejecutan y cuánto tiempo tardan.

**Factores de tamaño importantes**

Al emplear estas fórmulas, recuerde tener en cuenta estos factores:

* **Duración del trabajo ($D\_avg,m$):** su promedio debe incluir trabajos que **expiran** (generalmente ~3 minutos), ya que estos mantienen un núcleo durante toda su duración.
* **Errores de trabajo y reintentos:** cuando un monitor falla, se vuelve a intentar automáticamente. Estos reintentos son trabajos adicionales que se suman a la carga total. Un monitor que falla y vuelve a intentarlo constantemente **multiplica efectivamente su periodo**, lo que afecta significativamente el rendimiento.
* **Escalamiento horizontal:** además de agregar más núcleos a un host (escalamiento vertical), puede implementar administradores de trabajos sintéticos adicionales con la misma clave de ubicación privada para equilibrar la carga de trabajos en múltiples entornos (escalamiento horizontal).

Es importante tener en cuenta que un solo Sintéticos Job Manager (SJM) tiene un límite de rendimiento de **aproximadamente 15 trabajos pesados por minuto**. Esto se debe a una estrategia de subprocesamiento interno que favorece la competencia eficiente de trabajos entre múltiples SJM sobre la cantidad bruta de trabajos procesados por SJM. Si sus cálculos indican la necesidad de un mayor rendimiento, deberá **ampliar la escala** implementando SJM adicionales. Puede [verificar si su cola de trabajos está creciendo](/docs/synthetics/synthetic-monitoring/private-locations/job-manager-maintenance-monitoring/) para determinar si se necesitan más SJM.

Agregar más SJM con la misma clave de ubicación privada ofrece varios beneficios:

* **Equilibrio de carga**: los trabajos para la ubicación privada se distribuyen entre todos los SJM disponibles.
* **Protección contra conmutación por error**: si una instancia de SJM deja de funcionar, las demás pueden continuar procesando trabajos.
* **Mayor rendimiento total**: el rendimiento total de su ubicación privada se convierte en la suma del rendimiento de cada SJM (por ejemplo, dos SJM proporcionan hasta ~30 trabajos/minuto).

### Consulta de NRQL para diagnóstico

Puede ejecutar estas consultas en el [generador de consultas](/docs/query-your-data/explore-query-data/get-started/introduction-querying-new-relic-data/) para obtener los insumos para la fórmula de diagnóstico. Cerciorar de establecer el rango de tiempo en un periodo lo suficientemente largo para obtener un promedio estable.

**1. Calcular la tasa de trabajos procesados por minuto ($R\_proc$)**: Esta consulta cuenta el número de trabajos que no son de ping (pesados) completados durante el último día y muestra la tasa promedio por minuto.

```sql
FROM SyntheticCheck
SELECT rate(uniqueCount(id), 1 minute) AS 'job rate per minute'
WHERE location = 'YOUR_PRIVATE_LOCATION' AND type != 'SIMPLE'
SINCE 1 day ago
```

**2. Encuentra la tasa de crecimiento de la cola por minuto ($R\_growth$)**: Esta consulta calcula el crecimiento promedio por minuto de la cola `jobManagerHeavyweightJobs` en un gráfico de seriales temporales. Una línea por encima de cero indica que la cola está creciendo, mientras que una línea por debajo de cero significa que está disminuyendo.

```sql
FROM SyntheticsPrivateLocationStatus
SELECT derivative(jobManagerHeavyweightJobs, 1 minute) AS 'queue growth rate per minute'
WHERE name = 'YOUR_PRIVATE_LOCATION'
TIMESERIES SINCE 1 day ago
```

<Callout variant="tip">
  Cerciorar de seleccionar la cuenta donde existe la ubicación privada. Es mejor ver esta consulta como un serial de tiempo porque la función derivada puede variar mucho. El objetivo es obtener una estimación de la tasa de crecimiento de la cola por minuto. Play con diferentes rangos de tiempo para ver qué funciona mejor.
</Callout>

**3. Encontrar el número total de monitores pesados ($N\_mon$)**: Esta consulta encuentra el recuento único de monitores pesados.

```sql
FROM SyntheticCheck
SELECT uniqueCount(monitorId) AS 'monitor count'
WHERE location = 'YOUR_PRIVATE_LOCATION' AND type != 'SIMPLE'
SINCE 1 day ago
```

**4. Calcular la duración promedio del trabajo en minutos ($D\_avg,m$)**: Esta consulta encuentra la duración promedio de ejecución de los trabajos no ping completados y convierte el resultado de milisegundos a minutos. `executionDuration` representa el tiempo que tardó el trabajo en ejecutar en el host.

```sql
FROM SyntheticCheck
SELECT average(executionDuration)/60e3 AS 'avg job duration (m)'
WHERE location = 'YOUR_PRIVATE_LOCATION' AND type != 'SIMPLE'
SINCE 1 day ago
```

**5. Encuentre el periodo promedio del monitor de peso pesado ($P\_avg,m$):** si la cola `jobManagerHeavyweightJobs` de la ubicación privada está creciendo, no es preciso calcular el periodo promedio del monitor a partir de los resultados existentes. Esto deberá estimar a partir de la lista de monitores en la página [Monitores Sintéticos](https://one.newrelic.com/synthetics). Cerciórate de seleccionar la cuenta New Relic correcta y es posible que debas filtrar por `privateLocation`.

<Callout variant="tip">
  Los monitores sintéticos pueden existir en múltiples subcuentas. Si tienes más subcuentas de las que se pueden seleccionar en el generador de consultas, elige las cuentas con más monitores.
</Callout>

### Nota sobre los monitores de ping y la cola `pingJobs`

**Los monitores de ping son diferentes.** Son trabajos ligeros que no consumen un núcleo de CPU completo cada uno. En su lugar, emplean una cola separada (`pingJobs`) y se ejecutan en un grupo de subprocesos de trabajo.

Si bien consumen menos recursos, un gran volumen de trabajos de ping, especialmente los fallidos, aún pueden causar problemas de rendimiento. Tenga en cuenta estos puntos:

* **Modelo de recursos:** los trabajos de ping emplean subprocesos de trabajo, no núcleos de CPU dedicados. En estos casos no se aplica el cálculo de núcleo por puesto de trabajo.
* **Tiempo de espera y reintento:** un trabajo de ping fallido puede ocupar un hilo de trabajo durante hasta **60 segundos**. Primero intenta una solicitud HTTP HEAD (tiempo de espera de 30 segundos). Si eso falla, vuelve a intentarlo inmediatamente con una solicitud HTTP GET (otro tiempo de espera de 30 segundos).
* **Escalamiento:** aunque la fórmula de dimensionamiento es diferente, se aplican los mismos principios. Para manejar un gran volumen de trabajos de ping y evitar que la cola `pingJobs` crezca, es posible que necesite escalar verticalmente u horizontalmente. Escalar significa aumentar los recursos de CPU y memoria por host o namespace. Escalar horizontalmente significa agregar más instancias del tiempo de ejecución de ping. Esto se puede hacer implementando más administradores de trabajos en más hosts, en más espacios de nombres o incluso [dentro del mismo namespace](/docs/synthetics/synthetic-monitoring/private-locations/job-manager-configuration#scaling-out-with-multiple-sjm-instances). Alternativamente, el `ping-runtime` en Kubernetes le permite establecer [una mayor cantidad de réplicas](https://github.com/newrelic/helm-charts/blob/41c03e287dafd41b9c914e5a6c720d5aa5c01ace/charts/synthetics-job-manager/values.yaml#L173) por implementación.

## Consideraciones de dimensionamiento para Kubernetes y OpenShift [#kubernetes-sizing]

Cada entorno de ejecución empleado por el administrador de trabajos Kubernetes y OpenShift Sintético se puede dimensionar de forma independiente configurando valores en el [gráfico de Helm](https://github.com/newrelic/helm-charts/tree/master/charts/synthetics-job-manager). El [node-api-runtime](https://github.com/newrelic/helm-charts/tree/master/charts/synthetics-job-manager/charts/node-api-runtime) y el[ node-browser-runtime](https://github.com/newrelic/helm-charts/tree/master/charts/synthetics-job-manager/charts/node-browser-runtime) se dimensionan de forma independiente empleando una combinación de las configuraciones `parallelism` y `completions`.

* La configuración `parallelism` controla cuántos pods de un entorno de ejecución particular se ejecutan simultáneamente.
* La configuración `completions` controla cuántos pods deben completar antes de que `CronJob` inicie otro trabajo Kubernetes para ese tiempo de ejecución.

### Mejores prácticas para dimensionar su implementación

A menudo no es posible calcular con precisión los valores necesarios de paralelismo y finalizaciones porque la duración promedio observada en New Relic podría no ser exacta, especialmente si la ubicación privada existente no funciona correctamente. Siga este enfoque práctico para ajustar el paralelismo y las finalizaciones. Las siguientes ecuaciones se pueden utilizar para obtener valores aproximados como punto de partida.

**1. Estimar finalizaciones y paralelismo**

Haga lo posible por estimar la duración promedio de ejecución y el número de trabajos cada 5 minutos. Esto le proporciona un punto de partida aproximado para el siguiente paso, que implicará prueba y error para ajustar los valores de paralelismo y finalizaciones en un clúster en funcionamiento. Asegúrese de escalarlos proporcionalmente, por ejemplo, pasando de los valores predeterminados de 1 y 6 a 10 y 60.

**Finalizaciones estimadas:** Esto determina cuánto tiempo tardará en completarse su carga de trabajo de 5 minutos.

```sql
-- Get average execution duration in minutes
FROM SyntheticCheck
SELECT average(executionDuration / 60e3) AS 'Avg Duration (min)'
WHERE type != 'SIMPLE' AND location = 'YOUR_PRIVATE_LOCATION'
SINCE 1 hour ago
```

$$ Finalizaciones = \frac5D\_avg,m $$

Donde $D\_avg,m$ es su **duración promedio de ejecución del trabajo en minutos**.

**Paralelismo estimado:** Esto determina cuántos trabajadores (pods) necesita que se ejecuten simultáneamente para manejar su carga de trabajo de 5 minutos.

```sql
-- Get jobs per 5 minutes
FROM SyntheticCheck
SELECT rate(uniqueCount(id), 5 minutes) AS 'Number of monitor jobs per 5 minutes'
WHERE type != 'SIMPLE' AND location = 'YOUR_PRIVATE_LOCATION'
SINCE 1 hour ago
```

$$ P\_est = \fracN\_mCompletions $$

Donde $N\_m$ es su **número de trabajos por 5 minutos**. Este valor $P\_est$ es su **paralelismo estimado**.

**2. Realice un despliegue de Helm**

Realice un despliegue de Helm con valores estimados de paralelismo y finalizaciones, y su mejor estimación para `ping-runtime.replicaCount` dada la cantidad de núcleos de CPU por nodo y el número de monitores de ping que deben ejecutarse por minuto.

**3. Monitorear el crecimiento de la cola**

Con los monitores sintéticos configurados para enviar trabajos a la ubicación privada, verifique el crecimiento de la cola en un gráfico de líneas de serie de tiempo para `pingJobs` y `jobManagerHeavyweightJobs`.

* Si la cola `pingJobs` tiene una pendiente positiva, aumente `ping-runtime.replicaCount` y vuelva a desplegar.
* Si la cola `jobManagerHeavyweightJobs` tiene una pendiente positiva, aumente `parallelism` y `completions` proporcionalmente hasta que la cola deje de crecer (pendiente negativa).

Una pendiente negativa indica que el administrador de trabajos tiene suficiente paralelismo para manejar la demanda de trabajos. Eventualmente llegará a cero con una pendiente negativa.

```sql
FROM SyntheticsPrivateLocationStatus
SELECT average(jobManagerHeavyweightJobs) AS 'Heavyweight Queue Growth', average(pingJobs) AS 'Ping Queue Growth'
WHERE name = 'YOUR_PRIVATE_LOCATION'
SINCE 1 day ago TIMESERIES
```

**4. Ajustar según el estado de ejecución del pod**

Con la cola disminuyendo o en cero, busque pods `node-api-runtime` y `node-browser-runtime` que estén en estado &quot;running&quot; durante más de 10 minutos. Esto indica que el paralelismo está configurado demasiado alto y hay más pods de los necesarios.

Para evitar desperdiciar recursos innecesariamente, disminuya `parallelism` y `completions` para reducir la antigüedad de cada pod de runtime &quot;running&quot;. Si el objetivo es una antigüedad de trabajo de Kubernetes de 5 minutos, los pods de tiempo de ejecución deben estar en estado de ejecución durante menos de 5 minutos, lo que significa que el pod se creó, recibió rápidamente un trabajo para ejecutar y se completó.

**5. Escalar horizontalmente si es necesario**

Si la cola no disminuye, pero hay muchos pods en estado &quot;running&quot; durante más de 10 minutos, es probable que el gestor de trabajos esté alcanzando su cuello de botella de rendimiento. Lo siguiente que debe hacer es disminuir el paralelismo y escalar horizontalmente con una o más implementaciones adicionales.

Por ejemplo, con `parallelism: 100`, `completions: 600` la cola sigue creciendo, pero hay muchos pods en estado &quot;running&quot; durante más de 10 minutos, y la antigüedad del Job de Kubernetes es de 20 minutos ... configure `parallelism: 50`, `completions: 200` y escale horizontalmente (hacia afuera) agregando 2 despliegues adicionales. Esto genera un total de 150 pods paralelos y debería reducir la antigüedad del trabajo de K8s a menos de 20 minutos, a la vez que reduce la cantidad de pods &quot;running&quot; de larga duración. Apunte a una antigüedad del trabajo de K8s de 5 a 10 minutos.

Para obtener más información sobre cómo agregar implementaciones, consulte [Escalamiento con múltiples implementaciones de SJM](#scaling-out-with-multiple-sjm-deployments).

<Callout variant="tip">
  Puede utilizar la siguiente consulta para ayudar a determinar si necesita escalar horizontalmente.

  **Nota:** Los monitores pueden existir en múltiples subcuentas.

  ```sql
  -- monitors per minute per SJM
  FROM SyntheticCheck SELECT
    round(rate(uniqueCount(id), 1 minute)/uniqueCount(minionId),0.1) AS 'heavy jobs per minute per SJM',
    uniqueCount(minionId) AS 'number of SJMs (namespaces)',
    round(rate(uniqueCount(id), 1 minute),0.1) AS 'heavy jobs per minute total'
  WHERE minionContainerSystem = 'KUBERNETES' AND minionDeploymentMode = 'private' AND location = 'YOUR_PRIVATE_LOCATION' AND type != 'SIMPLE' FACET location SINCE 1 hour ago TIMESERIES
  ```
</Callout>

<Callout variant="tip">
  Reducir el número de ciclos de trabajo de K8s también puede mejorar el rendimiento. A medida que cada ciclo alcanza el número establecido de finalizaciones, hay cada vez menos pods &quot;en ejecución&quot; para asumir nuevos trabajos de Synthetics. Por ejemplo, con completions establecido en 200 y parallelism establecido en 50, inicialmente tenemos 50 pods en ejecución, pero esto comienza a disminuir a medida que superamos los 150 completions. A las 199 finalizaciones, solo queda 1 pod en ejecución.

  Configurar un valor mayor para las finalizaciones no es una mala idea, pero puede generar eventos de advertencia en K8s sobre `TooManyMissedTimes` para el cronjob.
</Callout>

### Escalado horizontal con múltiples despliegues de SJM

Para escalar más allá del rendimiento de ~15 trabajos/minuto de un solo SJM, debe instalar **varias versiones separadas de SJM Helm**.

<Callout variant="important">
  No utilice réplicas para escalar el pod del administrador de trabajos. La arquitectura SJM requiere una relación 1:1 entre un pod de tiempo de ejecución y su pod SJM padre. Si los pods de tiempo de ejecución envían resultados a la réplica de SJM incorrecta (p. ej., a través de un servicio de Kubernetes), esos resultados se perderán. Sin embargo, se puede usar ping-runtime.replicaCount.
</Callout>

La estrategia correcta es desplegar múltiples instancias de SJM, cada una como su propia versión Helm. Cada SJM competirá por los trabajos desde la misma ubicación privada, lo que proporcionará equilibrio de carga, protección contra fallos y un mayor rendimiento total de trabajos.

#### Estrategia de escalado horizontal

Si necesita escalar horizontalmente, puede simplificar el mantenimiento tratando cada implementación de SJM como una unidad de capacidad fija.

1. **Establecer paralelismo:** Para *cada* SJM, establezca `parallelism` en el mismo máximo que un solo SJM pueda manejar sin crear demasiados pods de tiempo de ejecución &quot;en ejecución&quot; de larga duración. Esto maximiza el rendimiento potencial de cada SJM sin desperdiciar recursos.
2. **Establecer finalizaciones:** Para *cada* SJM, establezca `completions` en el mismo valor fijo también. Ajuste según sea necesario para lograr una antigüedad de job de Kubernetes de 5 minutos por runtime, es decir, node-browser-runtime y node-api-runtime.
3. **Instalar lanzamientos:** Instale tantos lanzamientos de Helm separados como necesite para manejar su demanda total de trabajos; es decir, lleve la cola a cero o el gráfico de líneas a una pendiente negativa.
4. **Monitorear y agregar:** Monitoree la cola de trabajos de la ubicación privada. Si comienza a crecer (pendiente positiva), simplemente instale otro release de Helm (p. ej., `sjm-delta`) usando la misma configuración fija.

Al fijar el paralelismo y las finalizaciones en valores estáticos, aumentar o disminuir la capacidad se convierte en un proceso más sencillo de **agregar o eliminar releases de Helm**. Esto ayuda a evitar el desperdicio de recursos del clúster en un valor de paralelismo superior al que el SJM puede utilizar eficazmente.

#### Ejemplo de instalación

Al instalar varias versiones de SJM, debe proporcionar un **nombre único para cada versión**. Todas las instancias deben configurar con la **misma clave de ubicación privada**.

Se recomienda encarecidamente establecer `fullnameOverride` para crear nombres de recursos más cortos y manejables. Por ejemplo, para instalar dos SJM llamados `sjm-alpha` y `sjm-beta` en el namespace `newrelic` (ambos usando el mismo `values.yaml` con su paralelismo y autocompletado fijos):

```sh
# Install the first SJM deployment
helm upgrade --install sjm-alpha newrelic/synthetics-job-manager \
  -n newrelic \
  -f values.yaml \
  --set fullnameOverride=sjm-alpha \
  --set ping-runtime.fullnameOverride=sjm-alpha-ping \
  --set node-api-runtime.fullnameOverride=sjm-alpha-api \
  --set node-browser-runtime.fullnameOverride=sjm-alpha-browser
```

```sh
# Install the second SJM deployment to add capacity
helm upgrade --install sjm-beta newrelic/synthetics-job-manager \
  -n newrelic \
  -f values.yaml \
  --set fullnameOverride=sjm-beta
  --set ping-runtime.fullnameOverride=sjm-beta-ping \
  --set node-api-runtime.fullnameOverride=sjm-beta-api \
  --set node-browser-runtime.fullnameOverride=sjm-beta-browser
```

Puedes continuar este patrón (`sjm-charlie`, `sjm-delta` etc.) para tantos SJM como sea necesario para evitar que la cola de trabajos crezca.