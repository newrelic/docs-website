---
title: Integración de Databricks
tags:
  - Databricks
  - databricks integration
  - New Relic integration
metaDescription: Use the Databricks integration to collect telemetry from the Databricks Data Intelligence Platform
freshnessValidatedDate: '2026-01-26T00:00:00.000Z'
translationType: machine
---

La integración de Databricks es un proyecto de la comunidad de código abierto que proporciona un conjunto completo de capacidades de recopilación de telemetría en todo su entorno de Databricks. Estas capacidades garantizan que tenga todos los datos en contexto que necesita para un análisis y una optimización profundos.

La integración recopila los siguientes tipos de telemetría:

* Métricas de la aplicación Apache Spark, como la memoria del ejecutor de Spark y las métricas de la CPU, la duración de los trabajos de Spark, la duración y las métricas de E/S de las etapas y tareas de Spark, y las métricas de memoria y disco de Spark RDD
* Métricas de ejecución de trabajos de Databricks Lakeflow, como duraciones, horas de inicio y finalización, y códigos y tipos de terminación para las ejecuciones de trabajos y tareas.
* Métricas de actualización de la canalización declarativa de Databricks Lakeflow, como duraciones, horas de inicio y finalización, y estado de finalización de las actualizaciones y flujos.
* Logs de eventos de la canalización declarativa de Databricks Lakeflow
* Métricas de consulta de Databricks, incluyendo tiempos de ejecución y métricas de E/S de consulta.
* Métricas y logs de estado del clúster de Databricks, como las métricas de memoria y CPU del controlador y del trabajador, y los logs del controlador y del ejecutor.
* Datos de consumo y costos de Databricks que se pueden usar para mostrar el consumo de DBU y los costos estimados de Databricks.

## Instalar la integración [#setup]

La integración de Databricks está diseñada para ser implementada en el nodo del controlador de un [clúster](https://docs.databricks.com/en/getting-started/concepts.html#cluster) de Databricks de uso general, trabajo o pipeline. Para implementar la integración de esta manera, siga los pasos para [implementar la integración en un clúster de Databricks](https://github.com/newrelic/newrelic-databricks-integration/docs/installation.md#deploy-the-integration-to-a-databricks-cluster).

La integración de Databricks también se puede implementar de forma remota en un entorno de host compatible. Para implementar la integración de esta manera, sigue los pasos para [implementar la integración de forma remota](https://github.com/newrelic/newrelic-databricks-integration/docs/installation.md#deploy-the-integration-remotely).

## Verifique la instalación [#verify-installation]

Una vez que la integración de Databricks se haya ejecutado durante unos minutos, use el [generador de consultas](https://one.newrelic.com/data-exploration/query-builder) en New Relic para ejecutar la siguiente consulta, reemplazando `[YOUR_CLUSTER_NAME]` con el *nombre* del clúster de Databricks *donde se instaló la integración* (tenga en cuenta que si el nombre de su clúster incluye un `'`, debe escaparlo con un `\`):

`SELECT uniqueCount(executorId) AS Executors FROM SparkExecutorSample WHERE databricksClusterName = '[YOUR_CLUSTER_NAME]'`

El resultado de la consulta debe ser **un número mayor que cero**.

## Importe los paneles de ejemplo (opcional) [#add-dashboard]

Para ayudarlo a comenzar a usar la telemetría recopilada, instale nuestros paneles preconstruidos usando la [instalación guiada](https://one.newrelic.com/marketplace?state=34e67b15-4fe1-28ef-ff41-99658fb36820).

Alternativamente, puede instalar los paneles preconstruidos siguiendo las instrucciones que se encuentran en [Importar los paneles de ejemplo](https://github.com/newrelic/newrelic-databricks-integration/docs/example-dashboards.md).

## Aprende más

Para obtener más información sobre la integración de Databricks, visite el [repositorio](https://github.com/newrelic/newrelic-databricks-integration) oficial de la integración de New Relic Databricks.