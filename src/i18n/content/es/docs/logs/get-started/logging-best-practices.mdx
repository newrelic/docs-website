---
title: Guía de mejores prácticas de registro
tags:
  - New Relic solutions
  - Best practices guides
  - Logs
  - Logging
metaDescription: Best practices for using New Relic logs
freshnessValidatedDate: never
translationType: machine
---

Bienvenido a la guía de mejores prácticas de registro de New Relic. Aquí encontrará recomendaciones detalladas sobre cómo optimizar nuestra característica de registro y administrar el consumo de datos.

<Callout variant="tip">
  ¿Tienes muchos logs? Consulte nuestro [tutorial sobre cómo optimizarlos y administrarlos](/docs/tutorial-large-logs/get-started-managing-large-logs/).
</Callout>

## Registro de reenvío [#forwarding-logs]

A continuación se ofrecen algunos consejos sobre el reenvío de registros para complementar nuestros [documentos de reenvío de registros](/docs/logs/forward-logs/enable-log-management-new-relic):

* Al reenviar el registro, recomendamos utilizar nuestro [agente New Relic Infrastructure ](/docs/logs/forward-logs/forward-your-logs-using-infrastructure-agent)y/o [agente APM](/docs/apm/new-relic-apm/getting-started/get-started-logs-context#agents). Si no puede utilizar el agente New Relic , utilice otro agente compatible (como FluentBit, Fluentd y Logstash).

  Aquí hay algunos ejemplos de configuración de Github para el agente de registro admitido:

  * [Ejemplos de FluentBit](https://github.com/newrelic/fluentbit-examples)

  * [Ejemplos Fluentd](https://github.com/newrelic/fluentd-examples/)

  * [Ejemplos de logstash](https://github.com/newrelic/logstash-examples)

    <Callout variant="important">
      Si su registro está almacenado en un directorio en un host/contenedor subyacente y nuestro agente de infraestructura lo instrumenta para recopilar el registro, es posible que vea registros duplicados recopilados tanto por el agente de infraestructura como por el agente <InlinePopover type="apm" />. Para evitar enviar registros duplicados, consulte nuestras recomendaciones en [esta guía](/docs/logs/logs-context/upgrade-to-automatic-logs-context).
    </Callout>

* Añade un atributo `logtype` a todos los datos que reenvíes. El atributo es <DNT>**required**</DNT> para usar nuestras reglas de análisis integradas y también se puede usar para crear reglas de análisis personalizadas basadas en el tipo de datos. El atributo `logtype` se considera un atributo bien conocido y se utiliza en nuestro dashboard de inicio rápido para obtener información resumida log .

* Utilice nuestras [reglas de análisis integradas](/docs/logs/ui-data/built-log-parsing-rules) para tipos de registros conocidos. Analizaremos automáticamente los registros de muchos tipos log conocidos diferentes cuando configure el atributo `logtype` relevante.

  A continuación se muestra un ejemplo de cómo agregar el atributo `logtype` a un log reenviado por nuestro agente de infraestructura:

  ```yml
  logs:
    - name: mylog
      file: /var/log/mylog.log
      attributes:
        logtype: mylog
  ```

* Utilice la integración New Relic para reenviar registros para otros tipos de datos comunes, como:

  * Entornos de contenedor: [Kubernetes (K8S)](/docs/kubernetes-pixie/kubernetes-integration/get-started/introduction-kubernetes-integration)
  * Integración de proveedores de nube: [AWS](/docs/infrastructure/amazon-integrations/get-started/introduction-aws-integrations/), [Azure](/docs/infrastructure/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations) o [GCP](/docs/infrastructure/google-cloud-platform-integrations/get-started/introduction-google-cloud-platform-integrations)
  * Cualquiera de nuestras otras [integraciones admitidas en el host con registro](/docs/infrastructure/host-integrations/get-started/introduction-host-integrations)

## Particiones de datos [#partitions]

Si consume o planea consumir una cantidad significativa de datos log cada día, definitivamente debería trabajar en un plan de gobernanza de ingesta para el registro, incluido un plan para particionar los datos de una manera que proporcione agrupaciones funcionales y temáticas. Puede obtener mejoras significativas en el rendimiento con el uso adecuado de particiones de datos. Si envía todos sus registros a un &quot;cubo&quot; gigante (la partición log predeterminada) en una sola cuenta, podría experimentar una consulta lenta o una consulta fallida, ya que tendrá que escanear todos los registros de su cuenta para obtener resultados para cada consulta. Para obtener más detalles, consulte [los límites de frecuencia de consultaNRQL ](/docs/query-your-data/nrql-new-relic-query-language/get-started/rate-limits-nrql-queries/#query-limits).

Una forma de mejorar el rendimiento de la consulta es limitar el rango de tiempo en el que se realiza la búsqueda. La búsqueda de registros durante largos periodos de tiempo arrojará más resultados y requerirá más tiempo. Evite las búsquedas en ventanas de tiempo largas cuando sea posible y emplee el selector de rango de tiempo para limitar las búsquedas a ventanas de tiempo más pequeñas y específicas.

Otra forma de mejorar el rendimiento de la búsqueda es mediante el uso [de particiones de datos](/docs/logs/ui-data/data-partitions/). Estas son algunas de las mejores prácticas para particiones de datos:

* Asegúrese de utilizar particiones al principio de su proceso de incorporación de registros. Cree una estrategia para usar particiones para que su usuario sepa dónde buscar y encontrar un registro específico. De esa manera, no es necesario modificar sus alertas, dashboard de control y vistas guardadas si implementa particiones más adelante en su recorrido de registro.

* Cree particiones de datos que se alineen con las categorías de su entorno u organización que son estáticas o cambian con poca frecuencia (por ejemplo, por unidad de negocios, equipo, entorno, servicio, etc.).

* Crea particiones para optimizar la cantidad de eventos que deben escanear para tu consulta más común. Si tiene un alto volumen de ingesta, tendrá más eventos en ventanas de tiempo más cortas, lo que provoca que las búsquedas durante periodos de tiempo más largos tomen más tiempo y potencialmente se agoten. No hay una regla estricta, pero generalmente log de eventos &quot;tal como se escanea&quot; supera los 500 millones (especialmente los 1000 millones). Para una consulta común, es posible que desees considerar ajustar tu partición.

* Incluso si su volumen de ingesta es bajo, también puede usar particiones de datos para una separación lógica de los datos o simplemente para mejorar el rendimiento de la consulta entre tipos de datos separados.

* Para [buscar particiones de datos](/docs/logs/ui-data/data-partitions#search) en la UI <DNT>**Logs**</DNT>, debe seleccionar las particiones apropiadas, abrir el selector de particiones y marcar las particiones que desea buscar. Si está utilizando NRQL, utilice la cláusula `FROM` para especificar `Log` o `Log_<partion>` para buscar. Por ejemplo:

  ```sql
  FROM Log_<my_partition_name> SELECT * SINCE 1 hour ago
  ```

  O para buscar registros en múltiples particiones:

  ```sql
  FROM Log, Log_<my_partition_name> SELECT * SINCE 1 hour ago
  ```

## Protección del acceso a los datos de log [#securing-log-data]

Cuando se trabaja con datos de log que pueden contener información confidencial, es importante controlar quién puede acceder a logs específicos. Aquí tienes las mejores prácticas para proteger tus datos de log:

* **Emplee el control de acceso a datos**: Implemente políticas [de control de acceso a datos](/docs/accounts/accounts-billing/new-relic-one-user-management/data-access-control) para restringir qué usuarios pueden ver [particiones de log](/docs/logs/ui-data/data-partitions/) específicas. Esto es especialmente importante cuando los logs contienen información de identificación personal (PII), datos de seguridad o información confidencial para el negocio.

* **Tenga en cuenta los riesgos de exposición de datos**: ciertas funcionalidades pueden eludir las restricciones de acceso a los datos. Revise [los posibles riesgos de exposición de datos](/docs/accounts/accounts-billing/new-relic-one-user-management/data-access-control#data-exposure-risks) y siga estas mejores prácticas:

  * Limitar quién tiene <DNT>**Modify**</DNT> acceso a la capacidad <DNT>**Data partitions rules**</DNT>, ya que esto permite al usuario reconfigurar particiones y potencialmente acceder a datos restringidos.
  * Restringir el acceso <DNT>**Modify**</DNT> a la capacidad <DNT>**Query keys**</DNT> solo al administrador de confianza, ya que consulta puede eludir las restricciones de acceso a datos a nivel de usuario.
  * Rotar o eliminar las claves de API de Insights legacy que no respetan las políticas de acceso a datos.
  * Cree roles personalizados sin capacidades de administración de alertas para usuarios que no deben acceder a datos confidenciales a través de condición de alerta o notificación.

* **Alinea las particiones con el control de acceso**: Al planear tu estrategia de particiones, considera qué equipos deberían tener acceso a qué logs. Agrupe los logs por equipo, servicio o nivel de sensibilidad para facilitar la gestión del control de acceso.

## Registro de análisis [#parsing-logs]

Analizar su registro durante la ingesta es la mejor manera de hacer que usted y otros usuarios de su organización puedan utilizar mejor sus datos log . Cuando analiza los atributos, puede usarlos fácilmente para buscar en la UI <DNT>**Logs**</DNT> y en NRQL sin tener que analizar los datos en el momento de la consulta. Esto también le permite usarlos fácilmente en <InlinePopover type="alerts" />y en el dashboard.

Para analizar el registro le recomendamos que:

* Analice su registro en la ingesta para crear `attributes` (o campos), que puede usar al buscar o crear <InlinePopover type="dashboards" />y alertas. El atributo puede ser cadenas de datos o valores numéricos.

* Utilice el atributo `logtype` que agregó a su registro durante la ingesta, junto con otras cláusulas `WHERE` NRQL para que coincidan con los datos que desea analizar. Escriba reglas de coincidencia específicas para filtrar el registro con la mayor precisión posible. Por ejemplo:

  ```sql
  WHERE logtype='mylog' AND message LIKE '%error%'
  ```

* Utilice nuestras [reglas de análisis integradas](/docs/logs/ui-data/built-log-parsing-rules/) y el atributo `logtype` asociado siempre que sea posible. Si las reglas integradas no funcionan para sus datos, use un nombre de atributo `logtype` diferente (es decir, `apache_logs` frente a `apache`, `iis_w3c_custom` frente a `iis_w3c`) y luego cree una nueva regla de análisis en la UI utilizando una versión modificada de las reglas integradas para que funcione para su formato de datos log .

* Utilice nuestra UI <DNT>**Parsing**</DNT> para probar y validar sus reglas de Grok. Usando la opción `Paste log` , puede pegar uno de sus mensajes de registro para probar su expresión de Grok antes de crear y guardar una regla de análisis permanente.

  <img title="parsing example" alt="Parsing example in the UI" src="/images/logs_screenshot-full_parsing-example.webp" />

* Utilice la configuración externa de FluentBit para analizar el registro de varias líneas y para otro análisis previo más extenso antes de incorporarlo a New Relic. Para obtener detalles y configuración del análisis multilínea con nuestro agente de infraestructura, consulte [esta publicación de blog](https://newrelic.com/blog/how-to-relic/parse-multiline-log-messages-fluent-bit-plugin).

* Cree patrones de Grok optimizados para que coincidan con el registro filtrado para extraer el atributo. Evite el uso excesivo de patrones costosos de Grok como GREEDYDATA. Si necesita ayuda para identificar reglas de análisis subóptimas, hable con su representante de cuenta de New Relic.

### GROK mejores prácticas

* Utilice tipos de Grok para especificar el tipo de valor de atributo que se extraerá. Si se omite, los valores se extraen como cadenas. Esto es importante especialmente para valores numéricos si desea poder utilizar funciones NRQL (es decir, `monthOf()`, `max()`, `avg()`, `>`, `<`, etc.) en estos atributos.
* Utilice la UI <DNT>**Parsing**</DNT> para probar sus patrones de Grok. Puede pegar un registro de muestra en la UI <DNT>**Parsing**</DNT> para validar sus patrones Grok o Regex y si están extrayendo el atributo como esperaba.
* Agregue anclajes a la lógica de análisis (es decir, `^`) para indicar el comienzo de una línea o `$` al final de una línea.
* Utilice `()?` alrededor de un patrón para identificar campos opcionales
* Evite el uso excesivo de patrones Grok costosos como `'%{GREEDYDATA}`. Intente utilizar siempre un patrón Grok válido y un tipo Grok al extraer atributos.

## Reglas de filtro de caída [#drop-rules]

### Soltar registro durante la ingesta

* Cree [reglas de filtrado de caída](/docs/logs/ui-data/drop-data-drop-filter-rules#create) para descartar registros que no sean útiles o que no sean necesarios para satisfacer ningún caso de uso para el dashboard, alertas o resolución de problemas.

### Elimina el atributo de tu registro al ingerir

* Cree reglas de eliminación para eliminar atributos no utilizados de su registro.
* Elimine el atributo `message` después del análisis. Si analiza el atributo del mensaje para crear un nuevo atributo a partir de los datos, suelte el campo del mensaje.
* Ejemplo: si está reenviando datos desde la infraestructura de AWS, puede crear reglas de eliminación para eliminar cualquier atributo de AWS que pueda crear un exceso de datos no deseados.

## New Relic Logs [#sizing]

* La forma en que facturamos el almacenamiento puede diferir de la de algunos de nuestros competidores. La forma en que contabilizamos los datos log es similar a cómo contabilizamos y facturamos otros tipos de datos, que se define en [Cálculo de uso](/docs/accounts/accounts-billing/new-relic-one-pricing-billing/data-ingest-billing#usage-calculation).

* Si nuestra integración en la nube (AWS, Azure, GCP) está instalada, agregaremos metadatos de la nube a cada log , lo que se sumará a la factura de ingesta general. Sin embargo, estos datos se pueden eliminar para reducir la ingesta.

* Los principales impulsores de la sobrecarga de datos log se encuentran a continuación, en orden de impacto:

  * Integracion en la nube
  * Formato JSON
  * patrones de registros (Puedes [habilitar/deshabilitar patrones en la UI <DNT>**Logs**</DNT> ](/docs/logs/ui-data/find-unusual-logs-log-patterns#availability).)

## Registro de búsqueda [#searching-logs]

* Para búsquedas log comunes, cree y utilice <DNT>**Saved views**</DNT> en la UI. Cree una búsqueda para sus datos y haga clic en <DNT>**+ Add Column**</DNT> para agregar un atributo adicional a la tabla de la UI . Luego puede mover las columnas para que aparezcan en el orden que desee y luego guardarlas como una vista guardada con permisos públicos o privados. Configure las vistas guardadas para que sean públicas, de modo que usted y otros usuarios puedan ejecutar fácilmente búsquedas comunes mostrando todos los datos de atributos relevantes. Esta es una buena práctica para aplicaciones de terceros como Apache, nginx, etc., para que el usuario pueda ver fácilmente esos registros sin buscar.

* Emplee el [generador de consultas](/docs/query-your-data/explore-query-data/query-builder/introduction-query-builder) para ejecutar búsquedas empleando NRQL, empleando sus funciones avanzadas. Para consultar logs de varias cuentas e identificarlos con sus ID de cuenta correspondientes, incluya `accountId() as accountId` en la declaración `SELECT` de su consulta NRQL .

* Cree <InlinePopover type="dashboards" />o utilice [el dashboard de inicio rápido](https://newrelic.com/instant-observability) disponible para responder preguntas comunes sobre su registro y ver sus datos log a lo largo del tiempo en gráficos de series temporales. Cree un dashboard con múltiples dashboards para dividir y dividir sus datos log de muchas maneras diferentes.

* Utilice nuestras funciones NRQL avanzadas como [capture()](/docs/query-your-data/nrql-new-relic-query-language/get-started/nrql-syntax-clauses-functions#func-capture) o [aparse()](/docs/query-your-data/nrql-new-relic-query-language/get-started/nrql-syntax-clauses-functions#func-aparse) para analizar datos en el momento de la búsqueda.

* Instale el dashboard <DNT>**Logs analysis**</DNT> y/o <DNT>**APM logs monitoring quickstart**</DNT> para obtener rápidamente más información valiosa en sus datos log . Para agregar estos dashboards, vaya a <DNT>**[one.newrelic.com](https://one.newrelic.com) &gt; Integrations &amp; Agents &gt; Logging &gt; Dashboards**</DNT>.

## ¿Que sigue?

Consulte [Comenzar con <InlinePopover type="logs" />](/docs/logs/get-started/get-started-log-management/).