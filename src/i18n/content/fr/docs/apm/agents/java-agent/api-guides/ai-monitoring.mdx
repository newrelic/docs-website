---
title: monitoring de l'APIIA
tags:
  - Agents
  - Java agent
  - API guides
metaDescription: For information about customizing New Relic's Java agent for AI monitoring.
freshnessValidatedDate: never
translationType: machine
---

Lorsque vous avez instrumenté votre application avec monitoring de l&apos;IA, vous pouvez accéder à certaines API pour collecter le nombre de jetons et les commentaires des utilisateurs. Pour utiliser monitoring l&apos;de l&apos;IA,API vérifiez que votre agent Java est mis à jour vers la version 8.12.0 ou supérieure.

Ce document fournit des procédures de mise à jour de votre code pour accéder à l&apos;[ API de comptage de jetons et de commentaires des utilisateurs](https://newrelic.github.io/java-agent-api/javadoc/com/newrelic/api/agent/AiMonitoring.html#recordLlmFeedbackEvent).

## Nombre record de jetons [#token-count]

Si vous avez désactivé l&apos;agent avec `ai_monitoring.record_content.enabled=false`, vous pouvez utiliser l&apos;API `setLlmTokenCountCallback(LlmTokenCountCallback llmTokenCountCallback)` pour calculer l&apos;attribut de nombre de jetons. Cela calcule le nombre de jetons pour les événements liés aux processus d&apos;incorporation (intégration) et d&apos;achèvement de LLM sans enregistrer le contenu des messages eux-mêmes. Si vous souhaitez collecter des jetons, suivez ces étapes :

1. Implémentez le `LlmTokenCountCallback` afin qu&apos;il remplace la méthode `calculateLlmTokenCount(String model, String content)` . Cela calcule un nombre de jetons en fonction d&apos;un nom de modèle LLM donné et du contenu ou prompt du message LLM :

   ```java
       class MyTokenCountCallback implements LlmTokenCountCallback {
           @Override
           public int calculateLlmTokenCount(String model, String content) {
               // Implement custom token calculating logic here based on the LLM model and content.
               // Return an integer representing the calculated token count.
               return 0;
           }
       }
   ```

2. Créez une instance de l&apos;implémentation `LlmTokenCountCallback` pour enregistrer le rappel, puis transmettez-la à l&apos;API `setLlmTokenCountCallback` . Par exemple:

   ```java
       LlmTokenCountCallback myTokenCountCallback = new MyTokenCountCallback();
           // The callback needs to be registered at some point before invoking the LLM model
       NewRelic.getAgent().getAiMonitoring.setLlmTokenCountCallback(myTokenCountCallback);
   ```

Pour utiliser le rappel, implémentez `LlmTokenCountCallback` afin qu&apos;il renvoie un entier qui représente le nombre de jetons pour une prompt, un message d&apos;achèvement ou une incorporation (incorporation) particulier. Si les valeurs sont inférieures ou égales à `0`, `LlmTokenCountCallbacks` ne sera pas attaché à un événement. Gardez à l’esprit que vous ne devez appeler cette API qu’une seule fois. L&apos;appel de cette API plusieurs fois remplacera chaque rappel précédent.

## Enregistrer les commentaires des utilisateurs [#user-feedback]

monitoring de l&apos;IA permet de corréler les identifiants trace entre un message généré à partir de vos modèles LLM et le retour final d&apos;un utilisateur. L&apos;API `recordLlmFeedbackEvent` crée un argument avec une carte de la classe `LlmFeedbackEventAttributes.Builder` . Si vous souhaitez enregistrer les commentaires des utilisateurs, suivez ces étapes :

1. Utilisez l&apos;API [`TraceMetadata.getTraceId()`](https://newrelic.github.io/java-agent-api/javadoc/com/newrelic/api/agent/TraceMetadata.html#getTraceId) pour acquérir l&apos;ID de trace des transactions au fur et à mesure de leur exécution :

   ```java
   String traceId = NewRelic.getAgent().getTraceMetadata().getTraceId();
   ```

2. Ajoutez le [`recordLlmFeedbackEvent(Map<String, Object> llmFeedbackEventAttributes)`](https://newrelic.github.io/java-agent-api/javadoc/com/newrelic/api/agent/AiMonitoring.html#recordLlmFeedbackEvent) pour corréler l’ID de trace avec un événement de rétroaction. Voici un exemple de la manière dont vous pourriez enregistrer un événement de rétroaction LLM :

   ```java
   String traceId = ... // acquired directly from New Relic API or retrieved from some propagation mechanism

   Map<String, String> metadata = new HashMap<>();
   metadata.put("interestingKey", "interestingVal");

   LlmFeedbackEventAttributes.Builder llmFeedbackEvenAttrBuilder = new LlmFeedbackEventAttributes.Builder(traceId, ratingString);

   Map<String, Object> llmFeedbackEventAttributes = llmFeedbackEvenAttrBuilder
           .category("General")
           .message("Ok")
           .metadata(metadata)
           .build();

   NewRelic.getAgent().getAiMonitoring().recordLlmFeedbackEvent(llmFeedbackEventAttributes);
   ```

Si les commentaires de l&apos;utilisateur enregistrent un thread différent ou un service différent de celui où le prompt ou la réponse LLM s&apos;est produite, vous devez acquérir l&apos;ID trace à partir du thread ou du service d&apos;origine. Une fois que vous avez acquis l&apos;ID de trace, propagez-le à l&apos;endroit où l&apos;événement de commentaires de l&apos;utilisateur sera enregistré.

Pour afficher le paramètre pris par la classe `LlmFeedbackEventAttributes.Builder`, [consultez les détails de la méthode dans notre documentation monitoring de l&apos;API IA](https://newrelic.github.io/java-agent-api/javadoc/com/newrelic/api/agent/AiMonitoring.html#recordLlmFeedbackEvent).

## Ajouter un attribut LLM personnalisé [#custom-attributes]

Vous pouvez ajuster votre agent pour collecter des attributs LLM personnalisés :

* Tout attribut personnalisé ajouté avec l&apos;[`NewRelic.addCustomParameter(...)`](https://newrelic.github.io/java-agent-api/javadoc/com/newrelic/api/agent/NewRelic.html#addCustomParameter\(java.lang.String,boolean\)) API peut être préfixé par `llm.`. Cela copie automatiquement ces attributs dans `LlmEvent`.
* Si vous ajoutez un attribut personnalisé à `LlmEvent`avec l&apos;API `addCustomParameters`, assurez-vous que l&apos;appel de l&apos;API se produit avant d&apos;appeler le SDK Bedrock.
* Un attribut personnalisé facultatif avec une signification particulière est `llm.conversation_id`. Vous pouvez l&apos;utiliser pour regrouper les messages LLM dans des conversations spécifiques dans APM.