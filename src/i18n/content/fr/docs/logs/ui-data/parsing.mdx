---
title: analyser les données log
tags:
  - Logs
  - Log management
  - UI and data
metaDescription: How New Relic uses parsing and how to send customized log data.
freshnessValidatedDate: never
translationType: machine
---

log <DNT>parsing</DNT> est le processus de traduction des données log non structurées en attributs (paires valeur-clé) en fonction des règles que vous définissez. Vous pouvez utiliser ces attributs dans votre requête NRQL pour facetter ou filtrer le log de manière utile.

New Relic analyse automatiquement les données log en fonction de certaines règles d&apos;analyse. Dans ce document, vous apprendrez comment fonctionne l&apos;analyse des logs et comment créer vos propres règles d&apos;analyse personnalisées.

Vous pouvez également créer, interroger et gérer vos règles d&apos;analyse des logen utilisant NerdGraph, notre API GraphQL . Un outil utile pour cela est notre [explorateur d&apos;API Nerdgraph](https://api.newrelic.com/graphiql). Pour plus d&amp;apos;informations, consultez notre [tutoriel NerdGraph pour l&apos;analyse](/docs/apis/nerdgraph/examples/nerdgraph-log-parsing-rules-tutorial/).

Voici une vidéo de 5 minutes sur l&apos;analyse des log :

<Video id="xPWM46yw3bQ" type="youtube" />

## Exemple d&apos;analyse [#parsing-defined]

Un bon exemple est un log d’accès NGINX par défaut contenant du texte non structuré. C&apos;est utile pour la recherche mais pas pour grand chose d&apos;autre. Voici un exemple d&apos;une ligne typique :

```
127.180.71.3 - - [10/May/1997:08:05:32 +0000] "GET /downloads/product_1 HTTP/1.1" 304 0 "-" "Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)"
```

Dans un format non analysé, vous devrez effectuer une recherche en texte intégral pour répondre à la plupart des questions. Après analyse, le log est organisé en attributs, comme `response code` et `request URL`:

```json
{
  "remote_addr":"93.180.71.3",
  "time":"1586514731",
  "method":"GET",
  "path":"/downloads/product_1",
  "version":"HTTP/1.1",
  "response":"304",
  "bytesSent": 0,
  "user_agent": "Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)"
}
```

analyse facilite la création [de requêtes personnalisées](/docs/using-new-relic/data/understand-data/query-new-relic-data) qui s&amp;apos;appuient sur ces valeurs. Cela vous aide à comprendre la distribution des codes de réponse par URL de demande et à trouver rapidement les pages problématiques.

## Comment fonctionne l&apos;analyse des log [#how-it-works]

Voici un aperçu de la manière dont New Relic implémente l&apos;analyse des logs :

<table>
  <thead>
    <tr>
      <th style={{ width: "100px" }}>
        analyser des logs
      </th>

      <th>
        Comment ça marche
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        Quoi
      </td>

      <td>
        * L&apos;analyse est appliquée à un champ sélectionné spécifique. Par défaut, le champ `message` est utilisé. Cependant, n&amp;apos;importe quel champ/attribut peut être choisi, même celui qui n&amp;apos;existe pas actuellement dans vos données.
        * Chaque règle d&apos;analyse est créée à l&apos;aide d&apos;une clause NRQL `WHERE` qui détermine quel log la règle tentera d&amp;apos;analyser.
        * Pour simplifier le processus de correspondance, nous vous recommandons d&apos;ajouter un attribut [`logtype`](#logtype) à votre log. Cependant, vous n&amp;apos;êtes pas limité à l&amp;apos;utilisation de `logtype`; un ou plusieurs attributs peuvent être utilisés comme critères de correspondance dans la clause NRQL `WHERE`.
      </td>
    </tr>

    <tr>
      <td>
        Quand
      </td>

      <td>
        * analyser ne sera appliqué qu&apos;une seule fois à chaque message de log. Si plusieurs règles d&apos;analyse correspondent au log, seule la première qui réussit sera appliquée.
        * Les règles d’analyse ne sont pas ordonnées. Si plusieurs règles d&apos;analyse correspondent à un log, une est choisie au hasard. Assurez-vous de créer vos règles d’analyse de manière à ce qu’elles ne correspondent pas au même log.
        * L&apos;analyse a lieu lors de l&apos;ingestion log , avant que les données ne soient écrites dans NRDB. Une fois les données écrites sur le stockage, elles ne peuvent plus être analysées.
        * L&apos;analyse se produit dans le pipeline <DNT>**before**</DNT> des enrichissements de données ont lieu. Soyez prudent lorsque vous définissez les critères de correspondance pour une règle d’analyse. Si les critères sont basés sur un attribut qui n&amp;apos;existe pas avant l&amp;apos;analyse ou l&amp;apos;enrichissement, ces données ne seront pas présentes dans le log lors de la correspondance. Par conséquent, aucune analyse n’aura lieu.
      </td>
    </tr>

    <tr>
      <td>
        Comment
      </td>

      <td>
        * Les règles peuvent être écrites en [Grok](#grok), regex ou un mélange des deux. Grok est une collection de modèles qui font abstraction des expressions régulières complexes.
        * Nous prenons en charge la syntaxe Java Regex dans notre interface utilisateur d&apos;analyse. Pour les noms d&apos;attributs ou de champs dans les groupes de capture, Java Regex autorise uniquement \[A-Za-z0-9].
      </td>
    </tr>
  </tbody>
</table>

## Analyser l&apos;attribut à l&apos;aide de Grok [#grok]

Les modèles d&apos;analyse sont spécifiés à l&apos;aide de Grok, une norme industrielle pour l&apos;analyse des messages de log. Tout log entrant avec un champ `logtype` sera vérifié par rapport à nos [règles d&apos;analyse intégrées](#built-in-rules) et, si possible, le modèle Grok associé est appliqué au log.

Grok est un sur-ensemble d&apos;expressions régulières qui ajoute des modèles nommés intégrés à utiliser à la place des expressions régulières complexes littérales. Par instance, au lieu de devoir se rappeler qu&apos;un entier peut être associé aux expressions régulières `(?:[+-]?(?:[0-9]+))`, vous pouvez simplement écrire `%{INT}` pour utiliser le modèle Grok `INT`, qui représente les mêmes expressions régulières.

Les modèles Grok ont la syntaxe :

```
%{PATTERN_NAME[:OPTIONAL_EXTRACTED_ATTRIBUTE_NAME[:OPTIONAL_TYPE[:OPTIONAL_PARAMETER]]]}
```

Où:

* `PATTERN_NAME` est l&amp;apos;un des modèles Grok pris en charge. Le nom du motif est juste un nom convivial représentant des expressions régulières. Elles sont exactement égales aux expressions régulières correspondantes.
* `OPTIONAL_EXTRACTED_ATTRIBUTE_NAME`, s&amp;apos;il est fourni, est le nom de l&amp;apos;attribut qui sera ajouté à votre message de log avec la valeur correspondant au nom du modèle. Cela équivaut à utiliser un groupe de capture nommé à l’aide d’expressions régulières. Si cela n&amp;apos;est pas fourni, la règle d&amp;apos;analyse correspondra simplement à une région de votre chaîne, mais n&amp;apos;extrairea pas un attribut avec sa valeur.
* `OPTIONAL_TYPE` spécifie le type de valeur d&amp;apos;attribut à extraire. Si elles sont omises, les valeurs sont extraites sous forme de chaînes. Par instance, pour extraire la valeur `123` de `"File Size: 123"` sous forme de nombre dans l&amp;apos;attribut `file_size`, utilisez `value: %{INT:file_size:int}`.
* `OPTIONAL_PARAMETER` spécifie un paramètre facultatif pour certains types. Actuellement, seul le type `datetime` prend un paramètre, voir ci-dessous pour plus de détails.

Vous pouvez également utiliser un mélange d&apos;expressions régulières et de noms de modèles Grok dans votre chaîne correspondante.

Cliquez sur ce lien pour une liste des [modèles Grok](https://github.com/thekrakken/java-grok/tree/master/src/main/resources/patterns) pris en charge et ici pour une liste des [types Grok](#grok-types) pris en charge.

Notez que les noms de variables doivent être explicitement définis et être en minuscules comme `%{URI:uri}`. Des expressions telles que `%{URI}` ou `%{URI:URI}` ne fonctionneraient pas.

<CollapserGroup>
  <Collapser id="grok-example" title="Exemple de Grok : extraire des données utiles de votre log">
    Un enregistrement log pourrait ressembler à ceci :

    ```json
    {
      "message": "54.3.120.2 2048 0"
    }
    ```

    Cette information est exacte, mais sa signification n’est pas vraiment intuitive. Les modèles Grok vous aident à extraire et comprendre les données télémétriques que vous souhaitez. Par exemple, un enregistrement log comme celui-ci est beaucoup plus facile à utiliser :

    ```json
    {
      "host_ip": "43.3.120.2",
      "bytes_received": 2048,
      "bytes_sent": 0
    }
    ```

    Pour ce faire, créez un modèle Grok qui extrait ces trois champs ; par exemple :

    ```
    %{IP:host_ip} %{INT:bytes_received} %{INT:bytes_sent}
    ```

    Après le traitement, votre enregistrement log comprendra les champs `host_ip`, `bytes_received` et `bytes_sent`. Vous pouvez désormais utiliser ces champs dans New Relic pour filtrer, facetter et effectuer des opérations statistiques sur vos données log . Pour plus de détails sur la façon d&amp;apos;analyser les logs avec les modèles Grok dans New Relic, consultez [notre article de blog](https://newrelic.com/blog/how-to-relic/how-to-use-grok-log-parsing).
  </Collapser>

  <Collapser id="grok-ui" title="Exemple d'interface utilisateur : création d'une règle d'analyse Grok">
    Si vous disposez des autorisations appropriées, vous pouvez créer des règles d&apos;analyse dans notre interface utilisateur pour créer, tester et activer l&apos;analyse Grok. Par exemple, pour obtenir un type spécifique de message d’erreur pour l’un de vos microservices appelé Inventory Services, vous devez créer une règle d’analyse Grok qui recherche un message d’erreur et un produit spécifiques. Pour ce faire :

    1. Donnez un nom à la règle ; par exemple, `Inventory Services error parsing`.

    2. Sélectionnez un champ existant à analyser (par défaut = `message`) ou entrez un nouveau nom de champ.

    3. Identifiez la clause NRQL `WHERE` qui agit comme un pré-filtre pour le log entrant ; par exemple, `entity.name='Inventory Service'`. Ce pré-filtre réduit le nombre de logs qui doivent être traités par votre règle, supprimant ainsi les traitements inutiles.

    4. Sélectionnez un log correspondant s&apos;il existe, ou cliquez sur l&apos;onglet Coller log pour coller un exemple log.

    5. Ajoutez la règle d’analyse Grok ; par exemple :

       ```
       Inventory error: %{DATA:error_message} for product %{INT:product_id}
       ```

       Où:

    * `Inventory error`: Le nom de votre règle d&amp;apos;analyse
    * `error_message`:Le message d&amp;apos;erreur que vous souhaitez sélectionner
    * `product_id`: L&amp;apos;ID du produit pour le service d&amp;apos;inventaire

    6. Activez et enregistrez la règle d’analyse.

       Bientôt, vous verrez que votre log de service d&apos;inventaire est enrichi de deux nouveaux champs : `error_message` et `product_id`. À partir de là, vous pouvez effectuer des requêtes sur ces champs, créer des graphiques et des tableaux de bord, définir des alertes, etc.

       Pour plus de détails, consultez [notre documentation sur l&apos;ajout de règles d&apos;analyse personnalisées dans l&apos;interface utilisateur](#custom-parsing).
  </Collapser>

  <Collapser id="grok-types" title="Types de Grok pris en charge">
    Le champ `OPTIONAL_TYPE` spécifie le type de valeur d&amp;apos;attribut à extraire. Si elles sont omises, les valeurs sont extraites sous forme de chaînes.

    Les types pris en charge sont :

    <table>
      <thead>
        <tr>
          <th>
            Type spécifié dans Grok
          </th>

          <th>
            Type stocké dans la base de données New Relic
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `boolean`
          </td>

          <td>
            `boolean`
          </td>
        </tr>

        <tr>
          <td>
            `byte` `short` `int` `integer`
          </td>

          <td>
            `integer`
          </td>
        </tr>

        <tr>
          <td>
            `long`
          </td>

          <td>
            `long`
          </td>
        </tr>

        <tr>
          <td>
            `float`
          </td>

          <td>
            `float`
          </td>
        </tr>

        <tr>
          <td>
            `double`
          </td>

          <td>
            `double`
          </td>
        </tr>

        <tr>
          <td>
            `string` (défaut) `text`
          </td>

          <td>
            `string`
          </td>
        </tr>

        <tr>
          <td>
            `date` `datetime`
          </td>

          <td>
            Le temps comme un `long`

            Par défaut, il est interprété comme ISO 8601. Si `OPTIONAL_PARAMETER` est présent, il spécifie la [chaîne de modèle de date et d&apos;heure](https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/text/SimpleDateFormat.html)à utiliser pour interpréter `datetime`.

            Notez que ceci n&apos;est disponible que pendant l&apos;analyse. Nous avons une [étape d’interprétation d’horodatage supplémentaire et distincte](/docs/logs/ui-data/timestamp-support) qui se produit pour tous les logs plus tard dans le pipeline d’ingestion.
          </td>
        </tr>

        <tr>
          <td>
            `json`
          </td>

          <td>
            Données structurées JSON. Voir [analyser JSON mélangé à du texte brut](#parsing-json) pour plus d&amp;apos;informations.
          </td>
        </tr>

        <tr>
          <td>
            `csv`
          </td>

          <td>
            Données CSV. Voir [Analyser CSV](#parsing-csv) pour plus d&amp;apos;informations.
          </td>
        </tr>

        <tr>
          <td>
            `geo`
          </td>

          <td>
            Localisation géographique à partir des adresses IP. Consultez [Géolocalisation des adresses IP (GeoIP)](#geo) pour plus d&amp;apos;informations.
          </td>
        </tr>

        <tr>
          <td>
            `key value pairs`
          </td>

          <td>
            valeur clé Paire . Voir [analyser les paires de valeurs clés](#parsing-key-value-pairs) pour plus d&amp;apos;informations.
          </td>
        </tr>
      </tbody>
    </table>
  </Collapser>

  <Collapser id="grok-multiline" title="Analyse multiligne de Grok">
    Si vous avez un log multiligne, sachez que le modèle Grok `GREEDYDATA` ne correspond pas aux nouvelles lignes (il est équivalent à `.*`).

    Ainsi, au lieu d&apos;utiliser `%{GREEDYDATA:some_attribute}` directement, vous devrez ajouter l&amp;apos;indicateur multiligne devant lui : `(?s)%{GREEDYDATA:some_attribute}`
  </Collapser>

  <Collapser id="parsing-json" title="analyser du JSON mélangé à du texte brut">
    Le pipeline New Relic Logs analyse vos messages log JSON par défaut, mais vous avez parfois des messages de log JSON mélangés à du texte brut. Dans cette situation, vous souhaiterez peut-être pouvoir les analyser, puis pouvoir filtrer à l’aide de l’attribut JSON. Si tel est le cas, vous pouvez utiliser le [type grok](#grok-syntax) `json` , qui analysera le JSON capturé par le modèle grok. Ce format repose sur 3 parties principales : la syntaxe grok, le préfixe que vous souhaitez attribuer à l&amp;apos;attribut json analysé et le [type grok](#grok-syntax) `json` . En utilisant le [type grok](#grok-syntax) `json` , vous pouvez extraire et analyser du JSON à partir de logs qui ne sont pas correctement formatés ; par exemple, si vos logs sont préfixés par une chaîne de date/heure :

    ```json
    2015-05-13T23:39:43.945958Z {"event": "TestRequest", "status": 200, "response": {"headers": {"X-Custom": "foo"}}, "request": {"headers": {"X-Custom": "bar"}}}
    ```

    Afin d&apos;extraire et d&apos;analyser les données JSON de ce format log , créez l&apos;expression Grok suivante :

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json}
    ```

    Le log résultant est :

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    my_attribute_prefix.event: "TestRequest"
    my_attribute_prefix.status: 200
    my_attribute_prefix.response.headers.X-Custom: "foo"
    my_attribute_prefix.request.headers.X-Custom: "bar"
    ```

    Vous pouvez définir la liste des attributs à extraire ou à supprimer avec les options `keepAttributes` ou `dropAttributes`. Par exemple, avec l&amp;apos;expression Grok suivante :

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json({"keepAttributes": ["my_attribute_prefix.event", "my_attribute_prefix.response.headers.X-Custom"]})}
    ```

    Le log résultant est :

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    my_attribute_prefix.event: "TestRequest"
    my_attribute_prefix.request.headers.X-Custom: "bar"
    ```

    Si vous souhaitez omettre le préfixe `my_attribute_prefix` , vous pouvez inclure le `"noPrefix": true` dans la configuration.

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json({"noPrefix": true})}
    ```

    Si vous souhaitez omettre le préfixe `my_attribute_prefix` et conserver uniquement l&amp;apos;attribut `status` , vous pouvez inclure `"noPrefix": true` et `"keepAttributes: ["status"]` dans la configuration.

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json({"noPrefix": true, "keepAttributes": ["status"]})}
    ```

    Si votre JSON a été échappé, vous pouvez utiliser l&apos;option `isEscaped` pour pouvoir l&amp;apos;analyser. Si votre JSON a été échappé puis cité, vous devez également faire correspondre les guillemets, comme indiqué ci-dessous. Par exemple, avec l&amp;apos;expression Grok suivante :

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} "%{GREEDYDATA:my_attribute_prefix:json({"isEscaped": true})}"
    ```

    Serait capable d&apos;analyser le message échappé :

    ```
    2015-05-13T23:39:43.945958Z "{\"event\": \"TestRequest\", \"status\": 200, \"response\": {\"headers\": {\"X-Custom\": \"foo\"}}, \"request\": {\"headers\": {\"X-Custom\": \"bar\"}}}"
    ```

    Le log résultant est :

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    my_attribute_prefix.event: "TestRequest"
    my_attribute_prefix.status: 200
    my_attribute_prefix.response.headers.X-Custom: "foo"
    my_attribute_prefix.request.headers.X-Custom: "bar"
    ```

    Pour configurer le [type Grok](#grok-syntax) `json` , utilisez `:json(_CONFIG_)`:

    * `json({"dropOriginal": true})`: Supprimez le snippet JSON qui a été utilisé dans l&amp;apos;analyse. Lorsqu&amp;apos;elle est définie sur `true` (valeur par défaut), la règle d&amp;apos;analyse supprimera le snippet JSON d&amp;apos;origine. Notez que l’attribut JSON restera dans le champ de message.
    * `json({"dropOriginal": false})`:Cela affichera la charge utile JSON qui a été extraite. Lorsqu&amp;apos;il est défini sur `false`, la charge utile complète uniquement JSON sera affichée sous un attribut nommé dans `my_attribute_prefix` ci-dessus. Notez que l&amp;apos;attribut JSON restera également dans le champ de message ici, donnant à l&amp;apos;utilisateur 3 vues différentes des données JSON. Si le stockage des trois versions est un problème, il est recommandé d&amp;apos;utiliser la valeur par défaut de `true` ici.
    * `json({"depth": 62})`: Niveaux de profondeur auxquels vous souhaitez analyser la valeur JSON (par défaut 62).
    * `json({"keepAttributes": ["attr1", "attr2", ..., "attrN"]})`:Spécifie quel attribut sera extrait du JSON. La liste fournie ne peut pas être vide. Si cette option configuration n&amp;apos;est pas définie, tous les attributs sont extraits.
    * `json({"dropAttributes": ["attr1", "attr2", ..., "attrN"]})`:Spécifie l&amp;apos;attribut à supprimer du JSON. Si cette option de configuration n&amp;apos;est pas définie, aucun attribut n&amp;apos;est supprimé.
    * `json({"noPrefix": true})`: Définissez cette option sur `true` pour supprimer le préfixe de l&amp;apos;attribut extrait du JSON.
    * `json({"isEscaped": true})`: Définissez cette option sur `true` pour analyser le JSON qui a été échappé (ce que vous voyez généralement lorsque le JSON est transformé en chaîne, par exemple `{\"key\": \"value\"}`)
  </Collapser>

  <Collapser id="parsing-csv" title="analyser CSV">
    Si votre système envoie un log de valeurs séparées par des virgules (CSV) et que vous devez les analyser dans New Relic, vous pouvez utiliser le [type Grok](#grok-syntax) `csv` , qui analyse le CSV capturé par le modèle Grok. Ce format repose sur 3 parties principales : la syntaxe Grok, le préfixe que vous souhaitez attribuer à l&amp;apos;attribut CSV analysé et le [type Grok](#grok-syntax) `csv` . En utilisant le [type Grok](#grok-syntax) `csv` , vous pouvez extraire et analyser le CSV du log.

    Étant donné la ligne log CSV suivante à titre d’exemple :

    ```
    "2015-05-13T23:39:43.945958Z,202,POST,/shopcart/checkout,142,10"
    ```

    Et une règle d&apos;analyse avec la forme suivante :

    ```
    %{GREEDYDATA:log:csv({"columns": ["timestamp", "status", "method", "url", "time", "bytes"]})}
    ```

    Analysera votre log comme suit :

    ```
    log.timestamp: "2015-05-13T23:39:43.945958Z"
    log.status: "202"
    log.method: "POST"
    log.url: "/shopcart/checkout"
    log.time: "142"
    log.bytes: "10"
    ```

    Si vous devez omettre le préfixe « log », vous pouvez inclure le `"noPrefix": true` dans la configuration.

    ```
    %{GREEDYDATA:log:csv({"columns": ["timestamp", "status", "method", "url", "time", "bytes"], "noPrefix": true})}
    ```

    ### configuration des colonnes :

    * Il est obligatoire d&apos;indiquer les colonnes dans la configuration de type CSV Grok (qui doit être un JSON valide).
    * Vous pouvez ignorer n&apos;importe quelle colonne en définissant « \_ » (trait de soulignement) comme nom de colonne pour la supprimer de l&apos;objet résultant.

    ### Options de configuration facultatives :

    Bien que la configuration des « colonnes » soit obligatoire, il est possible de modifier l&apos;analyse du CSV avec les paramètres suivants.

    * <DNT>**dropOriginal**</DNT>: (La valeur par défaut est `true`) Supprimez le snippet CSV utilisé dans l&amp;apos;analyse. Lorsque la valeur est `true` (valeur par défaut), la règle d&amp;apos;analyse supprime le champ d&amp;apos;origine.
    * <DNT>**noPrefix**</DNT>: (La valeur par défaut est `false`) N&amp;apos;inclut pas le nom du champ Grok comme préfixe sur l&amp;apos;objet résultant.
    * <DNT>**separator**</DNT>: (Par défaut `,`) Définit le caractère/la chaîne qui divise chaque colonne.
      * Un autre scénario courant est celui des valeurs séparées par des tabulations (TSV), pour cela vous devez indiquer `\t` comme séparateur, ex. `%{GREEDYDATA:log:csv({"columns": ["timestamp", "status", "method", "url", "time", "bytes"], "separator": "\t"})`
    * <DNT>**quoteChar**</DNT>: (Par défaut `"`) Définit le caractère qui entoure éventuellement le contenu d&amp;apos;une colonne.
  </Collapser>

  <Collapser id="geo" title="Géolocalisation des adresses IP (GeoIP)">
    Si votre système envoie un log contenant des adresses IPv4, New Relic peut les localiser géographiquement et enrichir l&apos;événement de log avec l&apos;attribut spécifié. Vous pouvez utiliser le [type Grok](#grok-syntax) `geo` , qui trouve la position d&amp;apos;une adresse IP capturée par le modèle Grok. Ce format peut être configuré pour renvoyer un ou plusieurs champs liés à l&amp;apos;adresse, tels que la ville, le pays et la latitude/longitude de l&amp;apos;IP.

    Étant donné la ligne log suivante à titre d&apos;exemple :

    ```
    2015-05-13T23:39:43.945958Z 146.190.212.184
    ```

    Et une règle d&apos;analyse avec la forme suivante :

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:ip:geo({"lookup":["city","region","countryCode", "latitude","longitude"]})}
    ```

    Nous allons analyser votre log comme suit :

    ```
    ip: 146.190.212.184
    ip.city: North Bergen
    ip.countryCode: US
    ip.countryName: United States
    ip.latitude: 40.793
    ip.longitude: -74.0247
    ip.postalCode: 07047
    ip.region: NJ
    ip.regionName: New Jersey
    containerTimestamp:2015-05-13T23:39:43.945958Z
    ISO8601_TIMEZONE:Z
    ```

    ### configuration de la recherche :

    Il est obligatoire de spécifier les champs `lookup` souhaités renvoyés par l&amp;apos;action `geo` . Au moins un élément est requis parmi les options suivantes.

    * <DNT>**city**</DNT>: Nom de la ville
    * <DNT>**countryCode**</DNT>: Abréviation du pays
    * <DNT>**countryName**</DNT>: Nom du pays
    * <DNT>**latitude**</DNT>: Latitude
    * <DNT>**longitude**</DNT>: Longitude
    * <DNT>**postalCode**</DNT>: Code postal, code zip ou similaire
    * <DNT>**region**</DNT>:Abréviation d&amp;apos;État, de province ou de territoire
    * <DNT>**regionName**</DNT>: Nom de l&amp;apos;État, de la province ou du territoire
  </Collapser>

  <Collapser id="parsing-key-value-pairs" title="analyser la valeur clé Pairs">
    Le pipeline New Relic Logs analyse votre message de log par défaut, mais parfois vous avez des messages de log formatés sous forme de paires valeur/clé. Dans cette situation, vous souhaiterez peut-être pouvoir les analyser, puis pouvoir filtrer à l&apos;aide de l&apos;attribut valeur clé.

    Si tel est le cas, vous pouvez utiliser le [type grok](#grok-syntax) `keyvalue` , qui analysera les paires valeur-clé capturées par le modèle grok. Ce format repose sur 3 parties principales : la syntaxe grok, le préfixe que vous souhaitez attribuer à l&amp;apos;attribut valeur clé analysé et le [type grok](#grok-syntax) `keyvalue` . En utilisant le [type grok](#grok-syntax) `keyvalue` , vous pouvez extraire et analyser les paires valeur/clé du log qui ne sont pas correctement formatées ; par exemple, si votre log est préfixé par une chaîne date/heure :

    ```json
      2015-05-13T23:39:43.945958Z key1=value1,key2=value2,key3=value3
    ```

    Afin d&apos;extraire et d&apos;analyser les données de valeur clé de ce format log , créez l&apos;expression Grok suivante :

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:keyvalue()}
    ```

    Le log résultant est :

    ```
      containerTimestamp: "2015-05-13T23:39:43.945958Z"
      my_attribute_prefix.key1: "value1"
      my_attribute_prefix.key2: "value2"
      my_attribute_prefix.key3: "value3"
    ```

    Vous pouvez également définir le délimiteur et le séparateur personnalisés pour extraire les paires valeur-clé requises.

    ```json
    2015-05-13T23:39:43.945958Z event:TestRequest request:bar
    ```

    Par exemple, avec l&apos;expression Grok suivante :

    ```
      %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:keyvalue({"delimiter": " ", "keyValueSeparator": ":"})}
    ```

    Le log résultant est :

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    my_attribute_prefix.event: "TestRequest"
    my_attribute_prefix.request: "bar"
    ```

    Si vous souhaitez omettre le préfixe `my_attribute_prefix` , vous pouvez inclure le `"noPrefix": true` dans la configuration.

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:keyValue({"noPrefix": true})}
    ```

    Le log résultant est :

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    event: "TestRequest"
    request: "bar"
    ```

    Si vous souhaitez définir votre préfixe de caractère de citation personnalisé, vous pouvez inclure « quoteChar » : dans la configuration.

    ```json
    2015-05-13T23:39:43.945958Z nbn_demo='INFO',message='This message contains information with spaces ,sessionId='abc123'
    ```

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:keyValue({"quoteChar": "'"})}
    ```

    Le log résultant est :

    ```
    "my_attribute_prefix.message": "'This message contains information with spaces",
    "my_attribute_prefix.nbn_demo": "INFO",
    "my_attribute_prefix.sessionId": "abc123"
    ```

    ### Paramètres du modèle Grok

    Vous pouvez personnaliser le comportement de l&apos;analyse avec les options suivantes en fonction de vos formats log :

    * **délimiteur**

      * **Description :** Chaîne séparant chaque paire de valeur clé.
      * **Valeur par défaut :** `,` (virgule)
      * **Remplacement :** définissez le champ `delimiter` pour modifier ce comportement.

    * **Séparateur de valeur clé**

      * **Description :** Chaîne utilisée pour attribuer des valeurs aux clés.
      * **Valeur par défaut :** `=`
      * **Remplacement :** définissez le champ `keyValueSeparator` pour l&amp;apos;utilisation d&amp;apos;un séparateur personnalisé.

    * **citationChar**

      * **Description :** Caractère utilisé pour entourer des valeurs avec des espaces ou des caractères spéciaux.
      * **Valeur par défaut :** `"` (guillemets doubles)
      * **Remplacement :** définissez un caractère personnalisé à l’aide de `quoteChar`.

    * **dropOriginal**

      * **Description :** Supprime le message d&amp;apos;origine du log après analyse. Utile pour réduire le stockage log .
      * **Valeur par défaut :** `true`
      * **Remplacement :** définissez `dropOriginal` sur `false` pour conserver le message d&amp;apos;origine du log.

    * **pas de préfixe**

      * **Description :** Lorsque `true`, exclut le nom du champ Grok comme préfixe dans l&amp;apos;objet résultant.
      * **Valeur par défaut :** `false`
      * **Remplacement :** activer en définissant `noPrefix` sur `true`.

    * **échapperChar**

      * **Description :** Définissez un caractère d&amp;apos;échappement personnalisé pour gérer les caractères log spéciaux.
      * **Valeur par défaut :** « » (barre oblique inverse)
      * **Remplacer :** Personnaliser avec `escapeChar`.

    * **trimValeurs**

      * **Description :** permet de supprimer les valeurs contenant des espaces.
      * **Valeur par défaut :** `false`
      * **Remplacement :** définissez `trimValues` sur `true` pour activer le rognage.

    * **touches de finition**

      * **Description :** Permet de rogner les clés contenant des espaces.
      * **Valeur par défaut :** `true`
      * **Remplacement :** définissez `trimKeys` sur `true` pour activer le rognage.
  </Collapser>
</CollapserGroup>

## Organisation par type de log [#type]

Le pipeline d&apos;ingestion de log de New Relic peut analyser les données en faisant correspondre un événement de log à une règle décrivant comment le log doit être analysé. Il existe deux façons d&apos;analyser un événement de log :

* Utiliser une [règle intégrée](#built-in-rules).
* Définir une [règle personnalisée](#custom-parsing).

Les règles sont une combinaison de logique de correspondance et de logique d&apos;analyse. La correspondance est effectuée en définissant une correspondance de requête sur un attribut du log. Les règles ne sont pas appliquées rétroactivement. Les logs collectés avant la création d&apos;une règle ne sont pas analysés par cette règle.

La manière la plus simple d&apos;organiser votre log et la manière dont il est analysé est d&apos;inclure le champ `logtype` dans votre événement de log. Cela indique à New Relic quelle règle intégrée appliquer au log.

<Callout variant="important">
  Une fois qu&apos;une règle d&apos;analyse est active, les données analysées par la règle sont modifiées de manière permanente. Cela ne peut pas être annulé.
</Callout>

## Limites

L&apos;analyse est coûteuse en termes de calcul, ce qui introduit des risques. l&apos;analyse est effectuée pour les règles personnalisées définies dans un compte et pour faire correspondre les modèles à un log. Un grand nombre de modèles ou de règles personnalisées mal définies consommeront une énorme quantité de mémoire et de ressources CPU tout en prenant beaucoup de temps à exécuter.

Afin d&apos;éviter les problèmes, nous appliquons deux limites d&apos;analyse : par message, par règle et par compte.

<table>
  <thead>
    <tr>
      <th style={{ width: "200px" }}>
        Limite
      </th>

      <th>
        Description
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        Par message et par règle
      </td>

      <td>
        La limite par message et par règle empêche que le temps consacré à l&apos;analyse d&apos;un seul message soit supérieur à 100 ms. Si cette limite est atteinte, le système cessera de tenter d&apos;analyser le message de log avec cette règle.

        Le pipeline d&apos;ingestion tentera d&apos;exécuter tout autre élément applicable à ce message, et le message sera toujours transmis via le pipeline d&apos;ingestion et stocké dans NRDB. Le message de log sera dans son format original, non analysé.
      </td>
    </tr>

    <tr>
      <td>
        Par compte
      </td>

      <td>
        La limite par compte existe pour empêcher les comptes d’utiliser plus que leur juste part de ressources. La limite prend en compte le temps total passé à traiter <DNT>**all**</DNT> message de log pour un compte par minute.
      </td>
    </tr>
  </tbody>
</table>

<Callout variant="tip">
  Pour vérifier facilement si vos limites de débit ont été atteintes, accédez à [la page <DNT>**Limits**</DNT> de votre système](/docs/telemetry-data-platform/ingest-manage-data/manage-data/view-system-limits#limits-ui) dans l&amp;apos;interface utilisateur de New Relic.
</Callout>

## Règles d&apos;analyse intégrées [#built-in-rules]

Les formats log courants ont des règles d&apos;analyse bien établies déjà créées pour eux. Pour bénéficier des règles d’analyse intégrées, ajoutez l’attribut `logtype` lors du transfert du log. Définissez la valeur sur une valeur répertoriée dans le tableau suivant et les règles pour ce type de log seront appliquées automatiquement.

### Liste des règles intégrées [#rulesets]

Les valeurs d’attribut `logtype` suivantes correspondent à une règle d’analyse prédéfinie. Par exemple, pour interroger l&amp;apos;équilibreur de charge d&amp;apos;application :

* Depuis l&apos;interface utilisateur de New Relic, utilisez le format `logtype:"alb"`.
* Depuis [NerdGraph](/docs/apis/nerdgraph/examples/nerdgraph-log-parsing-rules-tutorial/), utilisez le format `logtype = 'alb'`.

Pour savoir quels champs sont analysés pour chaque règle, consultez notre documentation sur [les règles d&apos;analyse intégrées](/docs/logs/ui-data/built-log-parsing-rules).

<table>
  <thead>
    <tr>
      <th style={{ width: "200px" }}>
        `logtype`
      </th>

      <th>
        source du log
      </th>

      <th>
        Exemple de requête correspondante
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        [`apache`](/docs/logs/ui-data/built-log-parsing-rules#apache)
      </td>

      <td>
        Log d&apos;accès Apache
      </td>

      <td>
        `logtype:"apache"`
      </td>
    </tr>

    <tr>
      <td>
        [`apache_error`](/docs/logs/ui-data/built-log-parsing-rules#apache_error)
      </td>

      <td>
        Log des erreurs Apache
      </td>

      <td>
        `logtype:"apache_error"`
      </td>
    </tr>

    <tr>
      <td>
        [`alb`](/docs/logs/ui-data/built-log-parsing-rules#application-load-balancer)
      </td>

      <td>
        Log de l&apos;équilibreur de charge d&apos;application
      </td>

      <td>
        `logtype:"alb"`
      </td>
    </tr>

    <tr>
      <td>
        [`cassandra`](/docs/logs/ui-data/built-log-parsing-rules#cassandra)
      </td>

      <td>
        Log de Cassandre
      </td>

      <td>
        `logtype:"cassandra"`
      </td>
    </tr>

    <tr>
      <td>
        [`cloudfront-web`](/docs/logs/ui-data/built-log-parsing-rules#cloudfront)
      </td>

      <td>
        CloudFront (log Web standard)
      </td>

      <td>
        `logtype:"cloudfront-web"`
      </td>
    </tr>

    <tr>
      <td>
        [`cloudfront-rtl`](/docs/logs/ui-data/built-log-parsing-rules#cloudfront-rtl)
      </td>

      <td>
        CloudFront (log web temps réel)
      </td>

      <td>
        `logtype:"cloudfront-rtl"`
      </td>
    </tr>

    <tr>
      <td>
        [`elb`](/docs/logs/ui-data/built-log-parsing-rules#elastic-load-balancer)
      </td>

      <td>
        Log d&apos; Elastic Load Balancer
      </td>

      <td>
        `logtype:"elb"`
      </td>
    </tr>

    <tr>
      <td>
        [`haproxy_http`](/docs/logs/ui-data/built-log-parsing-rules#haproxy)
      </td>

      <td>
        Log HAProxy
      </td>

      <td>
        `logtype:"haproxy_http"`
      </td>
    </tr>

    <tr>
      <td>
        [`ktranslate-health`](/docs/logs/ui-data/built-log-parsing-rules#ktranslate-health)
      </td>

      <td>
        Log de santé du conteneur KTranslate
      </td>

      <td>
        `logtype:"ktranslate-health"`
      </td>
    </tr>

    <tr>
      <td>
        [`linux_cron`](/docs/logs/ui-data/built-log-parsing-rules/#linux_cron)
      </td>

      <td>
        Cron Linux
      </td>

      <td>
        `logtype:"linux_cron"`
      </td>
    </tr>

    <tr>
      <td>
        [`linux_messages`](/docs/logs/ui-data/built-log-parsing-rules/#linux_messages)
      </td>

      <td>
        Messages Linux
      </td>

      <td>
        `logtype:"linux_messages"`
      </td>
    </tr>

    <tr>
      <td>
        [`iis_w3c`](/docs/logs/ui-data/built-log-parsing-rules/#iis)
      </td>

      <td>
        Log du serveur Microsoft IIS - format W3C
      </td>

      <td>
        `logtype:"iis_w3c"`
      </td>
    </tr>

    <tr>
      <td>
        [`mongodb`](/docs/logs/ui-data/built-log-parsing-rules#mongodb)
      </td>

      <td>
        Logs de MongoDB
      </td>

      <td>
        `logtype:"mongodb"`
      </td>
    </tr>

    <tr>
      <td>
        [`monit`](/docs/logs/ui-data/built-log-parsing-rules#monit)
      </td>

      <td>
        Monit logs
      </td>

      <td>
        `logtype:"monit"`
      </td>
    </tr>

    <tr>
      <td>
        [`mysql-error`](/docs/logs/ui-data/built-log-parsing-rules#mysql-error)
      </td>

      <td>
        Log des erreurs MySQL
      </td>

      <td>
        `logtype:"mysql-error"`
      </td>
    </tr>

    <tr>
      <td>
        [`nginx`](/docs/logs/ui-data/built-log-parsing-rules#nginx)
      </td>

      <td>
        Log d&apos;accès NGINX
      </td>

      <td>
        `logtype:"nginx"`
      </td>
    </tr>

    <tr>
      <td>
        [`nginx-error`](/docs/logs/ui-data/built-log-parsing-rules#nginx-error)
      </td>

      <td>
        Log des erreurs NGINX
      </td>

      <td>
        `logtype:"nginx-error"`
      </td>
    </tr>

    <tr>
      <td>
        [`postgresql`](/docs/logs/ui-data/built-log-parsing-rules#postgresql)
      </td>

      <td>
        Log Postgresql
      </td>

      <td>
        `logtype:"postgresql"`
      </td>
    </tr>

    <tr>
      <td>
        [`rabbitmq`](/docs/logs/ui-data/built-log-parsing-rules#rabbitmq)
      </td>

      <td>
        Log de Rabbitmq
      </td>

      <td>
        `logtype:"rabbitmq"`
      </td>
    </tr>

    <tr>
      <td>
        [`redis`](/docs/logs/ui-data/built-log-parsing-rules#redis)
      </td>

      <td>
        Log Redis
      </td>

      <td>
        `logtype:"redis"`
      </td>
    </tr>

    <tr>
      <td>
        [`route-53`](/docs/logs/ui-data/built-log-parsing-rules#route53)
      </td>

      <td>
        Log de la Route 53
      </td>

      <td>
        `logtype:"route-53"`
      </td>
    </tr>

    <tr>
      <td>
        [`syslog-rfc5424`](/docs/logs/ui-data/built-log-parsing-rules/#syslog-rfc5424)
      </td>

      <td>
        Logs système au format RFC5424
      </td>

      <td>
        `logtype:"syslog-rfc5424"`
      </td>
    </tr>
  </tbody>
</table>

### Ajoutez le `logtype` [#logattr]

Lors de l&apos;agrégation des logs, il est important de fournir des métadonnées qui facilitent l&apos;organisation, la recherche et l&apos;analyse de ces logs. Une façon simple de procéder consiste à ajouter l&apos;attribut `logtype` au message de log lors de leur expédition. [Les règles d’analyse intégrées](#built-in-rules) sont appliquées par défaut à certaines valeurs `logtype` .

<Callout variant="tip">
  Les champs `logType`, `logtype` et `LOGTYPE` sont tous pris en charge pour les règles intégrées. Pour faciliter la recherche, nous vous recommandons de vous aligner sur une syntaxe unique dans votre organisation.
</Callout>

Voici quelques exemples de la façon d&apos;ajouter `logtype` au log envoyé par certaines de nos [méthodes d&apos;expédition prises en charge](/docs/logs/enable-new-relic-logs).

<CollapserGroup>
  <Collapser className="freq-link" id="infrastructure-log-forwarder-example" title="Exemple d'agent d'infrastructure New Relic">
    Ajoutez `logtype` comme [`attribute`](/docs/logs/forward-logs/forward-your-logs-using-infrastructure-agent#attributes). Vous devez définir le type de log pour chaque source nommée.

    ```yml
    logs:
      - name: file-simple
        file: /path/to/file
        attributes:
          logtype: fileRaw
      - name: nginx-example
        file: /var/log/nginx.log
        attributes:
          logtype: nginx
    ```
  </Collapser>

  <Collapser className="freq-link" id="fluentd-example" title="Exemple de Fluentd">
    Ajoutez un bloc de filtre au fichier `.conf` , qui utilise un `record_transformer` pour ajouter un nouveau champ. Dans cet exemple, nous utilisons un `logtype` sur `nginx` pour déclencher la règle d’analyse NGINX intégrée. Découvrez d’autres [exemples de Fluentd](https://github.com/newrelic/fluentd-examples).

    ```apacheconf
    <filter containers>
      @type record_transformer
      enable_ruby true
      <record>
        #Add logtype to trigger a built-in parsing rule for nginx access logs
        logtype nginx
        #Set timestamp from the value contained in the field "time"
        timestamp record["time"]
        #Add hostname and tag fields to all records
        hostname "#{Socket.gethostname}"
        tag ${tag}
      </record>
    </filter>
    ```
  </Collapser>

  <Collapser className="freq-link" id="fluentbit-example" title="Exemple de Fluent Bit">
    Ajoutez un bloc de filtre au fichier `.conf` qui utilise un `record_modifier` pour ajouter un nouveau champ. Dans cet exemple, nous utilisons un `logtype` sur `nginx` pour déclencher la règle d’analyse NGINX intégrée. Découvrez d’autres [exemples de Fluent Bit](https://github.com/newrelic/fluentbit-examples).

    ```ini
    [FILTER]
        Name   record_modifier
        Match  *
        Record logtype nginx
        Record hostname ${HOSTNAME}
        Record service_name Sample-App-Name
    ```
  </Collapser>

  <Collapser className="freq-link" id="logstash-example" title="Exemple de Logstash">
    Ajoutez un bloc de filtre à la configuration Logstash qui utilise un filtre de mutation `add_field` pour ajouter un nouveau champ. Dans cet exemple, nous utilisons un `logtype` sur `nginx` pour déclencher la règle d’analyse NGINX intégrée. Découvrez d&amp;apos;autres [exemples de Logstash](https://github.com/newrelic/logstash-examples).

    ```ini
    filter {
      mutate {
        add_field => {
          "logtype" => "nginx"
          "service_name" => "myservicename"
          "hostname" => "%{host}"
        }
      }
    }
    ```
  </Collapser>

  <Collapser className="freq-link" id="api-example" title="Exemple d'API de Log">
    Vous pouvez ajouter un attribut à la requête JSON envoyée à New Relic. Dans cet exemple, nous ajoutons un attribut `logtype` de valeur `nginx` pour déclencher la règle d’analyse NGINX intégrée. En savoir plus sur l’utilisation de l’ [API Logs](/docs/logs/new-relic-logs/log-api/introduction-log-api).

    ```
    POST /log/v1 HTTP/1.1
    Host: log-api.newrelic.com
    Content-Type: application/json
    X-License-Key: YOUR_LICENSE_KEY
    Accept: */*
    Content-Length: 133
    {
      "timestamp": TIMESTAMP_IN_UNIX_EPOCH,
      "message": "User 'xyz' logged in",
      "logtype": "nginx",
      "service": "login-service",
      "hostname": "login.example.com"
    }
    ```
  </Collapser>
</CollapserGroup>

## Créer et afficher des règles d&apos;analyse personnalisées [#custom-parsing]

De nombreux logs sont formatés ou structurés d’une manière unique. Afin de les analyser, une logique personnalisée doit être construite et appliquée.

<img title="Log parsing rules" alt="Screenshot of log parsing in UI" src="/images/logs_screenshot-full_parsing-ui.webp" />

<figcaption>
  Dans la navigation de gauche dans l&apos;interface utilisateur du log, sélectionnez <DNT>**Parsing**</DNT>, puis créez votre propre règle d&amp;apos;analyse personnalisée avec une clause NRQL `WHERE` valide et un modèle Grok.
</figcaption>

Pour créer et gérer vos propres règles d’analyse personnalisées :

1. Allez à <DNT>**[one.newrelic.com &gt; All capabilities](https://one.newrelic.com/all-capabilities) &amp;gt; Logs**</DNT>.
2. À partir de <DNT>**Manage data**</DNT> dans la navigation de gauche de l’interface utilisateur du log, cliquez sur <DNT>**Parsing**</DNT>, puis sur <DNT>**Create parsing rule**</DNT>.
3. Saisissez un nom pour la nouvelle règle d’analyse.
4. Sélectionnez un champ existant à analyser (par défaut = `message`) ou entrez un nouveau nom de champ.
5. Saisissez une clause NRQL `WHERE` valide pour correspondre au log que vous souhaitez analyser.
6. Sélectionnez un log correspondant s&apos;il existe, ou cliquez sur l&apos;onglet <DNT>**Paste log**</DNT> pour coller un exemple log. Notez que si vous copiez du texte depuis l&amp;apos;interface utilisateur du log ou le générateur de requêtes pour le coller dans l&amp;apos;interface utilisateur d&amp;apos;analyse, assurez-vous qu&amp;apos;il s&amp;apos;agit de la version <DNT>*Unformatted*</DNT> .
7. Saisissez la règle d&apos;analyse et validez son fonctionnement en affichant les résultats dans la section <DNT>**Output**</DNT> . Pour en savoir plus sur Grok et les règles d&amp;apos;analyse personnalisées, lisez [notre article de blog sur la façon d&apos;analyser les logs avec les modèles Grok](https://blog.newrelic.com/product-news/how-to-use-grok-log-parsing).
8. Activez et enregistrez la règle d’analyse personnalisée.

Pour afficher les règles d’analyse existantes :

1. Allez à <DNT>**[one.newrelic.com &gt; All capabilities](https://one.newrelic.com/all-capabilities) &amp;gt; Logs**</DNT>.
2. À partir de <DNT>**Manage data**</DNT> dans la barre de navigation de gauche de l’interface utilisateur du log, cliquez sur <DNT>**Parsing**</DNT>.

## Dépannage [#troubleshooting]

Si l&apos;analyse ne fonctionne pas comme prévu, cela peut être dû à :

* <DNT>**Logic:**</DNT> La logique de correspondance de la règle d&amp;apos;analyse ne correspond pas au log souhaité.
* <DNT>**Timing:**</DNT> Si votre règle de correspondance d&amp;apos;analyse cible une valeur qui n&amp;apos;existe pas encore, elle échouera. Cela peut se produire si la valeur est ajoutée plus tard dans le pipeline dans le cadre du processus d’enrichissement.
* <DNT>**Limits:**</DNT> Il y a une quantité de temps fixe disponible chaque minute pour traiter le log via l&amp;apos;analyse, les modèles, les filtres de suppression, etc. Si le temps maximum a été écoulé, l&amp;apos;analyse sera ignorée pour les enregistrements d&amp;apos;événements de log supplémentaires.

Pour résoudre ces problèmes, créez ou ajustez vos [règles d&apos;analyse personnalisées](#custom-parsing).