---
title: analyser les données log
tags:
  - Logs
  - Log management
  - UI and data
metaDescription: How New Relic uses parsing and how to send customized log data.
freshnessValidatedDate: never
translationType: machine
---

Log <DNT>parsing</DNT> transforme les données de logs non structurées en attributs interrogeables que vous pouvez utiliser pour obtenir des informations plus approfondies à partir de vos logs. Ces attributs vous permettent de filtrer, facetter et créer des alertes sur vos données avec précision.

## Choisissez votre stratégie d&apos;analyse [#choose]

Décidez si vous souhaitez analyser les données au moment de l&apos;ingestion ou lors de l&apos;exécution d&apos;une requête :

<table>
  <thead>
    <tr>
      <th style={{ width: "200px" }}>
        Type d&apos;analyse
      </th>

      <th>
        Description
      </th>

      <th>
        Idéal pour
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        **Analyse au moment de la requête**
      </td>

      <td>
        Crée des attributs temporaires à l&apos;aide de NRQL qui n&apos;existent que pendant l&apos;exécution de la requête. Idéal pour l&apos;analyse instantanée des données existantes sans attendre l&apos;arrivée de nouveaux logs. En savoir plus sur le [parsing au moment de la requête](/docs/logs/ui-data/query-time-parsing).
      </td>

      <td>
        * Dépannage et investigations ad hoc
        * Analyse exploratoire sur de petits jeux de données
        * Investigations ponctuelles
        * Extraction d&apos;attributs à partir de logs déjà stockés dans NRDB
      </td>
    </tr>

    <tr>
      <td>
        **Analyse au moment de l&apos;ingestion**
      </td>

      <td>
        Crée des attributs permanents stockés dans NRDB. Deux façons de créer des règles d&apos;analyse au moment de l&apos;ingestion :

        * **Règles de parsing intégrées :** Patterns préconfigurés pour les sources de logs courantes (Apache, NGINX, CloudFront, MongoDB, etc.). Ajoutez simplement un attribut `logtype` lors du transfert de logs. Consultez la [liste complète des règles intégrées](/docs/logs/ui-data/built-log-parsing-rules).

        * **Règles d&apos;analyse personnalisées :** Lorsque vos logs sont spécifiques à votre application, les règles d&apos;analyse personnalisées vous permettent de définir précisément les champs qui comptent pour votre activité.

          * **Analyse de logs sans code (IU) :** Détecte des motifs dans vos échantillons de logs. Idéal pour les utilisateurs qui souhaitent extraire des champs par pointer-cliquer.
          * **Grok/Regex personnalisé :** Saisie manuelle de code pour les formats de logs très complexes.
      </td>

      <td>
        * Volumes importants de logs
        * Attributs analysés nécessaires pour les alertes, les dashboards et le monitoring continu
      </td>
    </tr>
  </tbody>
</table>

Vous pouvez également créer, interroger et gérer vos règles d&apos;analyse des logen utilisant NerdGraph, notre API GraphQL . Un outil utile pour cela est notre [explorateur d&apos;API Nerdgraph](https://api.newrelic.com/graphiql). Pour plus d&apos;informations, consultez notre [tutoriel NerdGraph pour l&apos;analyse](/docs/apis/nerdgraph/examples/nerdgraph-log-parsing-rules-tutorial/).

<DNT>
  /* Here&apos;s a 5-minute video about log parsing: &lt;Video id=&quot;xPWM46yw3bQ&quot; type=&quot;youtube&quot; /&gt; */
</DNT>

## Comment fonctionne l&apos;analyse personnalisée au moment de l&apos;ingestion [#how-it-works]

L&apos;analyse personnalisée vous permet de définir exactement comment New Relic structure vos logs entrants. Avant de créer des règles, il est important de comprendre les contraintes techniques du pipeline d&apos;ingestion.

<table>
  <thead>
    <tr>
      <th style={{ width: "100px" }}>
        analyser des logs
      </th>

      <th>
        Comment ça marche
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        Quoi
      </td>

      <td>
        Les règles d&apos;analyse sont très ciblées. Lorsque vous créez une règle, vous définissez :

        * **Le champ ciblé :** L&apos;analyse est appliquée à un champ spécifique à la fois.
        * **La logique de correspondance :** Utilisez une clause NRQL `WHERE` pour filtrer précisément les logs que cette règle doit évaluer.
        * **La méthode d&apos;extraction :** Vous pouvez utiliser l&apos;**analyse de logs No Code** pour une expérience de détection de patterns automatique et guidée, ou écrire manuellement des **Grok/Regex** pour des structures de logs complexes et hautement personnalisées.
      </td>
    </tr>

    <tr>
      <td>
        Quand
      </td>

      <td>
        New Relic traite les logs de manière séquentielle. Cela affecte les conditions qui peuvent être mises en correspondance.

        * L&apos;analyse s&apos;effectue lors de l&apos;ingestion des données. Une fois qu&apos;un log est écrit dans la NRDB, les modifications apportées sont définitives.
        * Une fois une règle enregistrée et activée, les règles commencent immédiatement à traiter les logs entrants.
        * L&apos;analyse s&apos;effectue **avant** l&apos;enrichissement des données (tel que la synthèse d&apos;entités), leur rejet ou leur partitionnement.
      </td>
    </tr>

    <tr>
      <td>
        Validation
      </td>

      <td>
        Pour vous assurer que vos règles fonctionnent avant qu&apos;elles n&apos;affectent vos données ingérées, vous pouvez valider l&apos;aperçu du résultat sur un maximum de 10 exemples de logs des 30 dernières minutes. Il s&apos;agit d&apos;échantillons historiques stockés dans NRDB, et non de logs de streaming en temps réel.
      </td>
    </tr>
  </tbody>
</table>

## Créer une règle personnalisée [#custom-parsing]

Vous pouvez créer des règles de parsing en contexte lors de l&apos;investigation d&apos;un log. Cela évite le changement de contexte et réduit le temps moyen de détection (MTTD). Sinon, vous pouvez créer des règles à partir de zéro lors de l&apos;intégration d&apos;une nouvelle application ou d&apos;un nouveau service.

### Analyse de logs sans code [#no-code]

Utilisez l&apos;analyse de logs No Code pour détecter et extraire des champs de vos échantillons de logs. New Relic analyse vos exemples de logs et suggère des modèles que vous pouvez configurer.

<Steps>
  <Step>
    Pour créer une règle en contexte, allez dans <DNT>**[one.newrelic.com](https://one.newrelic.com) &gt; Logs**</DNT> et appliquez un filtre (ou sélectionnez une entité disposant de logs, telle que APM, Browser ou Mobile, et accédez à <DNT>**Logs in Context**</DNT>).

    Pour créer une règle sans contexte, accédez à <DNT>**[one.newrelic.com](https://one.newrelic.com) &gt; Logs**</DNT> sans définir de filtre, ou accédez à <DNT>**Logs &gt; Parsing**</DNT> et cliquez sur <DNT>**Create a parsing rule**</DNT>.
  </Step>

  <Step>
    Dans le processus de création de règles contextuelles :

    1. Cliquez sur un log pour l&apos;ouvrir <DNT>**Log details**</DNT>
    2. Sélectionnez l&apos;attribut de log que vous souhaitez parser (par exemple, `message`)
    3. Cliquez sur <DNT>**Create ingest time parsing rule**</DNT> et donnez un nom à votre règle

    Si vous avez appliqué un filtre dans l&apos;interface Logs avant de créer la règle, une condition de correspondance est automatiquement renseignée en fonction de ce filtre.

    <img title="Log parsing rules" alt="Screenshot of log filtering in UI" src="/images/logs_filtering.webp" />

    Dans le flux sans contexte, donnez un nom à votre règle et définissez une condition de filtre NRQL ou collez un exemple de log.

    * Si vous définissez un filtre de logs, cliquez sur <DNT>**Run your query**</DNT>, sélectionnez le champ que vous souhaitez analyser et cliquez sur <DNT>**Next**</DNT>.
    * Si vous collez un exemple de log, vous devez définir la clause NRQL `WHERE` pour correspondre à vos logs, sélectionner le champ que vous souhaitez analyser et cliquer sur <DNT>**Next**</DNT>.

    <img title="Log parsing rules" alt="Screenshot of creating a parsing rule in UI" src="/images/Create_a_parsing_rule_.webp" />
  </Step>

  <Step>
    Examinez le <DNT>**Patterns we detected**</DNT> dans l&apos;exemple de log sélectionné et la règle qui a été créée. Cliquez sur un motif mis en évidence pour afficher et modifier sa configuration.

    <img title="Screenshot of matching pattern in UI" alt="Screenshot of matching pattern in UI" src="/images/matching_pattern.webp" />

    <Callout variant="Preview" title="Note">
      * Lorsque vous nommez des attributs, utilisez des minuscules avec des tirets bas. Évitez les caractères spéciaux à l&apos;exception des traits de soulignement et ne commencez pas un nom d&apos;attribut par un chiffre.

      * Pour les sous-chaînes que vous souhaitez éviter d&apos;analyser et qui incluent des valeurs dynamiques, assurez-vous de les définir comme sous-chaînes dynamiques en les sélectionnant et en modifiant leur configuration sur <DNT>**Yes**</DNT>.
    </Callout>
  </Step>

  <Step>
    Pour un contrôle plus granulaire des champs à extraire, cliquez et faites glisser pour mettre en surbrillance l&apos;exemple de log.

    <img title="Log parsing rules" alt="Screenshot of creating a parsing rule in UI" src="/images/create_a_parsing_rule.webp" />

    Vous pouvez interagir avec les motifs de la façon suivante :

    * <DNT>**Auto detect patterns**</DNT>: Pour détecter des motifs dans n&apos;importe quelle partie de l&apos;exemple de log qui n&apos;est pas déjà mise en surbrillance, cliquez et faites glisser pour mettre cette sous-chaîne en surbrillance, puis cliquez sur <DNT>**Auto detect patterns**</DNT>. New Relic trouvera et mettra en évidence des motifs dans la portion sélectionnée. Pour une liste des noms de modèles Grok pris en charge, consultez [Noms de modèles Grok pris en charge](#grok-patterns).
    * <DNT>**Select text to parse**</DNT>: Sélectionnez ce mode pour l&apos;expérience de création de règles guidée. Ce mode offre une configuration motif par motif. Une fois les configurations de motifs définies, cliquez sur <DNT>**Add pattern to rule**</DNT> pour afficher la règle mise à jour et prévisualiser la sortie.

    Si les motifs détectés ne sont pas pertinents ou extraient des données indésirables, vous pouvez les supprimer de la règle créée en :

    * Surlignez le motif indésirable dans la fenêtre d&apos;exemple de log et cliquez sur <DNT>**Remove selected patterns**</DNT>, ou
    * Cliquez sur un motif et sélectionnez <DNT>**Remove**</DNT>.
  </Step>

  <Step>
    Examinez le panneau <DNT>**Preview output**</DNT>. Vérifiez que les exemples de logs affichent une coche verte, indiquant qu&apos;ils correspondent à votre règle et que les champs seront extraits lors de l&apos;ingestion.

    * Pour modifier votre échantillon, développez un log dans le volet <DNT>**Preview output**</DNT> et cliquez sur <DNT>**Use as sample**</DNT>.

      * Si vous avez sélectionné un log sans correspondance : l&apos;échantillon sélectionné s&apos;affichera dans la fenêtre de l&apos;échantillon de log, de nouveaux motifs seront détectés et une nouvelle règle sera créée.
      * Si vous avez sélectionné un log correspondant : l&apos;échantillon sélectionné s&apos;affichera dans la fenêtre des logs d&apos;échantillon.
  </Step>

  <Step>
    Cliquez sur <DNT>**Save rule**</DNT> pour activer immédiatement, ou sur <DNT>**Save as draft**</DNT> pour activer plus tard.
  </Step>
</Steps>

### Rédigez votre propre Grok/Regex personnalisé [#grok]

Pour les formats uniques, les utilisateurs avancés peuvent cliquer sur <DNT>**Write your own rule**</DNT> sur la page <DNT>**Create a parsing rule**</DNT> pour basculer vers l&apos;éditeur de code et modifier les modèles directement dans l&apos;éditeur de règles.

<img title="Screenshot depicting where to click to write your own custom Grok/regex" alt="Screenshot depicting where to click to write your own custom Grok/regex" src="/images/create_a_parsing_rule.webp" />

Une fois la modification de la règle terminée, cliquez sur <DNT>**Preview**</DNT> pour afficher la sortie d&apos;aperçu mise à jour et cliquez sur <DNT>**Save rule**</DNT> pour l&apos;activer.

<Callout variant="preview" title="Note">
  Pour passer à l&apos;ancien éditeur, cliquez sur <DNT>**Switch to original editor**</DNT> en haut à droite de la page <DNT>**Create a parsing rule**</DNT>.
</Callout>

#### Modèles de données pris en charge [#supported-patterns]

New Relic prend en charge l&apos;analyse de divers types et formats de données à l&apos;aide de modèles Grok. Les modèles d&apos;analyse sont spécifiés à l&apos;aide de Grok, un standard de l&apos;industrie pour l&apos;analyse des messages de log. Grok est un sur-ensemble d&apos;expressions régulières qui ajoute des motifs nommés intégrés à utiliser à la place d&apos;expressions régulières complexes littérales.

Les règles de parsing peuvent inclure un mélange d&apos;expressions régulières et de noms de patterns Grok dans votre chaîne de correspondance. Cliquez sur ce lien pour obtenir la liste des [patterns Grok](https://github.com/thekrakken/java-grok/tree/master/src/main/resources/patterns) pris en charge, et ici pour la liste des [types Grok](#grok-types) pris en charge.

<CollapserGroup>
  <Collapser id="grok-syntax" title="Syntaxe des patterns Grok">
    Les motifs Grok suivent une syntaxe prédéfinie :

    ```
    %{PATTERN_NAME[:OPTIONAL_EXTRACTED_ATTRIBUTE_NAME[:OPTIONAL_TYPE[:OPTIONAL_PARAMETER]]]}
    ```

    Où:

    * `PATTERN_NAME` est l&apos;un des modèles Grok pris en charge. Le nom du motif est juste un nom convivial représentant des expressions régulières. Elles sont exactement égales aux expressions régulières correspondantes.
    * `OPTIONAL_EXTRACTED_ATTRIBUTE_NAME`, s&apos;il est fourni, est le nom de l&apos;attribut qui sera ajouté à votre message de log avec la valeur correspondant au nom du modèle. Cela équivaut à utiliser un groupe de capture nommé à l’aide d’expressions régulières. Si cela n&apos;est pas fourni, la règle d&apos;analyse correspondra simplement à une région de votre chaîne, mais n&apos;extrairea pas un attribut avec sa valeur.
    * `OPTIONAL_TYPE` spécifie le type de valeur d&apos;attribut à extraire. Si elles sont omises, les valeurs sont extraites sous forme de chaînes. Par instance, pour extraire la valeur `123` de `"File Size: 123"` sous forme de nombre dans l&apos;attribut `file_size`, utilisez `value: %{INT:file_size:int}`.
    * `OPTIONAL_PARAMETER` spécifie un paramètre facultatif pour certains types. Actuellement, seul le type `datetime` prend un paramètre, voir ci-dessous pour plus de détails.
  </Collapser>

  <Collapser id="grok-types" title="Types de Grok pris en charge">
    Le champ `OPTIONAL_TYPE` spécifie le type de valeur d&apos;attribut à extraire. Si elles sont omises, les valeurs sont extraites sous forme de chaînes.

    Les types pris en charge sont :

    <table>
      <thead>
        <tr>
          <th>
            Type spécifié dans Grok
          </th>

          <th>
            Type stocké dans la base de données New Relic
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `boolean`
          </td>

          <td>
            `boolean`
          </td>
        </tr>

        <tr>
          <td>
            `byte` `short` `int` `integer`
          </td>

          <td>
            `integer`
          </td>
        </tr>

        <tr>
          <td>
            `long`
          </td>

          <td>
            `long`
          </td>
        </tr>

        <tr>
          <td>
            `float`
          </td>

          <td>
            `float`
          </td>
        </tr>

        <tr>
          <td>
            `double`
          </td>

          <td>
            `double`
          </td>
        </tr>

        <tr>
          <td>
            `string` (défaut) `text`
          </td>

          <td>
            `string`
          </td>
        </tr>

        <tr>
          <td>
            `date` `datetime`
          </td>

          <td>
            Le temps comme un `long`

            Par défaut, il est interprété comme ISO 8601. Si `OPTIONAL_PARAMETER` est présent, il spécifie la [chaîne de motif de date et d&apos;heure](https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/text/SimpleDateFormat.html)à utiliser pour interpréter le `datetime`.

            Notez que ceci n&apos;est disponible que pendant l&apos;analyse. Nous avons une [étape d’interprétation d’horodatage supplémentaire et distincte](/docs/logs/ui-data/timestamp-support) qui se produit pour tous les logs plus tard dans le pipeline d’ingestion.
          </td>
        </tr>

        <tr>
          <td>
            `json`
          </td>

          <td>
            Données structurées JSON. Voir [analyser JSON mélangé à du texte brut](#parsing-json) pour plus d&apos;informations.
          </td>
        </tr>

        <tr>
          <td>
            `csv`
          </td>

          <td>
            Données CSV. Voir [Analyser CSV](#parsing-csv) pour plus d&apos;informations.
          </td>
        </tr>

        <tr>
          <td>
            `geo`
          </td>

          <td>
            Localisation géographique à partir des adresses IP. Consultez [Géolocalisation des adresses IP (GeoIP)](#geo) pour plus d&apos;informations.
          </td>
        </tr>

        <tr>
          <td>
            `key value pairs`
          </td>

          <td>
            valeur clé Paire . Voir [analyser les paires de valeurs clés](#parsing-key-value-pairs) pour plus d&apos;informations.
          </td>
        </tr>
      </tbody>
    </table>
  </Collapser>

  <Collapser id="grok-multiline" title="Analyse multiligne de Grok">
    Si vous avez un log multiligne, sachez que le modèle Grok `GREEDYDATA` ne correspond pas aux nouvelles lignes (il est équivalent à `.*`).

    Ainsi, au lieu d&apos;utiliser `%{GREEDYDATA:some_attribute}` directement, vous devrez ajouter l&apos;indicateur multiligne devant lui : `(?s)%{GREEDYDATA:some_attribute}`
  </Collapser>

  <Collapser id="parsing-json" title="analyser du JSON mélangé à du texte brut">
    Le pipeline de logs New Relic analyse vos messages de log JSON par défaut, mais il arrive parfois que vous ayez des messages de log JSON mélangés à du texte brut. Dans cette situation, vous souhaiterez peut-être pouvoir les analyser, puis filtrer à l&apos;aide des attributs JSON. Dans ce cas, vous pouvez utiliser le [type Grok](#grok-syntax) `json`, qui analysera le JSON capturé par le motif Grok. Ce format repose sur 3 parties principales : la syntaxe Grok, le préfixe que vous souhaitez attribuer aux attributs JSON analysés et le [type Grok](#grok-syntax) `json`. En utilisant le [type Grok](#grok-syntax) `json`, vous pouvez extraire et analyser du JSON à partir de logs qui ne sont pas correctement formatés ; par exemple, si vos logs sont préfixés par une chaîne de date/heure :

    ```json
    2015-05-13T23:39:43.945958Z {"event": "TestRequest", "status": 200, "response": {"headers": {"X-Custom": "foo"}}, "request": {"headers": {"X-Custom": "bar"}}}
    ```

    Afin d&apos;extraire et d&apos;analyser les données JSON de ce format log , créez l&apos;expression Grok suivante :

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json}
    ```

    Le log résultant est :

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    my_attribute_prefix.event: "TestRequest"
    my_attribute_prefix.status: 200
    my_attribute_prefix.response.headers.X-Custom: "foo"
    my_attribute_prefix.request.headers.X-Custom: "bar"
    ```

    Vous pouvez définir la liste des attributs à extraire ou à supprimer avec les options `keepAttributes` ou `dropAttributes`. Par exemple, avec l&apos;expression Grok suivante :

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json({"keepAttributes": ["my_attribute_prefix.event", "my_attribute_prefix.response.headers.X-Custom"]})}
    ```

    Le log résultant est :

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    my_attribute_prefix.event: "TestRequest"
    my_attribute_prefix.request.headers.X-Custom: "bar"
    ```

    Si vous souhaitez omettre le préfixe `my_attribute_prefix` , vous pouvez inclure le `"noPrefix": true` dans la configuration.

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json({"noPrefix": true})}
    ```

    Si vous souhaitez omettre le préfixe `my_attribute_prefix` et conserver uniquement l&apos;attribut `status` , vous pouvez inclure `"noPrefix": true` et `"keepAttributes: ["status"]` dans la configuration.

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json({"noPrefix": true, "keepAttributes": ["status"]})}
    ```

    Si votre JSON a été échappé, vous pouvez utiliser l&apos;option `isEscaped` pour pouvoir l&apos;analyser. Si votre JSON a été échappé puis cité, vous devez également faire correspondre les guillemets, comme indiqué ci-dessous. Par exemple, avec l&apos;expression Grok suivante :

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} "%{GREEDYDATA:my_attribute_prefix:json({"isEscaped": true})}"
    ```

    Serait capable d&apos;analyser le message échappé :

    ```
    2015-05-13T23:39:43.945958Z "{\"event\": \"TestRequest\", \"status\": 200, \"response\": {\"headers\": {\"X-Custom\": \"foo\"}}, \"request\": {\"headers\": {\"X-Custom\": \"bar\"}}}"
    ```

    Le log résultant est :

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    my_attribute_prefix.event: "TestRequest"
    my_attribute_prefix.status: 200
    my_attribute_prefix.response.headers.X-Custom: "foo"
    my_attribute_prefix.request.headers.X-Custom: "bar"
    ```

    Pour configurer le [type Grok](#grok-syntax) `json` , utilisez `:json(_CONFIG_)`:

    * `json({"dropOriginal": true})`: Supprimez le snippet JSON qui a été utilisé dans l&apos;analyse. Lorsqu&apos;elle est définie sur `true` (valeur par défaut), la règle d&apos;analyse supprimera le snippet JSON d&apos;origine. Notez que l’attribut JSON restera dans le champ de message.
    * `json({"dropOriginal": false})`:Cela affichera la charge utile JSON qui a été extraite. Lorsqu&apos;il est défini sur `false`, la charge utile complète uniquement JSON sera affichée sous un attribut nommé dans `my_attribute_prefix` ci-dessus. Notez que l&apos;attribut JSON restera également dans le champ de message ici, donnant à l&apos;utilisateur 3 vues différentes des données JSON. Si le stockage des trois versions est un problème, il est recommandé d&apos;utiliser la valeur par défaut de `true` ici.
    * `json({"depth": 62})`: Niveaux de profondeur auxquels vous souhaitez analyser la valeur JSON (par défaut 62).
    * `json({"keepAttributes": ["attr1", "attr2", ..., "attrN"]})`:Spécifie quel attribut sera extrait du JSON. La liste fournie ne peut pas être vide. Si cette option configuration n&apos;est pas définie, tous les attributs sont extraits.
    * `json({"dropAttributes": ["attr1", "attr2", ..., "attrN"]})`:Spécifie l&apos;attribut à supprimer du JSON. Si cette option de configuration n&apos;est pas définie, aucun attribut n&apos;est supprimé.
    * `json({"noPrefix": true})`: Définissez cette option sur `true` pour supprimer le préfixe de l&apos;attribut extrait du JSON.
    * `json({"isEscaped": true})`: Définissez cette option sur `true` pour analyser le JSON qui a été échappé (ce que vous voyez généralement lorsque le JSON est transformé en chaîne, par exemple `{\"key\": \"value\"}`)
  </Collapser>

  <Collapser id="parsing-csv" title="analyser CSV">
    Si votre système envoie un log de valeurs séparées par des virgules (CSV) et que vous devez les analyser dans New Relic, vous pouvez utiliser le [type Grok](#grok-syntax) `csv` , qui analyse le CSV capturé par le modèle Grok. Ce format repose sur 3 parties principales : la syntaxe Grok, le préfixe que vous souhaitez attribuer à l&apos;attribut CSV analysé et le [type Grok](#grok-syntax) `csv` . En utilisant le [type Grok](#grok-syntax) `csv` , vous pouvez extraire et analyser le CSV du log.

    Étant donné la ligne log CSV suivante à titre d’exemple :

    ```
    "2015-05-13T23:39:43.945958Z,202,POST,/shopcart/checkout,142,10"
    ```

    Et une règle d&apos;analyse avec la forme suivante :

    ```
    %{GREEDYDATA:log:csv({"columns": ["timestamp", "status", "method", "url", "time", "bytes"]})}
    ```

    Analysera votre log comme suit :

    ```
    log.timestamp: "2015-05-13T23:39:43.945958Z"
    log.status: "202"
    log.method: "POST"
    log.url: "/shopcart/checkout"
    log.time: "142"
    log.bytes: "10"
    ```

    Si vous devez omettre le préfixe « log », vous pouvez inclure le `"noPrefix": true` dans la configuration.

    ```
    %{GREEDYDATA:log:csv({"columns": ["timestamp", "status", "method", "url", "time", "bytes"], "noPrefix": true})}
    ```

    #### configuration des colonnes :

    * Il est obligatoire d&apos;indiquer les colonnes dans la configuration de type CSV Grok (qui doit être un JSON valide).
    * Vous pouvez ignorer n&apos;importe quelle colonne en définissant « \_ » (trait de soulignement) comme nom de colonne pour la supprimer de l&apos;objet résultant.

    #### Options de configuration facultatives :

    Bien que la configuration des « colonnes » soit obligatoire, il est possible de modifier l&apos;analyse du CSV avec les paramètres suivants.

    * <DNT>**dropOriginal**</DNT>: (La valeur par défaut est `true`) Supprimez le snippet CSV utilisé dans l&apos;analyse. Lorsque la valeur est `true` (valeur par défaut), la règle d&apos;analyse supprime le champ d&apos;origine.
    * <DNT>**noPrefix**</DNT>: (La valeur par défaut est `false`) N&apos;inclut pas le nom du champ Grok comme préfixe sur l&apos;objet résultant.
    * <DNT>**separator**</DNT>: (Par défaut `,`) Définit le caractère/la chaîne qui divise chaque colonne.
      * Un autre scénario courant est celui des valeurs séparées par des tabulations (TSV), pour cela vous devez indiquer `\t` comme séparateur, ex. `%{GREEDYDATA:log:csv({"columns": ["timestamp", "status", "method", "url", "time", "bytes"], "separator": "\t"})`
    * <DNT>**quoteChar**</DNT>: (Par défaut `"`) Définit le caractère qui entoure éventuellement le contenu d&apos;une colonne.
  </Collapser>

  <Collapser id="geo" title="Géolocalisation des adresses IP (GeoIP)">
    Si votre système envoie un log contenant des adresses IPv4, New Relic peut les localiser géographiquement et enrichir l&apos;événement de log avec l&apos;attribut spécifié. Vous pouvez utiliser le [type Grok](#grok-syntax) `geo` , qui trouve la position d&apos;une adresse IP capturée par le modèle Grok. Ce format peut être configuré pour renvoyer un ou plusieurs champs liés à l&apos;adresse, tels que la ville, le pays et la latitude/longitude de l&apos;IP.

    Étant donné la ligne log suivante à titre d&apos;exemple :

    ```
    2015-05-13T23:39:43.945958Z 146.190.212.184
    ```

    Et une règle d&apos;analyse avec la forme suivante :

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:ip:geo({"lookup":["city","region","countryCode", "latitude","longitude"]})}
    ```

    Nous allons analyser votre log comme suit :

    ```
    ip: 146.190.212.184
    ip.city: North Bergen
    ip.countryCode: US
    ip.countryName: United States
    ip.latitude: 40.793
    ip.longitude: -74.0247
    ip.postalCode: 07047
    ip.region: NJ
    ip.regionName: New Jersey
    containerTimestamp:2015-05-13T23:39:43.945958Z
    ISO8601_TIMEZONE:Z
    ```

    #### configuration de la recherche :

    Il est obligatoire de spécifier les champs `lookup` souhaités renvoyés par l&apos;action `geo` . Au moins un élément est requis parmi les options suivantes.

    * <DNT>**city**</DNT>: Nom de la ville
    * <DNT>**countryCode**</DNT>: Abréviation du pays
    * <DNT>**countryName**</DNT>: Nom du pays
    * <DNT>**latitude**</DNT>: Latitude
    * <DNT>**longitude**</DNT>: Longitude
    * <DNT>**postalCode**</DNT>: Code postal, code zip ou similaire
    * <DNT>**region**</DNT>:Abréviation d&apos;État, de province ou de territoire
    * <DNT>**regionName**</DNT>: Nom de l&apos;État, de la province ou du territoire
  </Collapser>

  <Collapser id="parsing-key-value-pairs" title="analyser la valeur clé Pairs">
    Le pipeline New Relic Logs analyse votre message de log par défaut, mais parfois vous avez des messages de log formatés sous forme de paires valeur/clé. Dans cette situation, vous souhaiterez peut-être pouvoir les analyser, puis pouvoir filtrer à l&apos;aide de l&apos;attribut valeur clé.

    Dans ce cas, vous pouvez utiliser le [type Grok](#grok-syntax) `keyvalue`, qui analysera les paires clé-valeur capturées par le motif Grok. Ce format repose sur 3 parties principales : la syntaxe Grok, le préfixe que vous souhaitez attribuer aux attributs clé-valeur analysés et le `keyvalue` [type Grok](#grok-syntax). En utilisant le [type Grok](#grok-syntax) `keyvalue`, vous pouvez extraire et analyser des paires clé-valeur à partir de logs qui ne sont pas correctement formatés ; par exemple, si vos logs sont préfixés par une chaîne date/heure :

    ```json
      2015-05-13T23:39:43.945958Z key1=value1,key2=value2,key3=value3
    ```

    Pour extraire et analyser les données clé-valeur de ce format de log, créez l&apos;expression Grok suivante :

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:keyvalue()}
    ```

    Le log résultant est :

    ```
      containerTimestamp: "2015-05-13T23:39:43.945958Z"
      my_attribute_prefix.key1: "value1"
      my_attribute_prefix.key2: "value2"
      my_attribute_prefix.key3: "value3"
    ```

    Vous pouvez également définir le délimiteur et le séparateur personnalisés pour extraire les paires valeur-clé requises.

    ```json
    2015-05-13T23:39:43.945958Z event:TestRequest request:bar
    ```

    Par exemple, avec l&apos;expression Grok suivante :

    ```
      %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:keyvalue({"delimiter": " ", "keyValueSeparator": ":"})}
    ```

    Le log résultant est :

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    my_attribute_prefix.event: "TestRequest"
    my_attribute_prefix.request: "bar"
    ```

    Si vous souhaitez omettre le préfixe `my_attribute_prefix` , vous pouvez inclure le `"noPrefix": true` dans la configuration.

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:keyValue({"noPrefix": true})}
    ```

    Le log résultant est :

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    event: "TestRequest"
    request: "bar"
    ```

    Si vous souhaitez définir votre préfixe de caractère de citation personnalisé, vous pouvez inclure « quoteChar » : dans la configuration.

    ```json
    2015-05-13T23:39:43.945958Z nbn_demo='INFO',message='This message contains information with spaces ,sessionId='abc123'
    ```

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:keyValue({"quoteChar": "'"})}
    ```

    Le log résultant est :

    ```
    "my_attribute_prefix.message": "'This message contains information with spaces",
    "my_attribute_prefix.nbn_demo": "INFO",
    "my_attribute_prefix.sessionId": "abc123"
    ```

    #### Paramètres du modèle Grok

    Vous pouvez personnaliser le comportement de l&apos;analyse avec les options suivantes en fonction de vos formats log :

    * **délimiteur**

      * **Description :** Chaîne séparant chaque paire de valeur clé.
      * **Valeur par défaut :** `,` (virgule)
      * **Remplacement :** définissez le champ `delimiter` pour modifier ce comportement.

    * **Séparateur de valeur clé**

      * **Description :** Chaîne utilisée pour attribuer des valeurs aux clés.
      * **Valeur par défaut :** `=`
      * **Remplacement :** définissez le champ `keyValueSeparator` pour l&apos;utilisation d&apos;un séparateur personnalisé.

    * **citationChar**

      * **Description :** Caractère utilisé pour entourer des valeurs avec des espaces ou des caractères spéciaux.
      * **Valeur par défaut :** `"` (guillemets doubles)
      * **Remplacement :** définissez un caractère personnalisé à l’aide de `quoteChar`.

    * **dropOriginal**

      * **Description :** Supprime le message d&apos;origine du log après analyse. Utile pour réduire le stockage log .
      * **Valeur par défaut :** `true`
      * **Remplacement :** définissez `dropOriginal` sur `false` pour conserver le message d&apos;origine du log.

    * **pas de préfixe**

      * **Description :** Lorsque `true`, exclut le nom du champ Grok comme préfixe dans l&apos;objet résultant.
      * **Valeur par défaut :** `false`
      * **Remplacement :** activer en définissant `noPrefix` sur `true`.

    * **échapperChar**

      * **Description :** Définissez un caractère d&apos;échappement personnalisé pour gérer les caractères log spéciaux.
      * **Valeur par défaut :** « » (barre oblique inverse)
      * **Remplacer :** Personnaliser avec `escapeChar`.

    * **trimValeurs**

      * **Description :** permet de supprimer les valeurs contenant des espaces.
      * **Valeur par défaut :** `false`
      * **Remplacement :** définissez `trimValues` sur `true` pour activer le rognage.

    * **touches de finition**

      * **Description :** Permet de rogner les clés contenant des espaces.
      * **Valeur par défaut :** `true`
      * **Remplacement :** définissez `trimKeys` sur `true` pour activer le rognage.
  </Collapser>

  <Collapser id="grok-patterns" title="Motifs Grok pris en charge">
    New Relic prend en charge les patterns Grok suivants :

    * IP
    * TIMESTAMP\_ISO8601
    * HTTPDATE
    * TIME
    * UUID
    * MONTH
    * SPACE
    * DATESTAMP
    * DATE
    * COMBINEDAPACHELOG
    * ISO8601\_TIMEZONE
    * MAC
    * DATE\_EU
    * TZ
    * DATE\_US
    * DAY
    * LOGLEVEL
    * NUMBER
    * INT
    * QUOTEDSTRING
    * SYSLOGTIMESTAMP
    * PATH
    * SYSLOGBASE
    * COMMONAPACHELOG
    * IPV6
    * COMMONMAC
    * DATESTAMP\_OTHER
    * ISO8601\_SECOND
    * DATESTAMP\_EVENTLOG
    * SYSLOGBASE2
    * HAPROXYHTTP
    * RUBY\_LOGGER
    * WINDOWSMAC
    * WORD
    * DATA
    * GREEDYDATA
    * NOTSPACE
    * BASE16FLOAT
    * QS
    * BASE10NUM
    * USER
    * IPORHOST
    * USERNAME
    * IPV4
    * MONTHDAY
    * YEAR
    * HOSTNAME
    * POSINT
    * URIPATHPARAM
    * URI
    * URIPATH
    * MONTHNUM
    * NONNEGINT
    * MINUTE
    * SECOND
    * HOUR
    * URIHOST
    * URIPROTO
    * URIPARAM
    * SYSLOGHOST
    * BASE16NUM
    * SYSLOGPROG
    * HÔTE
    * HOSTPORT
    * JAVACLASS
    * PROG
    * UNIXPATH
    * WINPATH
    * MONTHNUM2
    * RUBY\_LOGLEVEL
    * SYSLOGFACILITY
    * CRON\_ACTION
    * HAPROXYCAPTUREDREQUESTHEADERS
    * HAPROXYCAPTUREDRESPONSEHEADERS
    * HAPROXYDATE
    * CISOMAC
  </Collapser>
</CollapserGroup>

## Gérer les règles d&apos;analyse [#manage-rules]

Après avoir créé des règles d&apos;analyse, vous pouvez les gérer depuis <DNT>**Logs &gt; Parsing**</DNT>. Les règles brouillon sont enregistrées mais pas encore activées. Vous pouvez les activer lorsque vous êtes prêt à les appliquer aux logs entrants.

Pour modifier une règle d&apos;analyse :

1. Dans votre liste de règles de parsing, cliquez sur le nom de la règle ou cliquez sur <DNT>**... &gt; Edit**</DNT> et effectuez les modifications nécessaires. Pour passer à l&apos;éditeur de code, cliquez sur <DNT>**Write your own rule**</DNT> pour écrire ou modifier directement des motifs Grok/Regex.
2. Cliquez sur <DNT>**Save rule**</DNT> (ou <DNT>**Save as draft**</DNT> si vous souhaitez le laisser désactivé).

Les modifications s&apos;appliquent aux logs ingérés après la mise à jour. Pour activer, désactiver ou supprimer une règle de parsing :

1. Trouvez la règle dans votre liste de règles de parsing et cliquez sur le menu <DNT>**...**</DNT>.

2. Choisissez une action :

   * <DNT>**Enable:**</DNT> Active la règle brouillon (s&apos;applique immédiatement aux logs nouvellement ingérés)
   * <DNT>**Disable:**</DNT> Met temporairement en pause la règle active
   * <DNT>**Delete:**</DNT> Supprime complètement la règle

## Limites

L&apos;analyse est gourmande en ressources de calcul. Pour garantir la stabilité de la plateforme, New Relic applique ce qui suit :

* **Limite par message**: Une règle dispose de 100 ms pour analyser un seul message. Si cette limite est dépassée, l&apos;analyse s&apos;arrête pour ce message.
* **Limite par compte**: Le temps de traitement total est plafonné par minute. Si vous atteignez cette limite, les logs restent non analysés (stockés dans leur format d&apos;origine).
* **Chronologie du pipeline**: L&apos;analyse s&apos;effectue avant l&apos;enrichissement. Vous ne pouvez pas faire correspondre une règle de parsing à un attribut qui n&apos;a pas encore été ajouté (comme un tag ajouté plus tard dans le pipeline).
* **La règle de la première correspondance**: Les règles d&apos;analyse ne sont pas ordonnées. Si plusieurs règles correspondent à un même log, New Relic en applique une au hasard. Assurez-vous que vos clauses NRQL `WHERE` sont suffisamment spécifiques pour éviter les correspondances qui se chevauchent.

<Callout variant="tip">
  Pour vérifier facilement si vos limites de débit ont été atteintes, accédez à [la page <DNT>**Limits**</DNT> de votre système](/docs/telemetry-data-platform/ingest-manage-data/manage-data/view-system-limits#limits-ui) dans l&apos;interface utilisateur de New Relic.
</Callout>

## Dépannage [#troubleshooting]

Si l&apos;analyse ne fonctionne pas comme prévu, cela peut être dû à :

* <DNT>**Logic:**</DNT> La logique de correspondance de la règle d&apos;analyse ne correspond pas au log souhaité.
* <DNT>**Timing:**</DNT> Si votre règle de correspondance d&apos;analyse cible une valeur qui n&apos;existe pas encore, elle échouera. Cela peut se produire si la valeur est ajoutée plus tard dans le pipeline dans le cadre du processus d’enrichissement.
* <DNT>**Limits:**</DNT> Il y a une quantité de temps fixe disponible chaque minute pour traiter le log via l&apos;analyse, les modèles, les filtres de suppression, etc. Si le temps maximum a été écoulé, l&apos;analyse sera ignorée pour les enregistrements d&apos;événements de log supplémentaires.

Pour résoudre ces problèmes, créez ou ajustez vos [règles d&apos;analyse personnalisées](#custom-parsing).

## Documentation associée [#related-docs]

<DocTiles>
  <DocTile title="Règles d'analyse des logintégrées" path="/docs/logs/ui-data/built-log-parsing-rules">
    Explorez les modèles prédéfinis de New Relic.
  </DocTile>

  <DocTile title="données log de requête" path="/docs/logs/log-management/ui-data/query-logs">
    Utilisez des attributs analysés dans les requêtes NRQL.
  </DocTile>
</DocTiles>