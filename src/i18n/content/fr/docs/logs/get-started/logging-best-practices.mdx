---
title: Guide des bonnes pratiques en matière d'exploitation forestière
tags:
  - New Relic solutions
  - Best practices guides
  - Logs
  - Logging
metaDescription: Best practices for using New Relic logs
freshnessValidatedDate: never
translationType: machine
---

Bienvenue dans le guide des bonnes pratiques de logging de New Relic. Vous trouverez ici des recommandations détaillées sur la façon d&apos;optimiser notre fonctionnalité de log et de gérer la consommation de données.

<Callout variant="tip">
  Vous avez beaucoup de logs ? Consultez notre [tutoriel pour savoir comment les optimiser et les gérer](/docs/tutorial-large-logs/get-started-managing-large-logs/).
</Callout>

## Log de transfert [#forwarding-logs]

Voici quelques conseils sur le transfert de logpour compléter notre [documentation sur le transfert de log](/docs/logs/forward-logs/enable-log-management-new-relic):

* Lors du transfert du log, nous vous recommandons d&apos;utiliser notre [agent New Relic Infrastructure](/docs/logs/forward-logs/forward-your-logs-using-infrastructure-agent) et/ou [nos agents APM](/docs/apm/new-relic-apm/getting-started/get-started-logs-context#agents). Si vous ne pouvez pas utiliser les agents New Relic, utilisez d’autres agents pris en charge (comme FluentBit, Fluentd et Logstash).

  Voici quelques exemples de configuration Github pour les agents de logging pris en charge :

  * [Exemples de FluentBit](https://github.com/newrelic/fluentbit-examples)

  * [Exemples de Fluentd](https://github.com/newrelic/fluentd-examples/)

  * [Exemples de Logstash](https://github.com/newrelic/logstash-examples)

    <Callout variant="important">
      Si vos logs sont stockés dans un répertoire sur un hôte/conteneur sous-jacent et sont instrumentés par notre agent infrastructure pour collecter les logs, vous pouvez voir des logs en double collectés à la fois par l&apos;agent infrastructure et l&apos;agent <InlinePopover type="apm" />. Pour éviter d&apos;envoyer des logs en double, consultez nos recommandations dans [ce guide](/docs/logs/logs-context/upgrade-to-automatic-logs-context).
    </Callout>

* Ajoutez un attribut `logtype` à toutes les données que vous transférez. L&apos;attribut <DNT>**required**</DNT> permet d&apos;utiliser nos règles d&apos;analyse intégrées et peut également être utilisé pour créer des règles d&apos;analyse personnalisées en fonction du type de données. L&apos;attribut `logtype` est considéré comme un attribut bien connu et est utilisé dans notre dashboard quickstart pour les informations récapitulatives log .

* Utilisez nos [règles d’analyse intégrées](/docs/logs/ui-data/built-log-parsing-rules) pour les types de logs connus. Nous analyserons automatiquement les logs de nombreux types log différents et bien connus lorsque vous définissez l&apos;attribut `logtype` approprié.

  Voici un exemple de la manière d&apos;ajouter l&apos;attribut `logtype` à un log transmis par notre agent infrastructure :

  ```yml
  logs:
    - name: mylog
      file: /var/log/mylog.log
      attributes:
        logtype: mylog
  ```

* Utilisez l&apos;intégration New Relic pour transférer le log d&apos;autres types de données courants tels que :

  * environnements conteneurisés : [Kubernetes (K8S)](/docs/kubernetes-pixie/kubernetes-integration/get-started/introduction-kubernetes-integration)
  * Intégration du fournisseur de cloud : [AWS](/docs/infrastructure/amazon-integrations/get-started/introduction-aws-integrations/), [Azure](/docs/infrastructure/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations) ou [GCP](/docs/infrastructure/google-cloud-platform-integrations/get-started/introduction-google-cloud-platform-integrations)
  * L&apos;une de nos autres [intégrations sur hôte prises en charge avec logging](/docs/infrastructure/host-integrations/get-started/introduction-host-integrations)

## Partitions de données [#partitions]

Si vous consommez ou prévoyez de consommer une quantité importante de données log chaque jour, vous devez absolument travailler sur un plan de gouvernance d&apos;ingestion pour les logs, y compris un plan de partitionnement des données de manière à fournir des regroupements fonctionnels et thématiques. Vous pouvez obtenir des améliorations de performances significatives grâce à une utilisation appropriée des partitions de données. Si vous envoyez tous vos logs vers un « bucket » géant (la partition log par défaut) dans un seul compte, vous risquez de rencontrer des requêtes lentes ou des requêtes échouées puisque vous devrez analyser tous les logs de votre compte pour renvoyer les résultats de chaque requête. Pour plus de détails, consultez [Limites de taux de requête NRQL](/docs/query-your-data/nrql-new-relic-query-language/get-started/rate-limits-nrql-queries/#query-limits).

Une façon d’améliorer les performances des requêtes consiste à limiter la plage horaire recherchée. La recherche de logs sur de longues périodes de temps renverra plus de résultats et nécessitera plus de temps. Évitez les recherches sur de longues fenêtres temporelles lorsque cela est possible et utilisez le sélecteur de plage temporelle pour restreindre les recherches à des fenêtres temporelles plus petites et plus spécifiques.

Une autre façon d’améliorer les performances de recherche est d’utiliser [des partitions de données](/docs/logs/ui-data/data-partitions/). Voici quelques bonnes pratiques pour les partitions de données :

* Assurez-vous d’utiliser des partitions au début de votre processus d’intégration des logs. Créez une stratégie d&apos;utilisation des partitions afin que votre utilisateur sache où rechercher et trouver un log spécifique. De cette façon, vos alertes, votre dashboard et vos vues enregistrées n&apos;ont pas besoin d&apos;être modifiés si vous implémentez des partitions plus tard dans votre parcours de log.

* Créez des partitions de données qui correspondent aux catégories de votre environnement ou de votre organisation qui sont statiques ou qui changent rarement (par exemple, par unité commerciale, équipe, environnement, service, etc.).

* Créez des partitions pour optimiser le nombre d&apos;événements qui doivent être analysés pour votre requête la plus courante. Si vous avez un volume d&apos;ingestion élevé, vous aurez plus d&apos;événements dans des fenêtres de temps plus courtes, ce qui entraînera des recherches sur des périodes plus longues qui prendront plus de temps et risqueront d&apos;expirer. Il n&apos;y a pas de règle absolue, mais en général, un événement de log « tel que scanné » obtient plus de 500 millions (en particulier plus d&apos;un milliard). Pour les requêtes courantes, vous souhaiterez peut-être envisager d&apos;ajuster votre partitionnement.

* Même si votre volume d&apos;ingestion est faible, vous pouvez également utiliser des partitions de données pour une séparation logique des données ou simplement pour améliorer les performances des requêtes sur des types de données distincts.

* Pour [rechercher des partitions de données](/docs/logs/ui-data/data-partitions#search) dans l&apos;interface utilisateur <DNT>**Logs**</DNT> , vous devez sélectionner la ou les partitions appropriées, ouvrir le sélecteur de partition et cocher les partitions que vous souhaitez rechercher. Si vous utilisez NRQL, utilisez la clause `FROM` pour spécifier le `Log` ou `Log_<partion>` à rechercher. Par exemple:

  ```sql
  FROM Log_<my_partition_name> SELECT * SINCE 1 hour ago
  ```

  Ou pour rechercher un log sur plusieurs partitions :

  ```sql
  FROM Log, Log_<my_partition_name> SELECT * SINCE 1 hour ago
  ```

## Analyser le log [#parsing-logs]

Analyser votre log lors de l&apos;ingestion est le meilleur moyen de rendre vos données log plus utilisables par vous et les autres utilisateurs de votre organisation. Lorsque vous analysez un attribut, vous pouvez facilement l&apos;utiliser pour effectuer une recherche dans l&apos;interface utilisateur <DNT>**Logs**</DNT> et dans NRQL sans avoir à analyser les données au moment de la requête. Cela vous permet également de les utiliser facilement dans <InlinePopover type="alerts" />et le dashboard.

Pour analyser le log, nous vous recommandons de :

* Analysez votre log lors de l&apos;ingestion pour créer `attributes` (ou des champs), que vous pouvez utiliser lors de la recherche ou de la création <InlinePopover type="dashboards" />et d&apos;alertes. L&apos;attribut peut être des chaînes de données ou des valeurs numériques.

* Utilisez l’attribut `logtype` que vous avez ajouté à votre log lors de l’ingestion, ainsi que d’autres clauses NRQL `WHERE` pour faire correspondre les données que vous souhaitez analyser. Écrivez des règles de correspondance spécifiques pour filtrer le log aussi précisément que possible. Par exemple:

  ```sql
  WHERE logtype='mylog' AND message LIKE '%error%'
  ```

* Utilisez nos [règles d’analyse intégrées](/docs/logs/ui-data/built-log-parsing-rules/) et l’attribut `logtype` associé chaque fois que possible. Si les règles intégrées ne fonctionnent pas pour vos données, utilisez un nom d&apos;attribut `logtype` différent (c&apos;est-à-dire `apache_logs` contre `apache`, `iis_w3c_custom` contre `iis_w3c`), puis créez une nouvelle règle d&apos;analyse dans l&apos;interface utilisateur à l&apos;aide d&apos;une version modifiée des règles intégrées afin qu&apos;elle fonctionne pour votre format de données log .

* Utilisez notre interface utilisateur <DNT>**Parsing**</DNT> pour tester et valider vos règles Grok. En utilisant l&apos;option `Paste log`, vous pouvez coller l&apos;un de vos messages de log pour tester votre expression Grok avant de créer et d&apos;enregistrer une règle d&apos;analyse permanente.

  <img title="parsing example" alt="Parsing example in the UI" src="/images/logs_screenshot-full_parsing-example.webp" />

* Utilisez la configuration externe FluentBit pour analyser le log multiligne et pour d&apos;autres pré-analyses plus approfondies avant l&apos;ingestion dans New Relic. Pour plus de détails et la configuration de l&apos;analyse multiligne avec notre agent d&apos;infrastructure, consultez [cet article de blog](https://newrelic.com/blog/how-to-relic/parse-multiline-log-messages-fluent-bit-plugin).

* Créez des modèles Grok optimisés pour correspondre au log filtré afin d&apos;extraire l&apos;attribut. Évitez d&apos;utiliser de manière excessive des modèles Grok coûteux comme GREEDYDATA. Si vous avez besoin d’aide pour identifier des règles d’analyse sous-optimales, contactez votre représentant de compte New Relic.

### GROK bonnes pratiques

* Utilisez les types Grok pour spécifier le type de valeur d&apos;attribut à extraire. Si elles sont omises, les valeurs sont extraites sous forme de chaînes. Ceci est particulièrement important pour les valeurs numériques si vous souhaitez pouvoir utiliser les fonctions NRQL (c&apos;est-à-dire `monthOf()`, `max()`, `avg()`, `>`, `<`, etc.) sur ces attributs.
* Utilisez l&apos;interface utilisateur <DNT>**Parsing**</DNT> pour tester vos modèles Grok. Vous pouvez coller un exemple de log dans l&apos;interface utilisateur <DNT>**Parsing**</DNT> pour valider vos modèles Grok ou Regex et s&apos;ils extraient l&apos;attribut comme prévu.
* Ajoutez des ancres à la logique d&apos;analyse (par exemple, `^`) pour indiquer le début d&apos;une ligne ou `$` à la fin d&apos;une ligne.
* Utilisez `()?` autour d&apos;un modèle pour identifier les champs facultatifs
* Évitez d&apos;utiliser à outrance des modèles Grok coûteux comme `'%{GREEDYDATA}`. Essayez de toujours utiliser un modèle Grok et un type Grok valides lors de l&apos;extraction d&apos;un attribut.

## Règles de filtrage des abandons [#drop-rules]

### Supprimer le log lors de l&apos;ingestion

* Créez [des règles de filtrage](/docs/logs/ui-data/drop-data-drop-filter-rules#create) pour supprimer les logs qui ne sont pas utiles ou qui ne sont pas nécessaires pour satisfaire les cas d&apos;utilisation du dashboard, des alertes ou du dépannage.

### Supprimer l&apos;attribut de votre log lors de l&apos;ingestion

* Créez des règles de suppression pour supprimer les attributs inutilisés de votre log.
* Supprimez l’attribut `message` après l’analyse. Si vous analysez l&apos;attribut de message pour créer un nouvel attribut à partir des données, supprimez le champ de message.
* Exemple : si vous transférez des données depuis infrastructure AWS, vous pouvez créer des règles de suppression pour supprimer tout attribut AWS susceptible de créer un gonflement indésirable des données.

## Dimensionnement des logs New Relic [#sizing]

* Notre façon de facturer le stockage peut différer de celle de certains de nos concurrents. La manière dont nous mesurons les données log est similaire à la manière dont nous mesurons et facturons d&apos;autres types de données, qui sont définies dans [Calcul d&apos;utilisation](/docs/accounts/accounts-billing/new-relic-one-pricing-billing/data-ingest-billing#usage-calculation).

* Si nos cloud d&apos;intégration (AWS, Azure, GCP) sont installés, nous ajouterons des métadonnées cloud à chaque enregistrement log , ce qui augmentera la facture d&apos;ingestion globale. Ces données peuvent cependant être supprimées pour réduire l&apos;ingestion.

* Les principaux facteurs de surcharge des données log sont les suivants, par ordre d&apos;impact :

  * Intégration dans le cloud
  * Formatage JSON
  * modèles de log (vous pouvez [désactiver/activer les modèles dans l&apos;interface utilisateur <DNT>**Logs**</DNT> ](/docs/logs/ui-data/find-unusual-logs-log-patterns#availability).)

## Logs de recherche [#searching-logs]

* Pour les recherches log courantes, créez et utilisez <DNT>**Saved views**</DNT> dans l&apos;interface utilisateur. Créez une recherche pour vos données et cliquez sur <DNT>**+ Add Column**</DNT> pour ajouter un attribut supplémentaire à la table de l&apos;interface utilisateur. Vous pouvez ensuite déplacer les colonnes afin qu&apos;elles apparaissent dans l&apos;ordre souhaité, puis les enregistrer en tant que vue enregistrée avec des autorisations privées ou publiques. Configurez les vues enregistrées pour qu&apos;elles soient publiques afin que vous et d&apos;autres utilisateurs puissiez facilement exécuter des recherches courantes avec toutes les données d&apos;attribut pertinentes affichées. Il s&apos;agit d&apos;une bonne pratique pour les applications tierces comme Apache, nginx, etc. afin que l&apos;utilisateur puisse facilement voir ces logs sans effectuer de recherche.

* Utilisez le [générateur de requêtes](/docs/query-your-data/explore-query-data/query-builder/introduction-query-builder) pour exécuter des recherches à l&apos;aide de NRQL, en utilisant ses fonctions avancées. Pour interroger les logs de plusieurs comptes et les identifier avec leurs identifiants de compte correspondants, incluez `accountId() as accountId` dans l&apos;instruction `SELECT` de votre requête NRQL .

* Créez <InlinePopover type="dashboards" />ou utilisez [le dashboardquickstart ](https://newrelic.com/instant-observability)disponible pour répondre aux questions courantes sur votre log et pour examiner les données de votre log au fil du temps dans des graphiques de séries chronologiques. Créez un dashboard avec plusieurs panneaux pour découper et analyser vos données log de différentes manières.

* Utilisez nos fonctions NRQL avancées telles que [capture()](/docs/query-your-data/nrql-new-relic-query-language/get-started/nrql-syntax-clauses-functions#func-capture) ou [aparse()](/docs/query-your-data/nrql-new-relic-query-language/get-started/nrql-syntax-clauses-functions#func-aparse) pour analyser les données au moment de la recherche.

* Installez le dashboard <DNT>**Logs analysis**</DNT> et/ou <DNT>**APM logs monitoring quickstart**</DNT> pour obtenir rapidement des informations plus détaillées sur vos données log . Pour ajouter ces dashboards, accédez à <DNT>**[one.newrelic.com](https://one.newrelic.com) &gt; Integrations &amp; Agents &gt; Logging &gt; Dashboards**</DNT>.

## Quelle est la prochaine étape ?

Voir [Démarrer avec <InlinePopover type="logs" />](/docs/logs/get-started/get-started-log-management/).