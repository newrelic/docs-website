---
title: Transférer le log depuis Google Cloud Platform
tags:
  - Logs
  - Enable log management in New Relic
  - Enable log monitoring in New Relic
  - GCP
  - Dataflow
metaDescription: 'Configure New Relic logging for a Google Cloud Platform Pub/Sub topic, so you can use enhanced log management capabilities.'
freshnessValidatedDate: never
translationType: machine
---

Nous prenons en charge deux méthodes différentes pour transférer votre log d&apos;une rubrique Google Cloud Platform Pub/Sub vers New Relic.

## Sélectionnez une option [#gcp-options]

Les éléments suivants peuvent vous aider à décider quelle option convient le mieux aux besoins de votre entreprise.

<table>
  <thead>
    <tr>
      <th style={{ width: "200px" }}>
        Options de transfert de logGCP
      </th>

      <th>
        Considérations
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        API sans en-tête
      </td>

      <td>
        * Idéal pour les faibles volumes log , car il effectue un appel d&apos;API pour chaque enregistrement log qu&apos;il envoie.
        * N&apos;entraîne aucun coût supplémentaire pour votre abonnement GCP.
        * Si le volume de vos log augmente, cette solution peut atteindre les limites de quota de votre compte New Relic.
      </td>
    </tr>

    <tr>
      <td>
        Tâche de flux de données
      </td>

      <td>
        * Idéal pour les volumes log plus importants, car il regroupe les enregistrements log par lots avant de les envoyer à New Relic.
        * Réduit le nombre d&apos;appel d&apos;API et vous permet de réduire votre utilisation de quota.
        * Des frais supplémentaires peuvent être occasionnés par votre abonnement GCP, en raison de l&apos;exécution d&apos;une tâche Dataflow dans vos locaux.
      </td>
    </tr>
  </tbody>
</table>

## Utiliser une API sans en-tête [#gcp-headerless-api]

Pour envoyer votre log GCP à New Relic à l&apos;aide de notre API sans en-tête :

<CollapserGroup>
  <Collapser id="ingest-url" title="1. Générez une URL d’ingestion GCP Pub/Sub.">
    Commencez par créer une URL d&apos;ingestion pour votre rubrique GCP Pub/Sub :

    1. Accédez à l’ [interface utilisateur<DNT>**Integrations &amp; Agents**</DNT> ](https://one.newrelic.com/marketplace), cliquez sur <DNT>**Logging**</DNT>, puis sur <DNT>**Google Cloud Platform**</DNT>.

    2. Sélectionnez le compte New Relic vers lequel vous souhaitez transférer le log et cliquez sur <DNT>**Continue**</DNT>.

    3. Facultatif : configurez les métadonnées (paires attribute-value) à inclure dans chaque événement de log envoyé à l&apos;URL d&apos;ingestion que vous générerez à l&apos;étape suivante.

    4. Cliquez sur <DNT>**Generate URL**</DNT>.

    5. Copiez votre <DNT>**ingest URL**</DNT> nouvellement généré et conservez-le dans un endroit sûr.

       Vous utiliserez votre nouvelle URL d’ingestion pour configurer une rubrique Pub/Sub qui envoie le log à New Relic.
  </Collapser>

  <Collapser id="create-gcp-topic" title="2. Créez une rubrique GCP Pub/Sub.">
    Ensuite, vous allez créer la rubrique GCP Pub/Sub que votre URL d’ingestion utilisera.

    <img title="Create GCP topic" alt="Create GCP topic" src="/images/logs_screenshot-full_GCP-logs-create-topic.webp" />

    1. Accédez à la [console GCP Pub/Sub](https://console.cloud.google.com/cloudpubsub/topic/list).
    2. Cliquez sur <DNT>**Create Topic**</DNT>.
    3. Saisissez un <DNT>**Topic ID**</DNT> significatif, puis configurez d’autres options selon vos besoins.
    4. Cliquez sur <DNT>**Create Topic**</DNT>.
  </Collapser>

  <Collapser id="configure-gcp-log-forwarding" title="3. Préparez la rubrique GCP Pub/Sub pour transférer le log vers New Relic.">
    Une fois que vous avez créé votre sujet Pub/Sub, créez un abonnement pour celui-ci.

    <img title="Create GCP Pub/Sub topic subscription" alt="Create GCP Pub/Sub topic subscription" src="/images/logs_screenshot-crop_GCP-create-subscription.webp" />

    1. Revenir à la [console GCP Pub/Sub](https://console.cloud.google.com/cloudpubsub/topic/list).

    2. Cliquez sur la [rubrique Pub/Sub](#create-gcp-topic) que vous avez créée précédemment.

    3. Faites défiler vers le bas et sélectionnez l’onglet <DNT>**Subscriptions**</DNT> , puis cliquez sur <DNT>**Create Subscription**</DNT> et sélectionnez <DNT>**Create a simple subscription**</DNT>.

       <img title="Select GCP Pub/Sub topic subscription type" alt="Select GCP Pub/Sub topic subscription type" src="/images/logs_screenshot-crop_GCP-create-subscription-type.webp" />

    4. Entrez un <DNT>**Subscription ID**</DNT>. Ensuite, sous <DNT>**Delivery Type**</DNT>, sélectionnez <DNT>**Push**</DNT>.

    5. Dans le champ <DNT>**Endpoint URL**</DNT> , collez l’ [URL d’ingestion](#ingest-url) que vous avez générée précédemment.

    6. Configurez les paramètres restants selon vos besoins et cliquez sur <DNT>**Create**</DNT>.
  </Collapser>

  <Collapser id="forward-logs" title="4. Créez un récepteur de routage pour transférer le log vers New Relic.">
    Pour terminer la configuration, créez un récepteur de routage pour votre rubrique GCP Pub/Sub qui transmettra votre log à New Relic.

    <img title="Create GCP Pub/Sub logs router sink" alt="Create GCP Pub/Sub logs router sink" src="/images/logs_screenshot-crop_GCP-create-sink.webp" />

    1. Accédez à la [console du routeur du log GCP](https://console.cloud.google.com/logs/router).
    2. Cliquez sur <DNT>**Create Sink**</DNT>.
    3. Entrez un <DNT>**Sink name**</DNT> et <DNT>**Sink description**</DNT>, puis cliquez sur <DNT>**Next**</DNT>.
    4. Sous <DNT>**Select sink service**</DNT>, sélectionnez <DNT>**Cloud Pub/Sub topic**</DNT> et sélectionnez la [rubrique Pub/Sub](#create-gcp-topic) que vous avez créée précédemment.
    5. Configurez les filtres restants selon vos besoins, puis cliquez sur <DNT>**Create sink**</DNT> pour terminer la configuration.
  </Collapser>
</CollapserGroup>

## Utiliser le travail Dataflow [#gcp-dataflow]

Pour envoyer votre log GCP à New Relic à l’aide d’une tâche Dataflow, vous utiliserez notre modèle Dataflow. Avant de commencer, assurez-vous que vous disposez des outils suivants sur votre ordinateur local :

* Un terminal Unix pour Linux ou macOS
* [Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)
* [JDK Java 8](https://adoptopenjdk.net/index.html)
* [Apache Maven 3.2 ou supérieur](https://maven.apache.org/). Nous avons vu des versions antérieures échouer pendant le processus de compilation.
* Le [Google Cloud SDK](https://cloud.google.com/sdk/docs/install), qui comprend les outils de ligne de commande `gcloud` et `gsutil`

<CollapserGroup>
  <Collapser id="dataflow-login-gcp" title="1. Connectez-vous à votre compte Google Cloud Platform .">
    Exécutez la commande suivante et suivez l’invite pour vous connectez à GCP et sélectionner votre projet cloud :

    ```bash
    gcloud init
    ```

    À l&apos;aide de l&apos;assistant, vous sélectionnerez un projet cloud à utiliser et vous pourrez éventuellement sélectionner une région et une zone de calcul par défaut pour les ressources que vous créez à l&apos;aide de `gcloud` ou `gsutil`. Nous ne présumerons pas de projet, d’emplacement ou de région par défaut pour les commandes suivantes.
  </Collapser>

  <Collapser id="dataflow-repo" title="2. Clonez le référentiel GitHub DataflowTemplates.">
    1. Clonez le référentiel GitHub DataflowTemplates à l&apos;aide de la commande suivante :

       ```bash
       git clone https://github.com/newrelic-forks/DataflowTemplates.git
       ```

    2. Entrez dans le répertoire que vous venez de créer :

       ```bash
       cd DataflowTemplates
       ```

       Passez ensuite à la section suivante pour exécuter des commandes supplémentaires.
  </Collapser>

  <Collapser id="dataflow-configuration" title="3. Compilez et exécutez le transitaire Dataflow.">
    Pour définir la configuration requise pour compiler et exécuter le transitaire Dataflow, exécutez les commandes suivantes dans votre répertoire `DataflowTemplates` . Les seules valeurs requises sont :

    * `PROJECT_ID`

    * `BUCKET_NAME`

    * `NR_LICENSE_KEY`

    * `INPUT_SUBSCRIPTION_NAME`

      Vous pouvez laisser les autres valeurs par défaut telles quelles.

      ```ini
      # The Google Cloud Platform project id where your logs are and where the Dataflow log forwarder will run
      PROJECT_ID=<your_project_id>
      # Temporary bucket that will store intermediary files as a result of compiling the Dataflow template. Its name must be unique.
      BUCKET_NAME=<a_nonexisting_gcs_bucket_name>
      # New Relic license key
      NR_LICENSE_KEY=<your_newrelic_license_key>
      # Input PubSub subscription to read logs from
      INPUT_SUBSCRIPTION_NAME=<your_pubsub_input_subscription_name>

      # Region where the created resources will be placed
      REGION=us-west1
      # Service account used to execute the Dataflow template
      SERVICE_ACCOUNT_NAME=nr-dataflow-forwarder-sa
      # File name where the service account credentials will be stored
      SERVICE_ACCOUNT_KEY_FILENAME=service-account-key.json
      # The name your Dataflow log forwarder job will have
      JOB_NAME=nr-log-forwarder
      ```
  </Collapser>

  <Collapser id="dataflow-bucket" title="4. Créez un bucket GCP pour le modèle Dataflow.">
    Créez un bucket dans GCP pour contenir le modèle Dataflow généré en exécutant la commande suivante :

    ```bash
    gsutil mb -p ${PROJECT_ID} -l ${REGION} gs://${BUCKET_NAME}
    ```
  </Collapser>

  <Collapser id="dataflow-account" title="5. Créez un compte de service.">
    Exécutez les commandes suivantes :

    1. Créer le compte de service :

       ```bash
       gcloud iam service-accounts create ${SERVICE_ACCOUNT_NAME}
       ```

    2. Accorder des autorisations au compte de service :

       ```bash
       gcloud projects add-iam-policy-binding ${PROJECT_ID} --member="serviceAccount:${SERVICE_ACCOUNT_NAME}@${PROJECT_ID}.iam.gserviceaccount.com" --role="roles/owner"
       ```

    3. Générer le fichier de clé de compte de service :

       ```bash
       gcloud iam service-accounts keys create ${SERVICE_ACCOUNT_KEY_FILENAME} --iam-account=${SERVICE_ACCOUNT_NAME}@${PROJECT_ID}.iam.gserviceaccount.com
       ```

    4. Faites référence à votre fichier de clé de compte de service à l’aide de la variable d’environnement `GOOGLE_APPLICATION_CREDENTIALS` , car elle sera utilisée par les commandes suivantes :

       ```bash
       export GOOGLE_APPLICATION_CREDENTIALS=${SERVICE_ACCOUNT_KEY_FILENAME}
       ```
  </Collapser>

  <Collapser id="dataflow-compile" title="6. Compilez et publiez le modèle PubsubToNewRelic.">
    Exécutez la commande suivante :

    ```bash
    mvn compile exec:java \
        -Dexec.mainClass=com.google.cloud.teleport.templates.PubsubToNewRelic \
        -Dexec.cleanupDaemonThreads=false \
        -Dexec.args=" \
            --project=${PROJECT_ID} \
            --region=${REGION} \
            --enableStreamingEngine \
            --stagingLocation=gs://${BUCKET_NAME}/staging/ \
            --gcpTempLocation=gs://${BUCKET_NAME}/temp/ \
            --templateLocation=gs://${BUCKET_NAME}/template/PubsubToNewRelic \
            --runner=DataflowRunner \
        "
    ```
  </Collapser>

  <Collapser id="dataflow-job" title="7. Exécutez le modèle en tant que tâche Dataflow.">
    Exécutez la commande suivante pour démarrer l&apos;expédition du log à l&apos;aide d&apos;une tâche Dataflow qui lit à partir de votre rubrique Pub/Sub :

    ```bash
    gcloud dataflow jobs run ${JOB_NAME} \
        --gcs-location=gs://${BUCKET_NAME}/template/PubsubToNewRelic \
        --region=${REGION} \
        --parameters "inputSubscription=projects/${PROJECT_ID}/subscriptions/${INPUT_SUBSCRIPTION_NAME},licenseKey=${NR_LICENSE_KEY}"
    ```

    Cette commande ne nécessite que ces deux valeurs :

    * L&apos;abonnement PubSub d&apos;entrée utilisé pour lire le message de log
    * New Relic <InlinePopover type="licenseKey" />utilisée pour envoyer votre log

    <CollapserGroup>
      <Collapser id="eu-customers-gcp-dataflow" title="Comptes de la région européenne">
        Les comptes européens doivent également ajouter le paramètre `logsApiUrl` avec `https://log-api.eu.newrelic.com/log/v1` comme valeur.

        ```bash
        gcloud dataflow jobs run ${JOB_NAME} \
            --gcs-location=gs://${BUCKET_NAME}/template/PubsubToNewRelic \
            --region=${REGION} \
            --parameters "inputSubscription=projects/${PROJECT_ID}/subscriptions/${INPUT_SUBSCRIPTION_NAME},licenseKey=${NR_LICENSE_KEY},logsApiUrl=https://log-api.eu.newrelic.com/log/v1"
        ```
      </Collapser>
    </CollapserGroup>

    <Callout variant="tip">
      Pour les autres valeurs, la commande utilise les paramètres de configuration par défaut que vous pouvez [personnaliser davantage selon vos besoins](#dataflow-config).
    </Callout>
  </Collapser>

  <Collapser id="dataflow-config" title="8. Facultatif : optimisez le travail de redirecteur de logde votre Dataflow.">
    Voici une référence des options disponibles que vous pouvez utiliser pour optimiser davantage l&apos;exécution de votre travail de redirecteur de logDataflow.

    <table>
      <thead>
        <tr>
          <th style={{ width: "250px" }}>
            Paramètre de configuration
          </th>

          <th>
            Description
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `licenseKey` <DNT>**Required.**</DNT>
          </td>

          <td>
            New Relic <InlinePopover type="licenseKey" />.
          </td>
        </tr>

        <tr>
          <td>
            `inputSubscription` <DNT>**Required.**</DNT>
          </td>

          <td>
            L&apos;abonnement Cloud Pub/Sub utilisé pour consommer le log. Utilisez ce format :

            ```
            projects/<project-id>/subscriptions/<subscription-name>
            ```
          </td>
        </tr>

        <tr>
          <td>
            `logsApiUrl`
          </td>

          <td>
            URL de New Relic pour l&apos;API du log. Cet itinéraire provient du VPC où s&apos;exécute le pipeline Dataflow.

            Défaut:

            ```
            https://log-api.newrelic.com/log/v1
            ```

            Région Europe :

            ```
            https://log-api.eu.newrelic.com/log/v1
            ```
          </td>
        </tr>

        <tr>
          <td>
            `batchCount`
          </td>

          <td>
            Nombre maximal d&apos;enregistrements log à regrouper dans un lot avant de les envoyer à New Relic dans une seule requête HTTP POST.

            Défaut: `100`
          </td>
        </tr>

        <tr>
          <td>
            `flushDelay`
          </td>

          <td>
            Nombre de secondes à attendre pour un log supplémentaire (jusqu&apos;à `batchCount`) depuis la réception du dernier enregistrement de log dans un lot non complet, avant de les vider dans New Relic.

            Défaut: `2`
          </td>
        </tr>

        <tr>
          <td>
            `parallelism`
          </td>

          <td>
            Nombre maximal de requests parallèles.

            Défaut: `1`
          </td>
        </tr>

        <tr>
          <td>
            `disableCertificateValidation`
          </td>

          <td>
            Désactiver la validation du certificat SSL.

            Défaut: `false`
          </td>
        </tr>

        <tr>
          <td>
            `useCompression`
          </td>

          <td>
            Compressez (dans GZIP) la charge envoyée à l&apos;API de Log de New Relic.

            Défaut: `true`
          </td>
        </tr>

        <tr>
          <td>
            `tokenKMSEncryptionKey`
          </td>

          <td>
            Clé de chiffrement KMS pour le jeton. Utilisez ce format :

            ```
            projects/{gcp_project}/locations/{key_region}/keyRings/{key_ring}/cryptoKeys/{kms_key_name}
            ```

            Défaut: `null`
          </td>
        </tr>
      </tbody>
    </table>
  </Collapser>
</CollapserGroup>

<InstallFeedback />

## Quelle est la prochaine étape ? [#what-next]

Explorez les données de logging sur votre plateforme avec notre [interface utilisateur de logs](/docs/logs/log-management/ui-data/use-logs-ui/).

* Obtenez une visibilité plus approfondie sur les données de performances de votre application et de votre plateforme en transmettant votre log avec nos capacités [de logs en contexte](/docs/logs/enable-log-management-new-relic/configure-logs-context/configure-logs-context-apm-agents/) .
* Configurer [des alertes](/docs/alerts-applied-intelligence/new-relic-alerts/alert-conditions/create-alert-conditions/).
* [interrogez vos données](/docs/query-your-data/explore-query-data/get-started/introduction-querying-new-relic-data/) et [créez un dashboard](/docs/query-your-data/explore-query-data/dashboards/introduction-dashboards/).

## Désactiver le transfert de log [#disable]

Pour désactiver les fonctionnalités de transfert de log , suivez les procédures standard dans [la documentation de Google Cloud Platform ](https://cloud.google.com/sdk/docs/). Vous n&apos;avez rien d&apos;autre à faire dans New Relic.