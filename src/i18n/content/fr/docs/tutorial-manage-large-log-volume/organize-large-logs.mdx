---
title: Organisez votre ingestion log volumineux
metaDescription: Organize your logs into managable partitions and tag their attributes with logs parsing
freshnessValidatedDate: never
translationType: machine
---

Une fois que vous avez décidé quels logs ingérer et stocker, il est temps d&apos;organiser vos logs. Vous ingérez probablement encore des centaines de gigaoctets ou des dizaines de téraoctets de logs. Même si c&apos;est beaucoup moins que ce que vous aviez au départ, vous devrez quand même travailler dur pour essayer de les utiliser efficacement.

Pour résoudre ce problème, nous allons regrouper ces logs restants en partitions thématiques et analyser les logs pour extraire et tag les attributs précieux. En organisant vos logs de cette manière, vous pouvez :

* Requête pour n&apos;importe quel attribut dans vos logs
* Gérer des partitions spécifiques à la fois, telles que les logs du frontend et les logs du backend
* Réduire les temps de chargement des requêtes

## Pourquoi partitionner vos logs

Vous pouvez obtenir des améliorations de performances significatives grâce à une utilisation appropriée des partitions de données. L&apos;organisation de vos données en partitions discrètes vous permet d&apos;interroger uniquement les données dont vous avez besoin. Vous pouvez interroger une seule partition ou une liste de partitions séparées par des virgules. Les objectifs du partitionnement de vos données doivent être les suivants :

* Créez des partitions de données qui correspondent aux catégories de votre environnement ou de votre organisation qui sont statiques ou qui changent rarement (par exemple, par unité commerciale, équipe, environnement, service, etc.).
* Créez des partitions pour optimiser le nombre d&apos;événements qui doivent être analysés pour votre requête la plus courante. Il n&apos;y a pas de règle établie, mais généralement, lorsque l&apos;événement de log analysé dépasse 500 millions (en particulier plus d&apos;un milliard) pour votre requête `common`, vous pouvez envisager d&apos;ajuster votre partitionnement.

L&apos;espace de nommage d&apos;une partition détermine sa durée de conservation. Nous proposons deux options de conservation :

* <DNT>**Standard:**</DNT> La conservation par défaut du compte est déterminée par votre abonnement New Relic. Il s&apos;agit de la durée de conservation maximale disponible sur votre compte et de l&apos;espace de nommage que vous sélectionnerez pour la plupart de vos partitions.
* <DNT>**Secondary:**</DNT> Conservation de 30 jours. Tous les logs envoyés à une partition membre de l&apos;espace de nommage secondaire seront purgés sur une base continue 30 jours après avoir été ingérés.

## Pourquoi analyser les logs

Analyser vos logs lors de l&apos;ingestion est le meilleur moyen de rendre vos données de log plus utilisables par vous et les autres utilisateurs de votre organisation. Par exemple, comparez ces deux logs pré-analyse et post-analyse à l&apos;aide d&apos;une règle d&apos;analyse Grok :

<SideBySide>
  <Side>
    Pré-analyse :

    ```json
    {
      "message": "32 4329 store_Portland"
    }


    ```
  </Side>

  <Side>
    Post-analyse :

    ```json
    {
      "transaction_total": "32",
      "purchase_number": "4329",
      "store_location": "store_Portland",
    }
    ```
  </Side>
</SideBySide>

Cela vous permet d&apos;interroger facilement un attribut nouvellement défini tel que `transaction_total` dans les dashboards et les alertes.

## Organisez vos logs

Disons qu&apos;ACME Corp sait qu&apos;elle va ingérer environ 2 To de logs dans les mois à venir. Ils souhaitent créer des partitions pour les logs provenant de leur application Java et de leur agent infrastructure . Ils pensent qu&apos;ils devront peut-être interroger leurs logs Java dans un futur lointain et décident donc d&apos;utiliser une conservation standard. En attendant, ils n&apos;ont besoin que des logs infrastructure récents, ils utiliseront donc une rétention secondaire pour ceux-ci.

Pour ce faire :

<Steps>
  <Step>
    ## Accéder à l&apos;interface utilisateur

    Aller à <DNT>**[one.newrelic.com &gt; Logs](https://one.newrelic.com/launcher/logger.log-launcher)**</DNT>
  </Step>

  <Step>
    ## Partitionnez vos logs

    <SideBySide>
      <Side>
        1. À partir de <DNT>**Manage data**</DNT> dans la navigation de gauche, cliquez sur <DNT>**Data partitions**</DNT>, puis sur <DNT>**Create partition rule**</DNT>.
        2. Définissez un nom de partition sous la forme d&apos;une chaîne alphanumérique commençant par `Log_`. Dans ce cas `Log_java`.
        3. Ajoutez une description facultative.
        4. Sélectionnez la rétention d&apos;espace de nommage standard pour la partition.
        5. Définissez les critères de correspondance de votre règle : saisissez une clause NRQL `WHERE` valide pour faire correspondre les logs à stocker dans cette partition. Dans ce cas `logtype=java`.
        6. Cliquez sur <DNT>**Create**</DNT> pour enregistrer votre nouvelle partition.
      </Side>

      <Side>
        <img title="log-partition" alt="An image displaying New Relic's log partion UI" src="/images/logs_screenshot_full-partition.webp" />
      </Side>
    </SideBySide>

    Cela crée une partition de données avec rétention des données standard pour tous les logs Java . Pour organiser vos logs d&apos;infrastructure, vous devez suivre les mêmes étapes ci-dessus, en changeant uniquement la rétention en secondaire et la requête en `logtype=infrastructure`.
  </Step>

  <Step>
    ## Analysez vos logs

    Maintenant que vos logs sont partitionnés, il est temps de les analyser. les analyser vous permet d&apos;extraire des données pertinentes dans vos logs pour faciliter les requêtes et l&apos;accessibilité.

    Pour analyser vos logs :

    <SideBySide>
      <Side>
        1. À partir de <DNT>**Manage Data**</DNT> dans la navigation de gauche de l’interface utilisateur du log, cliquez sur <DNT>**Parsing**</DNT>, puis sur <DNT>**Create parsing rule**</DNT>.
        2. Saisissez un nom pour la nouvelle règle d’analyse.
        3. Sélectionnez un champ existant à analyser (la valeur par défaut est `message`) ou entrez un nouveau nom de champ.
        4. Saisissez une clause NRQL `WHERE` valide pour correspondre au log que vous souhaitez analyser.
        5. Sélectionnez un log correspondant s&apos;il existe, ou cliquez sur l&apos;onglet Coller log pour coller un exemple log.
        6. Saisissez la règle d&apos;analyse et validez son fonctionnement en affichant les résultats dans la section <DNT>**Output**</DNT> . (Voir exemple ci-dessous)
        7. Activez et enregistrez la règle d’analyse personnalisée.
      </Side>

      <Side>
        <img title="log-parsing" alt="An image displaying New Relic's log parsing UI" src="/images/logs_screenshot_full-parsing.webp" />
      </Side>
    </SideBySide>

    L&apos;exemple suivant vous guide à travers un exemple spécifique de création d&apos;une règle d&apos;analyse :

    <CollapserGroup>
      <Collapser id="example" title="Exemple d'analyse des logs">
        Travaillons avec l’exemple que nous avons utilisé plus tôt dans ce document. Vous avez des logs qui suivent ce modèle :

        ```json
        {
          "message": "32 4329 store_Portland"
        }
        ```

        Vous souhaitez extraire le montant de la transaction, le numéro de commande et l&apos;emplacement du magasin. Les règles d’analyse sont construites à l’aide de Grok, qui utilise le modèle suivant : `%{SYNTAX:SEMANTIC}`. `SYNTAX` est le modèle utilisé pour trouver le texte et `SEMANTIC` est l&apos;identifiant ou l&apos;attribut donné au résultat correspondant.

        Dans ce cas, notre règle d’analyse ressemblerait à :

        ```
        %{INT:transaction_total} %{INT:purchase_number} store%{DATA:store_location}
        ```

        Une fois la règle d&apos;analyse créée avec le modèle ci-dessus, elle renverra les logs de la manière suivante :

        ```json
        {
          "transaction_total": "32",
          "purchase_number": "4329",
          "store_location": "store_Portland",
        }
        ```
      </Collapser>
    </CollapserGroup>

    Pour un aperçu plus approfondi de la création de modèles Grok pour analyser les logs, [lisez notre article de blog](https://newrelic.com/blog/how-to-relic/how-to-use-grok-log-parsing).
  </Step>
</Steps>

## Et ensuite ?

Félicitations pour avoir découvert la véritable valeur de vos logs et pour avoir épargné à votre équipe des heures de frustration avec vos logs ! À mesure que votre système se développe et que vous ingérez, vous devrez assurer la maintenance des règles d&apos;analyse et des partitions. Si vous souhaitez en savoir plus sur ce que New Relic <InlinePopover type="logs" />peut faire pour vous, consultez ces documents :

* [analyser les données log ](/docs/logs/ui-data/parsing): un aperçu plus approfondi de l&apos;analyse des logs avec Grok et apprenez à créer, interroger et gérer vos règles d&apos;analyse des logs en utilisant NerdGraph, notre API GraphQL.
* [Modèles de logs](/docs/logs/ui-data/find-unusual-logs-log-patterns/): les modèles de logs sont le moyen le plus rapide de découvrir de la valeur dans les données log sans recherche.
* [Obfuscation des logs](/docs/logs/ui-data/obfuscation-ui/): Avec log la règle d&apos;obfuscation, vous pouvez empêcher l&apos;enregistrement de certains types d&apos;informations dans New Relic.
* [Rechercher des données dans des logs longs (blobs)](/docs/logs/ui-data/long-logs-blobs/): des données de log complètes peuvent vous aider à résoudre les problèmes. Mais que se passe-t-il si un attribut de votre log contient des milliers de caractères ? Quelle quantité de ces données New Relic peut-il stocker ? Et comment trouver des informations utiles dans toutes ces données ?

<DocTiles numbered>
  <DocTile title="Get started" path="/docs/tutorial-large-logs/get-started-managing-large-logs" />

  <DocTile title="Filter and reduce your log ingest" path="/docs/tutorial-large-logs/filter-large-logs" />

  <DocTile title="Organize your logs" label={{text: 'You are here', color: '#FCD672'}} path="/docs/tutorial-large-logs/organize-large-logs" />
</DocTiles>