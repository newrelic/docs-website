---
title: Composants d'intégration de Kubernetes
tags:
  - Integrations
  - Kubernetes integration
  - Kubernetes components
metaDescription: Learn what components are deployed after installing the Kubernetes integration.
freshnessValidatedDate: never
translationType: machine
---

L&apos;intégration New Relic Kubernetes vous offre une visibilité complète sur la santé et les performances de votre cluster, que vous exécutiez Kubernetes sur site ou dans le cloud. Il vous donne une visibilité sur l&apos;espace de nommage, le déploiement, les réplicasets, les nœuds, le pod et le conteneur Kubernetes .

Le [graphique`newrelic-infrastructure` ](https://github.com/newrelic/nri-kubernetes/tree/main/charts/newrelic-infrastructure)est le composant principal de l&apos;intégration. Apprenez-en plus sur son architecture [ici](#architecture).

Le [graphique`nri-bundle` ](https://github.com/newrelic/helm-charts/tree/master/charts/nri-bundle)regroupe les graphiques individuels de l&apos;intégration, y compris `newrelic-infrastructure`, ce qui vous permet d&apos;installer et de mettre à niveau facilement l&apos;intégration à l&apos;aide d&apos;un seul graphique. Découvrez les composants par défaut et facultatifs que vous pouvez installer avec ce tableau [ici](#components).

## Architecture [#architecture]

Il y a 3 composants dans le graphique `newrelic-infrastructure` : `nrk8s-ksm`, `nrk8s-kubelet` et `nrk8s-controlplane`. Le premier est un déploiement et les deux suivants sont des DaemonSets.

Chacun des composants possède 2 conteneurs :

1. Un conteneur pour l&apos;intégration, chargé de collecter les métriques.
2. Un conteneur avec l&apos;agent New Relic Infrastructure, qui est utilisé pour envoyer les métriques à New Relic.

### Composant Kube-state-métriques [#nrk8s-ksm]

Nous construisons nos métriques d’état de cluster sur la base du projet OSS[`kube-state-metrics`](https://github.com/kubernetes/kube-state-metrics), qui est hébergé sous l’organisation Kubernetes elle-même.

Un déploiement spécifique `nrk8s-ksm` se charge de trouver KSM et de le récupérer. Ce pod étant unique et de longue durée de vie, il peut utiliser en toute sécurité un informateur de point de terminaison pour localiser l&apos;IP du pod KSM et le récupérer. L&apos;informateur met automatiquement en cache la liste des informateurs du cluster localement et monitore les nouveaux, évitant ainsi de submerger le serveur API avec requests pour déterminer où se trouvait le pod .

Les clients doivent permettre à toutes les métriques cibles d&apos;être surveillées sur KSM si ces métriques ne sont pas activées par défaut. Par exemple, KSM v2+ désactive les métriques d&apos;étiquettes par défaut. les clients peuvent activer les métriques d&apos;étiquette cible à surveiller en utilisant l&apos;option `metric-labels-allowlist` décrite [ici](https://github.com/kubernetes/kube-state-metrics/blob/main/docs/developer/cli-arguments.md) dans votre cluster Kubernetes.

Consultez [Apportez votre propre KSM](https://github.com/newrelic/helm-charts/blob/master/charts/nri-bundle/README.md#bring-your-own-ksm) pour plus d&apos;informations sur les versions prises en charge par KSM.

<Callout variant="important">
  Utilisez `customAttributes` pour ajouter un attribut aux échantillons liés à l&apos;entité qui ne sont pas strictement liés à un nœud particulier : `K8sNamespaceSample`, `K8sDeploymentSample`, `K8sReplicasetSample`, `K8sDaemonsetSample`, `K8sStatefulsetSample`, `K8sServiceSample` et `K8sHpaSample`.
</Callout>

### Composant Kubelet [#nrk8s-kubelet]

Le Kubelet est l&apos;« agent Kubernetes », un service qui s&apos;exécute sur chaque nœud Kubernetes et est responsable de la création du conteneur selon les instructions du control plane. Étant donné que c&apos;est le Kubelet qui travaille en étroite collaboration avec le conteneur Runtime, il constitue la principale source de métriques infrastructure pour notre intégration, telles que l&apos;utilisation du CPU, de la mémoire, du disque, du réseau, etc. Bien que non entièrement documentée, l&apos;API Kubelet est la source de facto de la plupart des métriques Kubernetes.

Le scraping du Kubelet est généralement une opération nécessitant peu de ressources. Compte tenu de cela et de notre intention de minimiser le trafic entre les nœuds autant que possible, `nrk8s-kubelet` est exécuté en tant que DaemonSet où chaque instance collecte les métriques du Kubelet exécuté dans le même nœud qu&apos;elle.

`nrk8s-kubelet` se connecte au Kubelet en utilisant l&apos;IP du nœud. Si ce processus échoue, `nrk8s-kubelet` reviendra pour atteindre le nœud via le proxy du serveur API. Ce mécanisme de secours vous aide avec les clusters très volumineux, car le proxy de nombreux kubelets peut augmenter la charge sur le serveur API. Vous pouvez vérifier si le serveur API est utilisé comme proxy en recherchant un message comme celui-ci dans le log :

```
Trying to connect to kubelet through API proxy
```

### composant du plan de contrôle [#nrk8s-controlplane]

En tant que distributions Kubernetes majeures, nous avons déployé `nrk8s-controlplane` comme DaemonSet avec `hostNetwork: true`. La configuration est structurée pour prendre en charge la découverte automatique et le point de terminaison statique. Pour être compatible avec une large gamme de distributions prêtes à l&apos;emploi, nous fournissons une large gamme de valeurs par défaut connues comme entrées de configuration. Faire cela dans la configuration au lieu du code vous permet d&apos;ajuster la découverte automatique à vos besoins.

Il existe la possibilité d&apos;avoir plusieurs points de terminaison par sélecteur et d&apos;ajouter un mécanisme de sonde qui détecte automatiquement le bon. Cela vous permet d&apos;essayer différentes configurations telles que des ports ou des protocoles en utilisant le même sélecteur.

configuration de scraping pour le composant CP etcd ressemble à ce qui suit, où la même structure et les mêmes fonctionnalités s&apos;appliquent à tous les composants :

```yaml
config:
  etcd:
    enabled: true
    autodiscover:
      - selector: "tier=control-plane,component=etcd"
        namespace: kube-system
        matchNode: true
        endpoints:
          - url: https://localhost:4001
            insecureSkipVerify: true
            auth:
              type: bearer
          - url: http://localhost:2381
    staticEndpoint:
      url: https://url:port
      insecureSkipVerify: true
      auth: {}
```

Si `staticEndpoint` est défini, le composant essaie de le récupérer. Si le point de terminaison n&apos;est pas atteint, l&apos;intégration échoue, il n&apos;y a donc pas d&apos;erreurs silencieuses lorsque les points de terminaison manuels sont configurés.

Si `staticEndpoint` n&apos;est pas défini, le composant parcourra les entrées de découverte automatique à la recherche du premier pod correspondant à `selector` dans le `namespace` spécifié et s&apos;exécutant éventuellement dans le même nœud du DaemonSet (si `matchNode` est défini sur `true`). Une fois qu&apos;un pod est découvert, le composant sonde, en émettant une requête http `HEAD`, le point de terminaison répertorié dans l&apos;ordre et récupère le premier point sondé avec succès en utilisant le type d&apos;autorisation sélectionné.

Bien que nous montrions ci-dessus un extrait de configuration pour le composant `etcd` , la logique de scraping est la même pour les autres composants.

Pour des instructions plus détaillées sur la façon de configurer control plane monitoring, veuillez consulter la page [control plane monitoring](/docs/kubernetes-pixie/kubernetes-integration/advanced-configuration/configure-control-plane-monitoring) .

### Cartes de Helm [#helm-charts]

Helm est le principal moyen que nous proposons pour déployer notre solution dans votre cluster. Le graphique Helm gère un déploiement et deux DaemonSets où chacun a une configuration légèrement différente. Cela vous donne plus de flexibilité pour adapter la solution à vos besoins, sans avoir besoin d&apos;appliquer des correctifs manuels sur le graphique et les manifestes générés.

Certaines des nouvelles fonctionnalités de notre nouvel exposant de cartes Helm sont les suivantes :

* Contrôle total du `securityContext` pour tous les pods
* Contrôle total du pod `labels` pour tous les pods
* Possibilité d&apos;ajouter des variables d&apos;environnement supplémentaires, `volumes` et `volumeMounts`
* Contrôle total sur la configuration de l&apos;intégration, y compris les points de terminaison atteints, le comportement de découverte automatique et les intervalles de scraping
* Meilleur alignement avec les idiomes et les normes Helm

Vous pouvez voir les détails complets de tous les commutateurs que vous pouvez basculer dans le [graphique`README.md` ](https://github.com/newrelic/nri-kubernetes/blob/main/charts/newrelic-infrastructure/README.md).

## Composants [#components]

Lorsque vous installez l’intégration Kubernetes à l’aide de `nri-bundle`, ces 2 composants sont installés par défaut :

<table>
  <thead>
    <tr>
      <th style={{ width: "200px" }}>
        Composant
      </th>

      <th>
        Description
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        [infrastructure newrelic](https://github.com/newrelic/nri-kubernetes/tree/main/charts/newrelic-infrastructure)
      </td>

      <td>
        Envoie des métriques sur les nœuds, les objets cluster (par exemple, le déploiement, le pod) et le control plane à New Relic.
      </td>
    </tr>

    <tr>
      <td>
        [nri-metadata-injection](https://github.com/newrelic/k8s-metadata-injection/tree/main/charts/nri-metadata-injection)
      </td>

      <td>
        Enrichit les applications instrumentées New Relic (APM) avec les informations Kubernetes .
      </td>
    </tr>
  </tbody>
</table>

Il s&apos;agit de composants facultatifs que vous pouvez installer à l&apos;aide de `nri-bundle` ou séparément :

<table>
  <thead>
    <tr>
      <th style={{ width: "200px" }}>
        Composant
      </th>

      <th>
        Description
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        [métriques de l&apos;état du kube](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics)
      </td>

      <td>
        Nécessaire pour que newrelic-infrastructure collecte des métriques au niveau du cluster.
      </td>
    </tr>

    <tr>
      <td>
        [nri-kube-events](https://github.com/newrelic/nri-kube-events/tree/main/charts/nri-kube-events)
      </td>

      <td>
        Signale l’événement Kubernetes à New Relic.
      </td>
    </tr>

    <tr>
      <td>
        [opérateur d&apos;infrastructure newrelic](https://github.com/newrelic/newrelic-infra-operator/tree/main/charts/newrelic-infra-operator)
      </td>

      <td>
        (Bêta) Utilisé avec Fargate ou des environnements sans serveur pour injecter newrelic-infrastructure en tant que side-car au lieu du DaemonSet habituel.
      </td>
    </tr>

    <tr>
      <td>
        [Adaptateur de métriques newrelic-k8s](https://github.com/newrelic/newrelic-k8s-metrics-adapter/tree/main/charts/newrelic-k8s-metrics-adapter)
      </td>

      <td>
        (Bêta) Fournit une source de données pour les autoscalers de pods horizontaux (HPA) basés sur une requête NRQL de New Relic.
      </td>
    </tr>

    <tr>
      <td>
        [newrelic-logging](https://github.com/newrelic/helm-charts/tree/master/charts/newrelic-logging)
      </td>

      <td>
        Envoie le log des composants Kubernetes et de la charge de travail exécutés sur le cluster à New Relic.
      </td>
    </tr>

    <tr>
      <td>
        [nri-prometheus](https://github.com/newrelic/nri-prometheus/tree/main/charts/nri-prometheus)
      </td>

      <td>
        Envoie les métriques des applications exposant les métriques Prometheus à New Relic.
      </td>
    </tr>

    <tr>
      <td>
        [newrelic-Prometheus-configurateur](https://github.com/newrelic/newrelic-prometheus-configurator/tree/master/charts/newrelic-prometheus-agent)
      </td>

      <td>
        Configure l&apos;instance de Prometheus en mode Agent pour envoyer des métriques au point de terminaison New Relic Prometheus .
      </td>
    </tr>

    <tr>
      <td>
        [newrelic-pixie](https://github.com/newrelic/helm-charts/tree/master/charts/newrelic-pixie)
      </td>

      <td>
        Se connecte à l&apos;API Pixie et active le plugin New Relic dans Pixie. Le plugin vous permet d&apos;exporter des données de Pixie vers New Relic pour une conservation à long terme des données.
      </td>
    </tr>

    <tr>
      <td>
        [Pixie](https://docs.pixielabs.ai/installing-pixie/install-schemes/helm/#3.-deploy)
      </td>

      <td>
        Un outil d&apos;observabilité open source pour les applications Kubernetes qui utilise eBPF pour capturer automatiquement des données télémétriques sans avoir besoin d&apos; instrumentation manuelle.
      </td>
    </tr>
  </tbody>
</table>