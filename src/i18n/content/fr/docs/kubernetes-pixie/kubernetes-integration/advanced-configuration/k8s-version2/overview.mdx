---
title: Présentation
tags:
  - Integrations
  - Kubernetes integration v2
  - Changelog
metaDescription: Kubernetes integration v2
freshnessValidatedDate: '2024-07-22T00:00:00.000Z'
translationType: machine
---

La version 2 de l&apos;intégration Kubernetes présente des paramètres et des exigences différents de la version 3. Ce document passe en revue les paramètres qui sont différents de la version 3 et dont vous aurez besoin pour la version 2. Si nous ne spécifions rien de différent, vous pouvez utiliser les paramètres de la version 3.

<Callout variant="caution">
  New Relic a rendu obsolète la version 2 et recommande de ne pas l&apos;utiliser. Nous maintenons cette documentation pour les utilisateurs qui utilisent encore la version 2 même si nous ne la supportons plus.

  Consultez [Introduction à l’intégration de Kubernetes](/docs/kubernetes-pixie/kubernetes-integration/get-started/introduction-kubernetes-integration/) pour démarrer avec la version actuelle de Kubernetes.
</Callout>

Pour comprendre les modifications de la version 3, consultez le document [Modifications introduites dans l’intégration Kubernetes version 3](/docs/kubernetes-pixie/kubernetes-integration/advanced-configuration/k8s-version2/changes-since-v3) .

## Monitoring control plane avec la version d&apos;intégration 2 [#monitoring-control-plane]

Cette section explique comment configurer control plane monitoring sur les versions 2 et antérieures de l&apos;intégration.

Veuillez noter que ces versions avaient des options de découverte automatique moins flexibles et ne prenaient pas en charge le point de terminaison externe. Nous vous recommandons fortement de mettre à jour vers [la version 3](/docs/kubernetes-pixie/kubernetes-integration/advanced-configuration/k8s-version2/changes-since-v3) dès que possible.

### Découverte automatique et configuration par défaut : `hostNetwork` et `privileged` [#hostnetwork-privileged]

Dans les versions inférieures à la v3, lorsque l&apos;intégration est effectuée à l&apos;aide de `privileged: false`, le paramètre `hostNetwork` pour le composant control plane sera également défini sur `false`.

### Découverte des control plane et des composants control plane [#discover-nodes-components]

L&apos;intégration Kubernetes s&apos;appuie sur les conventions d&apos;étiquetage [`kubeadm`](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/) pour découvrir les nœuds control plane et les composants control plane . Cela signifie que les nœuds control plane doivent être étiquetés avec `node-role.kubernetes.io/control-plane=""`.

Les composants control plane doivent avoir soit les étiquettes `k8s-app`, soit les étiquettes `tier` et `component`. Consultez ce tableau pour connaître les combinaisons et valeurs d&apos;étiquettes acceptées :

<table>
  <thead>
    <tr>
      <th style={{ width: "110px" }}>
        Composant
      </th>

      <th>
        Étiquette
      </th>

      <th style={{ width: "200px" }}>
        Point de terminaison
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        Serveur API
      </td>

      <td>
        <DNT>
          **Kubeadm / Kops / ClusterAPI**
        </DNT>

        `k8s-app=kube-apiserver`

        `tier=control-plane component=kube-apiserver`

        <DNT>
          **OpenShift**
        </DNT>

        `app=openshift-kube-apiserver apiserver=true`
      </td>

      <td>
        `localhost:443/metrics` par défaut (peut être configuré) si la demande échoue revient à `localhost:8080/metrics`
      </td>
    </tr>

    <tr>
      <td>
        etcd
      </td>

      <td>
        <DNT>
          **Kubeadm / Kops / ClusterAPI**
        </DNT>

        `k8s-app=etcd-manager-main`

        `tier=control-plane component=etcd`

        <DNT>
          **OpenShift**
        </DNT>

        `k8s-app=etcd`
      </td>

      <td>
        `localhost:4001/metrics`
      </td>
    </tr>

    <tr>
      <td>
        Planificateur
      </td>

      <td>
        <DNT>
          **Kubeadm / Kops / ClusterAPI**
        </DNT>

        `k8s-app=kube-scheduler`

        `tier=control-plane component=kube-scheduler`

        <DNT>
          **OpenShift**
        </DNT>

        `app=openshift-kube-scheduler scheduler=true`
      </td>

      <td>
        `localhost:10251/metrics`
      </td>
    </tr>

    <tr>
      <td>
        Responsable contrôleur
      </td>

      <td>
        <DNT>
          **Kubeadm / Kops / ClusterAPI**
        </DNT>

        `k8s-app=kube-controller-manager`

        `tier=control-plane component=kube-controller-manager​`

        <DNT>
          **OpenShift**
        </DNT>

        `app=kube-controller-manager kube-controller-manager=true`
      </td>

      <td>
        `localhost:10252/metrics`
      </td>
    </tr>
  </tbody>
</table>

Lorsque l&apos;intégration détecte qu&apos;elle s&apos;exécute à l&apos;intérieur d&apos;un nœud control plane , elle essaie de trouver les composants qui s&apos;exécutent sur le nœud en recherchant les pods qui correspondent aux étiquettes répertoriées dans le tableau ci-dessus. Pour chaque composant en cours d’exécution, l’intégration envoie une demande à son point de terminaison de métriques.

### Configuration

monitoring du plan de contrôle est automatique pour les agents exécutés à l&apos;intérieur des nœuds control plane . Le seul composant qui nécessite une étape supplémentaire pour s&apos;exécuter est etcd, car il utilise l&apos;authentification TLS mutuelle (mTLS) pour requests des clients. Le serveur API peut également être configuré pour être interrogé à l&apos;aide du [port sécurisé](https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/#api-server-ports-and-ips).

<Callout variant="important">
  monitoring du plan de contrôle pour [OpenShift](http://learn.openshift.com/?extIdCarryOver=true&sc_cid=701f2000001OH7iAAG) 4.x nécessite configuration supplémentaire. Pour plus d&apos;informations, consultez la section [Configuration d&apos;OpenShift 4.x.](#openshift-4x-configuration)
</Callout>

#### etcd

Afin de configurer mTLS pour interroger etcd, vous devez définir ces deux options de configuration :

<table>
  <thead>
    <tr>
      <th style={{ width: "280px" }}>
        Option
      </th>

      <th>
        Valeur
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        `ETCD_TLS_SECRET_NAME`
      </td>

      <td>
        Nom d&apos;un secret Kubernetes qui contient la configuration mTLS.

        Le secret doit contenir les clés suivantes :

        * `cert`: le certificat qui identifie le client effectuant la demande. Il doit être signé par une autorité de certification de confiance etcd.

        * `key`: la clé privée utilisée pour générer le certificat client.

        * `cacert`: l&apos;autorité de certification racine utilisée pour identifier le certificat du serveur etcd.

          Si l&apos;option `ETCD_TLS_SECRET_NAME` n&apos;est pas définie, les métriques etcd ne seront pas récupérées.
      </td>
    </tr>

    <tr>
      <td>
        `ETCD_TLS_SECRET_NAMESPACE`
      </td>

      <td>
        L&apos;espace de nommage où le secret spécifié dans le `ETCD_TLS_SECRET_NAME` a été créé. S&apos;il n&apos;est pas défini, l&apos;espace de nommage `default` est utilisé.
      </td>
    </tr>
  </tbody>
</table>

#### Serveur API

Par défaut, les métriques du serveur API sont des requêtes utilisant le point de terminaison `localhost:8080` non sécurisé. Si ce port est désactivé, vous pouvez également interroger ces métriques via le port sécurisé. Pour activer cela, définissez l’option de configuration suivante dans le fichier manifeste d’intégration Kubernetes :

<table>
  <thead>
    <tr>
      <th style={{ width: "250px" }}>
        Option
      </th>

      <th>
        Valeur
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        `API_SERVER_ENDPOINT_URL`
      </td>

      <td>
        L&apos;URL (sécurisée) pour requêter les métriques. Le serveur API utilise `localhost:443` par défaut

        Assurez-vous que le `ClusterRole` a été mis à jour vers la version la plus récente trouvée dans le manifeste

        Ajouté dans la version 1.15.0
      </td>
    </tr>
  </tbody>
</table>

<Callout variant="important">
  Notez que le port peut être différent selon le port sécurisé utilisé par le serveur API.

  Par exemple, dans Minikube, le port sécurisé du serveur API est 8443 et donc `API_SERVER_ENDPOINT_URL` doit être défini sur `https://localhost:8443`
</Callout>

## configurationd&apos;OpenShift [#openshift-configuration]

les composants du plan de contrôle sur [OpenShift](http://learn.openshift.com/?extIdCarryOver=true&sc_cid=701f2000001OH7iAAG) 4.x utilisent des URL de point de terminaison qui nécessitent une authentification SSL et basée sur un compte de service. Par conséquent, vous ne pouvez pas utiliser [les URL de point de terminaison par défaut](/docs/kubernetes-pixie/kubernetes-integration/advanced-configuration/configure-control-plane-monitoring/#static-endpoints-limitations) .

<Callout variant="important">
  Lors de l&apos;installation de `openshift` via Helm, spécifiez la configuration pour inclure automatiquement ces points de terminaison. Les paramètres `openshift.enabled=true` et `openshift.version="4.x"` incluront le point de terminaison sécurisé et activeront l&apos;exécution `/var/run/crio.sock` .
</Callout>

Pour configurer control plane monitoring sur OpenShift, supprimez le commentaire des variables d’environnement suivantes dans le [manifeste personnalisé](/install/kubernetes/?dropdown1=manifest). Les valeurs d&apos;URL sont préconfigurées sur les URL de base par défaut pour les control plane monitoring métriques du point de terminaison dans [OpenShift](http://learn.openshift.com/?extIdCarryOver=true&sc_cid=701f2000001OH7iAAG) 4.x.

```yaml
        - name: "SCHEDULER_ENDPOINT_URL"
        value: "https://localhost:10259
        - name: "ETCD_ENDPOINT_URL"
        value: "https://localhost:9979"
        - name: "CONTROLLER_MANAGER_ENDPOINT_URL"
        value: "https://localhost:10257"
        - name: "API_SERVER_ENDPOINT_URL"
        value: "https://localhost:6443"
```

<Callout variant="important">
  Même si le `ETCD_ENDPOINT_URL` personnalisé est défini, etcd nécessite que l&apos;authentification HTTPS et mTLS soit configurée. Pour en savoir plus sur la configuration de mTLS pour etcd dans OpenShift, consultez [Configurer mTLS pour etcd dans OpenShift](/docs/kubernetes-pixie/kubernetes-integration/advanced-configuration/configure-control-plane-monitoring/#mtls-how-to-openshift).
</Callout>

## Log de Kubernetes [#logs-version2]

Si vous souhaitez générer un log détaillé et obtenir des informations sur la version et configuration , consultez simplement les informations ci-dessous.

<CollapserGroup>
  <Collapser id="verbose-with-manifest" title="Activer le logging détaillé à l'aide d'un fichier manifeste">
    Pour l&apos;intégration Kubernetes , l&apos;agent infrastructure ajoute uniquement une entrée log en cas d&apos;erreur. Les erreurs les plus courantes sont affichées dans le log standard (non détaillé). Si vous effectuez une enquête plus approfondie par vous-même ou avec l&apos;assistance New Relic, vous pouvez activer le mode verbeux.

    <Callout variant="caution">
      Le mode verbeux augmente considérablement la quantité d&apos;informations envoyées au fichier de log. N&apos;activez ce mode que temporairement à des fins de dépannage et réinitialisez le niveau de log une fois terminé.
    </Callout>

    Pour activer le logging détaillé à l’aide d’un fichier manifeste :

    1. Activer le logging détaillé : dans le fichier de déploiement, définissez la valeur de `NRIA_VERBOSE` sur `1`.

    2. Appliquez la configuration modifiée en exécutant :

       ```shell
           kubectl apply -f your_newrelic_k8s.yaml
       ```

    3. Laissez le mode verbeux pendant quelques minutes ou jusqu&apos;à ce qu&apos;une activité suffisante se soit produite.

    4. Désactiver le mode verbeux : redéfinissez la valeur `NRIA_VERBOSE` sur `0`.

    5. Appliquez la configuration restaurée en exécutant :

       ```shell
           kubectl apply -f your_newrelic_k8s.yaml
       ```

    6. Obtenir une liste des nœuds dans l’environnement :

       ```shell
           kubectl get nodes --all-namespaces
       ```

    7. Obtenez une liste des infrastructure et du pod kube-state-métriques :

       ```shell
           kubectl get pods --all-namespaces -o wide | egrep 'newrelic|kube-state-metrics'
       ```

    8. [Récupérez le log du pod se connectant à `kube-state-metrics`](#logs-pod-kubestatemetrics).

    9. [Récupérer la configuration du service `kube-state-metrics` ](/docs/kubernetes-pixie/kubernetes-integration/advanced-configuration/get-logs-version/#kube-state-metrics-version).
  </Collapser>

  <Collapser className="freq-link" id="verbose-helm" title="Activer le logging détaillé à l'aide de Helm">
    Pour l&apos;intégration Kubernetes , l&apos;agent infrastructure ajoute uniquement une entrée log en cas d&apos;erreur. Les erreurs les plus courantes sont affichées dans le log standard (non détaillé). Si vous effectuez une enquête plus approfondie par vous-même ou avec l&apos;assistance New Relic, vous pouvez activer le mode verbeux.

    <Callout variant="caution">
      Le mode verbeux augmente considérablement la quantité d&apos;informations envoyées au fichier de log. N&apos;activez ce mode que temporairement à des fins de dépannage et réinitialisez le niveau de log une fois terminé.
    </Callout>

    Pour activer le logging détaillé à l’aide de Helm :

    1. Exécutez cette commande :

       ```shell
           helm upgrade -n <namespace> --reuse-values newrelic-bundle --set newrelic-infrastructure.verboseLog=true newrelic/nri-bundle
       ```

    2. Laissez le mode verbeux pendant quelques minutes ou jusqu&apos;à ce qu&apos;une activité suffisante se soit produite.

    3. Lorsque vous disposez des informations dont vous avez besoin, désactivez le logging détaillé :

       ```shell
           helm upgrade --reuse-values newrelic-bundle --set newrelic-infrastructure.verboseLog=false newrelic/nri-bundle
       ```

    4. Appliquez la configuration restaurée en exécutant :

       ```shell
           kubectl apply -f your_newrelic_k8s.yaml
       ```

    5. Obtenir une liste des nœuds dans l’environnement :

       ```shell
           kubectl get nodes --all-namespaces
       ```

    6. Obtenez une liste des infrastructure et du pod kube-state-métriques :

       ```shell
           kubectl get pods --all-namespaces -o wide | egrep 'newrelic|kube-state-metrics'
       ```

    7. [Récupérez le log du pod se connectant à `kube-state-metrics`](#logs-pod-kubestatemetrics).

    8. [Récupérer la configuration du service `kube-state-metrics` ](/docs/kubernetes-pixie/kubernetes-integration/advanced-configuration/get-logs-version/#kube-state-metrics-version).
  </Collapser>

  <Collapser id="logs-pod-kubestatemetrics" title="Obtenir le log du pod se connectant à kube-state-métriques">
    Pour obtenir le log du pod se connectant à `kube-state-metrics`:

    1. Exécutez cette commande :

       ```shell
           kubectl get pods --all-namespaces -o wide | grep kube-state-metrics
       ```

       Recherchez un résultat similaire à celui-ci :

       ```shell
           kube-system   kube-state-metrics-5c6f5cb9b5-pclhh     2/2
           Running   4          4d        172.17.0.3   minikube
       ```

    2. Obtenez le pod New Relic qui s&apos;exécute sur le même nœud que `kube-state-metrics`:

       ```shell
       kubectl describe node minikube | grep newrelic-infra
       ```

       Recherchez un résultat similaire à celui-ci :

       ```shell
       default                    newrelic-infra-5wcv6                     100m (5%)
       0 (0%)      100Mi (5%)       100Mi (5%)
       ```

    3. Récupérez le log des nœuds en exécutant :

       ```shell
           kubectl logs newrelic-infra-5wcv6
       ```
  </Collapser>

  <Collapser id="logs-pod-kubestatemetrics" title="Obtenir le log d'un pod exécuté sur un nœud control plane">
    Pour obtenir le log d’un pod exécuté sur un nœud control plane :

    1. Obtenez les nœuds étiquetés comme control plane:

       ```shell
       kubectl get nodes -l node-role.kubernetes.io/control-plane=""
       ```

       Recherchez un résultat similaire à celui-ci :

       ```shell
       NAME                         STATUS  ROLES          AGE   VERSION
       ip-10-42-24-4.ec2.internal   Ready   control-plane  42d   v1.14.8
       ```

    2. Récupérez le pod New Relic qui s’exécute sur l’un des nœuds renvoyés à l’étape précédente :

       ```shell
       kubectl get pods --field-selector spec.nodeName=ip-10-42-24-4.ec2.internal -l name=newrelic-infra --all-namespaces
       ```

       Recherchez un résultat similaire à celui-ci :

       ```shell
       newrelic-infra-whvzt
       ```

    3. Récupérez le log des nœuds en exécutant cette commande :

       ```shell
       kubectl logs newrelic-infra-whvzt
       ```
  </Collapser>
</CollapserGroup>

## Monitorer les services exécutés sur Kubernetes [#monitor-services]

Les services de monitoring dans Kubernetes fonctionnent en exploitant notre [agent infrastructure et notre intégration sur hôte](/docs/infrastructure/host-integrations/installation/install-infrastructure-host-integrations) et un mécanisme de découverte automatique pour les pointer vers un pod avec un sélecteur spécifié.

Consultez le document [Activer monitoring des services à l’aide du graphique Helm ](/docs/kubernetes-pixie/kubernetes-integration/advanced-configuration/monitor-services/monitor-services-running-kubernetes/#enable)pour savoir comment procéder. Consultez cet exemple pour la version 2, qui montre la configuration `yaml` pour l&apos;intégration Redis ajoutée au fichier `values.yml` du graphique `nri-bundle` .

```yaml
    newrelic-infrastructure:
    integrations_config:
    - name: nri-redis.yaml
    data:
    discovery:
    command:
    # Run NRI Discovery for Kubernetes
    # https://github.com/newrelic/nri-discovery-kubernetes
    exec: /var/db/newrelic-infra/nri-discovery-kubernetes --tls --port 10250
    match:
    label.app: redis
    integrations:
    - name: nri-redis
    env:
    # using the discovered IP as the hostname address
    HOSTNAME: ${discovery.ip}
    PORT: 6379
    labels:
    env: test
```

### Ajouter un service YAML à la configuration d&apos;intégration Kubernetes [#add-service-yaml]

Si vous utilisez la version 2 de l&apos;intégration Kubernetes, vous devez ajouter une entrée pour ce ConfigMap dans la section `volumes` et `volumeMounts` du `spec` du DaemonSet, afin de garantir que tous les fichiers du ConfigMap sont montés dans `/etc/newrelic-infra/integrations.d/`.