---
title: Configurer control plane monitoring
tags:
  - Integrations
  - Kubernetes integration
  - Installation
metaDescription: How to configure control plane monitoring for your Kubernetes integration with New Relic.
freshnessValidatedDate: never
translationType: machine
---

[New Relic](/docs/infrastructure/new-relic-infrastructure/getting-started/introduction-new-relic-infrastructure) fournit une prise en charge pour [control plane](https://kubernetes.io/docs/concepts/overview/components/#control-plane-components) votre Kubernetes intégration , vous permettant de monitorer et de collecter des métriques à partir cluster des control plane composants de votre . Ces données peuvent ensuite être trouvées dans New Relic et utilisées pour [créer des requêtes et des graphiques](/docs/using-new-relic/data/understand-data/query-new-relic-data).

<Callout variant="tip">
  Cette page fait référence à l&apos; [intégration Kubernetes v3](/docs/kubernetes-pixie/kubernetes-integration/advanced-configuration/k8s-version2/changes-since-v3). Si vous exécutez la version v2, découvrez [comment configurer control plane monitoring pour la version v2](/docs/kubernetes-pixie/kubernetes-integration/advanced-configuration/k8s-version2/overview/##monitoring-control-plane).
</Callout>

## Caractéristiques [#control-plane-integration-features]

Nous monitorons et collectons [les métriques](/docs/integrations/kubernetes-integration/understand-use-data/understand-use-data#metrics) des composants control plane suivants :

* <DNT>**etcd**</DNT>: informations sur le leader, taille de la mémoire résidente, nombre de threads du système d&apos;exploitation, données sur les propositions de consensus, etc. Pour une liste des métriques prises en charge, voir [etcd data](/docs/integrations/kubernetes-integration/understand-use-data/understand-use-data#etcd-data).
* <DNT>**API server**</DNT>: taux de `apiserver` requests, répartition de `apiserver` requests par méthode HTTP et code de réponse, etc. Pour la liste complète des métriques prises en charge, consultez [Données du serveur API](/docs/integrations/kubernetes-integration/understand-use-data/understand-use-data#api-server-data).
* <DNT>**Scheduler**</DNT>: CPU/mémoire demandés par rapport à la mémoire disponible sur le nœud, tolérances aux souillures, toute affinité ou anti-affinité définie, etc. Pour la liste complète des métriques prises en charge, voir [Données du planificateur](/docs/integrations/kubernetes-integration/understand-use-data/understand-use-data#scheduler-data).
* <DNT>**Controller manager**</DNT>: taille de la mémoire résidente, nombre de threads du système d&apos;exploitation créés, goroutines actuellement existantes, etc. Pour la liste complète des métriques prises en charge, voir [Données du gestionnaire de contrôleur](/docs/integrations/kubernetes-integration/understand-use-data/understand-use-data#controller-manager-data).

## Compatibilité et exigences [#compatibility]

* La plupart des clusters gérés, y compris AKS, EKS et GKE, n&apos;autorisent pas l&apos;accès extérieur à leurs composants control plane . C&apos;est pourquoi sur un cluster géré, New Relic ne peut obtenir que les métriques control plane pour le serveur API, et non pour etcd, le planificateur ou le gestionnaire de contrôleur.
* Lorsque vous utilisez la solution en [mode non privilégié](/docs/integrations/kubernetes-integration/installation/kubernetes-installation-configuration#unprivileged), la configuration control plane nécessitera [des étapes supplémentaires](#hostnetwork-privileged) et certaines mises en garde peuvent s&apos;appliquer.
* [OpenShift](http://learn.openshift.com/?extIdCarryOver=true&sc_cid=701f2000001OH7iAAG) 4.x utilise des points de terminaison métriques de composant control plane qui sont différents de ceux par défaut.

## composant du plan de contrôle [#component]

La tâche de monitoring du Kubernetes control plane est une responsabilité du `nrk8s-controlplane` composant, qui par défaut est déployé en tant que DaemonSet. Ce composant est automatiquement déployé sur les nœuds control plane , grâce à l&apos;utilisation d&apos;une liste par défaut de `nodeSelectorTerms` qui inclut des étiquettes couramment utilisées pour identifier les nœuds control plane , tels que `node-role.kubernetes.io/control-plane`. Quoi qu&apos;il en soit, ce sélecteur est exposé dans le fichier `values.yml` et peut donc être reconfiguré pour s&apos;adapter à d&apos;autres environnements.

les clusters qui n&apos;ont aucun nœud correspondant à ces sélecteurs pod n&apos;obtiendront aucun planifié, ne gaspillant ainsi aucune ressource et étant fonctionnellement équivalent à la désactivation totale control plane monitoring en définissant `controlPlane.enabled` sur `false` dans le Helm graphique .

Chaque composant du control plane dispose d&apos;une section dédiée, qui permet individuellement de :

* Activer ou désactiver monitoring de ce composant
* Définir des sélecteurs spécifiques et un espace de nommage pour découvrir ce composant
* Définir le point de terminaison et les chemins qui seront utilisés pour récupérer les métriques de ce composant
* Définir les mécanismes d&apos;authentification qui doivent être utilisés pour obtenir des métriques pour ce composant
* Spécifiez manuellement le point de terminaison qui ignore complètement la découverte automatique

<img title="Diagram showing a possible configuration scraping etcd with mTLS and API server with bearer Token." alt="Diagram showing a possible configuration scraping etcd with mTLS and API server with bearer Token. The monitoring is a DaemonSet deployed on control plane nodes only." src="/images/kubernetes_diagram_integration-cp.webp" />

Vous pouvez vérifier toutes les options configuration disponibles dans le fichier [values.yaml](https://github.com/newrelic/nri-kubernetes/blob/main/charts/newrelic-infrastructure/values.yaml) du graphique nri-Kubernetes sous la clé `controlPlane`.

Si vous installez l&apos;intégration via le graphique `nri-bundle` , vous devez [transmettre les valeurs au sous-graphique correspondant](https://helm.sh/docs/chart_template_guide/subcharts_and_globals/). Par exemple, pour désactiver la monitoring `etcd` dans le composant `controlPlane`, vous pouvez procéder comme suit :

```yaml
newrelic-infrastructure:
  controlPlane:
    config:
      etcd:
        enabled: false
```

### Découverte automatique et configuration par défaut [#autodiscovery-default]

Par défaut, notre [graphiqueHelm ](/docs/kubernetes-pixie/kubernetes-integration/installation/install-kubernetes-integration-using-helm)fournit une configuration qui devrait fonctionner immédiatement pour certains composants control plane pour les distributions sur site qui exécutent le control plane à l&apos;intérieur du cluster, tels que `Kubeadm` ou `minikube`.

Notez que, comme la découverte automatique repose sur les étiquettes pod comme mécanisme de découverte, elle ne fonctionne pas dans les environnements cloud ou lorsque les composants control plane ne s&apos;exécutent pas à l&apos;intérieur du cluster. Cependant, [le point de terminaison statique](#static-endpoints) peut être exploité dans ces scénarios si les composants control plane sont accessibles.

#### `hostNetwork` et `privileged` [#hostnetwork-privileged]

Dans les versions supérieures à 3, l&apos;indicateur `privileged` affecte uniquement les objets `securityContext` , c&apos;est-à-dire si le conteneur est exécuté en tant que root avec accès aux métriques de l&apos;hôte ou non. Tous les composants d&apos;intégration sont désormais par défaut sur `hostNetwork: false`, à l&apos;exception du pod qui obtient les métriques du control plane qui ont `hostNetwork: true` car il est nécessaire pour atteindre le point de terminaison control plane dans la plupart des distributions. La valeur `hostNetwork` de tous les composants peut être modifiée, individuellement ou globalement, à l&apos;aide [de la bascule `hostNetwork` dans votre `values.yaml`](https://github.com/newrelic/helm-charts/tree/master/library/common-library#values-managed-globally).

<Callout variant="tip">
  Pour les paramètres spécifiques liés à la version 2, voir [Découverte automatique et configuration par défaut : `hostNetwork` et `privileged`](/docs/kubernetes-pixie/kubernetes-integration/advanced-configuration/k8s-version2/overview/#hostnetwork-privileged).
</Callout>

Si l&apos;exécution du pod avec n&apos;est pas acceptable, en raison du `hostNetwork` cluster ou d&apos;autres politiques, control plane monitoring n&apos;est pas possible et doit être désactivée en `controlPlane.enabled` définissant `false` sur.

Si vous disposez d&apos;une configuration avancée qui inclut [la découverte automatique personnalisée](#autodiscovery-default) ou [un point de terminaison statique](#static-endpoints) qui peut être utilisé pour monitorer le control plane sans `hostNetwork`, consultez [READMEdu projet](https://github.com/newrelic/nri-kubernetes/blob/main/charts/newrelic-infrastructure/README.md) et recherchez `controlPlane.hostNetwork` dans le [fichier`values.yaml`](https://github.com/newrelic/nri-kubernetes/blob/main/charts/newrelic-infrastructure/values.yaml).

#### Découverte automatique personnalisée [#autodiscovery-default]

Les sélecteurs utilisés pour la découverte automatique sont entièrement exposés en tant qu&apos;entrées configuration dans le fichier `values.yaml`, ce qui signifie qu&apos;ils peuvent être modifiés ou remplacés pour s&apos;adapter à presque tous les environnements où le control plane est exécuté dans le cadre du cluster.

Une section de découverte automatique ressemble à ce qui suit :

```yaml
autodiscover:
  - selector: "tier=control-plane,component=etcd"
    namespace: kube-system
    # Set to true to consider only pods sharing the node with the scraper pod.
    # This should be set to `true` if Kind is Daemonset, `false` otherwise.
    matchNode: true
    # Try to reach etcd using the following endpoints.
    endpoints:
      - url: https://localhost:4001
        insecureSkipVerify: true
        auth:
          type: bearer
      - url: http://localhost:2381
  - selector: "k8s-app=etcd-manager-main"
    namespace: kube-system
    matchNode: true
    endpoints:
      - url: https://localhost:4001
        insecureSkipVerify: true
        auth:
          type: bearer
```

La section `autodiscover` contient une liste d’entrées de découverte automatique. Chaque entrée contient :

* `selector`:Un sélecteur d&apos;étiquette codé en chaîne qui sera utilisé pour rechercher un pod.
* `matchNode`: Si défini sur true, il limitera en outre la découverte au pod exécuté sur le même nœud que l&apos; instance particulière du DaemonSet effectuant la découverte.
* `endpoints`:Une liste de points de terminaison à essayer si un pod est trouvé pour le sélecteur spécifié.

De plus, chaque `endpoint` possède :

* `url`: URL vers la cible, y compris le schéma. Peut être `http` ou `https`.
* `insecureSkipVerify`: Si défini sur vrai, le certificat ne sera pas vérifié pour `https` URL.
* `auth.type`:Quel mécanisme utiliser pour authentifier la demande. Actuellement, les méthodes suivantes sont prises en charge :
* Aucun : si `auth` n’est pas spécifié, la demande ne contiendra aucune authentification.
* `bearer`:Le même jeton porteur utilisé pour s&apos;authentifier auprès de l&apos;API Kubernetes sera envoyé à cette demande.
* `mtls`: mTLS sera utilisé pour exécuter la requête.

##### mTLS [#mtls]

Pour le type `mtls` , les éléments suivants doivent être spécifiés :

```yaml
endpoints:
  - url: https://localhost:4001
    auth:
      type: mtls
      mtls:
        secretName: secret-name
        secretNamespace: secret-namespace
```

Où `secret-name` est le nom d&apos;un secret TLS Kubernetes, qui se trouve dans `secret-namespace` et contient le certificat, la clé et l&apos;autorité de certification requis pour se connecter à ce point de terminaison particulier.

L&apos;intégration récupère ce secret lors de l&apos;exécution plutôt que de le monter, ce qui signifie qu&apos;elle nécessite un rôle RBAC lui accordant l&apos;accès. Notre graphique Helm détecte automatiquement les entrées `auth.mtls` au moment du rendu et créera automatiquement des entrées pour ces secrets particuliers et cet espace de nommage pour vous, sauf si `rbac.create` est défini sur faux.

Notre intégration accepte un secret avec les clés suivantes :

* `cacert`:Le certificat CA codé en PEM utilisé pour signer le `cert`
* `cert`:Le certificat codé en PEM qui sera présenté à etcd
* `key`:La clé privée codée en PEM correspondant au certificat ci-dessus

Ces certificats doivent être signés par la même autorité de certification qu&apos;etcd utilise pour fonctionner.

La manière de générer ces certificats n&apos;entre pas dans le cadre de cette documentation, car elle varie considérablement selon les différentes distributions Kubernetes. Veuillez vous référer à la documentation de votre distribution pour voir comment récupérer les certificats homologues etcd requis. Dans Kubeadm, par exemple, ils peuvent être trouvés dans `/etc/kubernetes/pki/etcd/peer.{crt,key}` dans le nœud control plane .

Une fois que vous avez localisé ou généré les certificats homologues etcd, vous devez renommer les fichiers pour qu&apos;ils correspondent aux clés que nous nous attendons à voir présentes dans le secret et créer le secret dans le cluster.

```shell
mv peer.crt cert
mv peer.key key
mv ca.crt cacert

kubectl -n newrelic create secret tls newrelic-etcd-tls-secret --cert=./cert --key=./key --certificate-authority=./cacert
```

Enfin, vous pouvez saisir le nom secret (`newrelic-etcd-tls-secret`) et l&apos;espace de nommage (`newrelic`) dans le snippet de configuration affiché au début de cette section. N&apos;oubliez pas que le graphique Helm analysera automatiquement cette configuration et créera un rôle RBAC pour accorder l&apos;accès à ce secret spécifique et à cet espace de nommage pour le composant `nrk8s-controlplane`, aucune action manuelle n&apos;est donc nécessaire à cet égard.

### Point de terminaison statique [#static-endpoints]

Bien que la découverte automatique doive couvrir les cas où le control plane réside à l&apos;intérieur du cluster Kubernetes, certaines distributions ou environnements Kubernetes sophistiqués exécutent le control plane ailleurs, pour diverses raisons, notamment la disponibilité ou l&apos;isolement des ressources.

Dans ces cas, l&apos;intégration peut être configurée pour récupérer une URL fixe arbitraire, indépendamment du fait qu&apos;un pod avec une étiquette control plane soit trouvé dans le nœud. Cela se fait en spécifiant une entrée `staticEndpoint` . Par exemple, une instance etcd externe ressemblerait à ceci :

```yaml
controlPlane:
  etcd:
    staticEndpoint:
      url: https://url:port
      insecureSkipVerify: true
      auth: {}
```

<img title="Diagram showing a possible configuration scraping an external API server with bearer Token." alt="Diagram showing a possible configuration scraping an external API server with bearer Token. The monitoring is a Deployment with a single replica." src="/images/kubernetes_diagram_integration-cp-external.webp" />

`staticEndpoint` est le même type d&apos;entrée que `endpoints` dans l&apos;entrée `autodiscover` , dont les champs sont décrits ci-dessus. Les mécanismes et schémas d’authentification sont pris en charge ici.

Veuillez garder à l’esprit que si `staticEndpoint` est défini, la section `autodiscover` sera ignorée dans son intégralité.

#### Limites [#static-endpoints-limitations]

<Callout variant="important">
  Si vous utilisez `staticEndpoint` pointant vers un point de terminaison hors nœud (par exemple, pas `localhost`), vous devez modifier `controlPlane.kind` de `DaemonSet` à `Deployment`.
</Callout>

Lors de l&apos;utilisation `staticEndpoint`, tous les pods `nrk8s-controlplane` tenteront d&apos;atteindre et de gratter ledit point de terminaison. Cela signifie que, si `nrk8s-controlplane` est un DaemonSet (la valeur par défaut), toutes les instances du DaemonSet gratteront ce point de terminaison. Bien que cela soit acceptable si vous les pointez vers `localhost`, si le point de terminaison n&apos;est pas local sur le nœud, vous pourriez potentiellement produire des métriques en double et augmenter l&apos;utilisation facturable. Si vous utilisez `staticEndpoint` et le pointez vers une URL non locale, assurez-vous de remplacer `controlPlane.kind` par déploiement.

Pour la même raison ci-dessus, il n&apos;est actuellement pas possible d&apos;utiliser la découverte automatique pour certains composants control plane et un point de terminaison statique pour d&apos;autres. Il s’agit d’une limitation connue que nous travaillons à résoudre dans les futures versions de l’intégration.

Enfin, `staticEndpoint` permet uniquement de définir un seul point de terminaison par composant. Cela signifie que si vous avez plusieurs fragments control plane dans différents hôtes, il n&apos;est actuellement pas possible de pointer vers eux séparément. Il s’agit également d’une limitation connue sur laquelle nous travaillons pour remédier dans les versions futures. Pour le moment, une solution de contournement pourrait être d&apos;utiliser une métrique agrégée pour différents fragments ailleurs et de pointer l&apos;URL `staticEndpoint` vers la sortie agrégée.

#### monitoring du plan de contrôle pour les environnements gérés et cloud [#cloud-control-plane]

Certains environnements cloud, comme EKS ou GKE, permettent de récupérer des métriques à partir du serveur API Kubernetes. Cela peut être facilement configuré comme un point de terminaison statique :

```yaml
controlPlane:
  affinity: false  # https://github.com/helm/helm/issues/9136
  kind: Deployment
  # `hostNetwork` is not required for monitoring API Server on AKS, EKS
  hostNetwork: false
  config:
    etcd:
      enabled: false
    scheduler:
      enabled: false
    controllerManager:
      enabled: false
    apiServer:
     staticEndpoint:
       url: "https://kubernetes.default:443"
       insecureSkipVerify: true
       auth:
         type: bearer
```

Veuillez noter que cela s&apos;applique uniquement au serveur API et qu&apos;etcd, le planificateur et le gestionnaire de contrôleur restent inaccessibles dans les environnements cloud.

De plus, sachez que, selon l’environnement géré ou cloud spécifique, le service Kubernetes peut équilibrer la charge du trafic entre différentes instances du serveur API. Dans ce cas, les métriques qui dépendent de l’instance spécifique sélectionnée par l’équilibreur de charge ne sont pas fiables.

#### monitoring du plan de contrôle pour Rancher RKE1 [#rancher]

Le cluster déployé en utilisant Rancher Kubernetes Engine (RKE1) exécute les composants control plane en tant que conteneur Docker , et non en tant que pod géré par Kubelet. C&apos;est pourquoi l&apos;intégration ne peut pas détecter automatiquement ces conteneurs, et chaque point de terminaison doit être spécifié manuellement dans la section `staticEndpoint` de la configuration de l&apos;intégration.

L&apos;intégration doit pouvoir atteindre les différents points de terminaison soit en se connectant directement, avec les méthodes d&apos;authentification disponibles (jeton de compte de service, certificats mTLS personnalisés ou aucun), soit via un proxy.

Par exemple, pour rendre les métriques du planificateur et du gestionnaire de contrôleur accessibles, vous devrez peut-être ajouter l&apos; [indicateur](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/) `--authorization-always-allow-paths` , permettant `/metrics` ou `--authentication-kubeconfig` et `--authorization-kubeconfig` d&apos;activer l&apos;authentification jeton.

En supposant que chaque composant soit accessible sur le port spécifié, la configuration suivante monitorera le serveur API, le planificateur et le gestionnaire de contrôleur :

```yaml
controlPlane:
  kind: DaemonSet
  config:
    scheduler:
      enabled: true
      staticEndpoint:
        url: https://localhost:10259
        insecureSkipVerify: true
        auth:
          type: bearer
    controllerManager:
      enabled: true
      staticEndpoint:
        url: https://localhost:10257
        insecureSkipVerify: true
        auth:
          type: bearer
    apiServer:
      enabled: true
      staticEndpoint:
        url: https://localhost:6443
        insecureSkipVerify: true
        auth:
          type: bearer
```

Dans cet exemple, l&apos;intégration doit s&apos;exécuter sur le même nœud de chaque composant control plane dont l&apos;option `hostNetwork` est définie sur true, car elle se connecte localement à chaque `staticEndpoint`. Par conséquent, `controlPlane.kind` doit être conservé comme `DaemonSet`. De plus, le DaemonSet a besoin de règles d&apos;affinité, de nodeSelector et de tolérances configurées afin que toutes les instances s&apos;exécutent sur tous les nœuds control plane que vous souhaitez monitorer.

Vous pouvez reconnaître les nœuds control plane en vérifiant l’étiquette `node-role.kubernetes.io/controlplane`. Ce label est déjà pris en compte par les règles d&apos;affinité par défaut de l&apos;intégration.

Si vous utilisez la version 2 de l&apos;intégration, consultez [Monitoring control plane avec la version 2 de l&apos;intégration](/docs/kubernetes-pixie/kubernetes-integration/advanced-configuration/k8s-version2/overview/#monitoring-control-plane).

## Configuration d&apos;OpenShift [#openshift-4x-configuration]

La version 3 de l&apos;intégration Kubernetes inclut des paramètres par défaut qui détecteront automatiquement les composants control plane dans le cluster OpenShift. Elle devrait donc fonctionner immédiatement pour tous les composants, à l&apos;exception d&apos;etcd.

Etcd n’est pas pris en charge par défaut, car le point de terminaison des métriques est configuré pour exiger l’authentification mTLS dans les environnements OpenShift. Notre intégration prend en charge l’authentification mTLS pour récupérer les métriques etcd dans cette configuration, cependant vous devrez créer manuellement le certificat mTLS requis. Cela est nécessaire pour éviter d’accorder de larges autorisations à notre intégration sans l’approbation explicite de l’utilisateur.

Pour créer un secret mTLS, veuillez suivre les étapes de [cette section ci-dessous](#mtls-how-to-openshift), puis configurez l&apos;intégration pour utiliser le secret nouvellement créé comme décrit dans [la section mtls](#mtls).

Si vous utilisez la version 2 de l&apos;intégration, [configuration OpenShift sur la version 2 de l&apos;intégration](/docs/kubernetes-pixie/kubernetes-integration/advanced-configuration/k8s-version2/overview/#openshift-configuration).

### Configurer mTLS pour etcd dans OpenShift [#mtls-how-to-openshift]

Suivez ces instructions pour configurer l’authentification TLS mutuelle pour etcd dans [OpenShift](http://learn.openshift.com/?extIdCarryOver=true&sc_cid=701f2000001OH7iAAG) 4.x :

1. Exportez les certificats clients etcd du cluster vers un secret opaque. Dans un cluster OpenShift géré par défaut, le secret est nommé `kube-etcd-client-certs` et il est stocké dans l&apos;espace de nommage `openshift-monitoring`.

   ```shell
   kubectl get secret etcd-client -n openshift-etcd -o yaml > etcd-secret.yaml
   ```

   Le contenu du fichier etcd-secret.yaml ressemblerait à ce qui suit.

   ```yaml
    apiVersion: v1
    data:
      tls.crt: <CERT VALUE>
      tls.key: <KEY VALUE>
    kind: Secret
    metadata:
      creationTimestamp: "2023-03-23T23:19:17Z"
      name: etcd-client
      namespace: openshift-etcd
      resourceVersion: 
      uid: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
    type: kubernetes.io/tls   
   ```

2. Vous pouvez éventuellement modifier le nom secret et l&apos;espace de nommage par quelque chose de significatif. Les étapes suivantes supposent que le nom secret et l&apos;espace de nommage sont respectivement modifiés en `mysecret` et `newrelic` .

3. Supprimez ces clés inutiles dans la section métadonnées :

   * `creationTimestamp`
   * `resourceVersion`
   * `uid`

4. Installer le manifeste avec son nouveau nom et espace de nommage :

   ```shell
   kubectl apply -n newrelic -f etcd-secret.yaml
   ```

5. Configurez l&apos;intégration pour utiliser le secret nouvellement créé comme décrit dans [la section mtls](#mtls). Cela peut être fait en ajoutant la configuration suivante dans le `values.yaml` si vous installez l&apos;intégration via le graphique `nri-bundle` .

   ```yaml
   newrelic-infrastructure:
    controlPlane:
      enabled: true
      config:
        etcd:
          enabled: true
          autodiscover:
            - selector: "app=etcd,etcd=true,k8s-app=etcd"
              namespace: openshift-etcd
              matchNode: true
              endpoints:
                - url: https://<ETCD_ENDPOINT>:<PORT>
                  insecureSkipVerify: true
                  auth:
                    type: mTLS
                    mtls:
                      secretName: mysecret
                      secretNamespace: newrelic
   ```

## Consultez vos données [#check-integration-works]

Si l&apos;intégration est correctement configurée, vous verrez une vue avec tous les composants control plane et leurs statuts dans une section dédiée, comme indiqué ci-dessous.

<img title="new-relic-one-k8s-cluster-explorer-control-plane-parameters.png" alt="New Relic - Kubernetes cluster explorer - Control Plane section" src="/images/kubernetes_screenshot-crop_control-plane.webp" />

<figcaption>
  Accédez à <DNT>**[one.newrelic.com &gt; All capabilities](https://one.newrelic.com/all-capabilities) &gt; Kubernetes**</DNT> et cliquez sur <DNT>**Control plane**</DNT> dans le volet de navigation de gauche.
</figcaption>

Vous pouvez également vérifier les données control plane avec cette requête [NRQL](/docs/query-data/nrql-new-relic-query-language/getting-started/introduction-nrql) :

```sql
SELECT latest(timestamp) 
FROM K8sApiServerSample, K8sEtcdSample, K8sSchedulerSample, K8sControllerManagerSample FACET entityName 
WHERE clusterName = '_MY_CLUSTER_NAME_'
```

<Callout variant="tip">
  Si vous ne parvenez toujours pas à voir les données du plan de contrôle, consultez [cette page de dépannage](/docs/kubernetes-pixie/kubernetes-integration/troubleshooting/not-seeing-data).
</Callout>