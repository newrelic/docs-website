---
title: dbt Cloud avec intégration Airflow et Snowflake
tags:
  - New Relic integrations
  - Dbt Cloud integrations
metaDescription: Dbt Cloud with Airflow and Snowflake integration
freshnessValidatedDate: never
translationType: machine
---

Notre intégration <DNT>dbt Cloud</DNT> avec <DNT>Airflow</DNT> monitoree l&amp;apos;état de vos tâches et ressources <DNT>dbt Cloud</DNT> , vous aidant à identifier les problèmes tels que l&amp;apos;échec des exécutions, des modèles ou des tests.

Cette intégration s&apos;exécute sur <DNT>Apache Airflow</DNT> et la requête <DNT>Snowflake</DNT> pour tous les tests ayant échoué si elle est configurée pour le faire.

## Prérequis [#prerequisites]

* Compte dbt Cloud avec API activées et utilisant <DNT>Snowflake</DNT> comme base de données.
* Accès au compte <DNT>Snowflake</DNT> sur lequel le compte <DNT>dbt Cloud</DNT> s&amp;apos;exécute.
* Environnement <DNT>Airflow</DNT> existant version 2.8.1 ou supérieure, ou possibilité d&amp;apos;exécuter <DNT>Docker Compose</DNT>.

## Installer l&apos;intégration [#install]

Vous pouvez installer l&apos;intégration New Relic <DNT>dbt Cloud</DNT> avec <DNT>Airflow</DNT> de l&amp;apos;une des manières suivantes :

* Installation dans votre environnement Airflow existant. Ceci est recommandé pour l’environnement de production.
* Installation avec Docker Compose. Ceci est adapté aux POC rapides.

Sélectionnez l&apos;option la plus adaptée à vos besoins en cliquant sur son onglet :

<Tabs>
  <TabsBar>
    <TabsBarItem id="1">
      Installer sur un environnement Airflow existant
    </TabsBarItem>

    <TabsBarItem id="2">
      Installer avec Docker Compose
    </TabsBarItem>
  </TabsBar>

  <TabsPages>
    <TabsPageItem id="1">
      <Steps>
        <Step>
          Assurez-vous que vous disposez du fournisseur Snowflake, puis clonez le référentiel `newrelic-dbt-cloud-integration` en exécutant ces commandes :

          ```shell
          pip install apache-airflow-providers-snowflake>=3.0.0
          ```

          ```shell
          git clone https://github.com/newrelic-experimental/newrelic-dbt-cloud-integration.git
          ```
        </Step>

        <Step>
          Copiez le contenu de `airflow/dags` à la racine de votre dossier dags Airflow
        </Step>

        <Step>
          Créez les cinq connexions Airflow nécessaires au DAG. Le tableau suivant fournit le nom de la connexion et les informations nécessaires à sa configuration. Notez que pour tous ceux-ci, le type est `http`:

          <table>
            <thead>
              <tr>
                <th>
                  Nom de la connexion
                </th>

                <th>
                  Description
                </th>

                <th>
                  Type
                </th>

                <th>
                  Hôte et mot de passe
                </th>
              </tr>
            </thead>

            <tbody>
              <tr>
                <td class="children-nowrap">
                  `dbt_cloud_admin_api`
                </td>

                <td>
                  Vous permet de vous connecter à l&apos;API d&apos;administration de dbt Cloud avec <span class="children-nowrap">`SimpleHttpHook`</span>
                </td>

                <td>
                  <span class="children-nowrap">`http`</span>
                </td>

                <td>
                  **Hôte :** [https://cloud.getdbt.com/api/v2/accounts/ACCOUNT\_ID/](https://cloud.getdbt.com/api/v2/accounts/ACCOUNT_ID/) (Remplacez `ACCOUNT_ID` par votre identifiant de compte dbt Cloud)

                  **Mot de passe :** votre [jeton d&apos;API dbt Cloud (paramètres de profil) ou un jeton de compte de service](https://docs.getdbt.com/docs/dbt-cloud-apis/user-tokens#user-tokens)
                </td>

                <td />
              </tr>

              <tr>
                <td class="children-nowrap">
                  `dbt_cloud_discovery_api`
                </td>

                <td>
                  Permet de se connecter à l&apos;API de découverte dbt
                </td>

                <td>
                  <span class="children-nowrap">`http`</span>
                </td>

                <td>
                  **Hôte :** [https://metadata.cloud.getdbt.com/graphql](https://metadata.cloud.getdbt.com/graphql)

                  **Mot de passe :** [Compte cloud des services DBt jeton](https://docs.getdbt.com/docs/dbt-cloud-apis/user-tokens#user-tokens)
                </td>
              </tr>

              <tr>
                <td class="children-nowrap">
                  `nr_insights_insert`
                </td>

                <td>
                  Vous permet de télécharger un événement personnalisé sur New Relic
                </td>

                <td>
                  <span class="children-nowrap">`http`</span>
                </td>

                <td>
                  **Hôte :** [https://insights-collector.newrelic.com/v1/accounts/ACCOUNT\_ID/events](https://insights-collector.newrelic.com/v1/accounts/ACCOUNT_ID/events) (Remplacez `ACCOUNT_ID` par votre identifiant de compte)

                  **Mot de passe :** Vos [informations NR détaillées insérer clé API](https://one.newrelic.com/admin-portal/api-keys/insightkeys)
                </td>
              </tr>

              <tr>
                <td class="children-nowrap">
                  `nr_insights_query`
                </td>

                <td>
                  Permet de requêter New Relic événement personnalisé
                </td>

                <td>
                  <span class="children-nowrap">`http`</span>
                </td>

                <td>
                  **Hôte :** [https://insights-api.newrelic.com/v1/accounts/ACCOUNT\_ID/query](https://insights-api.newrelic.com/v1/accounts/ACCOUNT_ID/query) (Remplacez `ACCOUNT_ID` par votre identifiant de compte)

                  **Mot de passe :** Vos [informations NR détaillées requête clé API](https://one.newrelic.com/admin-portal/api-keys/insightkeys)
                </td>
              </tr>
            </tbody>
          </table>

          Une fois que vous avez configuré les quatre éléments ci-dessus, vous devez configurer la connexion Snowflake. Snowflake vous permet d&apos;interroger les lignes de test ayant échoué. Il existe [de nombreuses façons](https://airflow.apache.org/docs/apache-airflow-providers-snowflake/stable/connections/snowflake.html) de configurer une connexion Snowflake. Pour configurer à l&amp;apos;aide d&amp;apos;une paire de clés privées, renseignez l&amp;apos;attribut suivant :

          * `Type`: Flocon de neige
          * `Login`: Votre nom d&amp;apos;utilisateur Snowflake
          * `Account`: Votre compte Snowflake
          * `Warehouse`: Votre entrepôt Snowflake
          * `Role`:Votre rôle de Flocon de Neige. Le rôle doit avoir accès à toutes les bases de données utilisées dans dbt Cloud pour obtenir toutes les lignes de test ayant échoué.
          * `Private Key Text`:La clé privée complète utilisée pour cette connexion.
          * `Password`:Phrase de passe pour la clé privée si elle est cryptée. Vide s&amp;apos;il n&amp;apos;est pas chiffré.
        </Step>

        <Step>
          Terminez la configuration en activant le DAG `new_relic_data_pipeline_observability_get_dbt_run_metadata2` .
        </Step>
      </Steps>
    </TabsPageItem>

    <TabsPageItem id="2">
      <Steps>
        <Step>
          Exécutez la commande suivante pour cloner le référentiel `newrelic-dbt-cloud-integration` :

          ```shell
          git clone https://github.com/newrelic-experimental/newrelic-dbt-cloud-integration.git
          ```

          Ensuite `cd` dans le répertoire Airflow :

          ```shell
          cd newrelic-dbt-cloud-integration/airflow
          ```

          Initialisez ensuite et exécutez Docker compose en exécutant les commandes suivantes :

          ```shell
          docker-compose up airflow-init
          ```

          ```shell
          docker-compose up
          ```
        </Step>

        <Step>
          lancement de l&apos;UI Airflow : `http://localhost:8080`
        </Step>

        <Step>
          Créez les cinq connexions Airflow nécessaires au DAG. Le tableau suivant fournit le nom de la connexion et les informations nécessaires à sa configuration. Notez que pour tous ceux-ci, le type est `http`:

          <table>
            <thead>
              <tr>
                <th>
                  Nom de la connexion
                </th>

                <th>
                  Description
                </th>

                <th>
                  Type
                </th>

                <th>
                  Hôte et mot de passe
                </th>
              </tr>
            </thead>

            <tbody>
              <tr>
                <td class="children-nowrap">
                  `dbt_cloud_admin_api`
                </td>

                <td>
                  Vous permet de vous connecter à l&apos;API d&apos;administration de dbt Cloud avec <span class="children-nowrap">`SimpleHttpHook`</span>
                </td>

                <td>
                  <span class="children-nowrap">`http`</span>
                </td>

                <td>
                  **Hôte :** [https://cloud.getdbt.com/api/v2/accounts/ACCOUNT\_ID/](https://cloud.getdbt.com/api/v2/accounts/ACCOUNT_ID/) (Remplacez `ACCOUNT_ID` par votre identifiant de compte dbt Cloud)

                  **Mot de passe :** votre [jeton d&apos;API dbt Cloud (paramètres de profil) ou un jeton de compte de service](https://docs.getdbt.com/docs/dbt-cloud-apis/user-tokens#user-tokens)
                </td>

                <td />
              </tr>

              <tr>
                <td class="children-nowrap">
                  `dbt_cloud_discovery_api`
                </td>

                <td>
                  Permet de se connecter à l&apos;API de découverte dbt
                </td>

                <td>
                  <span class="children-nowrap">`http`</span>
                </td>

                <td>
                  **Hôte :** [https://metadata.cloud.getdbt.com/graphql](https://metadata.cloud.getdbt.com/graphql)

                  **Mot de passe :** [Compte cloud des services DBt jeton](https://docs.getdbt.com/docs/dbt-cloud-apis/user-tokens#user-tokens)
                </td>
              </tr>

              <tr>
                <td class="children-nowrap">
                  `nr_insights_insert`
                </td>

                <td>
                  Vous permet de télécharger un événement personnalisé sur New Relic
                </td>

                <td>
                  <span class="children-nowrap">`http`</span>
                </td>

                <td>
                  **Hôte :** [https://insights-collector.newrelic.com/v1/accounts/ACCOUNT\_ID/events](https://insights-collector.newrelic.com/v1/accounts/ACCOUNT_ID/events) (Remplacez `ACCOUNT_ID` par votre identifiant de compte)

                  **Mot de passe :** Vos [informations NR détaillées insérer clé API](https://one.newrelic.com/admin-portal/api-keys/insightkeys)
                </td>
              </tr>

              <tr>
                <td class="children-nowrap">
                  `nr_insights_query`
                </td>

                <td>
                  Permet de requêter New Relic événement personnalisé
                </td>

                <td>
                  <span class="children-nowrap">`http`</span>
                </td>

                <td>
                  **Hôte :** [https://insights-api.newrelic.com/v1/accounts/ACCOUNT\_ID/query](https://insights-api.newrelic.com/v1/accounts/ACCOUNT_ID/query) (Remplacez `ACCOUNT_ID` par votre identifiant de compte)

                  **Mot de passe :** Vos [informations NR détaillées requête clé API](https://one.newrelic.com/admin-portal/api-keys/insightkeys)
                </td>
              </tr>
            </tbody>
          </table>

          Une fois que vous avez configuré les quatre éléments ci-dessus, vous devez configurer la connexion Snowflake. Snowflake vous permet d&apos;interroger les lignes de test ayant échoué. Il existe [de nombreuses façons](https://airflow.apache.org/docs/apache-airflow-providers-snowflake/stable/connections/snowflake.html) de configurer une connexion Snowflake. Pour configurer à l&amp;apos;aide d&amp;apos;une paire de clés privées, renseignez l&amp;apos;attribut suivant :

          * `Type`: Flocon de neige
          * `Login`: Votre nom d&amp;apos;utilisateur Snowflake
          * `Account`: Votre compte Snowflake
          * `Warehouse`: Votre entrepôt Snowflake
          * `Role`:Votre rôle de Flocon de Neige. Le rôle doit avoir accès à toutes les bases de données utilisées dans dbt Cloud pour obtenir toutes les lignes de test ayant échoué.
          * `Private Key Text`:La clé privée complète utilisée pour cette connexion.
          * `Password`:Phrase de passe pour la clé privée si elle est cryptée. Vide s&amp;apos;il n&amp;apos;est pas chiffré.
        </Step>

        <Step>
          Terminez la configuration en activant le DAG `new_relic_data_pipeline_observability_get_dbt_run_metadata2` .
        </Step>
      </Steps>
    </TabsPageItem>
  </TabsPages>
</Tabs>

## Trouvez vos données [#find-data]

Cette intégration crée et rapporte trois événements personnalisés à New Relic :

<CollapserGroup>
  <Collapser id="collapser-1-in-group-2" title="`dbt_job_run`">
    `dbt_job_run`:Fournit des métadonnées et l&amp;apos;état de toutes les exécutions terminées. Cet événement n&amp;apos;inclut aucune donnée sur les modèles, les instantanés, les graines et les tests. les attributs incluent :

    * `project_name`
    * `environment_name`
    * `run_team`
    * Tous les champs répertoriés dans l&apos; [API dbt Cloud v2 pour les exécutions](https://docs.getdbt.com/dbt-cloud/api-v2#/operations/Retrieve%20Run).

    Tous les attributs autres que `project_name` et `environment_name` sont précédés de `run_`

    Exemple de requête :

    ```sql
    -- Get status of all job runs in the past seven days
    select 
        project_name,
        environment_name,
        job_name,
        run_created_at,
        run_run_duration_humanized,
        run_status,
        run_status_humanized,
        run_status_message
    from dbt_job_run
    since 7 days ago
    ```
  </Collapser>

  <Collapser id="collapser-1-in-group-2" title="`dbt_resource_run`">
    `dbt_resource_run` Fournit des métadonnées et des statuts pour toutes les ressources exécutées dans une tâche dbt. Les ressources comprennent des modèles, des instantanés, des graines et des tests. les attributs incluent :

    * Tous les attributs dans `dbt_job_run`
    * `team` (Configuré dans la méta du projet dbt)
    * `alert_failed_test_rows`
    * `failed_test_rows_limit`
    * `slack_mentions`
    * `message`
    * `resource_type`
    * `unique_id`
    * `database_name`
    * `schema_name`
    * `test_column_name`
    * `test_model_name`
    * `test_namespace`
    * `test_parameters`
    * `test_short_name`
    * `alias`
    * `severity`
    * `warn_if`
    * `error_if`
    * `tags`
    * `path`
    * `original_file_path`
    * `meta`
    * `meta_config`

    Exemple de requête :

    ```sql
    -- Get status of all resources run in the past day
    -- Status = 'None' means the resource exists in the project but was not executed in a particular run
    select 
        project_name,
        environment_name,
        job_name,
        run_created_at,
        resource_type,
        name,
        status
    from dbt_resource_run
    where status != 'None'
    since 1 day ago
    limit 200
    ```

    ```sql
    -- Get all resources types in the past day
    select 
        uniques(resource_type)
    from dbt_resource_run 
    since 1 day ago
    ```

    ```sql
    -- Get the count of all statuses in the last day
    -- Status = 'None' means the resource exists in the dbt project, but was not executed in a particular run
    select 
        count(*) as total_count
    from dbt_resource_run
    facet
        status
    since 1 day ago
    ```
  </Collapser>

  <Collapser id="collapser-3-in-group-2" title="`dbt_failed_test_rows`">
    `dbt_failed_test_rows`:Fournit des métadonnées et jusqu&amp;apos;aux dix premières colonnes des résultats d&amp;apos;une requête de test ayant échoué. Cet événement n&amp;apos;est créé que lorsque la méta-configuration d&amp;apos;un test dbt comporte `alert_failed_test_rows`: `true`. les attributs incluent :

    * Tous les attributs dans `dbt_resource_run`
    * `field_1` - `field_10` représentant les dix premières colonnes renvoyées dans une requête de test

    Exemple de requête :

    ```sql
    select 
        project_name,
        environment_name,
        job_name,
        run_created_at,
        name,
        field_1,
        field_2,
        field_3,
        field_4,
        field_5,
        field_6,
        field_7,
        field_8,
        field_9,
        field_10
    from dbt_failed_test_row
    since 7 days ago
    ```
  </Collapser>
</CollapserGroup>

## Configuration DAG

### Relations:

Ce DAG est destiné à fonctionner tel quel sans aucune configuration. Dans le même temps, nous réalisons que votre entreprise peut avoir ses propres conventions de dénomination pour les connexions. En tant que tel, nous avons une configuration simple à l&apos;intérieur `dag_config.yml` où vous pouvez définir le nom des différentes connexions.

```yaml
connections:
  dbt_cloud_admin_api: dbt_cloud_admin_api
  dbt_cloud_discovery_api: dbt_cloud_discovery_api 
  nr_insights_query: nr_insights_query 
  nr_insights_insert: nr_insights_insert
  snowflake_api: SNOWFLAKE 
```

### Équipe de course :

les tâches dbt peuvent appartenir à différentes équipes, mais il n&apos;y a aucun endroit pour définir cela dans dbt Cloud. Nous pouvons utiliser le code Python pour définir l’équipe de manière dynamique. Pour écrire votre propre code, modifiez `airflow/dags/nr_utils/nr_utils.py` et mettez toute logique nécessaire dans `get_team_from_run()`. Les données d’exécution transmises à cette fonction ont accès à l’attribut suivant.

* project\_name
* environment\_name
* Tous les champs répertoriés dans l&apos; [API dbt Cloud v2 pour les exécutions](https://docs.getdbt.com/dbt-cloud/api-v2#/operations/Retrieve%20Run). Tous les attributs sont précédés de « run\_ »

Voici un exemple de fonction :

```python
def get_team_from_run(run: dict) -> str:
    team = 'Data Engineering' 
    if run['project_id'] == '11111' and run['environment_id'] in ['55555', '33333']:
        team = 'Platform'
    if re.match(r'Catch-all', run['job_name']):
        team = 'Project Catch All'
    return team
```

## Configuration du projet DBt

Dans le cadre du projet Dbt, nous pouvons utiliser la méta-configuration pour définir une équipe supplémentaire et des paramètres spécifiques aux tests.

* `Team`:Bien que `run_team determines` soit propriétaire des tâches, nous avons parfois besoin que les équipes en amont ou en aval reçoivent des notifications d&amp;apos;alerte sur les ressources ayant échoué, comme les tests et les modèles. La constitution d’une équipe nous aide à y parvenir.
* `alert_failed_test_rows`: Le paramètre `True` activera les lignes de test ayant échoué où nous exécutons la requête pour les tests ayant échoué et enverrons jusqu&amp;apos;aux 10 premières colonnes à New Relic
* `failed_test_rows_limit`:Nombre maximal de lignes de test ayant échoué à envoyer à New Relic. Nous avons une limite codée en dur de 100 lignes pour éviter les situations où nous envoyons des quantités déraisonnables à New Relic.
* `slack_mentions`:Si vous activez les alertes Slack, ce champ vous permet de définir qui doit être mentionné dans le message.

La définition de cette valeur sur `dbt_project.yml` définirait l&amp;apos;équipe sur « Ingénierie des données » et activerait les lignes de test ayant échoué.

```yaml
models:
  dbt_fake_company:
    +meta:
      nr_config:
        team: 'Data Engineering'
        alert_failed_test_rows: False 
        failed_test_rows_limit: 5
        slack_mentions: '@channel, @business_users'
```

Nous pouvons ajouter un autre attribut appelé message aux ressources. Dans la configuration suivante, une équipe commerciale partenaire peut être alertée sur des tests spécifiques ayant échoué. De plus, nous pouvons définir des alertes sur les lignes de test ayant échoué elles-mêmes.

```yaml
models:
  - name: important_business_model
    tests:
      - some_custom_test:
        config:
          meta:
            nr_config:
              team: 'Upstream Business Team'
              alert_failed_test_rows: true 
              failed_test_rows_limit: 10 
              slack_mentions: '@channel, @business_user1, @engineer1'
              message: 'Important business process produced invalid data. Please check X tool' 
```

## Dépannage [#troubleshooting]

Différentes versions d&apos;Airflow combinées à différentes versions de fournisseurs peuvent induire des changements radicaux. Dans certains cas, vous devrez peut-être modifier le code pour qu&apos;il corresponde aux versions spécifiques de votre environnement Airflow. Nous suivons les problèmes connus dans notre [référentiel Github](https://github.com/newrelic-experimental/newrelic-dbt-cloud-integration/issues).