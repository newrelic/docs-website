---
title: Intégration Databricks
tags:
  - Databricks
  - databricks integration
  - New Relic integration
metaDescription: Use the Databricks integration to collect telemetry from the Databricks Data Intelligence Platform
freshnessValidatedDate: '2026-01-26T00:00:00.000Z'
translationType: machine
---

L&apos;intégration Databricks est une application autonome qui collecte des données de télémétrie à partir de la plateforme d&apos;intelligence de données Databricks, à utiliser pour le dépannage et l&apos;optimisation des charges de travail Databricks.

L&apos;intégration collecte les types de télémétrie suivants :

* Métriques des applications Apache Spark, telles que les métriques de mémoire et de CPU des exécuteurs Spark, les durées des tâches Spark, les durées et les métriques d&apos;E/S des étapes et des tâches Spark, et les métriques de mémoire et de disque des RDD Spark
* Métriques d&apos;exécution des tâches Databricks Lakeflow, telles que les durées, les heures de début et de fin, ainsi que les codes et types de terminaison pour les exécutions de tâches et de jobs.
* Mises à jour des métriques du pipeline déclaratif Databricks Lakeflow, telles que les durées, les heures de début et de fin, et l&apos;état d&apos;achèvement des mises à jour et des flux.
* Logs d&apos;événements du pipeline déclaratif Databricks Lakeflow
* Métriques de requête Databricks, y compris les temps d&apos;exécution et les métriques d&apos;E/S de requête.
* Métriques et logs d&apos;intégrité du cluster Databricks, tels que la mémoire et les métriques CPU du pilote et des workers, ainsi que les logs du pilote et de l&apos;exécuteur.
* Données de consommation et de coût Databricks qui peuvent être utilisées pour afficher la consommation DBU et les coûts Databricks estimés.

## Installer l&apos;intégration [#setup]

L&apos;intégration Databricks est destinée à être déployée sur le nœud du pilote d&apos;un [cluster](https://docs.databricks.com/en/getting-started/concepts.html#cluster) Databricks polyvalent, de tâche ou de pipeline. Pour déployer l&apos;intégration de cette manière, suivez les étapes pour [déployer l&apos;intégration sur un cluster Databricks](https://github.com/newrelic/newrelic-databricks-integration/docs/installation.md#deploy-the-integration-to-a-databricks-cluster).

L&apos;intégration Databricks peut également être déployée à distance sur un environnement hôte pris en charge. Pour déployer l&apos;intégration de cette manière, suivez les étapes pour [déployer l&apos;intégration à distance](https://github.com/newrelic/newrelic-databricks-integration/docs/installation.md#deploy-the-integration-remotely).

## Vérifier l&apos;installation [#verify-installation]

Une fois que l&apos;intégration Databricks a fonctionné pendant quelques minutes, utilisez le [générateur de requêtes](https://one.newrelic.com/data-exploration/query-builder) dans New Relic pour exécuter la requête suivante, en remplaçant `[YOUR_CLUSTER_NAME]` par le *nom* du cluster Databricks *où l&apos;intégration a été installée* (notez que si le nom de votre cluster inclut `'`, vous devez l&apos;échapper avec `\`) :

`SELECT uniqueCount(executorId) AS Executors FROM SparkExecutorSample WHERE databricksClusterName = '[YOUR_CLUSTER_NAME]'`

Le résultat de la requête doit être **un nombre supérieur à zéro**.

## Importer les exemples de dashboards (facultatif) [#add-dashboard]

Pour vous aider à démarrer avec la télémétrie collectée, installez nos dashboards pré-construits à l&apos;aide de l&apos;[installation guidée](https://one.newrelic.com/marketplace?state=34e67b15-4fe1-28ef-ff41-99658fb36820).

Alternativement, vous pouvez installer les dashboards pré-construits en suivant les instructions trouvées dans [Importer les exemples de tableaux de bord](https://github.com/newrelic/newrelic-databricks-integration/docs/example-dashboards.md).

## En savoir plus

Pour en savoir plus sur l&apos;intégration Databricks, consultez le [dépôt](https://github.com/newrelic/newrelic-databricks-integration) officiel de l&apos;intégration New Relic Databricks.