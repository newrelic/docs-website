---
title: Intégration du GPU NVIDIA
tags:
  - NVIDIA integration
  - GPU integration
  - New Relic integrations
metaDescription: Use New Relic infrastructure agent to get a dashboard with GPU metrics.
freshnessValidatedDate: '2023-08-10T00:00:00.000Z'
translationType: machine
---

Notre intégration GPU NVIDIA vous permet de monitorer l&apos;état de vos GPU. Cette intégration utilise notre agent d&apos;infrastructure avec l&apos;intégration Flex, qui nous permet d&apos;accéder à l&apos;utilitaire SMI de NVIDIA.

<img title="NVIDIA GPUs dashboard" alt="NVIDIA GPUs dashboard" src="/images/infrastructure_screenshot-full_NVIDIA-GPU-dashboard.webp" />

<figcaption>
  Après avoir configuré notre intégration GPU NVIDIA, nous vous fournissons un dashboard pour vos métriques GPU.
</figcaption>

Lors de l&apos;installation, vous obtiendrez un dashboard prédéfini contenant des mesures GPU cruciales :

* Utilisation du GPU
* Nombre d&apos;erreurs ECC
* Processus de calcul actifs
* États d&apos;horloge et de performances
* Température et vitesse du ventilateur
* Informations dynamiques et statiques sur chaque périphérique pris en charge

<Steps>
  <Step>
    ## Installer l&apos; agent d&apos;infrastructure

    Pour capturer des données avec New Relic, installez notre agent d&apos;infrastructure. Notre agent d&apos;infrastructure collecte et ingère des données afin que vous puissiez suivre les performances de vos GPU.

    Vous pouvez installer l&apos; agent d&apos;infrastructure de deux manières différentes :

    * Notre [guide d&apos;installation](https://one.newrelic.com/nr1-core?state=4f81feab-35f7-e97e-9903-52510f8542bd) est un outil CLI qui inspecte votre système et installe agent l&apos; d&apos;infrastructure aux côtés de l&apos;monitoring d&apos;applicationqui agent fonctionne le mieux pour votre système. Pour en savoir plus sur le fonctionnement de notre guide d&apos;installation, consultez notre [aperçu du guide d&apos;installation](/docs/infrastructure/host-integrations/installation/new-relic-guided-install-overview).
    * Si vous préférez installer notre agent d&apos;infrastructure manuellement, vous pouvez suivre un tutoriel d&apos;installation manuelle pour [Linux](/docs/infrastructure/install-infrastructure-agent/linux-installation/install-infrastructure-monitoring-agent-linux), [Windows](/docs/infrastructure/install-infrastructure-agent/windows-installation/install-infrastructure-monitoring-agent-windows/).
  </Step>

  <Step>
    ## Configurer l&apos;intégration Flex pour les GPU NVIDIA

    Flex est fourni avec l&apos;agent New Relic Infrastructure et peut être intégré à [NVIDIA SMI](https://developer.nvidia.com/nvidia-management-library-nvml), un utilitaire de ligne de commande permettant de monitorer les périphériques GPU NVIDIA.

    <Callout variant="important">
      nvidia-smi est livré préinstallé avec les pilotes d&apos;affichage GPU NVIDIA sur Linux et Windows Server.
    </Callout>

    Suivez ces étapes pour configurer Flex :

    1. Créez un fichier nommé `nvidia-smi-gpu-monitoring.yml` dans ce chemin :

    ```shell
    sudo touch /etc/newrelic-infra/integrations.d/nvidia-smi-gpu-monitoring.yml
    ```

    Vous pouvez également télécharger depuis le [référentiel git](https://github.com/newrelic/nri-flex/blob/master/examples/nvidia-smi-gpu-monitoring.yml).

    2. Mettre à jour le fichier `nvidia-smi-gpu-monitoring.yml` avec la configuration d&apos;intégration :

    ```yml
      --- 
      integrations:
        - name: nri-flex
          # interval: 30s
          config:
            name: NvidiaSMI
            variable_store:
              metrics: 
                "name,driver_version,count,serial,pci.bus_id,pci.domain,pci.bus,\
                pci.device_id,pci.sub_device_id,pcie.link.gen.current,pcie.link.gen.max,\
                pcie.link.width.current,pcie.link.width.max,index,display_mode,display_active,\
                persistence_mode,accounting.mode,accounting.buffer_size,driver_model.current,\
                driver_model.pending,vbios_version,inforom.img,inforom.oem,inforom.ecc,inforom.pwr,\
                gom.current,gom.pending,fan.speed,pstate,clocks_throttle_reasons.supported,\
                clocks_throttle_reasons.gpu_idle,clocks_throttle_reasons.applications_clocks_setting,\
                clocks_throttle_reasons.sw_power_cap,clocks_throttle_reasons.hw_slowdown,clocks_throttle_reasons.hw_thermal_slowdown,\
                clocks_throttle_reasons.hw_power_brake_slowdown,clocks_throttle_reasons.sw_thermal_slowdown,\
                clocks_throttle_reasons.sync_boost,memory.total,memory.used,memory.free,compute_mode,\
                utilization.gpu,utilization.memory,encoder.stats.sessionCount,encoder.stats.averageFps,\
                encoder.stats.averageLatency,ecc.mode.current,ecc.mode.pending,ecc.errors.corrected.volatile.device_memory,\
                ecc.errors.corrected.volatile.dram,ecc.errors.corrected.volatile.register_file,ecc.errors.corrected.volatile.l1_cache,\
                ecc.errors.corrected.volatile.l2_cache,ecc.errors.corrected.volatile.texture_memory,ecc.errors.corrected.volatile.cbu,\
                ecc.errors.corrected.volatile.sram,ecc.errors.corrected.volatile.total,ecc.errors.corrected.aggregate.device_memory,\
                ecc.errors.corrected.aggregate.dram,ecc.errors.corrected.aggregate.register_file,ecc.errors.corrected.aggregate.l1_cache,\
                ecc.errors.corrected.aggregate.l2_cache,ecc.errors.corrected.aggregate.texture_memory,ecc.errors.corrected.aggregate.cbu,\
                ecc.errors.corrected.aggregate.sram,ecc.errors.corrected.aggregate.total,ecc.errors.uncorrected.volatile.device_memory,\
                ecc.errors.uncorrected.volatile.dram,ecc.errors.uncorrected.volatile.register_file,ecc.errors.uncorrected.volatile.l1_cache,\
                ecc.errors.uncorrected.volatile.l2_cache,ecc.errors.uncorrected.volatile.texture_memory,ecc.errors.uncorrected.volatile.cbu,\
                ecc.errors.uncorrected.volatile.sram,ecc.errors.uncorrected.volatile.total,ecc.errors.uncorrected.aggregate.device_memory,\
                ecc.errors.uncorrected.aggregate.dram,ecc.errors.uncorrected.aggregate.register_file,ecc.errors.uncorrected.aggregate.l1_cache,\
                ecc.errors.uncorrected.aggregate.l2_cache,ecc.errors.uncorrected.aggregate.texture_memory,ecc.errors.uncorrected.aggregate.cbu,\
                ecc.errors.uncorrected.aggregate.sram,ecc.errors.uncorrected.aggregate.total,retired_pages.single_bit_ecc.count,\
                retired_pages.double_bit.count,retired_pages.pending,temperature.gpu,temperature.memory,power.management,power.draw,\
                power.limit,enforced.power.limit,power.default_limit,power.min_limit,power.max_limit,clocks.current.graphics,clocks.current.sm,\
                clocks.current.memory,clocks.current.video,clocks.applications.graphics,clocks.applications.memory,\
                clocks.default_applications.graphics,clocks.default_applications.memory,clocks.max.graphics,clocks.max.sm,clocks.max.memory,\
                mig.mode.current,mig.mode.pending"
            apis:
              - name: NvidiaGpu
                commands:
                  - run: nvidia-smi --query-gpu=${var:metrics} --format=csv # update this if you have an alternate path
                    output: csv
                rename_keys:
                  " ": ""
                  "\\[MiB\\]": ".MiB"
                  "\\[%\\]": ".percent"
                  "\\[W\\]": ".watts"
                  "\\[MHz\\]": ".MHz"
                value_parser:
                  "clocks|power|fan|memory|temp|util|ecc|stats|gom|mig|count|pcie": '\d*\.?\d+'
                  '.': '\[N\/A\]|N\/A|Not Active|Disabled|Enabled|Default'
    ```
  </Step>

  <Step>
    ## Confirmer que les métriques GPU sont ingérées

    La configuration Flex sera automatiquement détectée et exécutée par l&apos;agent d&apos;infrastructure, il n&apos;est pas nécessaire de redémarrer l&apos;agent. Vous pouvez confirmer que les métriques sont ingérées en exécutant cette requête NRQL :

    ```sql
      SELECT * FROM NvidiaGpuSample
    ```
  </Step>

  <Step>
    ## Monitorer votre application

    Vous pouvez utiliser notre modèle dashboard prédéfini pour monitorer vos métriques GPU. Suivez ces étapes :

    1. Allez à <DNT>**[one.newrelic.com](https://one.newrelic.com/)**</DNT> et cliquez sur <DNT>**Dashboards**</DNT>.
    2. Cliquez sur l&apos;onglet <DNT>**Import dashboard**</DNT> .
    3. Copiez le contenu du fichier (`.json`) depuis le [dashboard du GPU NVIDIA](https://raw.githubusercontent.com/newrelic/nri-flex/master/examples/nvidia-smi-gpu-monitoring-dashboard.json).
    4. Sélectionnez le compte cible où le dashboard doit être importé.
    5. Cliquez sur <DNT>**Import dashboard**</DNT> pour confirmer l&apos;action.

    Votre dashboard `NVIDIA GPU Monitoring` est considéré comme un dashboard personnalisé et peut être trouvé dans l&apos;UI <DNT>**Dashboards**</DNT>. Pour obtenir des documents sur l&apos;utilisation et la modification du dashboard, consultez [notre documentation dashboard ](/docs/query-your-data/explore-query-data/dashboards/introduction-dashboards).

    Voici une requête NRQL pour afficher toute la télémétrie disponible :

    ```sql
    SELECT * FROM NvidiaGpuSample
    ```
  </Step>
</Steps>

## Quelle est la prochaine étape ? [#next]

Vous pouvez adapter la configuration Flex pour inclure ou exclure des informations disponibles à partir de l&apos;utilitaire NVIDIA SMI.

Pour en savoir plus sur la création de requêtes NRQL et la génération de dashboards, consultez ces documents :

* [Introduction au générateur de requêtes](/docs/query-your-data/explore-query-data/query-builder/introduction-query-builder) pour créer des requêtes basiques et avancées.
* [Introduction au dashboard](/docs/query-your-data/explore-query-data/dashboards/introduction-dashboards) pour personnaliser votre dashboard et effectuer différentes actions.
* [Gérez votre dashboard](/docs/query-your-data/explore-query-data/dashboards/manage-your-dashboard) pour ajuster votre <InlinePopover type="dashboards" />mode d&apos;affichage ou pour ajouter plus de contenu à votre dashboard.