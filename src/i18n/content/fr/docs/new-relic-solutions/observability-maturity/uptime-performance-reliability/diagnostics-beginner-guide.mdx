---
title: 'Diagnostic technique de fiabilité : guide du débutant pour le dépannage des performances des applications'
tags:
  - Observability maturity
  - 'Uptime, performance, and reliability'
  - Site reliability engineering
  - SRE
metaDescription: 'New Relic observability maturity series: A beginner''s guide on identifying common application performance issues.'
freshnessValidatedDate: never
translationType: machine
---

Ce guide est une introduction pour améliorer vos compétences en matière de diagnostic des problèmes ayant un impact sur les clients. Vous pourrez récupérer plus rapidement des problèmes de performances des applications en suivant les procédures décrites dans ce guide.

Ce guide fait partie de notre [série sur la maturité de l&apos;observabilité](/docs/new-relic-solutions/observability-maturity/introduction).

## Prérequis

Voici quelques exigences et quelques recommandations pour l&apos;utilisation de ce guide :

* Couverture d&apos;observabilité de New Relic :

  * <DNT>**Required**</DNT> : [APM avec tracing distribué](/docs/apm/apm-ui-pages/monitoring/apm-summary-page-view-transaction-apdex-usage-data), [logs en contexte APM](/docs/apm/new-relic-apm/getting-started/get-started-logs-context) et [agent infrastructure](/docs/infrastructure/infrastructure-monitoring/get-started/get-started-infrastructure-monitoring)
  * Recommandé : [logs](/docs/logs/get-started/get-started-log-management) et [monitoring réseau](/docs/network-performance-monitoring/get-started/npm-introduction) (NPM)

* <DNT>**Required**</DNT>: [Gestion des niveaux de service](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide)

* Recommandé : une certaine expérience de l&apos;utilisation de New Relic APM, du tracing distribué, des requêtes NRQL et de l&apos;interface utilisateur<InlinePopover type="logs" />

* Recommandé : vous avez lu ces guides :

  * [Gestion de la qualité des alertes](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/alert-quality-management-guide)
  * [Gestion des niveaux de service](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide)

## Présentation [#overview]

Avant de commencer à utiliser ce guide, il vous aidera à comprendre ce que vous allez apprendre. Ce guide vous aidera à comprendre :

* Comment votre entreprise est impactée par l’amélioration de vos compétences en diagnostic.

* Les indicateurs de performances opérationnels clés utilisés pour mesurer le succès.

* Comment votre utilisateur final perçoit les différents types de problèmes de fiabilité.

* La différence entre la *cause directe* et la *cause sous-jacente* d’un problème.

* Les étapes de diagnostic de base pour trouver et résoudre les problèmes, qui comprennent :

  * Définition du problème – création d’un énoncé de problème
  * Trouver la source du problème
  * Trouver la cause directe de ce problème

* Certaines catégories de problèmes de performances (performances de sortie, performances d&apos;entrée et performances client) et la fonctionnalité New Relic utilisée pour diagnostiquer ces problèmes (APM, Synthetics<InlinePopover type="browser" />et monitoring des applications mobiles).

* Comment utiliser une matrice de problèmes, qui est une aide-mémoire pour comprendre les problèmes courants et leurs sources probables.

Vers la fin, nous passerons en revue quelques exemples de problèmes de performances, qui devraient vous aider à mieux comprendre ces concepts.

## Résultats souhaités [#desired-outcomes]

### Résumé

La valeur pour l&apos;entreprise est :

* Réduire le nombre d&apos;incidents perturbant l&apos;activité
* Réduire le temps nécessaire à la résolution des problèmes (MTTR)
* Réduire le coût opérationnel de l&apos;incident

La valeur pour les opérations informatiques et SRE est :

* Améliorer le temps de compréhension et de résolution

### résultats de l&apos;entreprise [#business-outcome]

En 2014, [Gartner a estimé](https://blogs.gartner.com/andrew-lerner/2014/07/16/the-cost-of-downtime) que le coût moyen des temps d’arrêt informatiques était de 5 600 $ par minute. Le coût cumulé d&amp;apos;un incident ayant un impact sur l&amp;apos;entreprise est déterminé par des facteurs tels que le temps de détection, la fréquence, le temps de réparation, l&amp;apos;impact sur les revenus et le nombre d&amp;apos;ingénieurs chargés de trier et de résoudre l&amp;apos;incident. En termes simples, vous souhaitez moins d’incidents ayant un impact sur l’entreprise, une durée incident plus courte et des diagnostics plus rapides, avec moins de personnes nécessaires pour résoudre les impacts sur les performances.

En fin de compte, l’objectif commercial est de maximiser le temps de disponibilité et de minimiser les temps d’arrêt, le coût des temps d’arrêt étant le suivant :

**`Downtime minutes x Cost per minute = Downtime cost`**

Le temps d&apos;arrêt est déterminé par le nombre d&apos;incidents perturbant l&apos;activité et leur durée. Le coût des temps d’arrêt comprend de nombreux facteurs, mais les plus directement mesurables sont les coûts opérationnels et les pertes de revenus.

L&apos;entreprise doit réduire les éléments suivants :

* nombre d&apos;incidents perturbant l&apos;activité
* coût opérationnel de l&apos;incident

### Résultat opérationnel [#operational-outcome]

Le résultat opérationnel requis est de maintenir la conformité aux objectifs de niveau de service au niveau du produit. Pour ce faire, vous devez diagnostiquer le niveau de service dégradé, communiquer votre diagnostic et effectuer une résolution rapide. Mais des dégradations et incidents inattendus surviennent toujours et vous devez réagir rapidement et efficacement.

Dans d’autres guides de cette série, nous nous concentrons sur l’amélioration <DNT>**time to know**</DNT>. Dans notre [guide de gestion de la qualité des alertes](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/alert-quality-management-guide), nous nous concentrons sur <DNT>**reactive**</DNT> les moyens d&amp;apos;améliorer le temps de connaissance, et dans notre [guide Gestion des niveaux de service,](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide) nous nous concentrons sur <DNT>**proactive**</DNT> les méthodes.

Dans le guide que vous lisez actuellement, nous nous concentrons sur l&apos;amélioration <DNT>**time to understand**</DNT> et <DNT>**time to resolve**</DNT>.

## indicateurs de performances clés - opérationnels [#operational-kpis]

De nombreuses mesures sont discutées et débattues dans le monde de la « gestion incident » et de la théorie SRE ; cependant, la plupart s&apos;accordent à dire qu&apos;il est important de se concentrer sur un petit ensemble d&apos;indicateurs de performances clés.

Les KPI ci-dessous sont les indicateurs les plus courants utilisés par les pratiques réussies SRE et de gestion incident .

<CollapserGroup>
  <Collapser id="slo-compliance" title="Conformité aux objectifs de niveau de service (SLO)">
    C&apos;est votre indicateur principal. Le niveau de service mesure le début de la dégradation des performances, la tendance des performances, l&apos;étendue de l&apos;impact et le moment où le problème a été résolu.

    Pour en savoir plus sur ce processus, consultez notre [guide Gestion des niveaux de service](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide).
  </Collapser>

  <Collapser id="time-to-know" title="Il est temps de savoir">
    C&apos;est à ce moment-là que l&apos;incident a été enregistré pour la première fois par un humain. Mesures du délai de connaissance entre le début de la dégradation du niveau de service et le moment où un enregistrement du problème de performance a été créé. Le [guide de gestion de la qualité Alert](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/alert-quality-management-guide) montre comment mesurer et améliorer cette mesure opérationnelle.
  </Collapser>

  <Collapser id="time-to-understand" title="Il est temps de comprendre">
    Il s’agit du temps écoulé entre l’enregistrement de l’incident (temps de savoir) et la résolution de l’impact (temps de résolution).
  </Collapser>

  <Collapser id="time-to-resolve" title="Il est temps de résoudre">
    Le délai de résolution est souvent appelé MTTR (temps moyen de restauration/réparation/résolution). Il est mesuré à partir du début de la dégradation des performances (déterminée par le niveau de service) jusqu&apos;au moment où le niveau de service revient au niveau de performances attendu.

    <DNT>**Note**</DNT>:Le temps de résolution ne signifie pas que la cause profonde a été identifiée et résolue de manière permanente. Les correctifs permanents font partie du processus de « gestion des problèmes », une fois l’incident résolu. Veuillez faire vos recherches sur la cause profonde par rapport à la cause directe et sur les « symptômes des causes profondes ».
  </Collapser>
</CollapserGroup>

## Perception de la fiabilité par l&apos;utilisateur final [#end-user-perception]

La façon dont les clients perçoivent les performances de votre produit est essentielle pour comprendre comment mesurer l’urgence et la priorité. De plus, comprendre le point de vue des clients permet de comprendre comment l’entreprise perçoit le problème, ainsi que de comprendre le workflow requis pour prendre en charge les capacités impactées. Une fois que vous comprenez la perception des clients et de l’entreprise, vous pouvez mieux comprendre ce qui pourrait avoir un impact sur la fiabilité de ces capacités.

En fin de compte, l’observabilité du point de vue du client est la première étape pour devenir proactif et compétent en ingénierie de fiabilité.

Il existe deux expériences principales qui ont un impact sur la perception qu&apos;a l&apos;utilisateur final des performances de votre produit numérique et de ses capacités. Les termes ci-dessous sont issus du point de vue des clients et utilisent le langage courant des clients.

<CollapserGroup>
  <Collapser id="availability" title="Disponibilité, autrement dit, ça ne marche pas">
    La disponibilité est également connue sous le nom de : connectivité, temps de disponibilité, accessibilité. Mais on le confond aussi avec le succès (sans erreurs).

    Un utilisateur final peut déclarer qu&apos;il ne peut pas accéder à une fonctionnalité requise, telle que la connexion, la navigation, la recherche ou l&apos;affichage de l&apos;inventaire. Ou bien ils peuvent simplement déclarer que l’ensemble du service n’est pas disponible. Il s’agit d’un symptôme de l’impossibilité de se connecter à un service ou d’un service renvoyant une erreur.

    Traditionnellement, la « disponibilité » ou le « temps de disponibilité » était mesuré selon une méthodologie binaire « UP/DOWN » en mesurant la capacité à se connecter à un service. La méthode traditionnelle présente une lacune critique dans la mesure où elle ne mesure que le moment où un service entier devient complètement indisponible. Cette mesure classique de fiabilité entraîne des écarts d&apos;observabilité importants, des diagnostics difficiles et l&apos;utilisateur final est significativement impacté avant de pouvoir réagir.

    La disponibilité est mesurée à la fois par « la capacité à atteindre un service », également appelée « temps de disponibilité », ET « la capacité du service à renvoyer la réponse attendue » (en d&apos;autres termes, « sans erreur »). framework de maturité d&apos;observabilité de New Relic distingue les deux par <DNT>**input performance**</DNT> (connectivité) et <DNT>**output performance**</DNT> (succès et latence de la réponse).
  </Collapser>

  <Collapser id="performance" title="Performances, autrement dit, c'est trop lent">
    Les performances sont également connues sous le nom de : latence et temps de réponse.

    Un utilisateur final peut déclarer que le service est trop lent.

    Pour les responsables informatiques comme pour les dirigeants d’entreprise, le terme « performance » peut englober un large éventail de problèmes. Dans la Gestion des niveaux de service de New Relic, la « lenteur » est mesurée à la fois dans les catégories « production » et « client ». Cependant, la majorité des problèmes de lenteur surviennent en raison d&apos;un problème de sortie, provenant de ce que l&apos;on appelle traditionnellement les « services backend ».
  </Collapser>
</CollapserGroup>

## Cause profonde vs cause directe [#root-cause-vs-direct-cause]

La cause profonde d’un problème **n’est pas** la même que la cause directe de ce problème. De même, corriger la cause directe (à court terme) ne signifie généralement pas que vous avez corrigé la cause profonde (à long terme) du problème. <DNT>**It&apos;s very important to make this distinction.**</DNT>

Lorsque vous recherchez un problème de performances, vous devez d&apos;abord tenter de trouver la cause directe du problème en posant la question « Qu&apos;est-ce qui a changé ? ». Le composant ou le comportement qui a changé n’est généralement pas la cause première, mais est en fait la cause directe que vous devez résoudre en premier. Résoudre la cause profonde est important, mais nécessite généralement une discussion rétroactive après l’incident et une gestion du problème à long terme.

Par exemple : le niveau de service de votre capacité de connexion chute soudainement. On constate immédiatement que le trafic est beaucoup plus important que d’habitude. Vous trace le problème de performances à une configuration de limite de connexion TCP ouverte qui entraîne une file d&apos;attente de connexion TCP beaucoup plus grande. Vous résolvez immédiatement le problème en déclenchant une augmentation de la limite TCP et une instance de serveur supplémentaire. Vous avez résolu la cause directe du problème à court terme, mais la cause profonde pourrait être une mauvaise planification de la capacité, une communication manquée du marketing ou un déploiement associé avec des conséquences imprévues sur les charges en amont.

Cette distinction est également faite dans ITIL/ITSM <DNT>**Incident management**</DNT> versus <DNT>**Problem management**</DNT>. Les causes profondes sont discutées lors des discussions post-incident, puis résolues dans des processus de gestion des problèmes à plus long terme.

## Étapes de diagnostic (aperçu) [#diagnostic-steps]

### Étape 1 : Définir le problème [#create-problem-statement]

La première règle est d’établir rapidement l’énoncé du problème. Il existe de nombreux guides sur la création d&apos;énoncés de problèmes, mais le meilleur est celui qui est simple et efficace. Un énoncé de problème bien formulé fera ce qui suit :

1. Décrivez ce que ressent l’utilisateur final. Quel est le problème rencontré par l’utilisateur final ?
2. Décrivez le comportement attendu de la capacité du produit. Quelle est l’expérience que l’utilisateur final devrait ressentir ?
3. Décrivez le comportement actuel de la capacité du produit. Quelle est l’évaluation technique de ce que vit l’utilisateur ?

Évitez toute hypothèse dans votre énoncé de problème. Restez fidèle aux faits.

### Étape 2 : Trouver la source [#find-source]

La « source » est le composant ou le code le plus proche de la cause directe du problème.

Pensez à de nombreuses conduites d’eau reliées par de nombreuses jonctions, séparateurs et vannes. Vous êtes alerté que votre niveau de service de production d&apos;eau est dégradé. Vous trace le problème depuis la sortie d’eau à travers les tuyaux jusqu’à ce que vous déterminiez quelle jonction, quelle division, quelle vanne ou quel tuyau est à l’origine du problème. Vous découvrez qu’une des vannes électriques est en court-circuit. Cette valve est la source de votre problème. Le court-circuit est la cause directe de votre problème. Vous résolvez facilement la cause directe en remplaçant la valeur. Gardez à l’esprit que la cause profonde peut être quelque chose de plus complexe, comme les conditions météorologiques, les produits chimiques dans l’eau ou la fabrication.

Il s’agit du même concept pour diagnostiquer une pile technologique complexe. Si votre capacité de connexion est limitée (sortie), vous devez trace au composant (source) à l&apos;origine de cette limite et le résoudre. Il peut s&apos;agir du logiciel API (limite de service), des services middleware, de la base de données, des contraintes de ressources, d&apos;un service tiers ou d&apos;autre chose.

En informatique, il existe trois catégories principales de points d’arrêt pour améliorer vos temps de réponse :

1. <DNT>
     **Output**
   </DNT>

2. <DNT>
     **Input**
   </DNT>

3. <DNT>
     **Client**
   </DNT>

Définir vos mesures de performance au sein de ces catégories, également appelées [niveau de service](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide), améliorera considérablement votre temps de réponse pour déterminer la source du problème. La mesure de ces catégories est abordée dans [notre guide Gestion des niveaux de service](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide). Pour comprendre comment les utiliser dans le diagnostic, continuez à lire.

### Étape 3 : Trouver la cause directe [#find-direct-cause]

Une fois que vous êtes proche de la source du problème, déterminez ce qui a changé. Cela vous aidera à déterminer rapidement comment résoudre immédiatement le problème à court terme. Dans l’exemple de [l’étape 2](#find-source), le changement était que la vanne ne fonctionnait plus en raison d’un matériel dégradé provoquant un court-circuit.

Voici quelques exemples de changements courants dans le domaine informatique :

1. débit (trafic)
2. Code (déploiement)
3. Ressources (allocation de matériel)
4. Changements de dépendance en amont ou en aval
5. Volume de données

Pour d’autres exemples courants de problèmes ayant un impact sur les performances, consultez la [matrice des problèmes](#problem-matrix) ci-dessous.

## Utiliser les points de données de santé [#health-data-points]

Comme mentionné précédemment, il existe trois catégories de performances principales qui lancent votre parcours de diagnostic. La compréhension de ces points de données sur la santé réduira considérablement le temps nécessaire pour comprendre la source du problème.

<CollapserGroup>
  <Collapser id="output-perf" title="Performances de sortie">
    <DNT>**This requires**</DNT>:APM

    Les performances de sortie sont la capacité de votre stack technologique interne à fournir les réponses attendues (sortie) à l&apos;utilisateur final. C&apos;est ce que l&apos;on appelle traditionnellement les services « backend».

    Dans la grande majorité des scénarios, les performances de sortie sont mesurées simplement par la vitesse de la réponse et la qualité de la réponse (en d&apos;autres termes, elle est sans erreur). N’oubliez pas la perspective utilisateur décrite ci-dessus. L’utilisateur final déclarera que le service est lent, ne fonctionne pas ou est inaccessible.

    Le problème le plus courant est la capacité à répondre aux requests finales des utilisateurs en temps opportun <DNT>**and**</DNT> et avec succès.

    Cela est facilement identifiable par une anomalie de latence ou une anomalie d’erreur sur les services qui prennent en charge la capacité du produit en difficulté.
  </Collapser>

  <Collapser id="input-perf" title="Performances d'entrée">
    <DNT>**This requires**</DNT>: Synthetics

    Les performances d&apos;entrée correspondent simplement à la capacité de vos services à recevoir une demande du client. Ce n’est pas la même chose que la capacité du client à envoyer une demande.

    Vos performances de sortie (services backend ) pourraient dépasser les niveaux de performances attendus. Cependant, quelque chose entre le client et vos services interrompt le cycle de vie de la demande-réponse. Cela peut être n’importe quoi entre le client et vos services.
  </Collapser>

  <Collapser id="client-perf" title="Performances des clients">
    <DNT>**This requires**</DNT>: monitoring des navigateurs et/ou monitoring des applications mobiles

    Les performances client correspondent à la capacité d&apos;un navigateur et/ou d&apos;une application mobile à la fois à effectuer requests et à rendre des réponses. Browser et/ou le mobile sont facilement identifiés comme la source du problème une fois que les performances de sortie (backend) et d&apos;entrée (Synthetics) ont été exclues.

    Les performances de sortie et d’entrée sont relativement faciles à exclure (ou à admettre). En raison de la profondeur des diagnostics d&apos;entrée et de sortie, le navigateur et le mobile seront abordés dans un guide de diagnostic avancé à l&apos;avenir.
  </Collapser>
</CollapserGroup>

## Matrice des problèmes [#problem-matrix]

La matrice des problèmes est une aide-mémoire des problèmes courants classés selon les trois points de données de santé.

Les sources de problèmes sont classées par ordre de fréquence, les plus courantes étant situées dans la rangée supérieure et à gauche. Une ventilation plus détaillée est présentée ci-dessous. Une gestion des niveaux de service bien effectuée vous aidera à éliminer rapidement deux de ces trois points de données.

Ce tableau est une matrice de problèmes triée par point de données de santé :

<table>
  <thead>
    <tr>
      <th>
        Point de données
      </th>

      <th>
        Fonctionalité de New Relic
      </th>

      <th>
        Sources de problèmes courants
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        Sortir
      </td>

      <td>
        APM, infrastructure, logs, NPM
      </td>

      <td>
        Application, sources de données, modification de la configuration matérielle, infrastructure, réseau interne, fournisseur tiers (AWS, GCP)
      </td>
    </tr>

    <tr>
      <td>
        Saisir
      </td>

      <td>
        Synthétique, bûche
      </td>

      <td>
        Routage externe (CDN, passerelles, etc.), routage interne, éléments sur Internet (FAI, etc.)
      </td>
    </tr>

    <tr>
      <td>
        Client
      </td>

      <td>
        Browser, mobile
      </td>

      <td>
        Code Browser ou mobile
      </td>
    </tr>
  </tbody>
</table>

Les problèmes ont tendance à s&apos;aggraver, mais l&apos;objectif est de « trouver la source » et de déterminer « ce qui a changé » afin de rétablir rapidement le niveau de service.

### Exemples de problèmes [#example-problem]

Regardons un exemple de problème. Disons que votre entreprise lance un nouveau produit et que l&apos;augmentation significative des requests entraîne des temps de réponse inacceptables. La source est découverte dans le service middleware de connexion. Le problème est un saut dans les temps d’attente TCP.

Voici un aperçu de cette situation :

* <DNT>**Category**</DNT>: performances de sortie
* <DNT>**Source**</DNT>: middleware de connexion
* <DNT>**Direct cause**</DNT>:Temps de file d&amp;apos;attente TCP en raison d&amp;apos;une charge de requête supplémentaire
* <DNT>**Solution**</DNT>: augmentation de la limite de connexion TCP et mise à l&amp;apos;échelle des ressources
* <DNT>**Root-cause**</DNT>: planification insuffisante des capacités et tests d&amp;apos;assurance qualité sur le service en aval, ce qui a un impact sur le middleware de connexion

### Un autre exemple de problème [#example-problem-2]

Voici un autre exemple de problème :

* Il y a eu une augmentation soudaine des erreurs de passerelle 500 lors de la connexion...
* Les temps de réponse de l&apos;API de connexion ont augmenté au point où les délais d&apos;attente ont commencé...
* Les délais d&apos;attente ont été tracés jusqu&apos;aux connexions de base de données dans la couche middleware...
* trace de transaction a révélé une augmentation significative du nombre de requêtes de base de données par demande de connexion...
* Un marqueur de déploiement a été trouvé pour un déploiement qui s&apos;est produit juste avant le problème.

Voici un aperçu de cette situation :

* <DNT>**Category**</DNT>:dégradation des performances de sortie entraînant une défaillance des performances d&amp;apos;entrée
* <DNT>**Source**</DNT>: service middleware appelant la base de données
* <DNT>**Direct cause**</DNT>: 10x augmentation des requêtes de base de données après déploiement du code
* <DNT>**Solution**</DNT>: restauration du déploiement
* <DNT>**Root-cause**</DNT>:tests d&amp;apos;assurance qualité insuffisants

### Matrice des problèmes par source [#problem-matrix-sources]

Voici un tableau avec la matrice des problèmes triée par sources.

<table>
  <thead>
    <tr>
      <th>
        <DNT>
          **Source**
        </DNT>
      </th>

      <th>
        <DNT>
          **Common direct causes**
        </DNT>
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        Application
      </td>

      <td>
        1. Déploiement récent (code)
        2. Contraintes de ressources matérielles
        3. Contraintes de la base de données
        4. Modification de configuration (matériel, routage ou réseau)
        5. Dépendance de tiers
      </td>
    </tr>

    <tr>
      <td>
        Source des données
      </td>

      <td>
        1. Contraintes de la base de données
        2. Changement dans la logique de la requête (n+1)
        3. fichier d&apos;attente des messages (entraînant généralement une mauvaise performance du producteur ou du consommateur)
      </td>
    </tr>

    <tr>
      <td>
        Réseaux internes et routage
      </td>

      <td>
        1. Équilibreurs de charge
        2. Procurations
        3. Gateways API
        4. Routeurs (rares)
        5. FAI/CDN (rare)
      </td>
    </tr>
  </tbody>
</table>

## Identification des anomalies du modèle de performance [#pattern-anomalies]

<Callout variant="tip">
  Avoir [un niveau de service](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide) bien défini sur vos limites de service relatives à la clé de transaction (capacités) vous aidera à identifier plus rapidement dans quel workflow de bout en bout réside le problème.
</Callout>

L’identification des anomalies de modèle améliorera votre capacité à identifier quelle est la cause directe des problèmes et où elle se situe.

Il existe de nombreuses informations utiles et des cours en ligne gratuits sur l&apos;identification des modèles, mais le concept général est plutôt simple et peut débloquer de puissantes capacités de diagnostic.

La clé pour identifier les modèles et les anomalies dans les données de performances est que vous n&apos;avez pas besoin de savoir comment le service devrait fonctionner : vous devez uniquement déterminer si le comportement récent a changé.

Les exemples fournis dans cette section utilisent le temps de réponse ou la latence comme métrique, mais vous pouvez appliquer la même analyse à presque tous les ensembles de données, tels que les erreurs, le débit, les métriques des ressources matérielles, les profondeurs de file d&apos;attente, etc.

### Normale [#normal]

Vous trouverez ci-dessous un exemple de graphique de temps de réponse apparemment volatil (7 jours) dans APM. En y regardant de plus près, vous pouvez voir que le comportement du temps de réponse est répétitif. En d’autres termes, il n’y a pas de changement radical dans le comportement sur une période de 7 jours. Les pics sont répétitifs et pas inhabituels par rapport au reste de la chronologie.

<img alt="normal pattern" title="Normal pattern" src="/images/solutions_screenshot-full_oma-upr-pattern-normal.webp" />

En fait, si vous changez la vue des données de <DNT>**average over time**</DNT> à <DNT>**percentiles over time**</DNT>, il devient encore plus clair à quel point les changements dans le temps de réponse sont « réguliers ».

<img alt="normal pattern with percentile" title="Normal pattern with percentile" src="/images/solutions_screenshot-full_normal-percentile-pattern.webp" />

### Anormal [#abnormal]

Ce graphique montre un temps de réponse d&apos;application qui semble avoir augmenté de manière inhabituelle par rapport au comportement récent.

<img alt="abnormal pattern" title="Abnormal pattern" src="/images/solutions_screenshot-full_pattern-abnormal.webp" />

Cela peut être confirmé en utilisant la comparaison d’une semaine à l’autre.

<img alt="abnormal pattern week-over-week" title="Abnormal pattern week-over-week comparison" src="/images/solutions_screenshot-full_pattern-abnormal-compare.webp" />

Nous constatons que la tendance a changé et qu’elle semble s’aggraver par rapport à la comparaison de la semaine dernière.

## Trouver la source [#finding-source]

Ensuite, nous verrons comment trouver la source dans New Relic. Notez que ce workflow repose sur le tracing distribué.

Tout d’abord, vous trouverez une application liée à la latence ou aux erreurs rencontrées par l’utilisateur final. Cela ne signifie pas que l&apos;application ou le code est le problème, mais trouver une application dans le flux (*en premier*) vous rapproche plus rapidement de la source. Une fois cette application trouvée, vous pouvez rapidement exclure des composants tels que le code, l&amp;apos;hôte, la base de données, la configuration et le réseau.

Une fois l’application identifiée, la question est de savoir quelles transactions au sein de cette application font partie du problème. Utilisez l’application que vous avez identifiée comme rencontrant un problème de performances et identifiez la ou les transactions concernées. Ici, vous pouvez répéter la compétence d&apos;anomalie du modèle de performance décrite ci-dessus dans [Identifier les anomalies du modèle de performance](#pattern-anomalies), mais cette fois sur les transactions elles-mêmes.

Les documents suivants vous aideront à utiliser New Relic pour identifier les transactions problématiques :

1. [Page Transactions : Rechercher des problèmes de performances spécifiques](/docs/apm/apm-ui-pages/monitoring/transactions-page-find-specific-performance-problems/)
2. [Transactions lentes sur la page de résumé du service](/whats-new/2021/03/slow-transactions)

Une fois les transactions problématiques identifiées, vous pouvez utiliser le tracing distribué pour examiner les composants de bout en bout prenant en charge cette transaction. Le tracing distribué vous aide à identifier rapidement où se trouve la latence et/ou où se produisent les erreurs sur l&apos;ensemble de votre stack, le tout dans une seule vue.

Les ressources suivantes vous aideront à apprendre à utiliser le tracing distribué pour identifier le composant source du problème :

1. [Introduction au tracing distribué](/docs/distributed-tracing/concepts/introduction-distributed-tracing)
2. [Comment utiliser l&apos;interface utilisateur de tracing distribué](/docs/distributed-tracing/ui-data/understand-use-distributed-tracing-ui/)
3. [Webinaires en ligne gratuits sur le tracing distribués](https://learn.newrelic.com/new-relic-distributed-tracing-tracking-across-your-application-stacks)
4. [Une vidéo sur l&apos;utilisation du tracing distribué pour l&apos;analyse des causes directes](https://www.youtube.com/watch?v=r9ImAQ5J5h4)

Voici un bref résumé des étapes de recherche de la source :

1. Examiner une application liée aux performances impactées.
2. Identifiez les transactions qui contribuent au problème.
3. Utilisez le tracing distribué pour identifier le composant problématique dans le flux de bout en bout.

Nous pouvons maintenant passer à l’étape finale, identifier la cause directe.

## Trouver la cause directe [#finding-direct-cause]

Une fois le composant source trouvé, vous pouvez commencer à déterminer la cause directe.

En utilisant les connaissances acquises lors des étapes précédentes, vous saurez si le problème est lié à la latence, au succès ou aux deux.

les problèmes de latence peuvent être détectés à l&apos;aide de trace de transaction et/ou &quot;span en cours de processus&quot; au sein des traces distribuées.

Les messages d&apos;erreur de problèmes de réussite peuvent également être vus dans la trace, mais les meilleurs détails sur les problèmes de réussite se trouvent généralement dans les logs d&apos;application.

Quoi qu&apos;il en soit, si vous êtes l&apos;intervenant de premier niveau incident ou un SRE, la recherche de la cause directe incombe aux experts en la matière (SME), qui sont généralement les développeurs et les ingénieurs responsables du composant source trouvé.

L’étape suivante la plus efficace après avoir découvert le composant source est de contacter les experts en la matière pour ledit composant. Montrez-leur les données découvertes lors du triage et des diagnostics que vous avez effectués pour avoir une longueur d&apos;avance sur le dépannage.

<Callout variant="tip">
  Notez que les logs en contexte et le tracing distribué sont activés par défaut avec nos nouveaux agents <InlinePopover type="apm" />. (Si vous n&amp;apos;avez pas mis à jour vos agents depuis un certain temps, nous vous recommandons [de mettre à jour régulièrement vos agents](/docs/new-relic-solutions/new-relic-one/install-configure/update-new-relic-agent).)

  Les logs en contexte et le tracing distribués sont des fonctionnalités essentielles nécessaires pour réduire votre temps de triage, de diagnostic et de résolution des problèmes à long terme.
</Callout>

Maintenant, allez-y et devenez un excellent ingénieur fiabilité des sites avec New Relic !

## Prochaines étapes [#next-steps]

Si vous ne l&apos;avez pas déjà fait, vous souhaiterez peut-être lire certains de nos [guides de maturité d&apos;observabilité](/docs/new-relic-solutions/observability-maturity/introduction) connexes, notamment :

* [Gestion de la qualité des alertes](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/alert-quality-management-guide)
* [Gestion des niveaux de service](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide)