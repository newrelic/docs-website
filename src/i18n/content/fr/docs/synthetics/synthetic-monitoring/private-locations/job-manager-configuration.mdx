---
title: Synthetics du gestionnaire de tâches configuration
tags:
  - synthetics
  - Synthetic monitoring
  - Private locations
metaDescription: Customize your New Relic synthetics job manager.
freshnessValidatedDate: '2024-07-29T00:00:00.000Z'
translationType: machine
---

Ce document vous guidera dans la configuration de votre [gestionnaire de tâches Synthetics](/docs/synthetics/synthetic-monitoring/private-locations/install-job-manager) en vous montrant comment :

* Utilisez [des variables d’environnement](#environment-variables) pour configurer votre gestionnaire de tâches Synthetics.
* Configurez [des modules personnalisés](#custom-modules) pour [l&apos;API scriptée](/docs/synthetics/synthetic-monitoring/scripting-monitors/write-synthetic-api-tests/) ou le moniteur [de navigateur scripté](/docs/synthetics/new-relic-synthetics/scripting-monitors/write-scripted-browsers) .
* Fournissez [des variables définies par l&apos;utilisateur](#user-defined-vars) dans votre configuration.

## configuration à l&apos;aide de variables d&apos;environnement [#environment-variables]

Les variables environnementales vous permettent d&apos;affiner la configuration du gestionnaire de tâches Synthetics pour répondre à vos besoins environnementaux et fonctionnels spécifiques.

<CollapserGroup>
  <Collapser id="docker-env-config" title="Docker de l'environnement configuration">
    Les variables sont fournies au démarrage à l&apos;aide de l&apos;argument `-e, --env` .

    Le tableau suivant présente toutes les variables d’environnement prises en charge par le gestionnaire de tâches Synthetics. `PRIVATE_LOCATION_KEY` est obligatoire et toutes les autres variables sont facultatives.

    <table>
      <thead>
        <tr>
          <th>
            Nom
          </th>

          <th>
            Description
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `PRIVATE_LOCATION_KEY`
          </td>

          <td>
            <DNT>**Required.**</DNT> clé du site privé, telle que trouvée sur la liste des entités du site privé.
          </td>
        </tr>

        <tr>
          <td>
            `DOCKER_API_VERSION`
          </td>

          <td>
            Format : `"vX.Y"` Version API à utiliser avec le service Docker donné.

            Défaut: `v1.35.`
          </td>
        </tr>

        <tr>
          <td>
            `DOCKER_HOST`
          </td>

          <td>
            Indique au gestionnaire de tâches Synthetics un `DOCKER_HOST` donné. En cas d&apos;absence, la valeur par défaut est `/var/run/docker.sock.`
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_ENDPOINT`
          </td>

          <td>
            Pour les comptes basés aux États-Unis, le point de terminaison est : `https://synthetics-horde.nr-data.net.`

            Pour les comptes [basés dans l&apos;UE](/docs/using-new-relic/welcome-new-relic/get-started/introduction-eu-region-data-center#partner-hierarchy) , le point de terminaison est : `https://synthetics-horde.eu01.nr-data.net/`

            Assurez-vous que votre gestionnaire de tâches Synthetics peut se connecter au point de terminaison approprié afin de servir votre moniteur.
          </td>
        </tr>

        <tr>
          <td>
            `DOCKER_REGISTRY`
          </td>

          <td>
            Le domaine du registre Docker où les images d&apos;exécution sont hébergées. Utilisez ceci pour remplacer `docker.io` par défaut.
          </td>
        </tr>

        <tr>
          <td>
            `DOCKER_REPOSITORY`
          </td>

          <td>
            Le référentiel Docker ou l&apos;organisation où les images d&apos;exécution sont hébergées. Utilisez ceci pour remplacer `newrelic` par défaut.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_HOST`
          </td>

          <td>
            Serveur proxy hôte utilisé pour la communication Horde. Format : `"localhost"`.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_PORT`
          </td>

          <td>
            Port du serveur proxy utilisé pour la communication Horde. Format : `8888`.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_USERNAME`
          </td>

          <td>
            Nom d&apos;utilisateur du serveur proxy utilisé pour la communication Horde. Format : `"username"`.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_PW`
          </td>

          <td>
            Mot de passe du serveur proxy utilisé pour la communication Horde. Format : `"password"`.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_ACCEPT_SELF_SIGNED_CERT`
          </td>

          <td>
            Accepter les certificats proxy auto-signés pour la connexion au serveur proxy utilisée pour la communication Horde ? Valeurs acceptables : `true`
          </td>
        </tr>

        <tr>
          <td>
            `CHECK_TIMEOUT`
          </td>

          <td>
            Le nombre maximal de secondes pendant lesquelles vos contrôles du monitoring sont autorisés à s&apos;exécuter. Cette valeur doit être un entier compris entre 0 seconde (exclue) et 900 secondes (incluses) (par exemple, de 1 seconde à 15 minutes).

            Par défaut : 180 secondes
          </td>
        </tr>

        <tr>
          <td>
            `LOG_LEVEL`
          </td>

          <td>
            Défaut: `INFO.`

            Options supplémentaires : `WARN`, `ERROR`, `DEBUG`
          </td>
        </tr>

        <tr>
          <td>
            `HEAVYWEIGHT_WORKERS`
          </td>

          <td>
            Le nombre de tâches lourdes simultanées (Browser/ Browser scripté et API scriptée) qui peuvent s&apos;exécuter simultanément.

            Par défaut : CPU disponibles - 1.
          </td>
        </tr>

        <tr>
          <td>
            `DESIRED_RUNTIMES`
          </td>

          <td>
            Un éventail qui peut être utilisé pour exécuter des images d&apos;exécution spécifiques. Format : \[&apos;newrelic/synthetics-ping-runtime:latest&apos;,&apos;newrelic/synthetics-node-api-runtime:latest&apos;,&apos;newrelic/synthetics-node-browser-runtime:latest&apos;]

            Par défaut : tous les derniers runtimes.
          </td>
        </tr>

        <tr>
          <td>
            `VSE_PASSPHRASE`
          </td>

          <td>
            Si défini, active <DNT>**verified script execution**</DNT> et utilise cette valeur comme <DNT>**passphrase**</DNT>.
          </td>
        </tr>

        <tr>
          <td>
            `USER_DEFINED_VARIABLES`
          </td>

          <td>
            Un ensemble hébergé localement de paires valeur clé définies par l&apos;utilisateur.
          </td>
        </tr>

        <tr>
          <td>
            `ENABLE_WASM`
          </td>

          <td>
            Si défini, active WebAssembly pour l&apos;exécution du navigateur de nœuds. Pour utiliser WebAssembly, la version minimale de votre gestionnaire de tâches Synthetics doit être sortie-367 ou supérieure et la version d&apos;exécution du navigateur de nœuds doit être 2.3.21 ou supérieure.
          </td>
        </tr>
      </tbody>
    </table>
  </Collapser>

  <Collapser id="podman-env-config" title="Configuration de l'environnement Podman">
    Les variables sont fournies au démarrage à l&apos;aide de l&apos;argument `-e, --env` .

    Le tableau suivant affiche toutes les variables d’environnement prises en charge par le gestionnaire de tâches Synthetics. `PRIVATE_LOCATION_KEY` est obligatoire et toutes les autres variables sont facultatives. Pour exécuter le gestionnaire de tâches Synthetics dans un environnement Podman, la version minimale doit être sortie-418 ou supérieure.

    <table>
      <thead>
        <tr>
          <th>
            Nom
          </th>

          <th>
            Description
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `PRIVATE_LOCATION_KEY`
          </td>

          <td>
            <DNT>**Required.**</DNT> clé du site privé, telle que trouvée sur la liste des entités du site privé.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_ENDPOINT`
          </td>

          <td>
            Pour les comptes basés aux États-Unis, le point de terminaison est : `https://synthetics-horde.nr-data.net.`

            Pour les comptes [basés dans l&apos;UE](/docs/using-new-relic/welcome-new-relic/get-started/introduction-eu-region-data-center#partner-hierarchy) , le point de terminaison est : `https://synthetics-horde.eu01.nr-data.net/`

            Assurez-vous que votre gestionnaire de tâches Synthetics peut se connecter au point de terminaison approprié afin de servir votre moniteur.
          </td>
        </tr>

        <tr>
          <td>
            `PODMAN_API_SERVICE_HOST`
          </td>

          <td>
            L&apos;entrée hôte ajoutée au pod créé où le SJM va s&apos;exécuter. Utilisez ceci pour remplacer `podman.service` par défaut.
          </td>
        </tr>

        <tr>
          <td>
            `PODMAN_API_SERVICE_PORT`
          </td>

          <td>
            Le port sur lequel le service API RESTful Podman LibPod s&apos;exécute dans l&apos;instance. Utilisez ceci pour remplacer `8000` par défaut.
          </td>
        </tr>

        <tr>
          <td>
            `PODMAN_API_VERSION`
          </td>

          <td>
            La version spécifique de l&apos;API RESTful Podman LibPod utilisée. Utilisez ceci pour remplacer `v5.0.0` par défaut.
          </td>
        </tr>

        <tr>
          <td>
            `PODMAN_POD_NAME`
          </td>

          <td>
            Le nom du pod dans lequel le conteneur SJM est exécuté. Utilisez ceci pour remplacer `SYNTHETICS` par défaut.
          </td>
        </tr>

        <tr>
          <td>
            `DOCKER_REGISTRY`
          </td>

          <td>
            Le domaine du registre Docker où les images d&apos;exécution sont hébergées. Utilisez ceci pour remplacer `docker.io` par défaut.
          </td>
        </tr>

        <tr>
          <td>
            `DOCKER_REPOSITORY`
          </td>

          <td>
            Le référentiel Docker ou l&apos;organisation où les images d&apos;exécution sont hébergées. Utilisez ceci pour remplacer `newrelic` par défaut.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_HOST`
          </td>

          <td>
            Serveur proxy hôte utilisé pour la communication Horde. Format : `"localhost"`.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_PORT`
          </td>

          <td>
            Port du serveur proxy utilisé pour la communication Horde. Format : `8888`.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_USERNAME`
          </td>

          <td>
            Nom d&apos;utilisateur du serveur proxy utilisé pour la communication Horde. Format : `"username"`.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_PW`
          </td>

          <td>
            Mot de passe du serveur proxy utilisé pour la communication Horde. Format : `"password"`.
          </td>
        </tr>

        <tr>
          <td>
            `HORDE_API_PROXY_ACCEPT_SELF_SIGNED_CERT`
          </td>

          <td>
            Accepter les certificats proxy auto-signés pour la connexion au serveur proxy utilisée pour la communication Horde ? Valeurs acceptables : `true`
          </td>
        </tr>

        <tr>
          <td>
            `CHECK_TIMEOUT`
          </td>

          <td>
            Le nombre maximal de secondes pendant lesquelles vos contrôles du monitoring sont autorisés à s&apos;exécuter. Cette valeur doit être un entier compris entre 0 seconde (exclue) et 900 secondes (incluses) (par exemple, de 1 seconde à 15 minutes).

            Par défaut : 180 secondes
          </td>
        </tr>

        <tr>
          <td>
            `LOG_LEVEL`
          </td>

          <td>
            Défaut: `INFO.`

            Options supplémentaires : `WARN`, `ERROR`, `DEBUG`
          </td>
        </tr>

        <tr>
          <td>
            `HEAVYWEIGHT_WORKERS`
          </td>

          <td>
            Le nombre de tâches lourdes simultanées (Browser/ Browser scripté et API scriptée) qui peuvent s&apos;exécuter simultanément.

            Par défaut : CPU disponibles - 1.
          </td>
        </tr>

        <tr>
          <td>
            `DESIRED_RUNTIMES`
          </td>

          <td>
            Un éventail qui peut être utilisé pour exécuter des images d&apos;exécution spécifiques. Format : \[&apos;newrelic/synthetics-ping-runtime:latest&apos;,&apos;newrelic/synthetics-node-api-runtime:latest&apos;,&apos;newrelic/synthetics-node-browser-runtime:latest&apos;]

            Par défaut : tous les derniers runtimes.
          </td>
        </tr>

        <tr>
          <td>
            `VSE_PASSPHRASE`
          </td>

          <td>
            Si défini, active <DNT>**verified script execution**</DNT> et utilise cette valeur comme <DNT>**passphrase**</DNT>.
          </td>
        </tr>

        <tr>
          <td>
            `USER_DEFINED_VARIABLES`
          </td>

          <td>
            Un ensemble hébergé localement de paires valeur clé définies par l&apos;utilisateur.
          </td>
        </tr>

        <tr>
          <td>
            `ENABLE_WASM`
          </td>

          <td>
            Si défini, active WebAssembly pour l&apos;exécution du navigateur de nœuds. Pour utiliser WebAssembly, la version minimale de votre gestionnaire de tâches Synthetics doit être sortie-367 ou supérieure et la version d&apos;exécution du navigateur de nœuds doit être 2.3.21 ou supérieure.
          </td>
        </tr>
      </tbody>
    </table>
  </Collapser>

  <Collapser id="kubernetes-env-config" title="Kubernetes de l'environnement configuration">
    Les variables sont fournies au démarrage à l&apos;aide de l&apos;argument `--set` .

    La liste suivante répertorie toutes les variables d’environnement prises en charge par le gestionnaire de tâches Synthetics. `synthetics.privateLocationKey` est obligatoire et toutes les autres variables sont facultatives.

    Un certain nombre de paramètres avancés supplémentaires sont disponibles et entièrement documentés dans [notre fichier README de la charte Helm](https://github.com/newrelic/helm-charts/blob/master/charts/synthetics-job-manager/README.md)

    <table>
      <thead>
        <tr>
          <th>
            Nom
          </th>

          <th>
            Description
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `synthetics.privateLocationKey`
          </td>

          <td>
            <DNT>**Required if `synthetics.privateLocationKeySecretName` is not set**</DNT>. [clé du site privé](/docs/synthetics/synthetic-monitoring/private-locations/install-job-manager/#private-location-key) du site privé, telle que trouvée sur la page web du site privé.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.privateLocationKeySecretName`
          </td>

          <td>
            <DNT>**Required if `synthetics.privateLocationKey` is not set**</DNT>. Nom du secret Kubernetes qui contient la clé `privateLocationKey`, qui contient la clé d&apos;authentification associée à votre site privé Synthetics.
          </td>
        </tr>

        <tr>
          <td>
            `imagePullSecrets`
          </td>

          <td>
            Le nom de l&apos;objet secret utilisé pour extraire une image d&apos;un registre de conteneurs spécifié.
          </td>
        </tr>

        <tr>
          <td>
            `fullnameOverride`
          </td>

          <td>
            Nom de remplacement utilisé pour votre déploiement, remplaçant la valeur par défaut.
          </td>
        </tr>

        <tr>
          <td>
            `appVersionOverride`
          </td>

          <td>
            sortie version de synthetics-job-manager à utiliser à la place de la version spécifiée dans [chart.yml](https://github.com/newrelic/helm-charts/blob/master/charts/synthetics-job-manager/Chart.yaml).
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.logLevel`
          </td>

          <td>
            Défaut: `INFO.`

            Options supplémentaires : `WARN`, `ERROR`
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.hordeApiEndpoint`
          </td>

          <td>
            Pour les comptes basés aux États-Unis, le point de terminaison est : `https://synthetics-horde.nr-data.net.`

            Pour les comptes [basés dans l&apos;UE](/docs/using-new-relic/welcome-new-relic/get-started/introduction-eu-region-data-center#partner-hierarchy) , le point de terminaison est : `https://synthetics-horde.eu01.nr-data.net/`

            Assurez-vous que votre gestionnaire de tâches Synthetics peut se connecter au point de terminaison approprié afin de servir votre moniteur.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.minionDockerRunnerRegistryEndpoint`
          </td>

          <td>
            Le registre Docker et l&apos;organisation où l&apos;image du Minion Runner est hébergée. Utilisez ceci pour remplacer `quay.io/newrelic` par défaut (par exemple, `docker.io/newrelic`).
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.vsePassphrase`
          </td>

          <td>
            S&apos;il est défini, il active <DNT>**verified script execution**</DNT> et utilise cette valeur comme <DNT>**passphrase**</DNT>.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.vsePassphraseSecretName`
          </td>

          <td>
            Si défini, active l&apos;exécution de script vérifiée et utilise cette valeur pour récupérer la phrase secrète d&apos;un secret Kubernetes avec une clé appelée `vsePassphrase`.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.enableWasm`
          </td>

          <td>
            Si défini, active WebAssembly pour l&apos;exécution du navigateur de nœuds. Pour utiliser WebAssembly, la version minimale de votre gestionnaire de tâches Synthetics doit être sortie-367 ou supérieure et la version d&apos;exécution du navigateur de nœuds doit être 2.3.21 ou supérieure.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.apiProxyHost`
          </td>

          <td>
            Serveur proxy utilisé pour la communication Horde. Format : `"host"`.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.apiProxyPort`
          </td>

          <td>
            Port du serveur proxy utilisé pour la communication Horde. Format : `port`.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.hordeApiProxySelfSignedCert`
          </td>

          <td>
            Acceptez les certificats auto-signés lors de l&apos;utilisation d&apos;un serveur proxy pour la communication Horde. Valeurs acceptables : `true`.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.hordeApiProxyUsername`
          </td>

          <td>
            Nom d&apos;utilisateur du serveur proxy pour la communication Horde. Format: `"username"`
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.hordeApiProxyPw`
          </td>

          <td>
            Mot de passe du serveur proxy pour la communication Horde. Format : `"password"`.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.userDefinedVariables.userDefinedJson`
          </td>

          <td>
            Une chaîne JSON de variables définies par l&apos;utilisateur. L&apos;utilisateur peut accéder à ces variables dans son script. Format : `'{"key":"value","key2":"value2"}'`.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.userDefinedVariables.userDefinedFile`
          </td>

          <td>
            Un chemin local à l&apos;utilisateur vers un fichier JSON contenant des variables définies par l&apos;utilisateur. Ceci est transmis via `--set-file` et ne peut pas être défini dans le fichier de valeurs.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.userDefinedVariables.userDefinedPath`
          </td>

          <td>
            Un chemin sur le PersistentVolume fourni par l&apos;utilisateur vers le fichier user\_defined\_variables.json. L&apos;utilisateur doit fournir un PersistentVolume ou un PersistentVolumeClaim si cette variable est renseignée.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.persistence.existingClaimName`
          </td>

          <td>
            Lors du montage d’un volume, l’utilisateur peut fournir un nom pour un PersistentVolumeClaim qui existe déjà dans le cluster. Présume l’existence d’un PersistentVolume correspondant.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.persistence.existingVolumeName`
          </td>

          <td>
            Si vous montez un volume et ne fournissez pas de PersistentVolumeClaim, l&apos;utilisateur doit au minimum fournir un nom PersistentVolume. Helm générera un PersistentVolumeClaim.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.persistence.storageClass`
          </td>

          <td>
            Le nom de la StorageClass pour le PersistentVolumeClaim généré. Cela doit correspondre au StorageClassName sur le PV existant. S&apos;il n&apos;y a pas de fournisseurs, Kubernetes utilisera la classe de stockage par défaut si elle est présente.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.persistence.size`
          </td>

          <td>
            La taille du volume pour le PersistentVolumeClaim généré. Format : `10Gi`. Par défaut 2Gi.
          </td>
        </tr>

        <tr>
          <td>
            `global.checkTimeout`
          </td>

          <td>
            Le nombre maximal de secondes pendant lesquelles vos contrôles du monitoring sont autorisés à s&apos;exécuter. Cette valeur doit être un entier compris entre 0 seconde (exclue) et 900 secondes (incluses) (par exemple, de 1 seconde à 15 minutes).

            Par défaut : 180 secondes
          </td>
        </tr>

        <tr>
          <td>
            `image.repository`
          </td>

          <td>
            Le conteneur à tirer.

            Défaut: `docker.io/newrelic/synthetics-job-runner`
          </td>
        </tr>

        <tr>
          <td>
            `image.pullPolicy`
          </td>

          <td>
            La politique d&apos;attraction.

            Défaut: `IfNotPresent`
          </td>
        </tr>

        <tr>
          <td>
            `podSecurityContext`
          </td>

          <td>
            Définissez un contexte de sécurité personnalisé pour le pod synthetics-job-manager.
          </td>
        </tr>

        <tr>
          <td>
            `ping-runtime.enabled`
          </td>

          <td>
            Si l&apos;exécution du ping persistant doit être déployée ou non. Cela peut être désactivé si vous n&apos;utilisez pas le moniteur de ping.

            Défaut: `true`
          </td>
        </tr>

        <tr>
          <td>
            `ping-runtime.replicaCount`
          </td>

          <td>
            Le nombre de conteneurs d&apos;exécution de ping à déployer. Augmentez le nombre de réplicas pour adapter le déploiement en fonction de vos besoins monitoring ping.

            Défaut: `1`
          </td>
        </tr>

        <tr>
          <td>
            `ping-runtime.image.repository`
          </td>

          <td>
            L&apos;image du conteneur à extraire pour l&apos;exécution du ping.

            Défaut: `docker.io/newrelic/synthetics-ping-runtime`
          </td>
        </tr>

        <tr>
          <td>
            `ping-runtime.image.pullPolicy`
          </td>

          <td>
            La politique d&apos;extraction pour le conteneur ping-runtime.

            Défaut: `IfNotPresent`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.enabled`
          </td>

          <td>
            Si l&apos;environnement d&apos;exécution de l&apos;API Node.js doit être déployé ou non. Cela peut être désactivé si vous n&apos;utilisez pas de moniteur d&apos;API scripté.

            Défaut: `true`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.parallelism`
          </td>

          <td>
            Le nombre d&apos;exécutions d&apos;API Node.js `CronJobs` à déployer. Le nombre maximal de tâches d&apos;API Node.js simultanées qui s&apos;exécuteront à tout moment. [Détails supplémentaires](#kubernetes-sizing).

            Défaut: `1`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.completions`
          </td>

          <td>
            Le nombre d&apos;exécutions de l&apos;API Node.js `CronJobs` à exécuter par minute. Augmentez ce paramètre avec le parallélisme pour améliorer le débit. Cela devrait être augmenté à chaque fois que le parallélisme est augmenté et les complétions devraient toujours être au moins supérieures ou égales au parallélisme. Augmentez ce paramètre si vous remarquez des périodes sans exécution de tâches d’exécution d’API. [Détails supplémentaires](#kubernetes-sizing).

            Défaut: `6`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.image.repository`
          </td>

          <td>
            L&apos;image du conteneur à extraire pour l&apos;exécution de l&apos;API Node.js.

            Défaut: `docker.io/newrelic/synthetics-node-api-runtime`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.image.pullPolicy`
          </td>

          <td>
            La politique d&apos;extraction pour le conteneur d&apos;exécution de l&apos;API Node.js.

            Défaut: `IfNotPresent`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.enabled`
          </td>

          <td>
            Si l&apos;environnement d&apos;exécution du navigateur Node.js doit être hiérarchisé ou non. Ceci peut être désactivé si vous n&apos;utilisez pas de script simple ou de moniteur de navigateur.

            Défaut: `true`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.parallelism`
          </td>

          <td>
            Le nombre d&apos;environnements d&apos;exécution du navigateur Chrome `CronJobs` à déployer. Le nombre maximal de tâches de navigateur Chrome simultanées qui s&apos;exécuteront à tout moment. [Détails supplémentaires](#kubernetes-sizing).

            Défaut: `1`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.completions`
          </td>

          <td>
            Le temps d&apos;exécution du navigateur Chrome `CronJobs` à effectuer par minute. Augmentez ce paramètre avec le parallélisme pour améliorer le débit. Cela devrait être augmenté à chaque fois que le parallélisme est augmenté et les complétions devraient toujours être au moins supérieures ou égales au parallélisme. Augmentez ce paramètre si vous remarquez des périodes pendant lesquelles aucun travail d&apos;exécution du navigateur n&apos;est en cours d&apos;exécution. [Détails supplémentaires](#kubernetes-sizing).

            Défaut: `6`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.image.repository`
          </td>

          <td>
            L&apos;image du conteneur à extraire pour l&apos;exécution du navigateur Node.js.

            Défaut: `docker.io/newrelic/synthetics-node-browser-runtime`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.image.pullPolicy`
          </td>

          <td>
            La politique d&apos;extraction pour le conteneur d&apos;exécution du navigateur Node.js.

            Défaut: `IfNotPresent`
          </td>
        </tr>
      </tbody>
    </table>
  </Collapser>

  <Collapser id="openshift-environment-config" title="Configurationde l'environnement OpenShift">
    Les variables sont fournies au démarrage à l&apos;aide de l&apos;argument `--set` .

    La liste suivante répertorie toutes les variables d’environnement prises en charge par le gestionnaire de tâches Synthetics. `synthetics.privateLocationKey` est obligatoire et toutes les autres variables sont facultatives.

    Un certain nombre de paramètres avancés supplémentaires sont disponibles et entièrement documentés dans [notre fichier README de la charte Helm](https://github.com/newrelic/helm-charts/blob/master/charts/synthetics-job-manager/README.md)

    <table>
      <thead>
        <tr>
          <th>
            Nom
          </th>

          <th>
            Description
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `synthetics.privateLocationKey`
          </td>

          <td>
            <DNT>**Required**</DNT>. [clé du site privé](/docs/synthetics/synthetic-monitoring/private-locations/install-job-manager/#private-location-key), telle que trouvée sur la liste des entités du site privé.
          </td>
        </tr>

        <tr>
          <td>
            `imagePullSecrets`
          </td>

          <td>
            Le nom de l&apos;objet secret utilisé pour extraire une image d&apos;un registre de conteneurs spécifié.
          </td>
        </tr>

        <tr>
          <td>
            `fullnameOverride`
          </td>

          <td>
            Nom de remplacement utilisé pour votre déploiement, remplaçant la valeur par défaut.
          </td>
        </tr>

        <tr>
          <td>
            `appVersionOverride`
          </td>

          <td>
            sortie version de synthetics-job-manager à utiliser à la place de la version spécifiée dans [chart.yml](https://github.com/newrelic/helm-charts/blob/master/charts/synthetics-job-manager/Chart.yaml).
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.logLevel`
          </td>

          <td>
            Défaut: `INFO.`

            Options supplémentaires : `WARN`, `ERROR`
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.hordeApiEndpoint`
          </td>

          <td>
            Pour les comptes basés aux États-Unis, le point de terminaison est : `https://synthetics-horde.nr-data.net.`

            Pour les comptes [basés dans l&apos;UE](/docs/using-new-relic/welcome-new-relic/get-started/introduction-eu-region-data-center#partner-hierarchy) , le point de terminaison est : `https://synthetics-horde.eu01.nr-data.net/`

            Assurez-vous que votre gestionnaire de tâches Synthetics peut se connecter au point de terminaison approprié afin de servir votre moniteur.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.vsePassphrase`
          </td>

          <td>
            S&apos;il est défini, il active <DNT>**verified script execution**</DNT> et utilise cette valeur comme <DNT>**passphrase**</DNT>.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.vsePassphraseSecretName`
          </td>

          <td>
            Si défini, active l&apos;exécution de script vérifiée et utilise cette valeur pour récupérer la phrase secrète d&apos;un secret Kubernetes avec une clé appelée `vsePassphrase`.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.enableWasm`
          </td>

          <td>
            Si défini, active WebAssembly pour l&apos;exécution du navigateur de nœuds. Pour utiliser WebAssembly, la version minimale de votre gestionnaire de tâches Synthetics doit être sortie-367 ou supérieure et la version d&apos;exécution du navigateur de nœuds doit être 2.3.21 ou supérieure.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.apiProxyHost`
          </td>

          <td>
            Serveur proxy utilisé pour la communication Horde. Format : `"host"`.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.apiProxyPort`
          </td>

          <td>
            Port du serveur proxy utilisé pour la communication Horde. Format : `port`.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.hordeApiProxySelfSignedCert`
          </td>

          <td>
            Acceptez les certificats auto-signés lors de l&apos;utilisation d&apos;un serveur proxy pour la communication Horde. Valeurs acceptables : `true`.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.hordeApiProxyUsername`
          </td>

          <td>
            Nom d&apos;utilisateur du serveur proxy pour la communication Horde. Format: `"username"`
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.hordeApiProxyPw`
          </td>

          <td>
            Mot de passe du serveur proxy pour la communication Horde. Format : `"password"`.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.userDefinedVariables.userDefinedJson`
          </td>

          <td>
            Une chaîne JSON de variables définies par l&apos;utilisateur. L&apos;utilisateur peut accéder à ces variables dans son script. Format : `'{"key":"value","key2":"value2"}'`.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.userDefinedVariables.userDefinedFile`
          </td>

          <td>
            Un chemin local à l&apos;utilisateur vers un fichier JSON contenant des variables définies par l&apos;utilisateur. Ceci est transmis via `--set-file` et ne peut pas être défini dans le fichier de valeurs.
          </td>
        </tr>

        <tr>
          <td>
            `synthetics.userDefinedVariables.userDefinedPath`
          </td>

          <td>
            Un chemin sur le fichier `PersistentVolume` fourni par l&apos;utilisateur vers le fichier` user_defined_variables.json` . L&apos;utilisateur doit fournir un `PersistentVolume` ou `PersistentVolumeClaim` si cette variable est renseignée.
          </td>
        </tr>

        <tr>
          <td>
            `global.persistence.existingClaimName`
          </td>

          <td>
            Lors du montage d&apos;un volume, l&apos;utilisateur peut fournir un nom pour un `PersistentVolumeClaim` qui existe déjà dans le cluster. Suppose l&apos;existence d&apos;un `PersistentVolume` correspondant.
          </td>
        </tr>

        <tr>
          <td>
            `global.persistence.existingVolumeName`
          </td>

          <td>
            Si vous montez un volume et ne fournissez pas de `PersistentVolumeClaim`, l&apos;utilisateur doit au moins fournir un nom `PersistentVolume` . Helm générera un `PersistentVolumeClaim`.
          </td>
        </tr>

        <tr>
          <td>
            `global.persistence.storageClass`
          </td>

          <td>
            Le nom du `StorageClass` pour le `PersistentVolumeClaim` généré. Cela devrait correspondre au `StorageClassName` sur le PV existant. S&apos;il n&apos;y a pas de fournisseurs, **Kubernetes** utilisera la classe de stockage par défaut si elle est présente.
          </td>
        </tr>

        <tr>
          <td>
            `global.persistence.size`
          </td>

          <td>
            La taille du volume pour le `PersistentVolumeClaim` généré. Format : `10Gi`. Par défaut `2Gi`.
          </td>
        </tr>

        <tr>
          <td>
            `global.checkTimeout`
          </td>

          <td>
            Le nombre maximal de secondes pendant lesquelles vos contrôles du monitoring sont autorisés à s&apos;exécuter. Cette valeur doit être un entier compris entre 0 seconde (exclue) et 900 secondes (incluses) (par exemple, de 1 seconde à 15 minutes).

            Par défaut : 180 secondes
          </td>
        </tr>

        <tr>
          <td>
            `image.repository`
          </td>

          <td>
            Le conteneur à tirer.

            Défaut: `docker.io/newrelic/synthetics-job-runner`
          </td>
        </tr>

        <tr>
          <td>
            `image.pullPolicy`
          </td>

          <td>
            La politique d&apos;attraction.

            Défaut: `IfNotPresent`
          </td>
        </tr>

        <tr>
          <td>
            `podSecurityContext`
          </td>

          <td>
            Définissez un contexte de sécurité personnalisé pour le pod `synthetics-job-manager` .
          </td>
        </tr>

        <tr>
          <td>
            `ping-runtime.enabled`
          </td>

          <td>
            Si l&apos;exécution du ping persistant doit être déployée ou non. Cela peut être désactivé si vous n&apos;utilisez pas le moniteur de ping.

            Défaut: `true`
          </td>
        </tr>

        <tr>
          <td>
            `ping-runtime.replicaCount`
          </td>

          <td>
            Le nombre de conteneurs d&apos;exécution de ping à déployer. Augmentez le `replicaCount` pour adapter le déploiement en fonction de vos besoins monitoring ping.

            Défaut: `1`
          </td>
        </tr>

        <tr>
          <td>
            `ping-runtime.image.repository`
          </td>

          <td>
            L&apos;image du conteneur à extraire pour l&apos;exécution du ping.

            Défaut: `docker.io/newrelic/synthetics-ping-runtime`
          </td>
        </tr>

        <tr>
          <td>
            `ping-runtime.image.pullPolicy`
          </td>

          <td>
            La politique d&apos;extraction pour le conteneur ping-runtime.

            Défaut: `IfNotPresent`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.enabled`
          </td>

          <td>
            Si l&apos;environnement d&apos;exécution de l&apos;API Node.js doit être déployé ou non. Cela peut être désactivé si vous n&apos;utilisez pas de moniteur d&apos;API scripté.

            Défaut: `true`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.parallelism`
          </td>

          <td>
            Le nombre d&apos;exécutions d&apos;API Node.js `CronJobs` à déployer. Le nombre maximal de tâches d&apos;API Node.js simultanées qui s&apos;exécuteront à tout moment. [Détails supplémentaires](#kubernetes-sizing).

            Défaut: `1`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.completions`
          </td>

          <td>
            Le nombre d&apos;exécutions de l&apos;API Node.js `CronJobs` à exécuter par minute. Augmentez ce paramètre avec le parallélisme pour améliorer le débit. Cela devrait être augmenté à chaque fois que le parallélisme est augmenté et les complétions devraient toujours être au moins supérieures ou égales au parallélisme. Augmentez ce paramètre si vous remarquez des périodes sans exécution de tâches d’exécution d’API. [Détails supplémentaires](#kubernetes-sizing).

            Défaut: `6`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.image.repository`
          </td>

          <td>
            L&apos;image du conteneur à extraire pour l&apos;exécution de l&apos;API Node.js.

            Défaut: `docker.io/newrelic/synthetics-node-api-runtime`
          </td>
        </tr>

        <tr>
          <td>
            `node-api-runtime.image.pullPolicy`
          </td>

          <td>
            La politique d&apos;extraction pour le conteneur d&apos;exécution de l&apos;API Node.js.

            Défaut: `IfNotPresent`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.enabled`
          </td>

          <td>
            Si l&apos;environnement d&apos;exécution du navigateur Node.js doit être hiérarchisé ou non. Ceci peut être désactivé si vous n&apos;utilisez pas de script simple ou de moniteur de navigateur.

            Défaut: `true`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.parallelism`
          </td>

          <td>
            Le nombre d&apos;environnements d&apos;exécution du navigateur Chrome `CronJobs` à déployer. Le nombre maximal de tâches de navigateur Chrome simultanées qui s&apos;exécuteront à tout moment. [Détails supplémentaires](#kubernetes-sizing).

            Défaut: `1`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.completions`
          </td>

          <td>
            Le temps d&apos;exécution du navigateur Chrome `CronJobs` à effectuer par minute. Augmentez ce paramètre avec le parallélisme pour améliorer le débit. Cela devrait être augmenté à chaque fois que le parallélisme est augmenté et les complétions devraient toujours être au moins supérieures ou égales au parallélisme. Augmentez ce paramètre si vous remarquez des périodes pendant lesquelles aucun travail d&apos;exécution du navigateur n&apos;est en cours d&apos;exécution. [Détails supplémentaires](#kubernetes-sizing).

            Défaut: `6`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.image.repository`
          </td>

          <td>
            L&apos;image du conteneur à extraire pour l&apos;exécution du navigateur Node.js.

            Défaut: `docker.io/newrelic/synthetics-node-browser-runtime`
          </td>
        </tr>

        <tr>
          <td>
            `node-browser-runtime.image.pullPolicy`
          </td>

          <td>
            La politique d&apos;extraction pour le conteneur d&apos;exécution du navigateur Node.js.

            Défaut: `IfNotPresent`
          </td>
        </tr>
      </tbody>
    </table>
  </Collapser>
</CollapserGroup>

## variables définies par l&apos;utilisateur pour le moniteur scripté [#user-defined-vars]

Les gestionnaires de tâches Private Synthetics vous permettent de configurer des variables d&apos;environnement pour le moniteur scripté. Ces variables sont gérées localement sur le SJM et sont accessibles via `$env.USER_DEFINED_VARIABLES`. Vous pouvez définir des variables définies par l&apos;utilisateur de deux manières. Vous pouvez monter un fichier JSON ou fournir une variable d&apos;environnement au SJM au lancement. Si les deux sont fournis, le SJM utilisera uniquement les valeurs fournies par l&apos;environnement.

<CollapserGroup>
  <Collapser id="user-file-example" title="Montage du fichier JSON">
    L&apos;utilisateur peut créer un fichier au format JSON et monter le volume où se trouve le fichier sur un chemin cible spécifié dans le conteneur SJM.

    Le fichier doit disposer d&apos;autorisations de lecture et contenir une carte au format JSON. Exemple de fichier de variables définies par l&apos;utilisateur :

    ```json
    {
      "KEY": "VALUE",
      "user_name": "MINION",
      "my_password": "PASSW0RD123",
      "my_URL": "https://newrelic.com/",
      "ETC": "ETC"
    }
    ```

    Placez le fichier dans le répertoire source sur l&apos;hôte. Le SJM s&apos;attend à ce que le nom du fichier soit user\_defined\_variables.json

    Exemple de Docker :

    Le répertoire cible attendu est : `/var/lib/newrelic/synthetics/variables/`

    ```sh
    docker run ... -v /variables:/var/lib/newrelic/synthetics/variables:rw ...
    ```

    Exemple de Podman :

    Dans le cas de SELinux, montez le volume en plus avec `:z` ou `:Z`. Pour plus d&apos;informations, reportez-vous [à la documentation de Podman.](https://docs.podman.io/en/latest/markdown/podman-run.1.html#volume-v-source-volume-host-dir-container-dir-options) Le répertoire cible attendu est : `/var/lib/newrelic/synthetics/variables/`

    ```sh
    podman run ... -v /variables:/var/lib/newrelic/synthetics/variables:rw,z ...
    ```

    Exemple de Kubernetes :

    L&apos;utilisateur dispose de deux options lorsqu&apos;il fournit un fichier au pod SJM dans Kubernetes. Ils peuvent :

    * Transmettre dans un fichier local.
    * Fournissez un PersistentVolume qui inclut le `user_defined_variables.json`.

    ### Passer dans un fichier local

    Cette option crée une ressource Kubernetes ConfigMap et la monte sur le pod SJM.

    ```sh
    helm install newrelic/synthetics-job-manager ... --set-file "synthetics.userDefinedVariables.userDefinedFile=[local-path]/user_defined_variables.json" ...
    ```

    ### Monter un `PersistentVolume`

    Cette option nécessite que l&apos;utilisateur fournisse un `PersistentVolume` qui inclut le fichier `user_defined_variables.json` ou un `PersistentVolumeClaim` au même. Pour plus de détails sur l&apos;installation du graphique de barre à l&apos;aide d&apos;un `PersistentVolume`, suivez les instructions sur [le stockage permanent des données](/docs/synthetics/synthetic-monitoring/private-locations/job-manager-configuration#permanent-data-storage).

    Une fois que l&apos;utilisateur a préparé un `PersistentVolume` comme décrit ci-dessous, lancez le SJM, en définissant le chemin où se trouve le fichier `user_defined_variables.json` et en définissant toutes les autres variables `synthetics.persistence` si nécessaire.

    ```sh
    helm install newrelic/synthetics-job-manger ... --set synthetics.userDefinedVariables.userDefinedPath="variables"
    ```
  </Collapser>

  <Collapser id="passing-env-var" title="Passer comme variable d'environnement">
    Les variables peuvent être transmises à leur système conteneur respectif via une variable d&apos;environnement.

    Exemple de Docker :

    Utilisez l&apos;indicateur `-e` pour configurer une variable d&apos;environnement nommée `USER_DEFINED_VARIABLES` et donnez-lui la valeur d&apos;une chaîne de carte au format JSON.

    ```sh
    docker run ... -e USER_DEFINED_VARIABLES='{"key":"value","name":"sjm"}' ...
    ```

    Exemple de Podman :

    Utilisez l&apos;indicateur `-e` pour configurer une variable d&apos;environnement nommée `USER_DEFINED_VARIABLES` et donnez-lui la valeur d&apos;une chaîne de carte au format JSON.

    ```sh
    podman run ... -e USER_DEFINED_VARIABLES='{"key":"value","name":"sjm"}' ...
    ```

    Exemple de Kubernetes :

    Utilisez l’indicateur `--set-literal` pour transmettre la chaîne au format JSON.

    ```sh
    helm install newrelic/synthetics-job-manager ... --set-literal synthetics.userDefinedVariables.userDefinedJson='{"key":"value","name":"sjm"}' ...
    ```
  </Collapser>
</CollapserGroup>

### Accéder aux variables d&apos;environnement définies par l&apos;utilisateur à partir d&apos;un script [#env-vars-scripts]

Pour référencer une variable d&apos;environnement configurée définie par l&apos;utilisateur, utilisez le `$env.USER_DEFINED_VARIABLES` réservé suivi du nom d&apos;une variable donnée avec une notation par points (par exemple, `$env.USER_DEFINED_VARIABLES.MY_VARIABLE`).

<Callout variant="caution">
  Les variables d&apos;environnement définies par l&apos;utilisateur ne sont pas nettoyées dans les logs. Pensez à utiliser la fonctionnalité [d’informations d’identification sécurisées](/docs/synthetics/new-relic-synthetics/using-monitors/secure-credentials-store-credentials-information-scripted-browsers) pour les informations sensibles.
</Callout>

## Modules de nœuds personnalisés [#custom-modules]

Des modules de nœuds personnalisés sont fournis à la fois dans CPM et dans SJM. Ils vous permettent de créer un ensemble personnalisé de [modules de nœuds](https://docs.npmjs.com/about-packages-and-modules) et de les utiliser dans un moniteur scripté (API scriptée et navigateur scripté) pour monitoring Synthétique.

### Configurez votre répertoire de modules personnalisés

Créez un répertoire avec un fichier `package.json` suivant [les directives officielles de npm](https://docs.npmjs.com/files/package.json) dans le dossier racine. Le SJM installera toutes les dépendances répertoriées dans le package.json champ `dependencies` . Ces dépendances seront disponibles lors de l&apos;exécution du moniteur sur le gestionnaire de tâches Synthetics privé. Voir un exemple ci-dessous.

#### Exemple

Dans cet exemple, un répertoire de modules personnalisé est utilisé avec la structure suivante :

```
/example-custom-modules-dir/
    ├── counter
    │   ├── index.js
    │   └── package.json
    └── package.json            ⇦ the only mandatory file
```

Le `package.json` définit `dependencies` à la fois comme un module local (par exemple, `counter`) et comme tout module hébergé (par exemple, `smallest` version `1.0.1`) :

```json
{
    "name": "custom-modules",
    "version": "1.0.0",                                ⇦ optional
    "description": "example custom modules directory", ⇦ optional
    "dependencies": {
    "smallest": "1.0.1",                               ⇦ hosted module
    "counter": "file:./counter"                        ⇦ local module
    }
}
```

### Ajoutez votre répertoire de modules personnalisés au SJM pour Docker, Podman ou Kubernetes

<CollapserGroup>
  <Collapser id="docker" title="Docker">
    Pour Docker, lancez SJM en montant le répertoire à `/var/lib/newrelic/synthetics/modules`. Par exemple:

    ```sh
    docker run ... -v /example-custom-modules-dir:/var/lib/newrelic/synthetics/modules:rw ...
    ```
  </Collapser>

  <Collapser id="podman" title="Podman">
    Pour podman, lancez SJM en montant le répertoire à `/var/lib/newrelic/synthetics/modules`. Dans le cas de SELinux, montez le volume en plus avec `:z` ou `:Z`. Pour plus d&apos;informations, reportez-vous [à la documentation Podman](https://docs.podman.io/en/latest/markdown/podman-run.1.html#volume-v-source-volume-host-dir-container-dir-options). Par exemple:

    ```sh
    podman run ... -v /example-custom-modules-dir:/var/lib/newrelic/synthetics/modules:rw,z ...
    ```
  </Collapser>

  <Collapser id="kubernetes" title="Kubernetes">
    Pour Kubernetes, le répertoire `/var/lib/newrelic/synthetics/modules` doit exister sur un PV avant de lancer le SJM avec les modules personnalisés activés.

    <Callout variant="tip">
      Le mode d&apos;accès PV doit être ReadWriteMany si vous devez partager le stockage entre plusieurs pods.
    </Callout>

    Une méthode consiste à créer un pod qui monte le PV uniquement dans le but de copier votre répertoire de modules personnalisés sur le PV. L&apos;exemple suivant utilise Amazon EFS avec Amazon EKS :

    #### Créer l&apos;espace de nommage, le volume persistant et la revendication de volume persistant

    1. Assurez-vous d&apos;avoir déjà configuré votre système de fichiers EFS et installé le [pilote EFS CSI](https://github.com/kubernetes-sigs/aws-efs-csi-driver) sur votre cluster. Vous aurez également besoin de votre ID de système de fichiers EFS pour les PV `spec.csi.volumeHandle`.

       ```sh
       kubectl apply -f - <<EOF
       apiVersion: v1
       kind: Namespace
       metadata:
         name: newrelic

       ---
       kind: StorageClass
       apiVersion: storage.k8s.io/v1
       metadata:
         name: efs-sc
       provisioner: efs.csi.aws.com

       ---
       apiVersion: v1
       kind: PersistentVolume
       metadata:
         name: custom-modules-pvc
       spec:
         capacity:
           storage: 5Gi
         volumeMode: Filesystem
         accessModes:
           - ReadWriteMany
         persistentVolumeReclaimPolicy: Retain
         storageClassName: efs-sc
         csi:
           driver: efs.csi.aws.com
           volumeHandle: <your-efs-filesystem-id>

       ---
       apiVersion: v1
       kind: PersistentVolumeClaim
       metadata:
         name: custom-modules-pvc
         namespace: newrelic
       spec:
         accessModes:
           - ReadWriteMany
         storageClassName: efs-sc
         resources:
           requests:
             storage: 5Gi
       EOF
       ```

    2. Passez à l&apos;espace de nommage `newrelic` dans votre `~/.kube/config`.

       ```sh
       kubectl config get-contexts
       kubectl config set-context YOUR_CONTEXT --namespace=newrelic
       kubectl config view --minify | grep namespace:
       ```

    3. À ce stade, le PVC doit être lié au PV avec le mode d’accès RWX.

       ```sh
       kubectl get pv,pvc
       [output] NAME                                  CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                         STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE
       [output] persistentvolume/custom-modules-pvc   5Gi        RWX            Retain           Bound    newrelic/custom-modules-pvc   efs-sc         <unset>                          4m46s
       [output]
       [output] NAME                                       STATUS   VOLUME               CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
       [output] persistentvolumeclaim/custom-modules-pvc   Bound    custom-modules-pvc   5Gi        RWX            efs-sc         <unset>                 4m10s
       ```

       #### Créez `mount-custom-mods-pod` pour copier votre répertoire de modules personnalisés

       ```sh
       kubectl apply -f - <<EOF
       apiVersion: v1
       kind: Pod
       metadata:
         name: mount-custom-mods-pod
       spec:
         containers:
         - name: mount-custom-mods-pod
           image: nginx
           resources:
             requests:
               memory: "64Mi"
               cpu: "250m"
             limits:
               memory: "128Mi"
               cpu: "500m"
           volumeMounts:
             - mountPath: "/var/lib/newrelic/synthetics/modules"
               name: custom-modules-storage
         volumes:
         - name: custom-modules-storage
           persistentVolumeClaim:
             claimName: custom-modules-pvc
       EOF
       ```

       À ce stade, le `mount-custom-mods-pod` doit être créé et configuré pour utiliser le volume.

       ```sh
       kubectl describe po mount-custom-mods-pod | grep -A4 Volumes:
       [output] Volumes:
       [output]   custom-modules-storage:
       [output]     Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
       [output]     ClaimName:  custom-modules-pvc
       [output]     ReadOnly:   false
       ```

       Vérifiez événement pour tous les avertissements liés au PV, au PVC ou `mount-custom-mods-pod`.

       ```sh
       kubectl get events --field-selector type=Warning --sort-by='.lastTimestamp'
       ```

       #### Copiez votre répertoire de modules personnalisés dans le PV

       Il n&apos;est pas nécessaire de copier `node_modules` car il sera généré par le SJM sur `npm install`.

       ```sh
       cd custom-modules
       rm -rf node_modules && cd ..
       ```

    4. Vérifiez que le `mount-custom-mods-pod` est en cours d’exécution.

       ```sh
       kubectl get po
       [output] NAME                    READY   STATUS    RESTARTS   AGE
       [output] mount-custom-mods-pod   1/1     Running   0          5m43s
       ```

    5. Copie au PV.

       ```sh
       kubectl cp custom-modules newrelic/mount-custom-mods-pod:/var/lib/newrelic/synthetics/modules
       ```

    6. Vérifiez que `/var/lib/newrelic/synthetics/modules/custom-modules/package.json` existe sur le PV.

       ```sh
       kubectl exec -it mount-custom-mods-pod -- bash
       [output] root@mount-custom-mods-pod:/# cd /var/lib/newrelic/synthetics/modules/
       [output] root@mount-custom-mods-pod:/var/lib/newrelic/synthetics/modules# ls -l
       [output] total 4
       [output] drwxr-xr-x 2 root root 6144 Jun 29 03:49 custom-modules
       [output] root@mount-custom-mods-pod:/var/lib/newrelic/synthetics/modules# ls -l custom-modules/
       [output] total 4
       [output] -rw-r--r-- 1 501 staff 299 Jun 29 03:49 package.json
       ```

       #### lancement du SJM avec la fonctionnalité des modules personnalisés activée

       Définissez les valeurs pour `persistence.existingClaimName` et `customNodeModules.customNodeModulesPath` dans la ligne de commande ou dans un fichier YAML lors de l&apos;installation. La valeur `customNodeModules.customNodeModulesPath` doit spécifier le sous-chemin sur le volume persistant où existent vos fichiers de modules personnalisés. Par exemple:

       ```sh
       helm upgrade --install synthetics-job-manager newrelic/synthetics-job-manager -n newrelic --set global.persistence.existingClaimName=custom-modules-pvc --set global.customNodeModules.customNodeModulesPath=custom-modules --set synthetics.privateLocationKey=YOUR_PRIVATE_LOCATION_KEY
       [output] Release "synthetics-job-manager" does not exist. Installing it now.
       [output] NAME: synthetics-job-manager
       [output] LAST DEPLOYED: Fri Jun 28 16:53:28 2024
       [output] NAMESPACE: newrelic
       [output] STATUS: deployed
       [output] REVISION: 1
       [output] TEST SUITE: None
       ```

       Le répertoire `custom-modules` doit maintenant contenir le package installé dans `node_modules`.

       ```sh
       kubectl exec -it mount-custom-mods-pod -- bash
       [output] root@mount-custom-mods-pod:/# cd /var/lib/newrelic/synthetics/modules/
       [output] root@mount-custom-mods-pod:/var/lib/newrelic/synthetics/modules# ls -l custom-modules/
       [output] total 16
       [output] -rw-r--r--  1 root root   836 Jun 29 03:51 README
       [output] drwxr-xr-x 18 root root  6144 Jun 29 03:51 node_modules
       [output] -rw-r--r--  1  501 staff  299 Jun 29 03:49 package.json
       [output] -rw-r--r--  1 root root   190 Jun 29 03:51 package.json.shasum
       ```

       Si les modules de nœuds personnalisés ne sont pas détectés, ajustez les autorisations sur le répertoire `custom-modules` et le fichier `package.json` .

       ```sh
       kubectl exec -it mount-custom-mods-pod -- bash
       [output] root@mount-custom-mods-pod:/# cd /var/lib/newrelic/synthetics/modules/
       [output] root@mount-custom-mods-pod:/var/lib/newrelic/synthetics/modules# chmod -R 777 custom-modules
       [output] root@mount-custom-mods-pod:/var/lib/newrelic/synthetics/modules# chown -R 2000:2000 custom-modules
       ```
  </Collapser>
</CollapserGroup>

Pour vérifier si les modules ont été installés correctement ou si des erreurs se sont produites, recherchez les lignes suivantes dans les logs [du conteneur](/docs/synthetics/new-relic-synthetics/private-locations/job-manager-maintenance-monitoring#monitor-docker-logs) ou [pod](/docs/synthetics/synthetic-monitoring/private-locations/job-manager-maintenance-monitoring/#review-kubernetes-logs) `synthetics-job-manager` :

```log
2024-06-29 03:51:28,407{UTC} [main] INFO  c.n.s.j.p.options.CustomModules - Detected mounted path for custom node modules
2024-06-29 03:51:28,408{UTC} [main] INFO  c.n.s.j.p.options.CustomModules - Validating permission for custom node modules package.json file
2024-06-29 03:51:28,409{UTC} [main] INFO  c.n.s.j.p.options.CustomModules - Installing custom node modules...
2024-06-29 03:51:44,670{UTC} [main] INFO  c.n.s.j.p.options.CustomModules - Custom node modules installed successfully.
```

Vous pouvez maintenant ajouter `"require('smallest');"` dans le [script](/docs/synthetics/new-relic-synthetics/scripting-monitors/write-scripted-browsers) du moniteur que vous envoyez sur ce site privé.

### Changement `package.json` [#change-package-json]

En plus des modules locaux et hébergés, vous pouvez également utiliser [des modules Node.js.](/docs/synthetics/new-relic-synthetics/scripting-monitors/import-nodejs-modules) Pour mettre à jour les modules personnalisés utilisés par votre SJM, apportez des modifications au fichier `package.json` et redémarrez le SJM. Pendant le processus de redémarrage, le SJM reconnaîtra le changement de configuration et effectuera automatiquement des opérations de nettoyage et de réinstallation pour garantir que les modules mis à jour sont appliqués.

<Callout variant="caution">
  Modules locaux : bien que votre `package.json` puisse inclure n’importe quel module local, ces modules doivent résider dans l’arborescence sous votre répertoire de modules personnalisés. Si stocké en dehors de l&apos;arbre, le processus d&apos;initialisation échouera et vous verrez un message d&apos;erreur dans les [logs Docker](/docs/synthetics/new-relic-synthetics/private-locations/job-manager-maintenance-monitoring#monitor-docker-logs) après le lancement de SJM.
</Callout>

## Stockage permanent des données [#permanent-data-storage]

l&apos;utilisateur peut souhaiter utiliser un stockage de données permanent pour fournir le fichier `user_defined_variables.json` ou prendre en charge des modules de nœuds personnalisés.

### Docker

Pour définir le stockage permanent des données sur Docker :

1. Créez un répertoire sur l’hôte où vous lancez le gestionnaire de travaux. Ceci est votre répertoire source.

2. lancement du Job Manager, en montant le répertoire source dans le répertoire cible `/var/lib/newrelic/synthetics`.

   Exemple:

   ```sh
   docker run ... -v /sjm-volume:/var/lib/newrelic/synthetics:rw ...
   ```

### Podman

Pour définir le stockage permanent des données sur Podman :

1. Créez un répertoire sur l’hôte où vous lancez le gestionnaire de travaux. Ceci est votre répertoire source.
2. lancement du Job Manager, en montant le répertoire source dans le répertoire cible `/var/lib/newrelic/synthetics`.

Exemple:

```sh
podman run ... -v /sjm-volume:/var/lib/newrelic/synthetics:rw,z ...
```

### Kubernetes

Pour définir un stockage permanent des données sur Kubernetes, l&apos;utilisateur dispose de deux options :

1. Fournissez un PersistentVolumeClaim (PVC) existant pour un PersistentVolume (PV) existant, en définissant la valeur de configuration `synthetics.persistence.existingClaimName` . Exemple:

   ```sh
   helm install ... --set synthetics.persistence.existingClaimName=sjm-claim ...
   ```

2. Fournissez un nom PersistentVolume (PV) existant, en définissant la valeur de configuration `synthetics.persistence.existingVolumeName` . Helm générera un PVC pour l&apos;utilisateur. L&apos;utilisateur peut également définir éventuellement les valeurs suivantes :

* `synthetics.persistence.storageClass`:La classe de stockage du PV existant. Si non fourni, Kubernetes utilisera la classe de stockage par défaut.

* `synthetics.persistence.size`:La taille de la réclamation. Si non défini, la valeur par défaut est actuellement 2Gi.

  ```sh
  helm install ... --set synthetics.persistence.existingVolumeName=sjm-volume --set synthetics.persistence.storageClass=standard ...
  ```

## Sizing considerations for Docker and Podman [#vm-sizing]

Pour garantir que votre site privé fonctionne efficacement, vous devez provisionner suffisamment de ressources CPU sur votre hôte pour gérer votre workload monitoring. De nombreux facteurs influencent le dimensionnement, mais vous pouvez rapidement estimer vos besoins. Vous aurez besoin **d&apos;un cœur de processeur pour chaque moniteur lourd** (c&apos;est-à-dire un navigateur simple, un navigateur scripté ou un moniteur d&apos;API scripté). Vous trouverez ci-dessous deux formules pour vous aider à calculer le nombre de cœurs dont vous avez besoin, que vous diagnostiquiez une configuration actuelle ou que vous en planifiiez une future.

### Formule 1 : Diagnostic d&apos;un emplacement existant

Si votre site privé actuel a du mal à suivre et que vous pensez que des tâches sont en file d&apos;attente, utilisez cette formule pour savoir de combien de cœurs vous avez réellement besoin. Il est basé sur les performances observable de votre système.

$$ C\_req = (R\_proc + R\_growth) \cdot D\_avg,m $$

* $C\_req$ = **Cœurs de processeur requis**.
* $R\_proc$ = Le **taux** de tâches lourdes **traitées** par minute.
* $R\_growth$ = Le **taux** **de croissance** de votre file d&apos;attente `jobManagerHeavyweightJobs` par minute.
* $D\_avg,m$ = La **durée moyenne** des tâches lourdes en **minutes**.

This formula calculates your true job arrival rate by adding the jobs your system *is processing* to the jobs that are *piling up* in the queue. Multiplying this total load by the average job duration tells you exactly how many cores you need to clear all the work without queuing.

### Formule 2 : Prévoir un emplacement nouveau ou futur

Si vous configurez un nouveau site privé ou prévoyez d&apos;ajouter plus de moniteurs, utilisez cette formule pour prévoir vos besoins à l&apos;avance.

$$ C\_req = N\_mon \cdot D\_avg,m \cdot \frac1P\_avg,m $$

* $C\_req$ = **Cœurs de processeur requis**.
* $N\_mon$ = Le **nombre** total de **moniteurs** lourds que vous prévoyez d&apos;exécuter.
* $D\_avg,m$ = La **durée moyenne** d&apos;un travail lourd en **minutes**.
* $P\_avg,m$ = La **période moyenne** pour un moniteur lourd en **minutes** (par exemple, un moniteur qui s&apos;exécute toutes les 5 minutes a $P\_avg,m = 5$).

This calculates your expected workload from first principles: how many monitors you have, how often they run, and how long they take.

**Facteurs de dimensionnement importants**

Lorsque vous utilisez ces formules, n’oubliez pas de tenir compte des facteurs suivants :

* **Durée du travail ($D\_avg,m$) :** votre moyenne doit inclure les travaux qui **expirent** (souvent environ 3 minutes), car ceux-ci conservent un cœur pendant toute leur durée.
* **Échecs et nouvelles tentatives de tâches :** lorsqu&apos;un moniteur échoue, il est automatiquement réessayé. Ces nouvelles tentatives sont des tâches supplémentaires qui s’ajoutent à la charge totale. Un moniteur qui échoue systématiquement et réessaye **multiplie efficacement sa période**, ce qui a un impact significatif sur le débit.
* **Mise à l&apos;échelle :** en plus d&apos;ajouter des cœurs supplémentaires à un hôte (mise à l&apos;échelle), vous pouvez déployer des gestionnaires de tâches Synthetics supplémentaires avec la même clé privée de site pour équilibrer la charge des tâches sur plusieurs environnements (mise à l&apos;échelle).

Il est important de noter qu&apos;un seul gestionnaire de tâches Synthetics (SJM) a une limite de débit d&apos; **environ 15 tâches lourdes par minute**. Cela est dû à une stratégie de threading interne qui favorise la concurrence efficace des tâches sur plusieurs SJM par rapport au nombre brut de tâches traitées par SJM. Si vos calculs indiquent un besoin de débit plus élevé, vous devez **augmenter la capacité** en déployant des SJM supplémentaires. Vous pouvez [vérifier si votre file d&apos;attente de tâches augmente](/docs/synthetics/synthetic-monitoring/private-locations/job-manager-maintenance-monitoring/) pour déterminer si davantage de SJM sont nécessaires.

L&apos;ajout de plusieurs SJM avec la même clé privée de site offre plusieurs avantages :

* **Équilibrage de charge**: Les jobs du site privé sont répartis sur tous les SJM disponibles.
* **Protection de basculement**: si une instance SJM tombe en panne, les autres peuvent continuer à traiter les tâches.
* **Débit total plus élevé**: Le débit total de votre site privé devient la somme du débit de chaque SJM (par exemple, deux SJM fournissent jusqu&apos;à \~30 jobs/minute).

### Requête NRQL pour le diagnostic

Vous pouvez exécuter ces requêtes dans le [générateur de requêtes](/docs/query-your-data/explore-query-data/get-started/introduction-querying-new-relic-data/) pour obtenir les entrées de la formule de diagnostic. Assurez-vous de définir la plage horaire sur une période suffisamment longue pour obtenir une moyenne stable.

**1. Find the rate of jobs processed per minute ($R\_proc$)**: This query counts the number of non-ping (heavyweight) jobs completed over the last day and shows the average rate per minute.

```sql
FROM SyntheticCheck
SELECT rate(uniqueCount(id), 1 minute) AS 'job rate per minute'
WHERE location = 'YOUR_PRIVATE_LOCATION' AND typeLabel != 'Ping'
SINCE 1 day ago
```

**2. Find the rate of queue growth per minute ($R\_growth$)**: This query calculates the average per-minute growth of the `jobManagerHeavyweightJobs` queue on a time series chart. A line above zero indicates the queue is growing, while a line below zero means it&apos;s shrinking.

```sql
FROM SyntheticsPrivateLocationStatus
SELECT derivative(jobManagerHeavyweightJobs, 1 minute) AS 'queue growth rate per minute'
WHERE name = 'YOUR_PRIVATE_LOCATION'
TIMESERIES SINCE 1 day ago
```

<Callout variant="tip">
  Assurez-vous de sélectionner le compte sur lequel le site privé existe. Il est préférable de considérer cette requête comme une série chronologique, car la fonction dérivée peut varier considérablement. L’objectif est d’obtenir une estimation du taux de croissance de la file d’attente par minute. Play avec différentes plages horaires pour voir ce qui fonctionne le mieux.
</Callout>

**3. Find total number of heavyweight monitors ($N\_mon$)**: This query finds the unique count of heavyweight monitors.

```sql
FROM SyntheticCheck
SELECT uniqueCount(monitorId) AS 'monitor count'
WHERE location = 'YOUR_PRIVATE_LOCATION' AND typeLabel != 'Ping'
SINCE 1 day ago
```

**4. Find average job duration in minutes ($D\_avg,m$)**: This query finds the average execution duration of completed non-ping jobs and converts the result from milliseconds to minutes. `executionDuration` represents the time the job took to execute on the host.

```sql
FROM SyntheticCheck
SELECT average(executionDuration)/60e3 AS 'avg job duration (m)'
WHERE location = 'YOUR_PRIVATE_LOCATION' AND typeLabel != 'Ping'
SINCE 1 day ago
```

**5. Trouver la période moyenne du moniteur lourd ($P\_avg,m$) :** si la file d&apos;attente `jobManagerHeavyweightJobs` du site privé augmente, il n&apos;est pas précis de calculer la période moyenne du moniteur à partir des résultats existants. Cela devra être estimé à partir de la liste des moniteurs sur la page [des moniteurs Synthétiques](https://one.newrelic.com/synthetics). Assurez-vous de sélectionner le bon compte New Relic et vous devrez peut-être filtrer par `privateLocation`.

<Callout variant="tip">
  Le moniteur synthétique peut exister en plusieurs sous-compte. Si vous avez plus de sous-comptes que ce qui peut être sélectionné dans le générateur de requêtes, choisissez les comptes avec le plus de monitorage.
</Callout>

### Remarque sur le moniteur de ping et la file d&apos;attente `pingJobs`

**Les moniteurs de ping sont différents.** Il s’agit de tâches légères qui ne consomment pas chacune un cœur de processeur complet. Au lieu de cela, ils utilisent une file d’attente distincte (`pingJobs`) et s’exécutent sur un pool de threads de travail.

Bien qu&apos;ils soient moins gourmands en ressources, un volume élevé de tâches de ping, en particulier celles qui échouent, peut néanmoins entraîner des problèmes de performances. Gardez ces points à l’esprit :

* **Modèle de ressources :** les tâches Ping utilisent des threads de travail et non des cœurs de processeur dédiés. Le calcul du nombre de cœurs par emploi ne s’applique pas à eux.
* **Délai d&apos;expiration et nouvelle tentative :** une tâche de ping en échec peut occuper un thread de travail jusqu&apos;à **60 secondes**. Il tente d’abord une requête HTTP HEAD (délai d’expiration de 30 secondes). Si cela échoue, il réessaye immédiatement avec une requête HTTP GET (un autre délai d&apos;attente de 30 secondes).
* **Mise à l&apos;échelle :** Bien que la formule de dimensionnement soit différente, les mêmes principes s&apos;appliquent. Pour gérer un volume important de tâches de ping et empêcher la file d&apos;attente `pingJobs` de croître, vous devrez peut-être effectuer une mise à l&apos;échelle verticale et/ou horizontale. La mise à l&apos;échelle signifie augmenter les ressources CPU et mémoire par hôte ou espace de nommage. La mise à l&apos;échelle signifie ajouter davantage d&apos;instances de l&apos;environnement d&apos;exécution ping. Cela peut être réalisé en déployant plus de gestionnaires de tâches sur plus d&apos;hôtes, dans plus d&apos;espace de nommage, ou même [au sein d&apos;un même espace de nommage](/docs/synthetics/synthetic-monitoring/private-locations/job-manager-configuration#scaling-out-with-multiple-sjm-instances). Alternativement, le `ping-runtime` dans Kubernetes vous permet de définir [un plus grand nombre de réplicas](https://github.com/newrelic/helm-charts/blob/41c03e287dafd41b9c914e5a6c720d5aa5c01ace/charts/synthetics-job-manager/values.yaml#L173) par déploiement.

## Sizing considerations for Kubernetes and OpenShift [#kubernetes-sizing]

Chaque runtime utilisé par le gestionnaire de tâches Kubernetes et OpenShift Synthétique peut être dimensionné indépendamment en définissant des valeurs dans le [graphique Helm](https://github.com/newrelic/helm-charts/tree/master/charts/synthetics-job-manager). Les [node-api-runtime](https://github.com/newrelic/helm-charts/tree/master/charts/synthetics-job-manager/charts/node-api-runtime) et [node-browser-runtime](https://github.com/newrelic/helm-charts/tree/master/charts/synthetics-job-manager/charts/node-browser-runtime) sont dimensionnés indépendamment à l&apos;aide d&apos;une combinaison des paramètres `parallelism` et `completions`.

* The `parallelism` setting controls how many pods of a particular runtime run concurrently.
* The `completions` setting controls how many pods must complete before the `CronJob` starts another Kubernetes Job for that runtime.

### How to Size Your Deployment: A Step-by-Step Guide

Your goal is to configure enough parallelism to handle your job load without exceeding the throughput limit of your SJM instances.

### Step 1: Estimate Your Required Workload

**Completions:** This determines how many runtime pods should complete before a new Kubernetes Job is started.

First, determine your private location&apos;s average job execution duration and job rate. Use `executionDuration` as it most accurately reflects the pod&apos;s active runtime.

```sql
-- Get average job execution duration (in seconds)
FROM SyntheticCheck
SELECT average(executionDuration / 60e3) AS 'D_avg_m'
WHERE typeLabel != 'Ping' AND location = 'YOUR_PRIVATE_LOCATION'
FACET typeLabel SINCE 1 hour ago
```

$$ Completions = \frac5D\_avg,m $$

Where $D\_avg,m$ is your **average job execution duration in seconds**.

**Required Parallelism:** This determines how many workers (pods) you need running concurrently to handle your 5-minute job load.

```sql
-- Get jobs per 5 minutes
FROM SyntheticCheck
SELECT rate(uniqueCount(id), 5 minutes) AS 'N_m'
WHERE typeLabel != 'Ping' AND location = 'YOUR_PRIVATE_LOCATION'
FACET typeLabel SINCE 1 hour ago
```

$$ P\_req = \fracN\_mCompletions $$

Where $N\_m$ is your **number of jobs per 5 minutes**. This $P\_req$ value is your **target total parallelism**.

### Step 2: Check Against the Single-SJM Throughput Limit

**Max Parallelism:** This determines how many workers (pods) your SJM can effectively utilize.

$$ P\_max \approx 15 \cdot D\_avg,m $$

This $P\_max$ value is your **system limit for one SJM Helm deployment**.

<Callout variant="tip">
  The above queries are based on current results. If your private location does not have any results or the job manager is not performing at its best, query results may not be accurate. In that case, start with the examples in the table below and adjust until your queue is stable.
</Callout>

<Callout variant="tip">
  A key consideration is that a **single SJM instance has a maximum throughput of approximately 15 heavyweight jobs per minute**. You can calculate the maximum effective parallelism ($P\_max$) a single SJM can support before hitting this ceiling.
</Callout>

### Step 3: Compare, Configure, and Scale

Compare your **required** parallelism ($P\_req$) from Step 1 to the **maximum** parallelism ($P\_max$) from Step 2.

<DNT>
  **Scenario A:** $P\_req \le P\_max$
</DNT>

* **Diagnosis:** Your job load is within the limit of a single SJM instance.

* **Action:**

  1. You will deploy **one** SJM Helm release.
  2. In your Helm chart `values.yaml`, set `parallelism` to your calculated $P\_req$.
  3. Set `completions` to your calculated **Completions**. For improved efficiency, this value should typically be 6-10x your `parallelism` setting.

<DNT>
  **Scenario B:** $P\_req &gt; P\_max$
</DNT>

* **Diagnosis:** Your job load **exceeds the \~15 jobs/minute limit** of a single SJM.

* **Action:**

  1. You must **scale out by deploying multiple, separate SJM Helm releases**.
  2. See the **[Scaling Out with Multiple SJM Deployments](#scaling-out-with-multiple-sjm-deployments)** section below for the correct procedure.
  3. **Do not** increase the `replicaCount` in your Helm chart.

### Step 4: Monitor Your Queue

After applying your changes, you must verify that your job queue is stable and not growing. A consistently growing queue means your location is still under-provisioned.

Run this query to check the queue&apos;s growth rate:

```sql
-- Check for queue growth (a positive value means the queue is growing)
SELECT derivative(jobManagerHeavyweightJobs, 1 minute) AS 'Heavyweight Queue Growth Rate (per min)'
FROM SyntheticsPrivateLocationStatus
WHERE name = 'YOUR_PRIVATE_LOCATION'
SINCE 1 hour ago TIMESERIES
```

If the &quot;Queue Growth Rate&quot; is consistently positive, you need to install more SJM Helm deployments (Scenario B) or re-check your `parallelism` settings (Scenario A).

### Configuration Examples and Tuning

The `parallelism` setting directly affects how many synthetics jobs per minute can be run. Too small a value and the queue may grow. Too large a value and nodes may become resource constrained.

<table>
  <thead>
    <tr>
      <th style={{ width: "300px" }}>
        Exemple
      </th>

      <th>
        Description
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        `parallelism=1` `completions=1`
      </td>

      <td>
        Le runtime exécutera 1 tâche Synthetics par minute. Une fois la tâche terminée, la configuration `CronJob` démarrera une nouvelle tâche à la minute suivante. <DNT>**Throughput will be extremely limited with this configuration.**</DNT>
      </td>
    </tr>

    <tr>
      <td>
        `parallelism=1` `completions=6`
      </td>

      <td>
        The runtime will execute 1 synthetics job at a time. After the job completes, a new job will start immediately. After 6 jobs complete, the `CronJob` configuration will start a new Kubernetes Job. <DNT>**Throughput will be limited.**</DNT> A single long-running synthetics job will block the processing of any other synthetics jobs of this type.
      </td>
    </tr>

    <tr>
      <td>
        `parallelism=3` `completions=24`
      </td>

      <td>
        The runtime will execute 3 synthetics jobs at once. After any of these jobs complete, a new job will start immediately. After 24 jobs complete, the `CronJob` configuration will start a new Kubernetes Job. <DNT>**Throughput is much better with this or similar configurations.**</DNT>
      </td>
    </tr>
  </tbody>
</table>

If your `parallelism` setting is working well (keeping the queue at zero), setting a higher `completions` value (e.g., 6-10x `parallelism`) can improve efficiency by:

* Accommodating variability in job durations.
* Reducing the number of completion cycles to minimize the &quot;nearing the end of completions&quot; inefficiency where the next batch can&apos;t start until the final job from the current batch completes.

Il est important de noter que la valeur `completions` ne doit pas être trop grande, sinon le CronJob rencontrera un événement d&apos;avertissement comme le suivant :

```sh
8m40s     Warning   TooManyMissedTimes   cronjob/synthetics-node-browser-runtime   too many missed start times: 101. Set or decrease .spec.startingDeadlineSeconds or check clock skew
```

<Callout variant="tip">
  New Relic n&apos;est pas responsable des modifications que vous apportez aux fichiers du gestionnaire de tâches Synthetics.
</Callout>

### Scaling out with multiple SJM deployments

To scale beyond the \~15 jobs/minute throughput of a single SJM, you must install **multiple, separate SJM Helm releases**.

<Callout variant="important">
  **Do not use `replicaCount` to scale the job manager pod.** You **cannot** scale by increasing the `replicaCount` for a single Helm release. The SJM architecture requires a 1:1 relationship between a runtime pod and its parent SJM pod. If runtime pods send results back to the wrong SJM replica (e.g., through a Kubernetes service), those results will be lost.
</Callout>

The correct strategy is to deploy multiple SJM instances, each as its own Helm release. Each SJM will compete for jobs from the same private location, providing load balancing, failover protection, and an increased total job throughput.

#### Simplified Scaling Strategy

Assuming $P\_req &gt; P\_max$ and you need to scale out, you can simplify maintenance by treating each SJM deployment as a fixed-capacity unit.

1. **Set Max Parallelism:** For *each* SJM, set `parallelism` to the same $P\_max$ value. This maximizes the potential throughput of each SJM.

2. **Set Completions:** For *each* SJM, set `completions` to a fixed value as well. The $P\_req$ formula from [Step 1](#step-1-estimate-your-required-workload) can be modified to estimate completions by substituting in the $P\_max$ value:

   $$ Completions = \fracN\_mP\_max $$

   Where $N\_m$ is your **number of jobs per 5 minutes**. Adjust as needed after deploying to target a 5 minute Kubernetes job age per runtime, i.e., node-browser-runtime and node-api-runtime.

3. **Install Releases:** Install as many separate Helm releases as you need to handle your total $P\_req$. For example, if your total $P\_req$ is 60 and you&apos;ve fixed each SJM&apos;s `parallelism` at 20 ($P\_max$ from [Step 2](#step-2-check-against-the-single-sjm-throughput-limit)), you would need **three** separate Helm deployments to meet the required job demand.

4. **Monitor and Add:** Monitor your job queue (see [Step 4](#step-4-monitor-your-queue)). If it starts to grow, simply install another Helm release (e.g., `sjm-delta`) using the same fixed configuration.

By fixing parallelism and completions to static values based on $P\_max$, increasing or decreasing capacity becomes a simpler process of **adding or removing Helm releases**. This helps to avoid wasting cluster resources on a parallelism value that is higher than the SJM can effectively utilize.

#### Installation Example

When installing multiple SJM releases, you must provide a **unique name for each release**. All instances must be configured with the **same private location key**.

Setting the `fullnameOverride` is highly recommended to create shorter, more manageable resource names. For example, to install two SJMs named `sjm-alpha` and `sjm-beta` into the `newrelic` namespace (both using the same `values.yaml` with your fixed parallelism and completions):

```sh
# Install the first SJM deployment
helm upgrade --install sjm-alpha newrelic/synthetics-job-manager \
  -n newrelic \
  -f values.yaml \
  --set fullnameOverride=sjm-alpha \
  --set ping-runtime.fullnameOverride=sjm-alpha-ping \
  --set node-api-runtime.fullnameOverride=sjm-alpha-api \
  --set node-browser-runtime.fullnameOverride=sjm-alpha-browser
```

```sh
# Install the second SJM deployment to add capacity
helm upgrade --install sjm-beta newrelic/synthetics-job-manager \
  -n newrelic \
  -f values.yaml \
  --set fullnameOverride=sjm-beta
  --set ping-runtime.fullnameOverride=sjm-beta-ping \
  --set node-api-runtime.fullnameOverride=sjm-beta-api \
  --set node-browser-runtime.fullnameOverride=sjm-beta-browser
```

You can continue this pattern (`sjm-charlie`, `sjm-delta`, etc.) for as many SJMs as needed to keep the job queue from growing.