---
title: Analisando dados log
tags:
  - Logs
  - Log management
  - UI and data
metaDescription: How New Relic uses parsing and how to send customized log data.
freshnessValidatedDate: never
translationType: machine
---

O Log <DNT>parsing</DNT> transforma dados de log não estruturados em atributos pesquisáveis que você pode usar para obter insights mais profundos dos seus logs. Esses atributos permitem filtrar, facetar e criar alertas sobre seus dados com precisão.

## Escolha sua estratégia de parsing [#choose]

Decida se deseja analisar os dados no momento da ingestão ou ao executar uma consulta:

<table>
  <thead>
    <tr>
      <th style={{ width: "200px" }}>
        Tipo de análise
      </th>

      <th>
        Descrição
      </th>

      <th>
        Melhor para
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        **Parsing em tempo de consulta**
      </td>

      <td>
        Cria atributos temporários usando NRQL que existem apenas durante a execução da consulta. Ideal para análise instantânea de dados existentes sem esperar que novos logs cheguem. Saiba mais sobre [análise em tempo de consulta](/docs/logs/ui-data/query-time-parsing).
      </td>

      <td>
        * Solução de problemas e investigações ad hoc
        * Análise exploratória em pequenos conjuntos de dados
        * Investigações pontuais
        * Extraindo atributos de logs já armazenados no NRDB
      </td>
    </tr>

    <tr>
      <td>
        **Análise em tempo de ingestão**
      </td>

      <td>
        Cria atributos permanentes armazenados no NRDB. Duas maneiras de criar regras de parsing em tempo de ingestão:

        * **Regras de parsing integradas:** Padrões pré-configurados para fontes de log comuns (Apache, NGINX, CloudFront, MongoDB, etc.). Basta adicionar um atributo `logtype` ao encaminhar logs. Consulte a [lista completa de regras integradas](/docs/logs/ui-data/built-log-parsing-rules).

        * **Regras de parsing personalizadas:** Quando seus logs são específicos da sua aplicação, as regras de parsing personalizadas permitem definir exatamente quais campos são importantes para o seu negócio.

          * **Parsing de logs sem código (UI):** Detecta padrões em seus logs de amostra. Melhor para usuários que desejam apontar e clicar para extrair campos.
          * **Grok/Regex personalizado:** Entrada manual de código para formatos de log altamente complexos.
      </td>

      <td>
        * Altos volumes de logs
        * Atributos analisados necessários para alertas, dashboards e monitoramento contínuo
      </td>
    </tr>
  </tbody>
</table>

Você também pode criar, consultar e gerenciar suas regras de análise de log usando NerdGraph, nossa API GraphQL. Uma ferramenta útil para isso é nosso [Nerdgraph API Explorer](https://api.newrelic.com/graphiql). Para obter mais informações, consulte nosso [tutorial NerdGraph para análise de arquivos](/docs/apis/nerdgraph/examples/nerdgraph-log-parsing-rules-tutorial/).

<DNT>
  /* Here&apos;s a 5-minute video about log parsing: &lt;Video id=&quot;xPWM46yw3bQ&quot; type=&quot;youtube&quot; /&gt; */
</DNT>

## Como funciona o parsing personalizado em tempo de ingestão [#how-it-works]

A análise personalizada permite que você defina exatamente como o New Relic estrutura seus logs de entrada. Antes de criar regras, é importante entender as restrições técnicas do pipeline de ingestão.

<table>
  <thead>
    <tr>
      <th style={{ width: "100px" }}>
        Análise de log
      </th>

      <th>
        Como funciona
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        O que
      </td>

      <td>
        As regras de análise são altamente direcionadas. Ao criar uma regra, você define:

        * **O campo alvo:** A análise é aplicada a um campo específico por vez.
        * **A lógica de correspondência:** Use uma cláusula `WHERE` do NRQL para filtrar exatamente quais logs esta regra deve avaliar.
        * **O método de extração:** Você pode usar o **No Code Log Parsing** para uma experiência de detecção de padrões automática e guiada ou escrever manualmente **Grok/Regex** para estruturas de log altamente personalizadas e complexas.
      </td>
    </tr>

    <tr>
      <td>
        Quando
      </td>

      <td>
        A New Relic processa logs em ordem sequencial. Isso afeta quais condições podem ser correspondidas.

        * A análise ocorre enquanto os dados são ingeridos. Assim que um log é gravado no NRDB, as alterações feitas são permanentes.
        * Assim que uma regra é salva e ativada, as regras começam a processar os logs recebidos imediatamente.
        * A análise ocorre **antes** do enriquecimento de dados (como síntese de entidades), descarte ou particionamento.
      </td>
    </tr>

    <tr>
      <td>
        Validação
      </td>

      <td>
        Para garantir que suas regras funcionem antes de afetarem seus dados ingeridos, você pode validar a pré-visualização da saída com até 10 logs de amostra dos últimos 30 minutos. Estas são amostras históricas armazenadas no NRDB, não logs de streaming em tempo real.
      </td>
    </tr>
  </tbody>
</table>

## Criar uma regra personalizada [#custom-parsing]

Você pode criar regras de parsing no contexto ao investigar um log. Isso evita a troca de contexto e reduz o Tempo Médio para Detecção (MTTD). Alternativamente, você pode criar regras do zero ao integrar uma nova aplicação ou serviço.

### Parsing de logs sem código [#no-code]

Use o No Code Log Parsing para detectar e extrair campos de seus logs de amostra. A New Relic analisa seus logs de amostra e sugere padrões que você pode configurar.

<Steps>
  <Step>
    Para criar uma regra no contexto, acesse <DNT>**[one.newrelic.com](https://one.newrelic.com) &gt; Logs**</DNT> e aplique um filtro (ou selecione qualquer entidade que tenha logs, como APM, Browser ou Mobile, e navegue até <DNT>**Logs in Context**</DNT>).

    Para criar uma regra sem contexto, vá para <DNT>**[one.newrelic.com](https://one.newrelic.com) &gt; Logs**</DNT> sem definir um filtro ou vá para <DNT>**Logs &gt; Parsing**</DNT> e clique em <DNT>**Create a parsing rule**</DNT>.
  </Step>

  <Step>
    No processo de criação de regras no contexto:

    1. Clique em um log para abrir <DNT>**Log details**</DNT>
    2. Selecione o atributo de log que você deseja analisar (por exemplo, `message`)
    3. Clique em <DNT>**Create ingest time parsing rule**</DNT> e forneça um nome para sua regra

    Se você aplicou um filtro na interface de Logs antes de criar a regra, uma condição de correspondência é preenchida automaticamente com base nesse filtro.

    <img title="Log parsing rules" alt="Screenshot of log filtering in UI" src="/images/logs_filtering.webp" />

    No fluxo sem contexto, dê um nome à sua regra e defina uma condição de filtro NRQL ou cole um exemplo de log.

    * Se você definir um filtro de log, clique em <DNT>**Run your query**</DNT>, selecione o campo que deseja analisar e clique em <DNT>**Next**</DNT>.
    * Se você colar uma amostra de log, você deve definir a cláusula NRQL `WHERE` para corresponder aos seus logs, selecionar o campo que deseja analisar e clicar em <DNT>**Next**</DNT>.

    <img title="Log parsing rules" alt="Screenshot of creating a parsing rule in UI" src="/images/Create_a_parsing_rule_.webp" />
  </Step>

  <Step>
    Revise o <DNT>**Patterns we detected**</DNT> no log de amostra selecionado e a regra que foi criada. Clique em um padrão destacado para visualizar e editar sua configuração.

    <img title="Screenshot of matching pattern in UI" alt="Screenshot of matching pattern in UI" src="/images/matching_pattern.webp" />

    <Callout variant="Preview" title="Observação">
      * Ao nomear atributos, use letras minúsculas com sublinhados. Evite caracteres especiais, exceto sublinhados, e não inicie um nome de atributo com um número.

      * Para substrings que você deseja evitar analisar e que incluem valores dinâmicos, certifique-se de defini-las como substrings dinâmicas, selecionando e alterando sua configuração para <DNT>**Yes**</DNT>.
    </Callout>
  </Step>

  <Step>
    Para um controle mais granular sobre os campos a extrair, clique e arraste para destacar o log de amostra.

    <img title="Log parsing rules" alt="Screenshot of creating a parsing rule in UI" src="/images/create_a_parsing_rule.webp" />

    Você pode interagir com os padrões das seguintes maneiras:

    * <DNT>**Auto detect patterns**</DNT>: Para detectar padrões em qualquer parte do log de amostra que ainda não esteja realçada, clique e arraste para realçar essa substring e clique em <DNT>**Auto detect patterns**</DNT>. O New Relic encontrará e destacará padrões na parte selecionada. Para uma lista de nomes de padrões Grok suportados, consulte [Nomes de padrões Grok suportados](#grok-patterns).
    * <DNT>**Select text to parse**</DNT>: Selecione este modo para a experiência guiada de criação de regras. Este modo oferece uma configuração padrão por padrão. Após definir as configurações de padrão, clique em <DNT>**Add pattern to rule**</DNT> para ver a regra atualizada e visualizar a saída.

    Se os padrões detectados não forem relevantes ou estiverem extraindo dados indesejados, você pode removê-los da regra criada:

    * Destaque o padrão indesejado na janela de log de amostra e clique em <DNT>**Remove selected patterns**</DNT>, ou
    * Clique em um padrão e selecione <DNT>**Remove**</DNT>.
  </Step>

  <Step>
    Revise o painel <DNT>**Preview output**</DNT>. Verifique se os logs de amostra mostram uma marca de seleção verde, indicando que correspondem à sua regra e que os campos serão extraídos no momento da ingestão.

    * Para alterar sua amostra, expanda qualquer log no painel <DNT>**Preview output**</DNT> e clique em <DNT>**Use as sample**</DNT>.

      * Se você selecionou um log sem correspondência: A amostra selecionada aparecerá na janela de log de amostra, novos padrões serão detectados e uma nova regra será criada.
      * Se você selecionou um log correspondente: A amostra selecionada será exibida na janela de log de amostra.
  </Step>

  <Step>
    Clique em <DNT>**Save rule**</DNT> para ativar imediatamente ou em <DNT>**Save as draft**</DNT> para ativar mais tarde.
  </Step>
</Steps>

### Escreva seu próprio Grok/Regex personalizado [#grok]

Para formatos exclusivos, usuários avançados podem clicar em <DNT>**Write your own rule**</DNT> na página <DNT>**Create a parsing rule**</DNT> para alternar para o editor de código e modificar padrões diretamente no editor de regras.

<img title="Screenshot depicting where to click to write your own custom Grok/regex" alt="Screenshot depicting where to click to write your own custom Grok/regex" src="/images/create_a_parsing_rule.webp" />

Ao terminar de editar a regra, clique em <DNT>**Preview**</DNT> para ver a saída de pré-visualização atualizada e clique em <DNT>**Save rule**</DNT> para ativá-la.

<Callout variant="preview" title="Observação">
  Para alternar para o editor legado, clique em <DNT>**Switch to original editor**</DNT> no canto superior direito da página <DNT>**Create a parsing rule**</DNT>.
</Callout>

#### Padrões de dados suportados [#supported-patterns]

A New Relic suporta a análise de vários tipos e formatos de dados usando padrões Grok. Os padrões de parsing são especificados usando Grok, um padrão da indústria para parsing de mensagens de log. O Grok é um superconjunto de expressões regulares que adiciona padrões nomeados integrados para serem usados no lugar de expressões regulares complexas literais.

Regras de parsing podem incluir uma combinação de expressões regulares e nomes de padrões Grok em sua string de correspondência. Clique neste link para ver uma lista de [padrões Grok](https://github.com/thekrakken/java-grok/tree/master/src/main/resources/patterns) suportados e aqui para ver uma lista de [tipos Grok](#grok-types) suportados.

<CollapserGroup>
  <Collapser id="grok-syntax" title="Sintaxe de padrão Grok">
    Padrões Grok seguem uma sintaxe predefinida:

    ```
    %{PATTERN_NAME[:OPTIONAL_EXTRACTED_ATTRIBUTE_NAME[:OPTIONAL_TYPE[:OPTIONAL_PARAMETER]]]}
    ```

    Onde:

    * `PATTERN_NAME` é um dos padrões Grok suportados. O nome do padrão é apenas um nome amigável que representa uma expressão regular. Eles são exatamente iguais à expressão regular correspondente.
    * `OPTIONAL_EXTRACTED_ATTRIBUTE_NAME`, se fornecido, é o nome do atributo que será adicionado à sua mensagem do log com o valor correspondente ao nome do padrão. É equivalente a usar um grupo de captura nomeado usando expressões regulares. Se isso não for fornecido, a regra de análise corresponderá apenas a uma região da sua string, mas não extrairá um atributo com seu valor.
    * `OPTIONAL_TYPE` especifica o tipo de valor de atributo a ser extraído. Se omitido, os valores serão extraídos como strings. Por exemplo, para extrair o valor `123` de `"File Size: 123"` como um número para o atributo `file_size`, use `value: %{INT:file_size:int}`.
    * `OPTIONAL_PARAMETER` especifica um parâmetro opcional para determinados tipos. Atualmente apenas o tipo `datetime` recebe um parâmetro. Veja detalhes abaixo.
  </Collapser>

  <Collapser id="grok-types" title="Tipos de Grok suportados">
    O campo `OPTIONAL_TYPE` especifica o tipo de valor de atributo a ser extraído. Se omitido, os valores serão extraídos como strings.

    Os tipos suportados são:

    <table>
      <thead>
        <tr>
          <th>
            Tipo especificado no Grok
          </th>

          <th>
            Tipo armazenado no banco de dados New Relic
          </th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>
            `boolean`
          </td>

          <td>
            `boolean`
          </td>
        </tr>

        <tr>
          <td>
            `byte` `short` `int` `integer`
          </td>

          <td>
            `integer`
          </td>
        </tr>

        <tr>
          <td>
            `long`
          </td>

          <td>
            `long`
          </td>
        </tr>

        <tr>
          <td>
            `float`
          </td>

          <td>
            `float`
          </td>
        </tr>

        <tr>
          <td>
            `double`
          </td>

          <td>
            `double`
          </td>
        </tr>

        <tr>
          <td>
            `string` (padrão) `text`
          </td>

          <td>
            `string`
          </td>
        </tr>

        <tr>
          <td>
            `date` `datetime`
          </td>

          <td>
            O tempo como `long`

            Por padrão, é interpretado como ISO 8601. Se `OPTIONAL_PARAMETER` estiver presente, ele especifica a [string de padrão de data e hora](https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/text/SimpleDateFormat.html)a ser usada para interpretar o `datetime`.

            Observe que isso só está disponível durante a análise. Temos uma [etapa adicional e separada de interpretação timestamp ](/docs/logs/ui-data/timestamp-support)que ocorre para todos os logs posteriormente no pipeline de ingestão.
          </td>
        </tr>

        <tr>
          <td>
            `json`
          </td>

          <td>
            Dados estruturados JSON. Consulte [Analisando JSON misturado com texto simples](#parsing-json) para obter mais informações.
          </td>
        </tr>

        <tr>
          <td>
            `csv`
          </td>

          <td>
            Dados CSV. Consulte [Analisando CSV](#parsing-csv) para obter mais informações.
          </td>
        </tr>

        <tr>
          <td>
            `geo`
          </td>

          <td>
            Localização geográfica de endereços IP. Consulte [Localização geográfica de endereços IP (GeoIP)](#geo) para obter mais informações.
          </td>
        </tr>

        <tr>
          <td>
            `key value pairs`
          </td>

          <td>
            valor principal Par . Veja [Análise de pares de valor principal](#parsing-key-value-pairs) para mais informações.
          </td>
        </tr>
      </tbody>
    </table>
  </Collapser>

  <Collapser id="grok-multiline" title="Análise multilinha Grok">
    Se você tiver log multilinha, esteja ciente de que o padrão `GREEDYDATA` Grok não corresponde a novas linhas (é equivalente a `.*`).

    Portanto, em vez de usar `%{GREEDYDATA:some_attribute}` diretamente, você precisará adicionar o sinalizador multilinha na frente dele: `(?s)%{GREEDYDATA:some_attribute}`
  </Collapser>

  <Collapser id="parsing-json" title="Analisando JSON misturado com texto simples">
    O pipeline de logs do New Relic analisa suas mensagens de log JSON por padrão, mas, às vezes, você tem mensagens de log JSON misturadas com texto simples. Nessa situação, você pode querer analisá-los e, em seguida, filtrar usando os atributos JSON. Se for esse o caso, você pode usar o [tipo Grok](#grok-syntax) `json`, que analisará o JSON capturado pelo padrão Grok. Este formato baseia-se em 3 partes principais: a sintaxe Grok, o prefixo que você deseja atribuir aos atributos JSON analisados e o `json` [tipo Grok](#grok-syntax). Usando o `json` [tipo Grok](#grok-syntax), você pode extrair e analisar JSON de logs que não estão formatados corretamente; por exemplo, se seus logs forem prefixados com uma string de data/hora:

    ```json
    2015-05-13T23:39:43.945958Z {"event": "TestRequest", "status": 200, "response": {"headers": {"X-Custom": "foo"}}, "request": {"headers": {"X-Custom": "bar"}}}
    ```

    Para extrair e analisar os dados JSON deste formato de log, crie a seguinte expressão Grok:

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json}
    ```

    O log resultante é:

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    my_attribute_prefix.event: "TestRequest"
    my_attribute_prefix.status: 200
    my_attribute_prefix.response.headers.X-Custom: "foo"
    my_attribute_prefix.request.headers.X-Custom: "bar"
    ```

    Você pode definir a lista de atributos a serem extraídos ou descartados com as opções `keepAttributes` ou `dropAttributes`. Por exemplo, com a seguinte expressão Grok:

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json({"keepAttributes": ["my_attribute_prefix.event", "my_attribute_prefix.response.headers.X-Custom"]})}
    ```

    O log resultante é:

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    my_attribute_prefix.event: "TestRequest"
    my_attribute_prefix.request.headers.X-Custom: "bar"
    ```

    Se quiser omitir o prefixo `my_attribute_prefix` , você poderá incluir `"noPrefix": true` na configuração.

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json({"noPrefix": true})}
    ```

    Se desejar omitir o prefixo `my_attribute_prefix` e manter apenas o atributo `status` , você poderá incluir `"noPrefix": true` e `"keepAttributes: ["status"]` na configuração.

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:json({"noPrefix": true, "keepAttributes": ["status"]})}
    ```

    Se seu JSON tiver escapado, você poderá usar a opção `isEscaped` para poder analisá-lo. Se o seu JSON tiver sido escapado e depois citado, você também precisará combinar as aspas, conforme mostrado abaixo. Por exemplo, com a seguinte expressão Grok:

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} "%{GREEDYDATA:my_attribute_prefix:json({"isEscaped": true})}"
    ```

    Seria capaz de analisar a mensagem escapada:

    ```
    2015-05-13T23:39:43.945958Z "{\"event\": \"TestRequest\", \"status\": 200, \"response\": {\"headers\": {\"X-Custom\": \"foo\"}}, \"request\": {\"headers\": {\"X-Custom\": \"bar\"}}}"
    ```

    O log resultante é:

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    my_attribute_prefix.event: "TestRequest"
    my_attribute_prefix.status: 200
    my_attribute_prefix.response.headers.X-Custom: "foo"
    my_attribute_prefix.request.headers.X-Custom: "bar"
    ```

    Para configurar o tipo `json` [Grok](#grok-syntax), use `:json(_CONFIG_)`:

    * `json({"dropOriginal": true})`: elimine o trecho JSON que foi usado na análise. Quando definido como `true` (valor padrão), a regra de análise eliminará o trecho JSON original. Observe que o atributo JSON permanecerá no campo da mensagem.
    * `json({"dropOriginal": false})`: isso mostrará a carga JSON que foi extraída. Quando definido como `false`, a carga completa somente JSON será exibida em um atributo nomeado em `my_attribute_prefix` acima. Observe que o atributo JSON permanecerá no campo de mensagem aqui, dando ao usuário 3 visualizações diferentes dos dados JSON. Se o armazenamento de todas as três versões for uma preocupação, é recomendado usar o padrão `true` aqui.
    * `json({"depth": 62})`: níveis de profundidade que você deseja analisar o valor JSON (padrão 62).
    * `json({"keepAttributes": ["attr1", "attr2", ..., "attrN"]})`: Especifica qual atributo será extraído do JSON. A lista fornecida não pode estar vazia. Se esta opção de configuração não estiver definida, todos os atributos serão extraídos.
    * `json({"dropAttributes": ["attr1", "attr2", ..., "attrN"]})`: Especifica qual atributo será retirado do JSON. Se esta opção de configuração não for definida, nenhum atributo será eliminado.
    * `json({"noPrefix": true})`: Defina esta opção como `true` para remover o prefixo do atributo extraído do JSON.
    * `json({"isEscaped": true})`: Defina esta opção como `true` para analisar o JSON que foi escapado (que normalmente você vê quando o JSON é stringificado, por exemplo `{\"key\": \"value\"}`)
  </Collapser>

  <Collapser id="parsing-csv" title="Analisando CSV">
    Se o seu sistema envia log de valores separados por vírgula (CSV) e você precisa analisá-los no New Relic, você pode usar o [tipo `csv` Grok](#grok-syntax), que analisa o CSV capturado pelo padrão Grok. Este formato depende de 3 partes principais: a sintaxe Grok, o prefixo que você gostaria de atribuir ao atributo CSV analisado e o `csv` [tipo Grok](#grok-syntax). Usando o tipo `csv` [Grok](#grok-syntax), você pode extrair e analisar CSV do log.

    Dada a seguinte linha de log CSV como exemplo:

    ```
    "2015-05-13T23:39:43.945958Z,202,POST,/shopcart/checkout,142,10"
    ```

    E uma regra de análise com o seguinte formato:

    ```
    %{GREEDYDATA:log:csv({"columns": ["timestamp", "status", "method", "url", "time", "bytes"]})}
    ```

    Irá analisar seu log da seguinte maneira:

    ```
    log.timestamp: "2015-05-13T23:39:43.945958Z"
    log.status: "202"
    log.method: "POST"
    log.url: "/shopcart/checkout"
    log.time: "142"
    log.bytes: "10"
    ```

    Se precisar omitir o prefixo &quot;log&quot;, você poderá incluir `"noPrefix": true` na configuração.

    ```
    %{GREEDYDATA:log:csv({"columns": ["timestamp", "status", "method", "url", "time", "bytes"], "noPrefix": true})}
    ```

    #### Configuração de colunas:

    * É obrigatório indicar as colunas na configuração do tipo CSV Grok (que deve ser um JSON válido).
    * Você pode ignorar qualquer coluna definindo &quot;\_&quot; (sublinhado) como o nome da coluna para removê-la do objeto resultante.

    #### Opções de configuração opcionais:

    Embora a configuração de “colunas” seja obrigatória, é possível alterar a análise do CSV com as seguintes configurações.

    * <DNT>**dropOriginal**</DNT>: (o padrão é `true`) Elimine o trecho CSV usado na análise. Quando definida como `true` (valor padrão), a regra de análise descarta o campo original.
    * <DNT>**noPrefix**</DNT>: (o padrão é `false`) Não inclui o nome do campo Grok como prefixo no objeto resultante.
    * <DNT>**separator**</DNT>: (O padrão é `,`) Define o caractere/string que divide cada coluna.
      * Outro cenário comum são os valores separados por tabulações (TSV), para isso você deve indicar `\t` como separador, ex. `%{GREEDYDATA:log:csv({"columns": ["timestamp", "status", "method", "url", "time", "bytes"], "separator": "\t"})`
    * <DNT>**quoteChar**</DNT>: (O padrão é `"`) Define o caractere que opcionalmente envolve o conteúdo de uma coluna.
  </Collapser>

  <Collapser id="geo" title="Geolocalização de endereços IP (GeoIP)">
    Se o seu sistema enviar log contendo endereços IPv4, New Relic poderá localizá-los geograficamente e enriquecer o evento de log com o atributo especificado. Você pode usar o tipo `geo` [Grok](#grok-syntax), que encontra a posição de um endereço IP capturado pelo padrão Grok. Este formato pode ser configurado para retornar um ou mais campos relacionados ao endereço, como cidade, país e latitude/longitude do IP.

    Dada a seguinte linha de log como exemplo:

    ```
    2015-05-13T23:39:43.945958Z 146.190.212.184
    ```

    E uma regra de análise com o seguinte formato:

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:ip:geo({"lookup":["city","region","countryCode", "latitude","longitude"]})}
    ```

    Analisaremos seu log da seguinte maneira:

    ```
    ip: 146.190.212.184
    ip.city: North Bergen
    ip.countryCode: US
    ip.countryName: United States
    ip.latitude: 40.793
    ip.longitude: -74.0247
    ip.postalCode: 07047
    ip.region: NJ
    ip.regionName: New Jersey
    containerTimestamp:2015-05-13T23:39:43.945958Z
    ISO8601_TIMEZONE:Z
    ```

    #### Configuração de pesquisa:

    É obrigatório especificar os campos `lookup` desejados retornados pela ação `geo` . É necessário pelo menos um item das opções a seguir.

    * <DNT>**city**</DNT>: Nome da cidade
    * <DNT>**countryCode**</DNT>: Abreviatura do país
    * <DNT>**countryName**</DNT>: Nome de país
    * <DNT>**latitude**</DNT>: Latitude
    * <DNT>**longitude**</DNT>: Longitude
    * <DNT>**postalCode**</DNT>: Código postal, CEP ou similar
    * <DNT>**region**</DNT>: Abreviatura de estado, província ou território
    * <DNT>**regionName**</DNT>: Nome do estado, província ou território
  </Collapser>

  <Collapser id="parsing-key-value-pairs" title="Analisando pares de valor principal">
    O pipeline do New Relic Logs analisa seu mensagem do log por padrão, mas às vezes você tem mensagem do log que são formatados como pares valor principal. Nessa situação, talvez você queira analisá-los e depois filtrar usando o atributo valor principal.

    Nesse caso, você pode usar o `keyvalue` [tipo Grok](#grok-syntax), que analisará os pares chave-valor capturados pelo padrão Grok. Este formato baseia-se em 3 partes principais: a sintaxe Grok, o prefixo que você deseja atribuir aos atributos de chave-valor analisados e o `keyvalue` [tipo Grok](#grok-syntax). Usando o [tipo Grok](#grok-syntax) `keyvalue`, você pode extrair e analisar pares chave-valor de logs que não estão formatados corretamente; por exemplo, se seus logs forem prefixados com uma string de data/hora:

    ```json
      2015-05-13T23:39:43.945958Z key1=value1,key2=value2,key3=value3
    ```

    Para extrair e analisar os dados de chave-valor deste formato de log, crie a seguinte expressão Grok:

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:keyvalue()}
    ```

    O log resultante é:

    ```
      containerTimestamp: "2015-05-13T23:39:43.945958Z"
      my_attribute_prefix.key1: "value1"
      my_attribute_prefix.key2: "value2"
      my_attribute_prefix.key3: "value3"
    ```

    Você também pode definir o delimitador e o separador personalizados para extrair os pares de valor principal necessários.

    ```json
    2015-05-13T23:39:43.945958Z event:TestRequest request:bar
    ```

    Por exemplo, com a seguinte expressão Grok:

    ```
      %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:keyvalue({"delimiter": " ", "keyValueSeparator": ":"})}
    ```

    O log resultante é:

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    my_attribute_prefix.event: "TestRequest"
    my_attribute_prefix.request: "bar"
    ```

    Se quiser omitir o prefixo `my_attribute_prefix` , você poderá incluir `"noPrefix": true` na configuração.

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:keyValue({"noPrefix": true})}
    ```

    O log resultante é:

    ```
    containerTimestamp: "2015-05-13T23:39:43.945958Z"
    event: "TestRequest"
    request: "bar"
    ```

    Se você quiser definir seu prefixo de caractere de aspas personalizado, você pode incluir &quot;quoteChar&quot;: na configuração.

    ```json
    2015-05-13T23:39:43.945958Z nbn_demo='INFO',message='This message contains information with spaces ,sessionId='abc123'
    ```

    ```
    %{TIMESTAMP_ISO8601:containerTimestamp} %{GREEDYDATA:my_attribute_prefix:keyValue({"quoteChar": "'"})}
    ```

    O log resultante é:

    ```
    "my_attribute_prefix.message": "'This message contains information with spaces",
    "my_attribute_prefix.nbn_demo": "INFO",
    "my_attribute_prefix.sessionId": "abc123"
    ```

    #### Parâmetro Grok Pattern

    Você pode personalizar o comportamento de análise com as seguintes opções para se adequar aos seus formatos de log:

    * **delimitador**

      * **Descrição:** String separando cada par de valor principal.
      * **Valor padrão:** `,` (vírgula)
      * **Substituir:** Defina o campo `delimiter` para alterar esse comportamento.

    * **Separador de Valor-chave**

      * **Descrição:** String usada para atribuir valores às chaves.
      * **Valor padrão:** `=`
      * **Substituir:** Defina o campo `keyValueSeparator` para uso de separador personalizado.

    * **citaçãoChar**

      * **Descrição:** Caractere usado para delimitar valores com espaços ou caracteres especiais.
      * **Valor padrão:** `"` (aspas duplas)
      * **Substituir:** Defina um caractere personalizado usando `quoteChar`.

    * **soltarOriginal**

      * **Descrição:** Descarta a mensagem original do log após a análise. Útil para reduzir o armazenamento de logs.
      * **Valor padrão:** `true`
      * **Substituir:** defina `dropOriginal` como `false` para manter a mensagem original do log.

    * **sem prefixo**

      * **Descrição:** Quando `true`, exclui o nome do campo Grok como um prefixo no objeto resultante.
      * **Valor padrão:** `false`
      * **Substituição:** Habilite definindo `noPrefix` como `true`.

    * **escapeChar**

      * **Descrição:** Defina um caractere de escape personalizado para manipular caracteres de log especiais.
      * **Valor padrão:** &quot;&quot; (barra invertida)
      * **Substituir:** personalizar com `escapeChar`.

    * **valores de trim**

      * **Descrição:** Permite o corte de valores que contêm espaços em branco.
      * **Valor padrão:** `false`
      * **Substituir:** defina `trimValues` como `true` para ativar o corte.

    * **Teclas de ajuste**

      * **Descrição:** Permite o corte de teclas que contêm espaços em branco.
      * **Valor padrão:** `true`
      * **Substituir:** defina `trimKeys` como `true` para ativar o corte.
  </Collapser>

  <Collapser id="grok-patterns" title="Padrões Grok suportados">
    A New Relic suporta os seguintes padrões Grok:

    * IP
    * TIMESTAMP\_ISO8601
    * HTTPDATE
    * TIME
    * UUID
    * MONTH
    * SPACE
    * DATESTAMP
    * DATE
    * COMBINEDAPACHELOG
    * ISO8601\_TIMEZONE
    * MAC
    * DATE\_EU
    * TZ
    * DATE\_US
    * DAY
    * LOGLEVEL
    * NUMBER
    * INT
    * QUOTEDSTRING
    * SYSLOGTIMESTAMP
    * PATH
    * SYSLOGBASE
    * COMMONAPACHELOG
    * IPV6
    * COMMONMAC
    * DATESTAMP\_OTHER
    * ISO8601\_SECOND
    * DATESTAMP\_EVENTLOG
    * SYSLOGBASE2
    * HAPROXYHTTP
    * RUBY\_LOGGER
    * WINDOWSMAC
    * WORD
    * DATA
    * GREEDYDATA
    * NOTSPACE
    * BASE16FLOAT
    * QS
    * BASE10NUM
    * USER
    * IPORHOST
    * USERNAME
    * IPV4
    * MONTHDAY
    * YEAR
    * HOSTNAME
    * POSINT
    * URIPATHPARAM
    * URI
    * URIPATH
    * MONTHNUM
    * NONNEGINT
    * MINUTE
    * SECOND
    * HOUR
    * URIHOST
    * URIPROTO
    * URIPARAM
    * SYSLOGHOST
    * BASE16NUM
    * SYSLOGPROG
    * HOSPEDAR
    * HOSTPORT
    * JAVACLASS
    * PROG
    * UNIXPATH
    * WINPATH
    * MONTHNUM2
    * RUBY\_LOGLEVEL
    * SYSLOGFACILITY
    * CRON\_ACTION
    * HAPROXYCAPTUREDREQUESTHEADERS
    * HAPROXYCAPTUREDRESPONSEHEADERS
    * HAPROXYDATE
    * CISOMAC
  </Collapser>
</CollapserGroup>

## Gerenciar regras de análise [#manage-rules]

Após criar regras de parsing, você pode gerenciá-las a partir de <DNT>**Logs &gt; Parsing**</DNT>. As regras de rascunho estão salvas, mas ainda não ativadas. Você pode ativá-los quando estiver pronto para aplicá-los aos logs de entrada.

Para editar uma regra de parsing:

1. Na sua lista de regras de parsing, clique no nome da regra ou clique em <DNT>**... &gt; Edit**</DNT> e faça as alterações necessárias. Para alternar para o editor de código, clique em <DNT>**Write your own rule**</DNT> para escrever ou modificar padrões Grok/Regex diretamente.
2. Clique em <DNT>**Save rule**</DNT> (ou <DNT>**Save as draft**</DNT> se quiser mantê-lo desativado).

As alterações se aplicam aos logs ingeridos após a atualização. Para ativar, desativar ou excluir uma regra de parsing:

1. Encontre a regra na sua lista de regras de parsing e clique no menu <DNT>**...**</DNT>.

2. Escolha uma ação:

   * <DNT>**Enable:**</DNT> Ativa a regra de rascunho (aplica-se imediatamente aos logs recém-ingeridos)
   * <DNT>**Disable:**</DNT> Pausa temporariamente a regra ativa
   * <DNT>**Delete:**</DNT> Remove a regra completamente

## Limites

O parsing é computacionalmente intensivo. Para garantir a estabilidade da plataforma, a New Relic impõe o seguinte:

* **Limite por mensagem**: Uma regra tem 100ms para analisar uma única mensagem. Se exceder isso, a análise será interrompida para essa mensagem.
* **Limite por conta**: O tempo total de processamento é limitado por minuto. Se você atingir isso, os logs permanecerão não analisados (armazenados em seu formato original).
* **Tempo do pipeline**: A análise ocorre antes do enriquecimento. Você não pode corresponder uma regra de parsing a um atributo que ainda não foi adicionado (como uma tag adicionada posteriormente no pipeline).
* **A regra da primeira correspondência**: As regras de parsing não são ordenadas. Se várias regras corresponderem a um único log, a New Relic aplica uma aleatoriamente. Certifique-se de que suas cláusulas `WHERE` do NRQL sejam específicas o suficiente para evitar correspondências sobrepostas.

<Callout variant="tip">
  Para verificar facilmente se seus limites de taxa foram atingidos, acesse [a página do sistema <DNT>**Limits**</DNT> ](/docs/telemetry-data-platform/ingest-manage-data/manage-data/view-system-limits#limits-ui)na interface do New Relic.
</Callout>

## Resolução de problemas [#troubleshooting]

Se a análise não estiver funcionando da maneira desejada, pode ser devido a:

* <DNT>**Logic:**</DNT> A lógica de correspondência de regras de análise não corresponde ao log desejado.
* <DNT>**Timing:**</DNT> Se a sua regra de análise de correspondência destino for um valor que ainda não existe, ela falhará. Isto pode ocorrer se o valor for adicionado posteriormente no pipeline como parte do processo de enriquecimento.
* <DNT>**Limits:**</DNT> Há um período fixo de tempo disponível a cada minuto para processar o log por meio de análise, padrões, filtros de eliminação, etc. Se o tempo máximo tiver sido gasto, a análise será ignorada para registros adicionais de eventos de log.

Para resolver esses problemas, crie ou ajuste suas [regras de análise personalizadas](#custom-parsing).

## Documentação relacionada [#related-docs]

<DocTiles>
  <DocTile title="Regras de análise de log integradas" path="/docs/logs/ui-data/built-log-parsing-rules">
    Explore padrões pré-criados da New Relic.
  </DocTile>

  <DocTile title="Consultar dados log" path="/docs/logs/log-management/ui-data/query-logs">
    Use atributos analisados em consultas NRQL.
  </DocTile>
</DocTiles>