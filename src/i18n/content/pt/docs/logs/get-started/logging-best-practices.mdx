---
title: Guia de práticas recomendadas de registro em log
tags:
  - New Relic solutions
  - Best practices guides
  - Logs
  - Logging
metaDescription: Best practices for using New Relic logs
freshnessValidatedDate: never
translationType: machine
---

Bem-vindo ao guia de práticas recomendadas de registro em log da New Relic. Aqui você encontrará recomendações detalhadas sobre como otimizar nosso recurso de log e gerenciar o consumo de dados.

<Callout variant="tip">
  Tem muito registro? Confira nosso [tutorial sobre como otimizá-los e gerenciá-los](/docs/tutorial-large-logs/get-started-managing-large-logs/).
</Callout>

## Registro de encaminhamento [#forwarding-logs]

Aqui estão algumas dicas sobre encaminhamento de logs para complementar nossos [documentos de encaminhamento de logs](/docs/logs/forward-logs/enable-log-management-new-relic):

* Ao encaminhar o log, recomendamos usar nosso [agente New Relic Infrastructure ](/docs/logs/forward-logs/forward-your-logs-using-infrastructure-agent)e/ou [agente APM](/docs/apm/new-relic-apm/getting-started/get-started-logs-context#agents). Se você não puder usar o agente New Relic , use outro agente compatível (como FluentBit, Fluentd e Logstash).

  Aqui estão alguns exemplos de configuração do Github para agente de registro compatível:

  * [Exemplos de FluentBit](https://github.com/newrelic/fluentbit-examples)

  * [Exemplos Fluentd](https://github.com/newrelic/fluentd-examples/)

  * [Exemplos de logstash](https://github.com/newrelic/logstash-examples)

    <Callout variant="important">
      Se o seu log estiver armazenado em um diretório em um host/contêiner subjacente e for instrumentado por nosso agente de infraestrutura para coletar log, você poderá ver logs duplicados coletados pelo agente de infraestrutura e pelo agente <InlinePopover type="apm"/>. Para evitar o envio de logs duplicados, consulte nossas recomendações [neste guia](/docs/logs/logs-context/upgrade-to-automatic-logs-context).
    </Callout>

* Adicione um atributo `logtype` a todos os dados que você encaminha. O atributo é <DNT>**required**</DNT> para usar nossas regras de análise integradas e também pode ser usado para criar regras de análise personalizadas com base no tipo de dados. O atributo `logtype` é considerado um atributo bem conhecido e é usado em nosso painel Início Rápido para informações resumidas log .

* Use nossas [regras de análise integradas](/docs/logs/ui-data/built-log-parsing-rules) para tipos de log conhecidos. Analisaremos automaticamente o registro de vários tipos log conhecidos quando você definir o atributo `logtype` relevante.

  Aqui está um exemplo de como adicionar o atributo `logtype` a um log encaminhado pelo nosso agente de infraestrutura:

  ```yml
  logs:
    - name: mylog
      file: /var/log/mylog.log
      attributes:
        logtype: mylog
  ```

* Use a integração New Relic para encaminhar log para outros tipos de dados comuns, como:

  * Ambientes de contêiner: [Kubernetes (K8S)](/docs/kubernetes-pixie/kubernetes-integration/get-started/introduction-kubernetes-integration)
  * Integração de provedor de nuvem: [AWS](/docs/infrastructure/amazon-integrations/get-started/introduction-aws-integrations/), [Azure](/docs/infrastructure/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations) ou [GCP](/docs/infrastructure/google-cloud-platform-integrations/get-started/introduction-google-cloud-platform-integrations)
  * Qualquer uma de nossas outras [integrações suportadas no host com registro](/docs/infrastructure/host-integrations/get-started/introduction-host-integrations)

## Partições de dados [#partitions]

Se estiver a consumir mais de 1 TB por dia de dados de log , definitivamente deverá trabalhar num plano de governação de ingestão para registo, incluindo um plano para particionar os dados de uma forma que forneça agrupamentos funcionais e temáticos. Sugerimos práticas recomendadas de no máximo 1 TB por partição. Isso serve tanto para desempenho quanto para usabilidade. Se você enviou todo o seu log para um "balde" gigante (a partição log padrão) em uma única conta, poderá experimentar uma consulta lenta ou falha na consulta. Para obter mais detalhes, consulte [Limites de taxa de consulta NRQL](/docs/query-your-data/nrql-new-relic-query-language/get-started/rate-limits-nrql-queries/#query-limits).

Uma maneira de melhorar o desempenho da consulta é limitar o intervalo de tempo pesquisado. A pesquisa de log por longos períodos retornará mais resultados e exigirá mais tempo. Evite pesquisas com duração superior a uma semana sempre que possível e use o seletor de intervalo de tempo para restringir as pesquisas a janelas de tempo menores e mais específicas.

Outra forma de melhorar o desempenho da pesquisa é usar [partições de dados](/docs/logs/ui-data/data-partitions/). Aqui estão algumas práticas recomendadas para partições de dados:

* Certifique-se de usar partições no início do processo de integração de log. Crie uma estratégia de utilização de partições para que seu usuário saiba onde pesquisar e encontrar log específico. Dessa forma, seus alertas, dashboard e visualizações salvas não precisam ser modificados se você implementar partições posteriormente em sua jornada de log.

* Para evitar limites e melhorar o tempo de consulta, crie uma [regra de partição de dados](/docs/logs/ui-data/data-partitions/) para tipos de dados específicos quando a ingestão diária total exceder 1 TB por dia. Certos logs de infraestrutura de alto volume (ou seja, log K8S, log CDN, log VPC, log Syslog ou log Web) podem ser bons candidatos para suas próprias partições.

* Mesmo que o volume de ingestão seja baixo, você também poderá usar partições de dados para uma separação lógica de dados ou apenas para melhorar o desempenho da consulta em tipos de dados separados.

* Use um [namespace de partição de dados secundário](/docs/logs/ui-data/data-partitions#namespace) para dados para os quais você deseja um tempo de retenção mais curto. As partições de dados secundárias têm um período de retenção padrão de 30 dias.

* Para [pesquisar partições de dados](/docs/logs/ui-data/data-partitions#search) na interface <DNT>**Logs**</DNT> , você deve selecionar as partições apropriadas, abrir o seletor de partição e marcar as partições que deseja pesquisar. Se você estiver usando NRQL, use a cláusula `FROM` para especificar `Log` ou `Log_<partion>` a ser pesquisado. Por exemplo:

  ```sql
  FROM Log_<my_partition_name> SELECT * SINCE 1 hour ago
  ```

  Ou para pesquisar log em múltiplas partições:

  ```sql
  FROM Log, Log_<my_partition_name> SELECT * SINCE 1 hour ago
  ```

## Registro de análise [#parsing-logs]

Analisar seu log na ingestão é a melhor maneira de tornar seus dados log mais utilizáveis por você e por outros usuários em sua organização. Ao analisar um atributo, você pode usá-los facilmente para pesquisar na interface <DNT>**Logs**</DNT> e no NRQL sem precisar analisar os dados no momento da consulta. Isso também permite que você os use facilmente no <InlinePopover type="alerts"/>e no painel.

Para analisar o log, recomendamos que você:

* Analise seu log na ingestão para criar `attributes` (ou campos), que você pode usar ao pesquisar ou criar <InlinePopover type="dashboards"/>e alertas. atributo pode ser sequências de dados ou valores numéricos.

* Use o atributo `logtype` que você adicionou ao seu log na ingestão, junto com outras cláusulas NRQL `WHERE` para corresponder aos dados que você deseja analisar. Escreva regras de correspondência específicas para filtrar o log com a maior precisão possível. Por exemplo:

  ```sql
  WHERE logtype='mylog' AND message LIKE '%error%'
  ```

* Use nossas [regras de análise integradas](/docs/logs/ui-data/built-log-parsing-rules/) e o atributo `logtype` associado sempre que possível. Se as regras integradas não funcionarem para seus dados, use um nome de atributo `logtype` diferente (ou seja, `apache_logs` vs `apache`, `iis_w3c_custom` vs `iis_w3c`) e, em seguida, crie uma nova regra de análise em a interface usando uma versão modificada das regras integradas para que funcione no seu formato de dados log .

* Use nossa interface <DNT>**Parsing**</DNT> para testar e validar suas regras Grok. Usando a opção `Paste log` , você pode colar uma de suas mensagens do log para testar sua expressão Grok antes de criar e salvar uma regra de análise permanente.

  <img
    title="parsing example"
    alt="Parsing example in the UI"
    src="/images/logs_screenshot-full_parsing-example.webp"
  />

* Use a configuração externa do FluentBit para analisar o log multilinha e para outras pré-análises mais extensas antes de ingerir no New Relic. Para obter detalhes e configuração da análise multilinha com nosso agente de infraestrutura, consulte [esta postagem do blog](https://newrelic.com/blog/how-to-relic/parse-multiline-log-messages-fluent-bit-plugin).

* Crie padrões Grok otimizados para corresponder ao log filtrado para extrair o atributo. Evite usar excessivamente padrões Grok caros, como GREEDYDATA. Se precisar de ajuda para identificar quaisquer regras de análise abaixo do ideal, fale com seu representante de conta New Relic .

### GROK práticas recomendadas

* Use tipos Grok para especificar o tipo de valor de atributo a ser extraído. Se omitido, os valores serão extraídos como strings. Isso é importante especialmente para valores numéricos se você quiser usar funções NRQL (ou seja, `monthOf()`, `max()`, `avg()`, `>`, `<`, etc.) nesses atributos.

* Use a interface

  <DNT>
    **Parsing**
  </DNT>

  para testar seus padrões Grok. Você pode colar o log de amostra na interface

  <DNT>
    **Parsing**
  </DNT>

  para validar seus padrões Grok ou Regex e se eles estão extraindo o atributo conforme o esperado.

* Adicione âncoras à lógica de análise (ou seja, `^`) para indicar o início de uma linha ou `$` no final de uma linha.

* Use `()?` em torno de um padrão para identificar campos opcionais

* Evite o uso excessivo de padrões Grok caros como `'%{GREEDYDATA}`. Tente sempre usar um padrão Grok e um tipo Grok válidos ao extrair o atributo.

## Eliminar regras de filtro [#drop-rules]

### Descartar registro na ingestão

* Crie [regras de filtro](/docs/logs/ui-data/drop-data-drop-filter-rules#create) para descartar logs que não são úteis ou que não são necessários para satisfazer quaisquer casos de uso de dashboard, alertas ou resolução de problemas

### Elimine o atributo do seu log na ingestão

* Crie regras de descarte para descartar atributos não utilizados do seu log.
* Elimine o atributo `message` após a análise. Se você analisar o atributo da mensagem para criar um novo atributo a partir dos dados, elimine o campo da mensagem.
* Exemplo: se você estiver encaminhando dados da infraestrutura AWS , poderá criar regras de descarte para descartar qualquer atributo AWS que possa criar um inchaço indesejado de dados.

## Dimensionamento New Relic Logs [#sizing]

* A forma como cobramos pelo armazenamento pode ser diferente de alguns de nossos concorrentes. A forma como medimos os dados log é semelhante à forma como medimos e cobramos outros tipos de dados, que é definido em [Cálculo de uso](/docs/accounts/accounts-billing/new-relic-one-pricing-billing/data-ingest-billing#usage-calculation).

* Se nossa integração na nuvem (AWS, Azure, GCP) estiver instalada, adicionaremos metadados de nuvem a cada registro de log , o que será adicionado à conta geral de ingestão. Esses dados podem ser descartados para reduzir a ingestão.

* Os principais impulsionadores da sobrecarga de dados de log estão abaixo, em ordem de impacto:

  * Integração na nuvem

  * Formatação JSON

  * Padrões de log (Você pode [ativar/desativar padrões na interface](/docs/logs/ui-data/find-unusual-logs-log-patterns#availability)

    <DNT>
      [**Logs**](/docs/logs/ui-data/find-unusual-logs-log-patterns#availability)
    </DNT>

    [](/docs/logs/ui-data/find-unusual-logs-log-patterns#availability).)

## Pesquisando registro [#searching-logs]

* Para pesquisas log comuns, crie e use <DNT>**Saved views**</DNT> na interface. Crie uma pesquisa para seus dados e clique em <DNT>**+ Add Column**</DNT> para adicionar um atributo adicional à tabela de interface. Você pode então mover as colunas para que apareçam na ordem desejada e salvá-las como uma visualização salva com permissões privadas ou públicas. Configure as visualizações salvas para serem públicas para que você e outros usuários possam facilmente executar pesquisas comuns com todos os dados de atributo relevantes exibidos. Esta é uma boa prática para aplicativos de terceiros, como Apache, nginx, etc., para que o usuário possa ver facilmente esses logs sem pesquisar.
* Utilize o [criador de consulta](/docs/query-your-data/explore-query-data/query-builder/introduction-query-builder) para realizar pesquisas utilizando NRQL. O criador de consulta permite utilizar todas as funções avançadas disponíveis no NRQL.
* Crie <InlinePopover type="dashboards"/>ou use [o painel Início Rápido](https://newrelic.com/instant-observability) disponível para responder perguntas comuns sobre seu registro e analisar seus dados log ao longo do tempo em gráficos de série temporal. Crie um painel com vários painéis para dividir seus dados log de muitas maneiras diferentes.
* Use nossas funções NRQL avançadas como [capture()](/docs/query-your-data/nrql-new-relic-query-language/get-started/nrql-syntax-clauses-functions#func-capture) ou [aparse()](/docs/query-your-data/nrql-new-relic-query-language/get-started/nrql-syntax-clauses-functions#func-aparse) para analisar dados no momento da pesquisa.
* Instale o painel <DNT>**Logs analysis**</DNT> e/ou <DNT>**APM logs monitoring quickstart**</DNT> para obter rapidamente mais insights sobre seus dados log . Para adicionar esses painéis, vá para <DNT>**[one.newrelic.com](https://one.newrelic.com) > Integrations & Agents > Logging > Dashboards**</DNT>.

## Qual é o próximo?

Consulte [Introdução ao <InlinePopover type="logs"/>](/docs/logs/get-started/get-started-log-management/).
