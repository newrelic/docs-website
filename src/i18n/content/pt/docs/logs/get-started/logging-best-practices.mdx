---
title: Guia de práticas recomendadas de registro em log
tags:
  - New Relic solutions
  - Best practices guides
  - Logs
  - Logging
metaDescription: Best practices for using New Relic logs
freshnessValidatedDate: never
translationType: machine
---

Bem-vindo ao guia de práticas recomendadas de registro em log da New Relic. Aqui você encontrará recomendações detalhadas sobre como otimizar nosso recurso de log e gerenciar o consumo de dados.

<Callout variant="tip">
  Tem muito registro? Confira nosso [tutorial sobre como otimizá-los e gerenciá-los](/docs/tutorial-large-logs/get-started-managing-large-logs/).
</Callout>

## Registro de encaminhamento [#forwarding-logs]

Aqui estão algumas dicas sobre encaminhamento de logs para complementar nossos [documentos de encaminhamento de logs](/docs/logs/forward-logs/enable-log-management-new-relic):

* Ao encaminhar o log, recomendamos usar nosso [agente New Relic Infrastructure ](/docs/logs/forward-logs/forward-your-logs-using-infrastructure-agent)e/ou [agente APM](/docs/apm/new-relic-apm/getting-started/get-started-logs-context#agents). Se você não puder usar o agente New Relic , use outro agente compatível (como FluentBit, Fluentd e Logstash).

  Aqui estão alguns exemplos de configuração do Github para agente de registro compatível:

  * [Exemplos de FluentBit](https://github.com/newrelic/fluentbit-examples)

  * [Exemplos Fluentd](https://github.com/newrelic/fluentd-examples/)

  * [Exemplos de logstash](https://github.com/newrelic/logstash-examples)

    <Callout variant="important">
      Se o seu log estiver armazenado em um diretório em um host/contêiner subjacente e for instrumentado por nosso agente de infraestrutura para coletar log, você poderá ver logs duplicados coletados pelo agente de infraestrutura e pelo agente <InlinePopover type="apm" />. Para evitar o envio de logs duplicados, consulte nossas recomendações [neste guia](/docs/logs/logs-context/upgrade-to-automatic-logs-context).
    </Callout>

* Adicione um atributo `logtype` a todos os dados que você encaminha. O atributo é <DNT>**required**</DNT> para usar nossas regras de análise integradas e também pode ser usado para criar regras de análise personalizadas com base no tipo de dados. O atributo `logtype` é considerado um atributo bem conhecido e é usado em nosso painel Início Rápido para informações resumidas log .

* Use nossas [regras de análise integradas](/docs/logs/ui-data/built-log-parsing-rules) para tipos de log conhecidos. Analisaremos automaticamente o registro de vários tipos log conhecidos quando você definir o atributo `logtype` relevante.

  Aqui está um exemplo de como adicionar o atributo `logtype` a um log encaminhado pelo nosso agente de infraestrutura:

  ```yml
  logs:
    - name: mylog
      file: /var/log/mylog.log
      attributes:
        logtype: mylog
  ```

* Use a integração New Relic para encaminhar log para outros tipos de dados comuns, como:

  * Ambientes de contêiner: [Kubernetes (K8S)](/docs/kubernetes-pixie/kubernetes-integration/get-started/introduction-kubernetes-integration)
  * Integração de provedor de nuvem: [AWS](/docs/infrastructure/amazon-integrations/get-started/introduction-aws-integrations/), [Azure](/docs/infrastructure/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations) ou [GCP](/docs/infrastructure/google-cloud-platform-integrations/get-started/introduction-google-cloud-platform-integrations)
  * Qualquer uma de nossas outras [integrações suportadas no host com registro](/docs/infrastructure/host-integrations/get-started/introduction-host-integrations)

## Partições de dados [#partitions]

Se você consome ou planeja consumir uma quantidade significativa de dados log todos os dias, você definitivamente deve trabalhar em um plano de governança de ingestão para log, incluindo um plano para particionar os dados de uma forma que forneça agrupamentos funcionais e temáticos. Você pode obter melhorias significativas de desempenho com o uso adequado de partições de dados. Se você enviar todos os seus logs para um &quot;bucket&quot; gigante (a partição log padrão) em uma única conta, poderá ter consultas lentas ou falhas, pois terá que escanear todos os logs na sua conta para retornar resultados para cada consulta. Para mais detalhes, consulte [Limites de taxa de consulta NRQL](/docs/query-your-data/nrql-new-relic-query-language/get-started/rate-limits-nrql-queries/#query-limits).

Uma maneira de melhorar o desempenho da consulta é limitar o intervalo de tempo pesquisado. Pesquisar por log por longos períodos retornará mais resultados e exigirá mais tempo. Evite pesquisas em intervalos de tempo longos sempre que possível e use o seletor de intervalo de tempo para restringir as pesquisas a intervalos de tempo menores e mais específicos.

Outra forma de melhorar o desempenho da pesquisa é usar [partições de dados](/docs/logs/ui-data/data-partitions/). Aqui estão algumas práticas recomendadas para partições de dados:

* Certifique-se de usar partições no início do processo de integração de log. Crie uma estratégia de utilização de partições para que seu usuário saiba onde pesquisar e encontrar log específico. Dessa forma, seus alertas, dashboard e visualizações salvas não precisam ser modificados se você implementar partições posteriormente em sua jornada de log.

* Crie partições de dados que se alinhem às categorias em seu ambiente ou organização que são estáticas ou mudam com pouca frequência (por exemplo, por unidade de negócios, equipe, ambiente, serviço, etc.).

* Crie partições para otimizar o número de eventos que devem ser escaneados para sua consulta mais comum. Se você tiver um alto volume de ingestão, terá mais eventos em janelas de tempo mais curtas, o que fará com que pesquisas em períodos mais longos demorem mais e possivelmente expirem. Não há uma regra rígida, mas geralmente o evento de log &quot;conforme escaneado&quot; ultrapassa 500 milhões (especialmente mais de 1 bilhão). Para consultas comuns, você pode considerar ajustar seu particionamento.

* Mesmo que o volume de ingestão seja baixo, você também poderá usar partições de dados para uma separação lógica de dados ou apenas para melhorar o desempenho da consulta em tipos de dados separados.

* Para [pesquisar partições de dados](/docs/logs/ui-data/data-partitions#search) na interface <DNT>**Logs**</DNT> , você deve selecionar as partições apropriadas, abrir o seletor de partição e marcar as partições que deseja pesquisar. Se você estiver usando NRQL, use a cláusula `FROM` para especificar `Log` ou `Log_<partion>` a ser pesquisado. Por exemplo:

  ```sql
  FROM Log_<my_partition_name> SELECT * SINCE 1 hour ago
  ```

  Ou para pesquisar log em múltiplas partições:

  ```sql
  FROM Log, Log_<my_partition_name> SELECT * SINCE 1 hour ago
  ```

## Registro de análise [#parsing-logs]

Analisar seu log na ingestão é a melhor maneira de tornar seus dados log mais utilizáveis por você e por outros usuários em sua organização. Ao analisar um atributo, você pode usá-los facilmente para pesquisar na interface <DNT>**Logs**</DNT> e no NRQL sem precisar analisar os dados no momento da consulta. Isso também permite que você os use facilmente no <InlinePopover type="alerts" />e no painel.

Para analisar o log, recomendamos que você:

* Analise seu log na ingestão para criar `attributes` (ou campos), que você pode usar ao pesquisar ou criar <InlinePopover type="dashboards" />e alertas. atributo pode ser sequências de dados ou valores numéricos.

* Use o atributo `logtype` que você adicionou ao seu log na ingestão, junto com outras cláusulas NRQL `WHERE` para corresponder aos dados que você deseja analisar. Escreva regras de correspondência específicas para filtrar o log com a maior precisão possível. Por exemplo:

  ```sql
  WHERE logtype='mylog' AND message LIKE '%error%'
  ```

* Use nossas [regras de análise integradas](/docs/logs/ui-data/built-log-parsing-rules/) e o atributo `logtype` associado sempre que possível. Se as regras integradas não funcionarem para seus dados, use um nome de atributo `logtype` diferente (ou seja, `apache_logs` vs `apache`, `iis_w3c_custom` vs `iis_w3c`) e, em seguida, crie uma nova regra de análise em a interface usando uma versão modificada das regras integradas para que funcione no seu formato de dados log .

* Use nossa interface <DNT>**Parsing**</DNT> para testar e validar suas regras Grok. Usando a opção `Paste log` , você pode colar uma de suas mensagens do log para testar sua expressão Grok antes de criar e salvar uma regra de análise permanente.

  <img title="parsing example" alt="Parsing example in the UI" src="/images/logs_screenshot-full_parsing-example.webp" />

* Use a configuração externa do FluentBit para analisar o log multilinha e para outras pré-análises mais extensas antes de ingerir no New Relic. Para obter detalhes e configuração da análise multilinha com nosso agente de infraestrutura, consulte [esta postagem do blog](https://newrelic.com/blog/how-to-relic/parse-multiline-log-messages-fluent-bit-plugin).

* Crie padrões Grok otimizados para corresponder ao log filtrado para extrair o atributo. Evite usar excessivamente padrões Grok caros, como GREEDYDATA. Se precisar de ajuda para identificar quaisquer regras de análise abaixo do ideal, fale com seu representante de conta New Relic .

### GROK práticas recomendadas

* Use tipos Grok para especificar o tipo de valor de atributo a ser extraído. Se omitido, os valores serão extraídos como strings. Isso é importante especialmente para valores numéricos se você quiser usar funções NRQL (ou seja, `monthOf()`, `max()`, `avg()`, `>`, `<`, etc.) nesses atributos.
* Use a interface <DNT>**Parsing**</DNT> para testar seus padrões Grok. Você pode colar o log de amostra na interface <DNT>**Parsing**</DNT> para validar seus padrões Grok ou Regex e se eles estão extraindo o atributo conforme o esperado.
* Adicione âncoras à lógica de análise (ou seja, `^`) para indicar o início de uma linha ou `$` no final de uma linha.
* Use `()?` em torno de um padrão para identificar campos opcionais
* Evite o uso excessivo de padrões Grok caros como `'%{GREEDYDATA}`. Tente sempre usar um padrão Grok e um tipo Grok válidos ao extrair o atributo.

## Eliminar regras de filtro [#drop-rules]

### Descartar registro na ingestão

* Crie [regras de filtro](/docs/logs/ui-data/drop-data-drop-filter-rules#create) para descartar logs que não são úteis ou que não são necessários para satisfazer quaisquer casos de uso de dashboard, alertas ou resolução de problemas

### Elimine o atributo do seu log na ingestão

* Crie regras de descarte para descartar atributos não utilizados do seu log.
* Elimine o atributo `message` após a análise. Se você analisar o atributo da mensagem para criar um novo atributo a partir dos dados, elimine o campo da mensagem.
* Exemplo: se você estiver encaminhando dados da infraestrutura AWS , poderá criar regras de descarte para descartar qualquer atributo AWS que possa criar um inchaço indesejado de dados.

## Dimensionamento New Relic Logs [#sizing]

* A forma como cobramos pelo armazenamento pode ser diferente de alguns de nossos concorrentes. A forma como medimos os dados log é semelhante à forma como medimos e cobramos outros tipos de dados, que é definido em [Cálculo de uso](/docs/accounts/accounts-billing/new-relic-one-pricing-billing/data-ingest-billing#usage-calculation).

* Se nossa integração na nuvem (AWS, Azure, GCP) estiver instalada, adicionaremos metadados de nuvem a cada registro de log , o que será adicionado à conta geral de ingestão. Esses dados podem ser descartados para reduzir a ingestão.

* Os principais impulsionadores da sobrecarga de dados de log estão abaixo, em ordem de impacto:

  * Integração na nuvem
  * Formatação JSON
  * Padrões de log (Você pode [ativar/desativar padrões na interface <DNT>**Logs**</DNT> ](/docs/logs/ui-data/find-unusual-logs-log-patterns#availability).)

## Pesquisando registro [#searching-logs]

* Para pesquisas log comuns, crie e use <DNT>**Saved views**</DNT> na interface. Crie uma pesquisa para seus dados e clique em <DNT>**+ Add Column**</DNT> para adicionar um atributo adicional à tabela de interface. Você pode então mover as colunas para que apareçam na ordem desejada e salvá-las como uma visualização salva com permissões privadas ou públicas. Configure as visualizações salvas para serem públicas para que você e outros usuários possam facilmente executar pesquisas comuns com todos os dados de atributo relevantes exibidos. Esta é uma boa prática para aplicativos de terceiros, como Apache, nginx, etc., para que o usuário possa ver facilmente esses logs sem pesquisar.

* Use o [criador de consulta](/docs/query-your-data/explore-query-data/query-builder/introduction-query-builder) para executar pesquisas usando NRQL, utilizando suas funções avançadas. Para consultar logs de várias contas e identificá-los com seus IDs de conta correspondentes, inclua `accountId() as accountId` na declaração `SELECT` da sua consulta NRQL .

* Crie <InlinePopover type="dashboards" />ou use [o painel Início Rápido](https://newrelic.com/instant-observability) disponível para responder perguntas comuns sobre seu registro e analisar seus dados log ao longo do tempo em gráficos de série temporal. Crie um painel com vários painéis para dividir seus dados log de muitas maneiras diferentes.

* Use nossas funções NRQL avançadas como [capture()](/docs/query-your-data/nrql-new-relic-query-language/get-started/nrql-syntax-clauses-functions#func-capture) ou [aparse()](/docs/query-your-data/nrql-new-relic-query-language/get-started/nrql-syntax-clauses-functions#func-aparse) para analisar dados no momento da pesquisa.

* Instale o painel <DNT>**Logs analysis**</DNT> e/ou <DNT>**APM logs monitoring quickstart**</DNT> para obter rapidamente mais insights sobre seus dados log . Para adicionar esses painéis, vá para <DNT>**[one.newrelic.com](https://one.newrelic.com) &gt; Integrations &amp; Agents &gt; Logging &gt; Dashboards**</DNT>.

## Qual é o próximo?

Consulte [Introdução ao <InlinePopover type="logs" />](/docs/logs/get-started/get-started-log-management/).