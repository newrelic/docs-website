---
title: 'Agente Java: instrumento Kafka fila de mensagens'
tags:
  - Agents
  - Java agent
  - Instrumentation
metaDescription: 'New Relic for Java includes built-in Kafka monitoring, as well as advanced event and distributed tracing data collection.'
freshnessValidatedDate: never
translationType: machine
---

O agente Java da New Relic coleta automaticamente dados da biblioteca de clientes Java do [Kafka](https://kafka.apache.org/documentation/). Como o Kafka é um sistema de mensagens de alto desempenho que gera muitos dados, você pode personalizar o agente para as taxas de transferência e casos de uso específicos do seu aplicativo.

Este documento explica como coletar e visualizar três tipos de dados Kafka:

* [Métrica Kafka](#view-kafka-metrics)
* [Evento Kafka](#collect-kafka-events)
* [Habilitar transação do Kafka Streams](#collect-kafka-streams-transactions)
* [Rastreamento distribuído Kafka](#collect-kafka-distributed-traces)

<Callout variant="tip">
  Também temos uma integração Kafka. Para obter detalhes sobre isso, consulte [integração de monitoramento Kafka](/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration).
</Callout>

## Requisitos [#requirements]

A instrumentação de clientes Kafka está disponível nas versões 4.12.0 ou superior do agente Java. A instrumentação de fluxos Kafka está disponível no agente Java versões 8.1.0 ou mais alto. Para ver todas as bibliotecas Kafka suportadas, verifique a [página de compatibilidade e requisitos do Java](/docs/agents/java-agent/getting-started/compatibility-requirements-java-agent). Observe que o Kafka Streams é executado em clientes Kafka, portanto, toda a instrumentação que se aplica aos clientes Kafka também se aplica ao Streams.

## Ver métrica Kafka

Após [a instalação](/docs/agents/java-agent/installation/install-java-agent), o agente reporta automaticamente métricas Kafka ricas com informações sobre taxas de mensagens, latência, atraso e muito mais. O agente Java coleta todas [as métricas de consumidor e produtor Kafka](https://kafka.apache.org/documentation/#monitoring) (mas não conecta ou transmite métricas).

Para visualizar essas métricas, crie um dashboard personalizado:

1. Vá para o [explorador métrico New Relic](/docs/insights/use-insights-ui/explore-data/metric-explorer-search-chart-metrics-sent-new-relic-agents).

2. Use o explorador métrico para localizar sua métrica. Aqui estão algumas pastas onde você pode encontrar métricas:

   * Métrica Kafka:

     ```
     MessageBroker/Kafka/Internal/KafkaMetricName
     ```

     Por exemplo, métrica `request-rate` :

     ```
     MessageBroker/Kafka/Internal/consumer-metrics/request-rate
     ```

   * Fluxos Kafka:

     ```
     Kafka/Streams/KafkaStreamsMetricName
     ```

     Por exemplo, métrica `poll-latency-avg` :

     ```
     Kafka/Streams/stream-thread-metrics/poll-latency-avg
     ```

   * Conexão Kafka:

     ```
     Kafka/Connect/KafkaConnectMetricName
     ```

     Por exemplo, métrica `connector-count` :

     ```
     Kafka/Connect/connect-worker-metrics/connector-count
     ```

3. Adicione a métrica que deseja monitor a um dashboard clicando em <DNT>**Add to dashboard**</DNT>.

<Callout variant="tip">
  Para obter uma lista completa das métricas de consumo, produtor e fluxos do Kafka, consulte a [documentação do Kafka](https://kafka.apache.org/documentation/#remote_jmx). As métricas nesses documentos podem ser pesquisadas via JMX. Lembre-se de que nem todas as métricas mencionadas na documentação serão exportadas para o New Relic. Isso pode ser devido a um destes motivos:

  * Na verdade, a métrica não é gerada por clientes Kafka ou Kafka Streams. Isso pode ser devido ao uso de uma versão mais antiga de clientes ou Streams ou com base em como você configurou e usa sua biblioteca Kafka.
  * A métrica não é numérica ou seu valor é `NaN`. New Relic só aceita métricas com valor numérico.
</Callout>

## Habilitar coleção de eventos Kafka [#collect-kafka-events]

Você pode configurar o agente para coletar dados de evento em vez de dados de métrica de fração de tempo (para saber a diferença entre métrica de fração de tempo e dados de evento, consulte [coleta de dados](/docs/using-new-relic/metrics/analyze-your-metrics/data-collection-metric-timeslice-event-data#overview)). Isso permite que você use [NRQL](/docs/insights/nrql-new-relic-query-language/using-nrql/introduction-nrql) para filtrar e facetar a métrica Kafka padrão. Quando ativado, o agente coleta um evento Kafka a cada 30 segundos. Este evento contém todos os dados do [consumidor Kafka e métricas de produção](https://kafka.apache.org/documentation/#monitoring) capturadas desde o evento anterior.

Se você estiver usando Kafka Streams, o agente gera um evento separado que contém todos os dados [da métrica do fluxo Kafka](https://kafka.apache.org/documentation/#remote_jmx) capturados desde o evento anterior. O evento também é coletado a cada 30 segundos.

<Callout variant="important">
  O agente registra até 2.000 eventos por [ciclo de colheita](/docs/using-new-relic/welcome-new-relic/getting-started/glossary#harvest-cycle), embora você possa alterar esse valor com [`max_samples_stored`](/docs/agents/java-agent/configuration/java-agent-configuration-config-file#ae-max_samples_stored). Os dados do evento Kafka estão incluídos neste pool. Se você usar a chamada de API `recordCustomEvent()` para enviar [evento personalizado](/docs/insights/insights-data-sources/custom-data/insert-custom-events-new-relic-apm-agents) para New Relic e enviar mais de 2.000 eventos, o agente descartará algum Kafka ou evento personalizado.
</Callout>

Para ativar a coleta de eventos Kafka:

1. Adicione o elemento `kafka.metrics.as_events.enabled` ao arquivo de configuração `newrelic.yml` :

   ```yml
   kafka.metrics.as_events.enabled: true
   ```

2. Reinicie sua JVM.

3. Use o [explorador de eventos](/docs/insights/use-insights-ui/explore-data/event-explorer-query-chart-your-event-data) para visualizar seus eventos Kafka, localizados no tipo de evento `KafkaMetrics` . Ou use [NRQL](/docs/insights/nrql-new-relic-query-language/using-nrql/introduction-nrql) para consultar seu evento diretamente. Por exemplo:

   ```sql
   SELECT average('producer-metrics.record-send-rate') from KafkaMetrics SINCE 30 minutes ago timeseries
   ```

   Se você estiver consultando a métrica do Kafka Streams, use o tipo de evento `KafkaStreamsMetrics` para acessar a métrica específica do stream.

<Callout variant="important">
  Lembre-se de que as limitações sobre o tipo de métrica Kafka que você pode enviar para o New Relic como métrica de intervalo de tempo também se aplicam a evento. Ou seja, métricas não numéricas e NaN não são incluídas como atributo do evento.
</Callout>

## Habilitar métrica do nó Kafka [#kafka-node-metrics]

Existe um módulo de instrumentação alternativo para clientes Kafka que fornecerá mais granularidade para a métrica Kafka. Este módulo de instrumentação está disponível desde o agente 8.6.0 e está desabilitado por padrão.

Para ativar este módulo de instrumentação, você deve desativar o módulo de instrumentação existente e ativar o novo adicionando o seguinte ao seu arquivo de configuração `newrelic.yml` :

```yml
class_transformer:
  kafka-clients-metrics:
    enabled: false
  kafka-clients-node-metrics:
    enabled: true
```

## Habilitar evento de configuração do Kafka [#kafka-config]

O módulo de instrumentação `kafka-clients-config` enviará periodicamente eventos com o conteúdo da configuração do cliente Kafka. Este módulo está disponível desde o agente 8.6.0 e está desabilitado por padrão.

Para ativar `kafka-clients-config` adicione o seguinte ao arquivo de configuração `newrelic.yml` :

```yml
class_transformer:
  kafka-clients-config:
    enabled: true
```

## Habilitar transação do Kafka Streams [#collect-kafka-streams-transactions]

Se você estiver usando Kafka Streams, por padrão não habilitamos a transação. Isso evita despesas desnecessárias porque os aplicativos Kafka tendem a ter altas taxas de transferência.

Ao contrário do JMS transação, os Kafka Streams transação não são processados por registro. Em vez disso, uma transação começa quando um consumidor Kafka pesquisa os registros e então os dados resultantes são processados.

Se você deseja criar transações, você precisa ativar um módulo `kafka-streams-spans` :

```yml
class_transformer:
  kafka-streams-spans:
    enabled: true
```

## Habilitar transação Kafka Connect [#collect-kafka-connect-transactions]

Se você estiver usando o Kafka Connect, por padrão não habilitamos a transação. Isso evita despesas desnecessárias porque os aplicativos Kafka tendem a ter altas taxas de transferência.

As transações do Kafka Connect são registradas para cada iteração da tarefa sink/source. Para uma tarefa de coletor, uma transação consiste em sondar um consumidor Kafka, converter cada mensagem e enviar dados ao destino. Para uma tarefa de origem, uma transação consiste na leitura do destino, convertendo os dados em mensagens e enviando cada mensagem a um produtor Kafka.

Se você deseja coletar essas transações, você precisa ativar um módulo `kafka-connect-spans` :

```yml
class_transformer:
  kafka-connect-spans:
    enabled: true
```

## Habilitar rastreamento distribuído Kafka [#collect-kafka-distributed-traces]

O agente Java também pode coletar [rastreamento distribuído](/docs/apm/distributed-tracing/getting-started/introduction-distributed-tracing) de clientes Kafka. Como o Kafka Streams é executado em clientes Kafka, as etapas para gerenciar distributed tracing também se aplicam. A ativação do rastreamento não afeta as operações normais do agente; ele ainda reportará dados métricos ou de eventos do Kafka.

Impactos e requisitos a considerar antes de permitir:

* <DNT>
    **The instrumentation adds a 150 to 200 byte payload to message headers.**
  </DNT>

  Se suas mensagens Kafka forem muito pequenas, o rastreamento poderá adicionar sobrecarga significativa de processamento e armazenamento. Esse tamanho de carga útil adicional pode fazer com que o Kafka descarte mensagens se elas excederem o limite de tamanho de mensagens do Kafka. Por esse motivo, recomendamos testar o rastreamento distribuído do Kafka em um ambiente de desenvolvimento antes de habilitá-lo na produção.

* Distributed tracing está disponível apenas para o cliente Kafka versões 0.11.0.0 ou superior.

* Se você ainda **não** habilitou distributed tracing para seu aplicativo, leia o [guia de transição](/docs/apm/distributed-tracing/getting-started/transition-guide-distributed-tracing) antes de habilitá-lo.

* Para propagar W3C Trace Context por meio de cabeçalhos de mensagens Kafka, consulte o [distributed tracing API guia de uso](/docs/agents/java-agent/api-guides/guide-using-java-agent-api#trace-calls) para obter detalhes sobre APIs que foram lançadas no agente Java 6.4.0. Observe que adicionar cabeçalhos adicionais às mensagens Kafka aumentará ainda mais o tamanho da carga útil. Para ver essas API em ação,[consulte Usando do agente Java trace API com Kafka](https://github.com/newrelic/newrelic-java-examples/tree/main/newrelic-java-agent/distributed-tracing/kafka-examples).

* Se estiver usando o Kafka Streams, você precisará habilitar um módulo de instrumentação de span (consulte a [seção de transação do Kafka Streams](#collect-kafka-streams-transactions)). Como uma transação não é registrada por registro, a aceitação de cabeçalhos distributed trace funcionará apenas para um registro.

O processo completo para habilitar isso é descrito abaixo, mas em alto nível envolve estas etapas básicas: 1) habilitar o rastreamento por meio da configuração do agente e 2) chamar a [API do agente Java](/docs/agents/java-agent/api-guides/guide-using-java-agent-api) para o instrumento de transação tanto do lado do produtor quanto do consumidor.

Para coletar rastreamento distribuído do Kafka:

<CollapserGroup>
  <Collapser
    id="enable-dt-kafka"
    title="1. Habilite distributed tracing no arquivo de configuração"
  >
    Se você ainda não habilitou distributed tracing para seu aplicativo, leia o [guia de transição distributed tracing ](/docs/apm/distributed-tracing/getting-started/transition-guide-distributed-tracing)antes de habilitá-lo.

    Para ativar o rastreamento distribuído do Kafka, essas duas configurações devem ser ativadas no [arquivo de configuração`newrelic.yml` ](/docs/agents/java-agent/configuration/java-agent-configuration-config-file#Structure):

    * Certifique-se de que o elemento [`distributed_tracing`](/docs/agents/java-agent/configuration/java-agent-configuration-config-file#distributed-tracing) esteja ativado:

      ```yml
      distributed_tracing:
        enabled: true
      ```

    * Habilite o recurso distributed tracing específico do Kafka adicionando o seguinte ao seu arquivo de configuração:

      ```yml
      class_transformer:
        com.newrelic.instrumentation.kafka-clients-spans-0.11.0.0:
          enabled: true
      ```
  </Collapser>

  <Collapser
    id="instrument-kafka-producer"
    title="2. instrumento do produtor Kafka"
  >
    Para instrumentar seu produtor Kafka, você precisará iniciar uma transação antes de qualquer chamada para `Producer.send(ProducerRecord<K, V> record)`. Para fazer isso, adicione a anotação do agente Java `@Trace(dispatcher = true)` ao método.

    Por exemplo:

    ```java
    @Trace(dispatcher = true)
    public static void createAndSend(KafkaProducer<String, String> producer){
      ProducerRecord<String, String> data = new ProducerRecord<String, String>("topic", "key", "value");
      producer.send(data);
    }
    ```

    <Callout variant="important">
      Se estiver usando Kafka Streams, você não precisa iniciar uma transação diretamente ou enviar dados a um produtor.
    </Callout>
  </Collapser>

  <Collapser
    id="instrument-kafka-consumer"
    title="3. Instrumento do consumidor Kafka"
  >
    Para instrumentar seu consumidor Kafka, você precisará iniciar uma transação quando a mensagem estiver sendo processada. O agente armazena o cabeçalho da carga útil distributed tracing na chave `newrelic` ou nas chaves `traceparent` e `tracestate` do W3C. Recupere o cabeçalho e chame a API de transação New Relic para aceitar a carga.

    Por exemplo:

    ```java
    @Trace(dispatcher = true)
    private static void processMessage(ConsumerRecord<String, String> rec) {
      // create a distributed trace headers map
      Headers dtHeaders = ConcurrentHashMapHeaders.build(HeaderType.MESSAGE);

      // Iterate through each record header and insert the trace headers into the dtHeaders map
      for (Header header : rec.headers()) {
        String headerValue = new String(header.value(), StandardCharsets.UTF_8);

        // using the newrelic key
        if (header.key().equals("newrelic")) {
          dtHeaders.addHeader("newrelic", headerValue);
        }

        // or using the W3C keys
        if (header.key().equals("traceparent")) {
          dtHeaders.addHeader("traceparent", headerValue);
        }

        if (header.key().equals("tracestate")) {
          dtHeaders.addHeader("tracestate", headerValue);
        }
      }

      // Accept distributed tracing headers to link this request to the originating request
      NewRelic.getAgent().getTransaction().acceptDistributedTraceHeaders(TransportType.Kafka, dtHeaders);
    }
    ```

    <Callout variant="important">
      Se você habilitou transações com Kafka Streams (consulte a [seção transação do Kafka Streams](#collect-kafka-streams-transactions)), mesmo que uma transação se aplique a muitos registros, a aceitação de cabeçalhos distributed trace se aplicará apenas a um registro.
    </Callout>
  </Collapser>
</CollapserGroup>
