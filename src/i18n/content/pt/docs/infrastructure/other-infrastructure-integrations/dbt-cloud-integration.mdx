---
title: Integração dbt Cloud com Airflow e Snowflake
tags:
  - New Relic integrations
  - Dbt Cloud integrations
metaDescription: Dbt Cloud with Airflow and Snowflake integration
freshnessValidatedDate: never
translationType: machine
---

Nossa <DNT>dbt Cloud</DNT> integração com <DNT>Airflow</DNT> monitora a integridade de seus <DNT>dbt Cloud</DNT> jobs e recursos, ajudando você a identificar problemas como quando execuções, modelos ou testes falham.

Essa integração é executada em <DNT>Apache Airflow</DNT> e consulta <DNT>Snowflake</DNT> em busca de testes com falha, se configurada para isso.

## Pré-requisitos [#prerequisites]

* Conta dbt Cloud com API habilitada e usando <DNT>Snowflake</DNT> como banco de dados.
* Acesso à conta <DNT>Snowflake</DNT> onde a conta <DNT>dbt Cloud</DNT> é executada.
* Ambiente <DNT>Airflow</DNT> existente versão 2.8.1 ou superior ou capacidade de executar <DNT>Docker Compose</DNT>.

## Instale a integração [#install]

Você pode instalar a integração New Relic <DNT>dbt Cloud</DNT> com <DNT>Airflow</DNT> :

* Instalando em seu ambiente Airflow existente. Isto é recomendado para ambiente de produção.
* Instalando com docker Compose. Isso é adequado para POCs rápidos.

Selecione a opção mais adequada às suas necessidades clicando em sua aba:

<Tabs>
  <TabsBar>
    <TabsBarItem id="1">
      Instalar no ambiente existente do Airflow
    </TabsBarItem>

    <TabsBarItem id="2">
      Instalar com docker Compose
    </TabsBarItem>
  </TabsBar>

  <TabsPages>
    <TabsPageItem id="1">
      <Steps>
        <Step>
          Certifique-se de ter o provedor Snowflake e clone o repositório `newrelic-dbt-cloud-integration` executando estes comandos:

          ```shell
          pip install apache-airflow-providers-snowflake>=3.0.0
          ```

          ```shell
          git clone https://github.com/newrelic-experimental/newrelic-dbt-cloud-integration.git
          ```
        </Step>

        <Step>
          Copie o conteúdo de `airflow/dags` para a raiz da pasta dags do Airflow
        </Step>

        <Step>
          Crie as cinco conexões Airflow necessárias para o DAG. A tabela a seguir fornece o nome da conexão e as informações para configurá-la. Observe que para todos eles, o tipo é `http`:

          <table>
            <thead>
              <tr>
                <th>
                  Nome da conexão
                </th>

                <th>
                  Descrição
                </th>

                <th>
                  Tipo
                </th>

                <th>
                  Host e senha
                </th>
              </tr>
            </thead>

            <tbody>
              <tr>
                <td class="children-nowrap">
                  `dbt_cloud_admin_api`
                </td>

                <td>
                  Permite que você se conecte à API de administração do dbt Cloud com <span class="children-nowrap">`SimpleHttpHook`</span>
                </td>

                <td>
                  <span class="children-nowrap">`http`</span>
                </td>

                <td>
                  **Host:** https://cloud.getdbt.com/api/v2/accounts/ACCOUNT\_ID/ (Substitua `ACCOUNT_ID` pelo ID da sua conta dbt Cloud)

                  **Senha:** Seu [token de API do dbt Cloud (configurações de perfil) ou um token de conta de serviço](https://docs.getdbt.com/docs/dbt-cloud-apis/user-tokens#user-tokens)
                </td>

                <td />
              </tr>

              <tr>
                <td class="children-nowrap">
                  `dbt_cloud_discovery_api`
                </td>

                <td>
                  Permite que você se conecte à API de descoberta dbt
                </td>

                <td>
                  <span class="children-nowrap">`http`</span>
                </td>

                <td>
                  **Host:** https://metadata.cloud.getdbt.com/graphql

                  **Senha:** [token de conta serviços na Dbt Cloud](https://docs.getdbt.com/docs/dbt-cloud-apis/user-tokens#user-tokens)
                </td>
              </tr>

              <tr>
                <td class="children-nowrap">
                  `nr_insights_insert`
                </td>

                <td>
                  Permite fazer upload de eventos personalizados para New Relic
                </td>

                <td>
                  <span class="children-nowrap">`http`</span>
                </td>

                <td>
                  **Host:** https://insights-collector.newrelic.com/v1/accounts/ACCOUNT\_ID/events (Substitua `ACCOUNT_ID` pelo ID da sua conta)

                  **Senha:** Sua [chave de API de inserção de insights do NR](https://one.newrelic.com/admin-portal/api-keys/insightkeys)
                </td>
              </tr>

              <tr>
                <td class="children-nowrap">
                  `nr_insights_query`
                </td>

                <td>
                  Permite consultar evento New Relic personalizado
                </td>

                <td>
                  <span class="children-nowrap">`http`</span>
                </td>

                <td>
                  **Host:** https://insights-api.newrelic.com/v1/accounts/ACCOUNT\_ID/query (Substitua `ACCOUNT_ID` pelo ID da sua conta)

                  **Senha:** Sua [chave de API de consulta de insights do NR](https://one.newrelic.com/admin-portal/api-keys/insightkeys)
                </td>
              </tr>
            </tbody>
          </table>

          Depois de configurar os quatro acima, você precisa configurar a conexão Snowflake. Snowflake permite que você consulte linhas de teste com falha. Há [muitas maneiras](https://airflow.apache.org/docs/apache-airflow-providers-snowflake/stable/connections/snowflake.html) de configurar uma conexão em floco de neve. Para configurar usando um par de chaves privadas, preencha o seguinte atributo:

          * `Type`: Floco de neve
          * `Login`: Seu nome de usuário do floco de neve
          * `Account`: Sua conta do floco de neve
          * `Warehouse`: Seu armazém Snowflake
          * `Role`: Seu papel de floco de neve. A função deve ter acesso a todos os bancos de dados usados no dbt Cloud para obter todas as linhas de teste com falha.
          * `Private Key Text`: a chave privada completa usada para esta conexão.
          * `Password`: frase secreta para a chave privada se ela estiver criptografada. Em branco se não estiver criptografado.
        </Step>

        <Step>
          Conclua a configuração ativando o `new_relic_data_pipeline_observability_get_dbt_run_metadata2` DAG.
        </Step>
      </Steps>
    </TabsPageItem>

    <TabsPageItem id="2">
      <Steps>
        <Step>
          Execute o seguinte comando para clonar o repositório `newrelic-dbt-cloud-integration` :

          ```shell
          git clone https://github.com/newrelic-experimental/newrelic-dbt-cloud-integration.git
          ```

          Em seguida, `cd` no diretório do Airflow:

          ```shell
          cd newrelic-dbt-cloud-integration/airflow
          ```

          Em seguida, inicialize e execute docker compose executando os seguintes comandos:

          ```shell
          docker-compose up airflow-init
          ```

          ```shell
          docker-compose up
          ```
        </Step>

        <Step>
          lançar a interface Airflow: `http://localhost:8080`
        </Step>

        <Step>
          Crie as cinco conexões Airflow necessárias para o DAG. A tabela a seguir fornece o nome da conexão e as informações para configurá-la. Observe que para todos eles, o tipo é `http`:

          <table>
            <thead>
              <tr>
                <th>
                  Nome da conexão
                </th>

                <th>
                  Descrição
                </th>

                <th>
                  Tipo
                </th>

                <th>
                  Host e senha
                </th>
              </tr>
            </thead>

            <tbody>
              <tr>
                <td class="children-nowrap">
                  `dbt_cloud_admin_api`
                </td>

                <td>
                  Permite que você se conecte à API de administração do dbt Cloud com <span class="children-nowrap">`SimpleHttpHook`</span>
                </td>

                <td>
                  <span class="children-nowrap">`http`</span>
                </td>

                <td>
                  **Host:** https://cloud.getdbt.com/api/v2/accounts/ACCOUNT\_ID/ (Substitua `ACCOUNT_ID` pelo ID da sua conta dbt Cloud)

                  **Senha:** Seu [token de API do dbt Cloud (configurações de perfil) ou um token de conta de serviço](https://docs.getdbt.com/docs/dbt-cloud-apis/user-tokens#user-tokens)
                </td>

                <td />
              </tr>

              <tr>
                <td class="children-nowrap">
                  `dbt_cloud_discovery_api`
                </td>

                <td>
                  Permite que você se conecte à API de descoberta dbt
                </td>

                <td>
                  <span class="children-nowrap">`http`</span>
                </td>

                <td>
                  **Host:** https://metadata.cloud.getdbt.com/graphql

                  **Senha:** [token de conta serviços na Dbt Cloud](https://docs.getdbt.com/docs/dbt-cloud-apis/user-tokens#user-tokens)
                </td>
              </tr>

              <tr>
                <td class="children-nowrap">
                  `nr_insights_insert`
                </td>

                <td>
                  Permite fazer upload de eventos personalizados para New Relic
                </td>

                <td>
                  <span class="children-nowrap">`http`</span>
                </td>

                <td>
                  **Host:** https://insights-collector.newrelic.com/v1/accounts/ACCOUNT\_ID/events (Substitua `ACCOUNT_ID` pelo ID da sua conta)

                  **Senha:** Sua [chave de API de inserção de insights do NR](https://one.newrelic.com/admin-portal/api-keys/insightkeys)
                </td>
              </tr>

              <tr>
                <td class="children-nowrap">
                  `nr_insights_query`
                </td>

                <td>
                  Permite consultar evento New Relic personalizado
                </td>

                <td>
                  <span class="children-nowrap">`http`</span>
                </td>

                <td>
                  **Host:** https://insights-api.newrelic.com/v1/accounts/ACCOUNT\_ID/query (Substitua `ACCOUNT_ID` pelo ID da sua conta)

                  **Senha:** Sua [chave de API de consulta de insights do NR](https://one.newrelic.com/admin-portal/api-keys/insightkeys)
                </td>
              </tr>
            </tbody>
          </table>

          Depois de configurar os quatro acima, você precisa configurar a conexão Snowflake. Snowflake permite que você consulte linhas de teste com falha. Há [muitas maneiras](https://airflow.apache.org/docs/apache-airflow-providers-snowflake/stable/connections/snowflake.html) de configurar uma conexão em floco de neve. Para configurar usando um par de chaves privadas, preencha o seguinte atributo:

          * `Type`: Floco de neve
          * `Login`: Seu nome de usuário do floco de neve
          * `Account`: Sua conta do floco de neve
          * `Warehouse`: Seu armazém Snowflake
          * `Role`: Seu papel de floco de neve. A função deve ter acesso a todos os bancos de dados usados no dbt Cloud para obter todas as linhas de teste com falha.
          * `Private Key Text`: a chave privada completa usada para esta conexão.
          * `Password`: frase secreta para a chave privada se ela estiver criptografada. Em branco se não estiver criptografado.
        </Step>

        <Step>
          Conclua a configuração ativando o `new_relic_data_pipeline_observability_get_dbt_run_metadata2` DAG.
        </Step>
      </Steps>
    </TabsPageItem>
  </TabsPages>
</Tabs>

## Encontre seus dados [#find-data]

Esta integração cria e reporta três eventos personalizados para New Relic:

<CollapserGroup>
  <Collapser id="collapser-1-in-group-2" title="`dbt_job_run`">
    `dbt_job_run`: fornece metadados e o status de todas as execuções concluídas. Este evento não inclui dados sobre modelos, instantâneos, sementes e testes. atributo inclui:

    * `project_name`
    * `environment_name`
    * `run_team`
    * Todos os campos listados na [API dbt Cloud v2 para execuções](https://docs.getdbt.com/dbt-cloud/api-v2#/operations/Retrieve%20Run).

    Todos os atributos diferentes de `project_name` e `environment_name` são prefixados com `run_`

    Exemplo de consulta:

    ```sql
    -- Get status of all job runs in the past seven days
    select 
        project_name,
        environment_name,
        job_name,
        run_created_at,
        run_run_duration_humanized,
        run_status,
        run_status_humanized,
        run_status_message
    from dbt_job_run
    since 7 days ago
    ```
  </Collapser>

  <Collapser id="collapser-1-in-group-2" title="`dbt_resource_run`">
    `dbt_resource_run` Fornece metadados e status para todos os recursos executados em uma execução de trabalho dbt. Os recursos incluem modelos, instantâneos, sementes e testes. atributo inclui:

    * Todos os atributos em `dbt_job_run`
    * `team` (Configurado na meta do projeto dbt)
    * `alert_failed_test_rows`
    * `failed_test_rows_limit`
    * `slack_mentions`
    * `message`
    * `resource_type`
    * `unique_id`
    * `database_name`
    * `schema_name`
    * `test_column_name`
    * `test_model_name`
    * `test_namespace`
    * `test_parameters`
    * `test_short_name`
    * `alias`
    * `severity`
    * `warn_if`
    * `error_if`
    * `tags`
    * `path`
    * `original_file_path`
    * `meta`
    * `meta_config`

    Exemplo de consulta:

    ```sql
    -- Get status of all resources run in the past day
    -- Status = 'None' means the resource exists in the project but was not executed in a particular run
    select 
        project_name,
        environment_name,
        job_name,
        run_created_at,
        resource_type,
        name,
        status
    from dbt_resource_run
    where status != 'None'
    since 1 day ago
    limit 200
    ```

    ```sql
    -- Get all resources types in the past day
    select 
        uniques(resource_type)
    from dbt_resource_run 
    since 1 day ago
    ```

    ```sql
    -- Get the count of all statuses in the last day
    -- Status = 'None' means the resource exists in the dbt project, but was not executed in a particular run
    select 
        count(*) as total_count
    from dbt_resource_run
    facet
        status
    since 1 day ago
    ```
  </Collapser>

  <Collapser id="collapser-3-in-group-2" title="`dbt_failed_test_rows`">
    `dbt_failed_test_rows`: fornece metadados e até as dez primeiras colunas dos resultados de uma consulta de teste com falha. Este evento só é criado quando a metaconfiguração para um teste dbt tem `alert_failed_test_rows`: `true`. atributo inclui:

    * Todos os atributos em `dbt_resource_run`
    * `field_1` - `field_10` representando as primeiras dez colunas retornadas em uma consulta de teste

    Exemplo de consulta:

    ```sql
    select 
        project_name,
        environment_name,
        job_name,
        run_created_at,
        name,
        field_1,
        field_2,
        field_3,
        field_4,
        field_5,
        field_6,
        field_7,
        field_8,
        field_9,
        field_10
    from dbt_failed_test_row
    since 7 days ago
    ```
  </Collapser>
</CollapserGroup>

## Configuração DAG

### Conexões:

Este DAG foi projetado para ser executado como está, sem configuração. Ao mesmo tempo, sabemos que sua empresa pode ter suas próprias convenções de nomenclatura para conexões. Assim, temos uma configuração simples dentro de `dag_config.yml` onde você pode definir o nome das diversas conexões.

```yaml
connections:
  dbt_cloud_admin_api: dbt_cloud_admin_api
  dbt_cloud_discovery_api: dbt_cloud_discovery_api 
  nr_insights_query: nr_insights_query 
  nr_insights_insert: nr_insights_insert
  snowflake_api: SNOWFLAKE 
```

### Equipe de execução:

Os trabalhos do dbt podem pertencer a equipes diferentes, mas não há lugar para definir isso no dbt Cloud. Podemos usar o código Python para definir a equipe dinamicamente. Para escrever seu próprio código, modifique `airflow/dags/nr_utils/nr_utils.py` e coloque qualquer lógica necessária em `get_team_from_run()`. Os dados de execução passados para essa função têm acesso ao seguinte atributo.

* Nome do Projeto
* nome\_do\_ambiente
* Todos os campos listados na [API dbt Cloud v2 para execuções](https://docs.getdbt.com/dbt-cloud/api-v2#/operations/Retrieve%20Run). Todos os atributos são prefixados com &quot;run\_&quot;

Aqui está um exemplo de função:

```python
def get_team_from_run(run: dict) -> str:
    team = 'Data Engineering' 
    if run['project_id'] == '11111' and run['environment_id'] in ['55555', '33333']:
        team = 'Platform'
    if re.match(r'Catch-all', run['job_name']):
        team = 'Project Catch All'
    return team
```

## Configuração do projeto Dbt

Dentro do projeto Dbt, podemos usar a metaconfiguração para definir uma equipe adicional e configurações específicas de teste.

* `Team`: embora `run_team determines` seja o proprietário dos trabalhos, às vezes precisamos que equipes upstream ou downstream recebam notificações de alerta sobre recursos com falha, como testes e modelos. Definir a equipe nos ajuda a fazer isso.
* `alert_failed_test_rows`: Definir como `True` ativará linhas de teste com falha onde executamos a consulta para testes com falha e enviamos até as 10 primeiras colunas para New Relic
* `failed_test_rows_limit`: número máximo de linhas de teste com falha para enviar ao New Relic. Temos um limite codificado de 100 linhas para evitar situações em que enviamos quantias excessivas para a New Relic.
* `slack_mentions`: se você ativar alertas de folga, este campo permite definir quem deve ser mencionado na mensagem.

Definir isso em `dbt_project.yml` definiria a equipe como &apos;Engenharia de Dados&apos; e ativaria linhas de teste com falha.

```yaml
models:
  dbt_fake_company:
    +meta:
      nr_config:
        team: 'Data Engineering'
        alert_failed_test_rows: False 
        failed_test_rows_limit: 5
        slack_mentions: '@channel, @business_users'
```

Podemos adicionar outro atributo chamado mensagem aos recursos. Na configuração a seguir, uma equipe de negócios parceira pode receber alertas sobre testes específicos que falharam. Além disso, podemos definir alertas nas próprias linhas de teste com falha.

```yaml
models:
  - name: important_business_model
    tests:
      - some_custom_test:
        config:
          meta:
            nr_config:
              team: 'Upstream Business Team'
              alert_failed_test_rows: true 
              failed_test_rows_limit: 10 
              slack_mentions: '@channel, @business_user1, @engineer1'
              message: 'Important business process produced invalid data. Please check X tool' 
```

## Resolução de problemas [#troubleshooting]

Diferentes versões do Airflow combinadas com diferentes versões de provedores podem induzir alterações significativas. Em alguns casos, pode ser necessário modificar o código para corresponder às versões específicas do seu ambiente Airflow. Rastreamos problemas conhecidos em nosso [repositório Github](https://github.com/newrelic-experimental/newrelic-dbt-cloud-integration/issues).