---
title: Configurar a integração do Prometheus OpenMetrics
tags:
  - Integrations
  - Prometheus integrations
  - Install and configure OpenMetrics
metaDescription: Configuration options and examples for your Prometheus OpenMetrics integration with New Relic in Docker and Kubernetes environments.
freshnessValidatedDate: never
translationType: machine
---

Salvo indicação em contrário, as opções de configuração para a integração do Prometheus OpenMetrics com New Relic se aplicam aos ambientes Docker e Kubernetes . No mínimo, os seguintes valores de configuração são <DNT>**required**</DNT>:

* <InlinePopover type="licenseKey"/>
* [Nome do cluster](#definitions-configuration-file)

Recomendação: Configure sua chave de licença do New Relic como uma variável de ambiente chamada `LICENSE_KEY`. Isso fornece um ambiente mais seguro, pois o New Relic pode carregar sua variável de ambiente a partir de um [segredo de autenticação TLS mútuo](/docs/integrations/prometheus-integrations/install-configure/add-mutual-tls-prometheus-endpoints).

## Configurar nri-prometheus-latest.yaml [#general-config]

O arquivo de manifesto `nri-prometheus-latest.yaml` inclui o mapa `nri-prometheus-cfg` mostrando um exemplo de configuração. Use o arquivo de manifesto para configurar o parâmetro a seguir.

<CollapserGroup>
  <Collapser
    id="example-configuration-file"
    title="Exemplo de arquivo de configuração"
  >
    A seguir está um exemplo de arquivo de configuração que você pode salvar e modificar para atender às suas necessidades. Para obter mais informações, consulte a documentação sobre [autenticação TLS mútua](/docs/integrations/prometheus-integrations/install-configure/add-mutual-tls-prometheus-endpoints) e [conversão de PromQL em NRQL](/docs/integrations/prometheus-integrations/view-query-data/translate-promql-queries-nrql).

    ```
    # The name of your cluster. It's important to match other New Relic products to relate the data.
    cluster_name: "<YOUR_CLUSTER_NAME>"

    # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true
    # standalone: true

    # How often the integration should run. Defaults to 30s.
    # scrape_duration: "30s"

    # The HTTP client timeout when fetching data from targets. Defaults to 5s.
    # scrape_timeout: "5s"

    # How old must the entries used for calculating the counters delta be
    # before the telemetry emitter expires them. Defaults to 5m.
    # telemetry_emitter_delta_expiration_age: "5m"

    # How often must the telemetry emitter check for expired delta entries.
    # Defaults to 5m.
    # telemetry_emitter_delta_expiration_check_interval: "5m"

    # Wether the integration should run in verbose mode or not. Defaults to false.
    verbose: false

    # Whether the integration should run in audit mode or not. Defaults to false.
    # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent.
    # It does not include verbose mode. This can lead to a high log volume, use with care.
    audit: false

    # Wether the integration should skip TLS verification or not. Defaults to false.
    insecure_skip_verify: false

    # The label used to identify scrapable targets. Defaults to "prometheus.io/scrape".
    scrape_enabled_label: "prometheus.io/scrape"

    # scrape_services Allows to enable scraping the service and not the endpoints behind.
    # When endpoints are scraped this is no longer needed
    scrape_services: true

    # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does.
    # Please notice that depending on the number of endpoints behind a service the load can increase considerably
    scrape_endpoints: false

    # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true.
    require_scrape_enabled_label_for_nodes: true

    # Number of worker threads used for scraping targets.
    # For large clusters with many (>400) targets, slowly increase until scrape
    # time falls between the desired `scrape_duration`.
    # Increasing this value too much will result in huge memory consumption if too
    # many metrics are being scraped.
    # Default: 4
    # worker_threads: 4

    # Maximum number of metrics to keep in memory until a report is triggered.
    # Changing this value is not recommended unless instructed by the New Relic support team.
    # max_stored_metrics: 10000

    # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms.
    # Changing this value is not recommended unless instructed by the New Relic support team.
    # min_emitter_harvest_period: 200ms

    # targets:
    #   - description: Secure etcd example
    #     urls: ["https://192.168.3.1:2379", "https://192.168.3.2:2379", "https://192.168.3.3:2379"]
    #     tls_config:
    #       ca_file_path: "/etc/etcd/etcd-client-ca.crt"
    #       cert_file_path: "/etc/etcd/etcd-client.crt"
    #       key_file_path: "/etc/etcd/etcd-client.key"

    # Proxy to be used by the emitters when submitting metrics. It should be
    # in the format [scheme]://[domain]:[port].
    # The emitter is the component in charge of sending the scraped metrics.
    # This proxy won't be used when scraping metrics from the targets.
    # By default it's empty, meaning that no proxy will be used.
    # emitter_proxy: "http://localhost:8888"

    # Certificate to add to the root CA that the emitter will use when
    # verifying server certificates.
    # If left empty, TLS uses the host's root CA set.
    # emitter_ca_file: "/path/to/cert/server.pem"

    # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account
    # having limited privileges. Defaults to false.
    # disable_autodiscovery: false

    # Whether the emitter should skip TLS verification when submitting data.
    # Defaults to false.
    # emitter_insecure_skip_verify: false

    # Histogram support is based on New Relic's guidelines for higher
    # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md.
    # To better support visualization of this data, percentiles are calculated
    # based on the histogram metrics and sent to New Relic.
    # By default, the following percentiles are calculated: 50, 95 and 99.
    #
    # percentiles:
    #   - 50
    #   - 95
    #   - 99

    # transformations:
    #   - description: "General processing rules"
    #     rename_attributes:
    #       - metric_prefix: ""
    #         attributes:
    #           container_name: "containerName"
    #           pod_name: "podName"
    #           namespace: "namespaceName"
    #           node: "nodeName"
    #           container: "containerName"
    #           pod: "podName"
    #           deployment: "deploymentName"
    #     ignore_metrics:
    #       # Ignore all the metrics except the ones listed below.
    #       # This is a list that complements the data retrieved by the New
    #       # Relic Kubernetes Integration, that's why Pods and containers are
    #       # not included, because they are already collected by the
    #       # Kubernetes Integration.
    #       - except:
    #         - kube_hpa_
    #         - kube_daemonset_
    #         - kube_statefulset_
    #         - kube_endpoint_
    #         - kube_service_
    #         - kube_limitrange
    #         - kube_node_
    #         - kube_poddisruptionbudget_
    #         - kube_resourcequota
    #         - nr_stats
    #     copy_attributes:
    #       # Copy all the labels from the time series with metric name
    #       # `kube_hpa_labels` into every time series with a metric name that
    #       # starts with `kube_hpa_` only if they share the same `namespace`
    #       # and `hpa` labels.
    #       - from_metric: "kube_hpa_labels"
    #         to_metrics: "kube_hpa_"
    #         match_by:
    #           - namespace
    #           - hpa
    #       - from_metric: "kube_daemonset_labels"
    #         to_metrics: "kube_daemonset_"
    #         match_by:
    #           - namespace
    #           - daemonset
    #       - from_metric: "kube_statefulset_labels"
    #         to_metrics: "kube_statefulset_"
    #         match_by:
    #           - namespace
    #           - statefulset
    #       - from_metric: "kube_endpoint_labels"
    #         to_metrics: "kube_endpoint_"
    #         match_by:
    #           - namespace
    #           - endpoint
    #       - from_metric: "kube_service_labels"
    #         to_metrics: "kube_service_"
    #         match_by:
    #           - namespace
    #           - service
    #       - from_metric: "kube_node_labels"
    #         to_metrics: "kube_node_"
    #         match_by:
    #           - namespace
    #           - node
    # integration definition files required to map metrics to entities
    # definition_files_path: /etc/newrelic-infra/definition-files
    ```
  </Collapser>

  <Collapser
    id="definitions-configuration-file"
    title="Nomes e definições principais"
  >
    Aqui estão alguns nomes e definições principais para o arquivo de configuração do Prometheus OpenMetrics.

    <table>
      <thead>
        <tr>
          <th style={{ width: "200px" }}>
            Nome da chave
          </th>

          <th>
            Descrição
          </th>
        </tr>
      </thead>

      <tbody>
        <tr id="cluster-name">
          <td>
            `cluster_name`

            <DNT>
              **Required.**
            </DNT>
          </td>

          <td>
            O nome do cluster. Este valor será incluído como atributo `clusterName` para todas as métricas.
          </td>
        </tr>

        <tr id="verbose">
          <td>
            `verbose`
          </td>

          <td>
            Booleano stringificado.

            * `true` (padrão): registra informações de depuração.
            * `false`: Registra apenas mensagem de erro.
          </td>
        </tr>

        <tr id="targets">
          <td>
            `targets`
          </td>

          <td>
            Configuração do endpoint estático a ser raspado pela integração. Ele contém uma lista de objetos. Para obter mais informações sobre essa estrutura, consulte a documentação sobre [configuração de destino](#target-config).
          </td>
        </tr>

        <tr id="scrape-enabled-label">
          <td>
            `scrape_enabled_label`

            <img
              style={{ width: '30px', height: '25px'}}
              class="inline"
              title="img-integration-k8.png"
              alt="img-integration-k8.png"
              src="/images/os_icon_k8.webp"
            />

            <DNT>
              **Kubernetes**
            </DNT>
          </td>

          <td>
            Corda. A integração verificará se o pod e o serviço do Kubernetes estão anotados ou possuem um rótulo com esse valor para decidir se deve ser copiado.

            Isto é particularmente útil quando você deseja limitar a quantidade de dados ignorando métricas ou incluindo métricas específicas que são enviadas para o New Relic. Como por padrão usamos o mesmo rótulo que o Prometheus usa para descobrir destinos que podem ser copiados, a maioria dos exportadores que você instala define esse rótulo automaticamente.

            Para manter um controle refinado sobre o destino que você deseja que a integração extraia, você pode definir esta opção para algum outro valor (como `newrelic/scrape`) e, em seguida, adicionar a anotação ou rótulo `newrelic/scrape: "true"` aos seus objetos Kubernetes . Se ambos estiverem definidos, as anotações terão precedência sobre os rótulos.

            Padrão: `"prometheus.io/scrape"`
          </td>
        </tr>

        <tr id="scrape-duration">
          <td>
            `scrape_duration`
          </td>

          <td>
            Com que frequência o raspador deve ser executado.

            * Para diminuir o uso de memória, aumente esse valor.

            * Para aumentar o uso de memória, diminua esse valor.

              O impacto no uso da memória se deve à distribuição da busca de destino durante o intervalo de coleta para evitar consultar (e armazenar em buffer) todos os dados de uma vez.

              O padrão é `30s`. Os valores válidos incluem `1s`, `15s`, `30s`, `1m`, `5m`, etc.
          </td>
        </tr>

        <tr id="scrape-timeout">
          <td>
            `scrape_timeout`
          </td>

          <td>
            O tempo limite do cliente HTTP ao buscar dados do endpoint.

            Padrão: `5s`. Os valores válidos incluem `1s`, `15s`, `30s`, `1m`, `5m`, etc.
          </td>
        </tr>

        <tr>
          <td>
            `worker_threads`
          </td>

          <td>
            Número de threads de trabalho usados para extrair destino. Pode ser aumentado em ambientes com alto número de destino ou destino com alta latência, mas pode aumentar o consumo de memória.

            Padrão: `4`. Não é recomendado usar mais de 10.
          </td>
        </tr>

        <tr>
          <td>
            `require_scrape_enabled_label_for_nodes`

            <img
              style={{ width: '30px', height: '25px'}}
              class="inline"
              title="img-integration-k8.png"
              alt="img-integration-k8.png"
              src="/images/os_icon_k8.webp"
            />

            <DNT>
              **Kubernetes**
            </DNT>
          </td>

          <td>
            Se os nós do Kubernetes precisam ou não de rótulos para serem copiados.

            Padrão: `true`.
          </td>
        </tr>

        <tr id="percentiles">
          <td>
            `percentiles`
          </td>

          <td>
            O suporte ao histograma é baseado nas [diretrizes da New Relic para abstrações métricas de nível superior](https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md).

            Para melhor auxiliar na visualização desses dados, são calculados percentuais com base no histograma métrico e enviados para New Relic. Os valores válidos incluem `50`, `95` e `99`.
          </td>
        </tr>

        <tr>
          <td id="emitter-proxy">
            `emitter_proxy`
          </td>

          <td>
            Proxy utilizada pela integração no envio de métricas:

            `[scheme]://[domain]:[port]`

            Este proxy não será utilizado na busca de métrica do destino.

            Por padrão, está vazio e nenhum proxy será usado.
          </td>
        </tr>

        <tr>
          <td id="emitter-ca-file">
            `emitter_ca_file`
          </td>

          <td>
            Certificado a ser adicionado à CA raiz que o emissor usará ao verificar os certificados do servidor. Se deixado em branco, o TLS usará o conjunto de CA raiz do host.
          </td>
        </tr>

        <tr id="emitter-insecure-skip-verify">
          <td>
            `emitter_insecure_skip_verify`
          </td>

          <td>
            Se o emissor deve ignorar a verificação TLS ao enviar dados. Padrão: `false`.
          </td>
        </tr>

        <tr id="disable-autodiscovery">
          <td>
            `disable_autodiscovery`
          </td>

          <td>
            Defina como verdadeiro para desabilitar a descoberta automática no cluster k8s. Pode ser útil ao executar o pod com uma conta de serviço com privilégios limitados. Padrão: `false`.
          </td>
        </tr>
      </tbody>
    </table>
  </Collapser>
</CollapserGroup>

## Configurar objetos na chave de destino [#target-config]

Se desejar que a chave de destino no arquivo de configuração contenha um ou mais objetos, use a seguinte estrutura na lista YAML:

<table>
  <thead>
    <tr>
      <th style={{ width: "250px" }}>
        Nome da chave
      </th>

      <th>
        Descrição
      </th>
    </tr>
  </thead>

  <tbody>
    <tr id="description">
      <td>
        `description`
      </td>

      <td>
        Uma descrição para os URLs neste destino.
      </td>
    </tr>

    <tr id="urls">
      <td>
        `urls`
      </td>

      <td>
        Uma lista de strings com os URLs a serem copiados.
      </td>
    </tr>

    <tr id="tls-config">
      <td>
        `tls_config`
      </td>

      <td>
        Configuração de autenticação utilizada para envio de solicitações. Suporta TLS e TLS mútuo. Para obter mais informações, consulte a documentação sobre [autenticação TLS mútua](/docs/integrations/prometheus-integrations/install-configure/add-mutual-tls-prometheus-endpoints).
      </td>
    </tr>
  </tbody>
</table>

<CollapserGroup>
  <Collapser
    id="specify-path-port"
    title={<><img src="/images/os_icon_k8.webp" title="img-integration-k8s@2x.png" alt="img-integration-k8s@2x.png" style={{ width: '30px', height: '25px' }} /></>}
  >
    A integração Prometheus OpenMetrics da New Relic descobre automaticamente qual destino extrair. Para especificar a porta e o caminho endpoint a serem usados ao construir o destino, você pode usar as anotações ou rótulo `prometheus.io/port` e `prometheus.io/path` em seu pod e serviços Kubernetes . As anotações têm precedência sobre os rótulos.

    * Se `prometheus.io/port` não estiver presente, a integração tentará extrair cada `port` ou `ContainerPort` definido para o serviço.
    * Se `prometheus.io/path` não estiver presente, a integração será padronizada como `/metrics`.
    * Se um serviço não estiver em execução no caminho `/my-metrics-path` padrão, adicione um rótulo ao pod `prometheus.io/path=my-metrics-path`. Se o caminho para o endpoint da métrica for mais complexo e não puder ser um valor de rótulo válido (por exemplo, `foo/bar`), use anotações.
  </Collapser>

  <Collapser
    id="example-port-path"
    title={<><img src="/images/os_icon_k8.webp" title="img-integration-k8s@2x.png" alt="img-integration-k8s@2x.png" style={{ width: '30px', height: '25px' }} /></>}
  >
    Neste exemplo você tem uma implantação no seu cluster, e o pod expõe a métrica do Prometheus na porta `8080` e no caminho `my-metrics.`

    Nos metadados `PodSpec` do manifesto de implantação, defina os rótulos `prometheus.io/port: "8080"` e `prometheus.io/path: "my-metrics"`. Quando a integração tentar recuperar a métrica do seu pod, ela enviará uma solicitação para `http://<pod-ip>:8080/my-metrics`.

    ```
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: my-deployment
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: my-app
      template:
        metadata:
          labels:
            app: my-app
            prometheus.io/scrape: "true"
            prometheus.io/port: "8080"
            prometheus.io/path: "my-metrics"
    ```
  </Collapser>
</CollapserGroup>

## Comportamento de serviços e raspagem de endpoint

Por padrão, os serviços são copiados diretamente em vez do endpoint subjacente, já que `scrape_services` está definido como `true` e `scrape_endpoints` como `false`.

Para alterar esse comportamento, defina `scrape_endpoints` como `true` configurando `Prometheus OpenMetrics integrations` para extrair o endpoint subjacente, como o servidor Prometheus faz nativamente, em vez de diretamente os serviços.

Observe que dependendo do número de endpoints por trás dos serviços no cluster a carga e os dados ingeridos podem aumentar consideravelmente, monitor e, se necessário, aumentar os requisitos de recursos.

Além disso, mesmo que seja possível definir `scrape_services` e `scrape_endpoints` como verdadeiros para garantir a retrocompatibilidade, isso levaria à duplicação de dados.

## Recarregue a configuração [#reload-config]

A integração do Prometheus OpenMetrics <DNT>**does not**</DNT> recarrega automaticamente a configuração quando você faz alterações no arquivo de configuração.

<img
  style={{ width: '30px', height: '25px'}}
  class="inline"
  title="Docker icon"
  alt="Docker icon"
  src="/images/os_icon_docker.webp"
/>

<DNT>
  **Docker**
</DNT>

Para recarregar a configuração, reinicie o contêiner que executa a integração:

```
docker restart nri-prometheus
```

<img
  style={{ width: '30px', height: '25px'}}
  class="inline"
  title="img-integration-k8.png"
  alt="img-integration-k8.png"
  src="/images/os_icon_k8.webp"
/>

<DNT>
  **Kubernetes**
</DNT>

Para recarregar a configuração, reinicie a integração. Recomendação: Reduza a implantação para zero réplicas e, em seguida, reduza-a para uma réplica:

```
kubectl scale deployment nri-prometheus --replicas=0
kubectl scale deployment nri-prometheus --replicas=1
```

## Docker: executa o arquivo de configuração anterior [#run-previous]

<img
  style={{ width: '30px', height: '25px'}}
  class="inline"
  title="Docker icon"
  alt="Docker icon"
  src="/images/os_icon_docker.webp"
/>

<DNT>**Docker:**</DNT> Para executar a integração com o arquivo de configuração anterior:

1. Copie o conteúdo e salve-o em um arquivo `config.yaml` .

2. No mesmo diretório, execute o comando:

   ```
   docker run -d --restart unless-stopped \
       --name nri-prometheus \
       -e CLUSTER_NAME="YOUR_CLUSTER_NAME" \
       -e LICENSE_KEY="YOUR_LICENSE_KEY" \
       -v "$(pwd)/config.yaml:/config.yaml" \
       newrelic/nri-prometheus:latest --configfile=/config.yaml
   ```
