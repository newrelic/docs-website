---
title: Recursos avançados e camadas de inteligência do ATP
metaDescription: 'Learn about ATP intelligent features including dynamic thresholds, multi-metric composite scoring, and anomaly detection.'
tags:
  - Open source telemetry integrations
  - OpenTelemetry
  - NRDOT
  - Adaptive Telemetry Processor
  - ATP
  - Dynamic thresholds
  - Anomaly detection
  - Multi-metric scoring
  - Intelligence
freshnessValidatedDate: never
translationType: machine
---

<Callout title="Visualização">
  Ainda estamos trabalhando nesse recurso, mas adoraríamos que você experimentasse!

  Atualmente, esse recurso é fornecido como parte de uma prévia, de acordo com nossas [políticas de pré-lançamento](/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy/).
</Callout>

O ATP inclui camadas de inteligência avançada que oferecem processamento de telemetria sofisticado e adaptável, além da simples filtragem baseada em limites.

## Opções de configuração avançada [#advanced-config]

Você pode adicionar estes **parâmetros opcionais** à configuração do seu processador ATP para habilitar recursos avançados.

<CollapserGroup>
  <Collapser id="dynamic" title="Limiares dinâmicos">
    ### Limiares dinâmicos [#dynamic-thresholds]

    Este recurso ajusta automaticamente os limites com base no comportamento histórico do seu sistema, ajudando você a detectar anomalias enquanto reduz falsos positivos.

    ```yaml
    processors:
      adaptivetelemetry:
        # ... existing config ...
        # Dynamic threshold configuration
        enable_dynamic_thresholds: true
        dynamic_smoothing_factor: 0.2
        min_thresholds:
          process.cpu.utilization: 0.04  # Can't go below 4%
          process.memory.utilization: 0.04  # Can't go below 4%
        max_thresholds:
          process.cpu.utilization: 0.30  # Can't exceed 30%
          process.memory.utilization: 0.30  # Can't exceed 30%
    ```
  </Collapser>

  <Collapser id="multi-metric" title="Pontuação composta multimétrica">
    ### Pontuação composta multimétrica [#multi-metric-scoring]

    Isso avalia várias métricas em conjunto para fornecer uma visão holística da saúde do processo, detectando processos problemáticos em múltiplas dimensões.

    ```yaml
    processors:
      adaptivetelemetry:
        # ... existing config ...
        # Multi-metric configuration
        enable_multi_metric: true
        composite_threshold: 1.2
        weights:
          process.cpu.utilization: 0.5
          process.memory.utilization: 0.5
    ```
  </Collapser>

  <Collapser id="anomaly-detection" title="Detecção de anomalia">
    ### Detecção de anomalia [#anomaly-detection]

    Isso detecta picos repentinos nas métricas acima das médias históricas, ajudando você a identificar processos que saíram do controle ou sofreram alterações inesperadas de comportamento.

    ```yaml
    processors:
      adaptivetelemetry:
        # ... existing config ...
        # Anomaly detection configuration
        enable_anomaly_detection: true
        anomaly_history_size: 15
        anomaly_change_threshold: 50.0  # 50% spike triggers anomaly
        anomaly_min_data_points: 3
    ```
  </Collapser>
</CollapserGroup>

## Exemplo completo de configuração avançada [#complete-example]

Aqui está um exemplo completo com todos os recursos avançados habilitados:

```yaml
processors:
  adaptivetelemetry:
    enable_storage: true
    retention_minutes: 30
    include_process_list:
      - "/usr/bin/postgres"
      - "/usr/sbin/nginx"
    metric_thresholds:
      process.cpu.utilization: 0.05
      process.memory.utilization: 0.05
    
    # All advanced features enabled
    enable_dynamic_thresholds: true
    dynamic_smoothing_factor: 0.2
    min_thresholds:
      process.cpu.utilization: 0.04
      process.memory.utilization: 0.04
    max_thresholds:
      process.cpu.utilization: 0.30
      process.memory.utilization: 0.30
    
    enable_multi_metric: true
    composite_threshold: 1.5
    weights:
      process.cpu.utilization: 0.5
      process.memory.utilization: 0.5
    
    enable_anomaly_detection: true
    anomaly_history_size: 15
    anomaly_change_threshold: 50.0
    anomaly_min_data_points: 3
```

## Guia de ajuste de configuração [#tuning-guide]

As seções a seguir explicam cada opção de configuração avançada em detalhes, juntamente com orientações sobre como ajustá-las para o seu ambiente.

<CollapserGroup>
  <Collapser id="basic" title="Configurações básicas">
    ### Configurações básicas

    * `enable_storage`: Habilite o armazenamento persistente para dados históricos de métricas. Isso é necessário para a persistência entre reinicializações.

      * Padrão: `true`

      * Os caminhos de armazenamento são determinados automaticamente pela plataforma:

        * **Linux**: `/var/lib/nrdot-collector/adaptiveprocess.db`
        * **Windows**: `%LOCALAPPDATA%\nrdot-collector\adaptiveprocess.db`

    * `retention_minutes`: Quanto tempo (em minutos) manter o rastreamento de um processo após ele ter excedido um limite pela última vez.

      * Padrão: 30 (máx.: 30)
      * Ajuste: Use valores mais baixos (5-10) para processos de curta duração; mantenha em 30 para cargas de trabalho estáveis
  </Collapser>

  <Collapser id="thresholds" title="Configurações de limite">
    ### Incluir lista de processos

    O `include_process_list` é uma lista de processos que sempre ignoram todos os filtros e são reportados independentemente dos limites.

    * Caso de uso: Processos críticos que você sempre deseja monitorar (por exemplo, banco de dados, servidor web)
    * Segurança: Use caminhos completos (`/usr/bin/postgres`) com separadores como `/usr/bin/postgres`. Entradas sem separadores de caminho, como `"postgres"`, não corresponderão a nenhum processo.

    **Exemplo:**

    ```yaml
    include_process_list:
      - "/usr/bin/postgres"  # Always include PostgreSQL
      - "/usr/sbin/nginx"    # Always include Nginx
    ```
  </Collapser>

  <Collapser id="metric-thresholds" title="Limiares de métrica">
    ### Limiares de métrica

    Os `metric_thresholds` são valores de limite estáticos para cada métrica. Um processo é sinalizado quando **excede** este valor.

    **Como otimizar:**

    * Comece com valores de linha de base da sua carga de trabalho média
    * Aumente os limites para reduzir o ruído e gerar menos alertas
    * Reduza os limiares para detectar anomalias menores
    * Para métricas de utilização (CPU/memória): Use porcentagens (0.0-1.0 = 0%-100%)
    * Para métricas de contagem (threads, descritores de arquivo): Use números absolutos

    **Valores de exemplo explicados:**

    ```yaml
    metric_thresholds:
      process.cpu.utilization: 0.0005      # 0.05% CPU - very sensitive
      process.memory.utilization: 0.0005   # 0.05% memory - very sensitive
      process.memory.virtual: 20971520     # 20 MB virtual memory
      process.threads: 16                  # 16 threads
      process.open_file_descriptors: 30    # 30 open files
      process.disk.io: 204800              # 200 KB disk I/O
      process.cpu.time: 0.01               # 0.01 seconds CPU time
    ```
  </Collapser>

  <Collapser id="dynamic-thresholds" title="Limiares dinâmicos">
    ### Limiares dinâmicos

    Ajusta automaticamente os limites com base no comportamento histórico. Útil para processos com padrões de carga de trabalho variáveis.

    * `enable_dynamic_thresholds`: Ativa o ajuste de limite adaptativo.

      * Quando habilitar: Processos com comportamento flutuante, como jobs em lote, servidores de API
      * Quando desativar: Processos com comportamento estável e previsível

    * `dynamic_smoothing_factor`: Quão rapidamente os limites se adaptam (0,0-1,0).

      * Menor (0,1): Adaptação lenta, mais estável (bom para mudanças graduais)
      * Maior (0,5): Adaptação rápida, mais responsivo (bom para cargas de trabalho voláteis)
      * Padrão: 0.2 (equilibrado)

    * `min_thresholds`: Valores de piso - os limiares não cairão abaixo destes. Isso é usado para evitar que os limites se tornem muito sensíveis.

    * `max_thresholds`: Valores de teto - os limiares não excederão estes. Isso é usado para evitar que os limiares se tornem muito permissivos.
  </Collapser>

  <Collapser id="multi-metric-scoring" title="Pontuação multimétrica">
    ### Pontuação multimétrica (pontuação composta)

    Avalia múltiplas métricas em conjunto em vez de individualmente. Útil para identificar processos que são &quot;agentes nocivos&quot; em múltiplas dimensões.

    * `enable_multi_metric`: Habilita a pontuação composta.

      * Quando habilitar: Deseja capturar processos que são problemáticos de várias maneiras (alto uso de CPU + alto uso de memória)
      * Quando desativar: Deseja alertar sobre violações de métricas individuais

    * `composite_threshold`: O limite da pontuação combinada. Um processo é sinalizado quando: (soma ponderada das métricas) &gt; composite\_threshold.

      * Inferior (0,5): Mais sensível, detecta casos marginais
      * Maior (2.0): Menos sensível, detecta apenas problemas significativos
      * Padrão: 1,5

    * `weights`: Importância de cada métrica na pontuação composta. Quanto maior o peso, mais influência a métrica tem.
  </Collapser>

  <Collapser id="anomaly-detection" title="Detecção de anomalia">
    ### Detecção de anomalia

    Isso detecta picos repentinos ou comportamento incomum comparando valores atuais com padrões históricos.

    * `enable_anomaly_detection`: Ativa a detecção de picos.

      * Quando habilitar: Para detectar mudanças repentinas (processo descontrolado)
      * Quando desabilitar: Para limites absolutos

    * `anomaly_history_size`: Número de pontos de dados recentes a serem mantidos para o cálculo da média de linha de base.

      * Maior (50-100): Linha de base mais suave, detecta anomalias maiores
      * Menor (5-15): Mais responsivo, captura picos menores
      * Padrão: 10 (máx: 100)

    * `anomaly_change_threshold`: Pico percentual acima da média histórica para acionar um alerta.

      * Exemplo: 50.0 = sinalizar se o valor atual for 50% maior que a média
      * Mais baixo (20-50): Mais sensível a mudanças
      * Mais alto (100-200): Capturar apenas picos drásticos
      * Padrão: 200,0

    * `anomaly_min_data_points`: Mínimo de pontos de dados históricos antes que a detecção de anomalias seja ativada.

      * Isso evita falsos positivos durante a inicialização
      * Recomendado: Manter em 3 (padrão)
      * Deve ser ≤ `anomaly_history_size`
  </Collapser>
</CollapserGroup>

## Estratégia de otimização [#tuning-strategy]

**Primeiros passos:**

1. Comece apenas com `metric_thresholds` (desabilite dinâmico/multimétrica/anomalia)
2. Observe por 1-2 dias, depois ajuste os limites para reduzir falsos positivos
3. Habilitar `enable_dynamic_thresholds` para cargas de trabalho variáveis
4. Adicione `enable_anomaly_detection` para detectar picos repentinos
5. Use `enable_multi_metric` se os processos apresentarem problemas de recursos correlacionados

**Padrões comuns:**

* **Serviços de produção estáveis**: use apenas limites estáticos
* **Jobs em lote**: use limiares dinâmicos + detecção de anomalias
* **Apps com uso intensivo de recursos**: use pontuação multimétrica
* **Processos críticos**: adicionar a `include_process_list`

## Recursos relacionados [#related-resources]

<DocTiles>
  <DocTile title="Resolução de problemas" path="/docs/opentelemetry/nrdot/atp/troubleshooting">
    Saiba como solucionar problemas de ATP no seu ambiente.
  </DocTile>

  <DocTile title="Consulte seus dados" path="/docs/opentelemetry/nrdot/atp/query">
    Saiba como consultar dados de ATP no New Relic usando NRQL.
  </DocTile>

  <DocTile title="Ver seus dados" path="/docs/opentelemetry/nrdot/atp/view-data">
    Saiba como visualizar os dados coletados pelo ATP no New Relic.
  </DocTile>
</DocTiles>