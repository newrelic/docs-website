---
title: 'Diagnóstico de engenharia de confiabilidade: um guia para iniciantes sobre resolução de problemas de desempenho do aplicativo'
tags:
  - Observability maturity
  - 'Uptime, performance, and reliability'
  - Site reliability engineering
  - SRE
metaDescription: 'New Relic observability maturity series: A beginner''s guide on identifying common application performance issues.'
freshnessValidatedDate: never
translationType: machine
---

Este guia é uma introdução para melhorar sua habilidade em diagnosticar problemas que impactam os clientes. Você poderá se recuperar de problemas de desempenho do aplicativo mais rapidamente seguindo os procedimentos deste guia.

Este guia faz parte de nossa [série sobre maturidade de observabilidade](/docs/new-relic-solutions/observability-maturity/introduction).

## Pré-requisitos

Aqui estão alguns requisitos e algumas recomendações para usar este guia:

* Cobertura de observabilidade da New Relic:

  * <DNT>**Required**</DNT> : [APM com distributed tracing](/docs/apm/apm-ui-pages/monitoring/apm-summary-page-view-transaction-apdex-usage-data), [logs contextualizados de APM](/docs/apm/new-relic-apm/getting-started/get-started-logs-context) e [agente de infraestrutura](/docs/infrastructure/infrastructure-monitoring/get-started/get-started-infrastructure-monitoring)
  * Recomendado: [log](/docs/logs/get-started/get-started-log-management) e [Monitoramento de rede](/docs/network-performance-monitoring/get-started/npm-introduction) (NPM)

* <DNT>**Required**</DNT>: [gerenciamento a nível de serviço](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide)

* Recomendado: alguma experiência com o uso New Relic APM, distributed tracing, consultas NRQL e <InlinePopover type="logs" />interface

* Recomendado: você leu estes guias:

  * [Gerenciamento de qualidade de alerta](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/alert-quality-management-guide)
  * [Gerenciamento do nível de serviço](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide)

## Visão geral [#overview]

Antes de começar a usar este guia, ele o ajudará a entender o que aprenderá. Este guia ajudará você a entender:

* Como seu negócio é impactado ao melhorar suas habilidades de diagnóstico.

* Os principais indicadores de desempenho operacionais usados para medir o sucesso.

* Como seu usuário final percebe diferentes tipos de problemas de confiabilidade.

* A diferença entre a *causa direta* e a *causa raiz subjacente* de um problema.

* As etapas básicas de diagnóstico para encontrar e resolver problemas, que incluem:

  * Definindo o problema - criando uma declaração do problema
  * Encontrando a origem do problema
  * Encontrando a causa direta desse problema

* Algumas categorias de problemas de desempenho (desempenho de saída, desempenho de entrada e desempenho do cliente) e o recurso New Relic utilizado para diagnosticar esses problemas (APM, Sintético, <InlinePopover type="browser" />e monitoramento de Mobile).

* Como usar uma matriz de problemas, que é uma folha de dicas para compreender problemas comuns e suas prováveis fontes.

No final, revisaremos alguns exemplos de problemas de desempenho, que devem ajudá-lo a entender melhor esses conceitos.

## Resultados desejados [#desired-outcomes]

### Resumo

O valor para o negócio é:

* Reduzir o número de incidentes que perturbam os negócios
* Reduza o tempo necessário para resolver problemas (MTTR)
* Reduza o custo operacional do incidente

O valor para as operações de TI e SRE é:

* Melhore o tempo para entender e resolver

### Resultado de business [#business-outcome]

Em 2014, [o Gartner estimou](https://blogs.gartner.com/andrew-lerner/2014/07/16/the-cost-of-downtime) que o custo médio do período de inatividade de TI foi de US$ 5.600 por minuto. O custo cumulativo do incidente que impacta os negócios é determinado por fatores como tempo para saber, frequência, tempo para reparo, impacto na receita e o número de engenheiros que fazem a triagem e resolvem o incidente. Simplificando, você deseja menos incidentes com impacto nos negócios, menor duração dos incidentes e diagnósticos mais rápidos, com menos pessoas necessárias para resolver os impactos no desempenho.

Em última análise, o objetivo do negócio é maximizar o tempo de operação e minimizar o período de inatividade onde o custo do período de inatividade é:

**`Downtime minutes x Cost per minute = Downtime cost`**

O período de inatividade é determinado pelo número de incidentes que perturbam os negócios e pela sua duração. O custo do período de inatividade inclui muitos fatores, mas os mais diretamente mensuráveis são os custos operacionais e a perda de receitas.

A empresa deve promover uma redução no seguinte:

* número de incidentes que perturbam os negócios
* custo operacional do incidente

### Resultado operacional [#operational-outcome]

O resultado operacional necessário é manter a conformidade dos objetivos de nível de serviço da camada de produto. Você faz isso diagnosticando níveis de serviço degradados, comunicando seu diagnóstico e realizando uma resolução rápida. Mas degradações e incidentes inesperados sempre acontecem e você precisa responder de forma rápida e eficaz.

Em outros guias desta série, nos concentramos em melhorar <DNT>**time to know**</DNT>. Em nosso [guia de gerenciamento de qualidade de alerta](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/alert-quality-management-guide), nos concentramos em <DNT>**reactive**</DNT> maneiras de melhorar o tempo de conhecimento, e em nosso [guia de gerenciamento a nível de serviço,](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide) nos concentramos em <DNT>**proactive**</DNT> métodos.

No guia que você está lendo agora, nos concentramos em melhorar <DNT>**time to understand**</DNT> e <DNT>**time to resolve**</DNT>.

## Principais indicadores de desempenho - operacionais [#operational-kpis]

Existem muitas métricas discutidas e debatidas no mundo da “gestão de incidentes” e da teoria SRE; contudo, a maioria concorda que é importante concentrar-se num pequeno conjunto de principais indicadores de desempenho.

Os KPIs abaixo são os indicadores mais comuns usados por práticas bem-sucedidas de SRE e gestão de incidentes.

<CollapserGroup>
  <Collapser id="slo-compliance" title="Conformidade com os objetivos de nível de serviço (SLO)">
    Este é o seu principal indicador. nível de serviço mede o início da degradação do desempenho, a tendência do desempenho, a extensão do impacto e quando o problema foi resolvido.

    Para saber mais sobre esse processo, consulte nosso [guia de gerenciamento a nível de serviço](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide).
  </Collapser>

  <Collapser id="time-to-know" title="Hora de saber">
    Este é o momento em que o incidente foi registrado pela primeira vez por um humano. Medidas de tempo para conhecimento entre o início da degradação do nível de serviço e o momento em que um registro do problema de desempenho foi criado. O [guia de gestão de qualidade alerta](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/alert-quality-management-guide) demonstra como medir e melhorar esta métrica operacional.
  </Collapser>

  <Collapser id="time-to-understand" title="Hora de entender">
    É o tempo entre o registro do incidente (tempo para saber) e a resolução do impacto (tempo para resolver).
  </Collapser>

  <Collapser id="time-to-resolve" title="Hora de resolver">
    O tempo de resolução é muitas vezes referido como MTTR (tempo médio para restaurar/reparar/resolver). É medido desde o início da degradação do desempenho (conforme determinado pelos níveis de serviço) até o momento em que o nível de serviço retorna ao nível de desempenho esperado.

    <DNT>**Note**</DNT>: O tempo de resolução não significa que a causa raiz foi identificada e corrigida permanentemente. Correções permanentes fazem parte do processo de “gerenciamento de problemas”, após a resolução do incidente. Por favor, faça sua pesquisa sobre causa raiz versus causa direta e “sintomas de causas raízes”.
  </Collapser>
</CollapserGroup>

## Percepção de confiabilidade do usuário final [#end-user-perception]

A forma como os clientes percebem o desempenho do seu produto é fundamental para entender como medir a urgência e a prioridade. Além disso, compreender a perspectiva dos clientes ajuda a compreender como o negócio vê o problema, bem como a compreender o fluxo de trabalho necessário para suportar as capacidades impactadas. Depois de entender a percepção dos clientes e do negócio, você poderá entender melhor o que pode estar impactando a confiabilidade dessas capacidades.

Em última análise, a observabilidade da perspectiva do cliente é o primeiro passo para se tornar proativo e proficiente em engenharia de confiabilidade.

Existem duas experiências principais que impactam a percepção do usuário final sobre o desempenho do seu produto digital e seus recursos. Os termos abaixo são da perspectiva dos clientes, usando a linguagem comum dos clientes.

<CollapserGroup>
  <Collapser id="availability" title="Disponibilidade, também conhecido como, não está funcionando">
    Disponibilidade também é conhecida como: conectividade, tempo de operação, acessibilidade. Mas também está associado ao sucesso (não erros).

    Um usuário final pode declarar que não pode acessar um recurso necessário, como fazer login, navegar, pesquisar, visualizar inventário. Ou podem simplesmente afirmar que todo o serviço está indisponível. Este é um sintoma da incapacidade de se conectar a um serviço ou de um serviço que retorna um erro.

    Tradicionalmente, a &quot;disponibilidade&quot; ou &quot;tempo de operação&quot; era medida numa metodologia binária &quot;UP/DOWN&quot;, medindo a capacidade de conexão a um serviço. O método tradicional tem uma lacuna crítica, pois só mede quando um serviço inteiro fica completamente indisponível. Essa medida clássica de confiabilidade resulta em lacunas significativas de observabilidade, diagnósticos difíceis e no usuário final sendo significativamente impactado antes que você possa reagir.

    A disponibilidade é medida tanto pela “capacidade de alcançar um serviço”, também conhecida como “tempo de operação”, E “pela capacidade do serviço de retornar a resposta esperada” (em outras palavras, “não erro”). framework de maturidade de observabilidade da New Relic distingue os dois por <DNT>**input performance**</DNT> (conectividade) e <DNT>**output performance**</DNT> (sucesso e latência da resposta).
  </Collapser>

  <Collapser id="performance" title="Desempenho, ou seja, é muito lento">
    O desempenho também é conhecido como: latência e tempo de resposta.

    Um usuário final pode afirmar que o serviço é muito lento.

    Tanto para os líderes de TI quanto para os de negócios, o termo “desempenho” pode abranger uma série de questões. No gerenciamento a nível de serviço da New Relic, a “lentidão” é medida tanto na categoria “produção” quanto na categoria “cliente”. No entanto, a maioria dos problemas de lentidão ocorre devido a um problema de saída, decorrente do que é tradicionalmente chamado de “serviços backend ”.
  </Collapser>
</CollapserGroup>

## Causa raiz vs causa direta [#root-cause-vs-direct-cause]

A causa raiz de um problema **não** é a mesma que a causa direta desse problema. Da mesma forma, corrigir a causa direta (curto prazo) geralmente não significa que você corrigiu a causa raiz (longo prazo) do problema. <DNT>**It&apos;s very important to make this distinction.**</DNT>

Ao procurar um problema de desempenho, você deve primeiro tentar encontrar a causa direta do problema fazendo a pergunta “O que mudou?”. O componente ou comportamento que mudou geralmente não é a causa raiz, mas é na verdade a causa direta que você precisa resolver primeiro. Resolver a causa raiz é importante, mas geralmente requer uma discussão retroativa pós-incidente e um gerenciamento de problemas de longo prazo.

Por exemplo: o nível de serviço da sua capacidade de login cai repentinamente. É imediatamente descoberto que os padrões de tráfego são muito mais elevados do que o normal. Você trace o problema de desempenho até uma configuração de limite de conexão TCP aberta que está causando uma fila de conexão TCP muito maior. Você resolve o problema imediatamente implantando um aumento de limite de TCP e alguma instância extra de servidor. Você resolveu a causa direta do problema no curto prazo, mas a causa raiz pode ser qualquer coisa, desde planejamento de capacidade inadequado, falta de comunicação do marketing ou uma implantação relacionada com consequências indesejadas nas cargas upstream.

Essa distinção também é feita em ITIL/ITSM <DNT>**Incident management**</DNT> versus <DNT>**Problem management**</DNT>. As causas profundas são discutidas em negociações pós-incidente e depois resolvidas em processos de gestão de problemas de longo prazo.

## Etapas de diagnóstico (visão geral) [#diagnostic-steps]

### Etapa 1: Defina o problema [#create-problem-statement]

A primeira regra é estabelecer rapidamente a definição do problema. Existem muitos guias sobre como criar declarações de problemas, mas simples e eficazes são o melhor. Uma declaração de problema bem formada fará o seguinte:

1. Descreva o que o usuário final está enfrentando. Qual é o problema que o usuário final está enfrentando?
2. Descreva o comportamento esperado da capacidade do produto. O que o usuário final deve experimentar?
3. Descreva o comportamento atual da capacidade do produto. Qual é a avaliação técnica do que o usuário está vivenciando?

Evite quaisquer suposições na declaração do problema. Atenha-se aos fatos.

### Etapa 2: Encontre a fonte [#find-source]

A “fonte” é o componente ou código que está mais próximo da causa direta do problema.

Pense em muitos canos de água conectados através de muitas junções, divisores e válvulas. Você está alertado que seu nível de serviço de saída de água está degradado. Você trace o problema desde a saída de água através dos canos até determinar qual junção, divisão, válvula ou tubo está causando o problema. Você descobre que uma das válvulas elétricas está em curto. Essa válvula é a fonte do seu problema. O curto é a causa direta do seu problema. Você resolve facilmente a causa direta substituindo o valor. Tenha em mente que a causa raiz pode ser algo mais complexo, como condições climáticas, produtos químicos na água ou fabricação.

Este é o mesmo conceito para diagnosticar pilha complexa de tecnologia. Se a sua capacidade de login for limitada (saída), você deverá trace o problema até o componente (fonte) que está causando esse limite e corrigi-lo. Pode ser o software API (limite de serviço), os serviços de middleware, o banco de dados, restrições de recursos, um serviço de terceiros ou qualquer outra coisa.

Em TI, existem três categorias principais de pontos de interrupção para melhorar seu ritmo de resposta:

1. <DNT>
     **Output**
   </DNT>

2. <DNT>
     **Input**
   </DNT>

3. <DNT>
     **Client**
   </DNT>

Definir sua métrica de desempenho dentro dessas categorias, também conhecida como [nível de serviço](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide), melhorará significativamente seu tempo de resposta para determinar onde está a origem do problema. A medição dessas categorias é abordada em [nosso guia de gerenciamento a nível de serviço](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide). Para entender como usá-los em diagnósticos, continue lendo.

### Etapa 3: Encontre a causa direta [#find-direct-cause]

Quando estiver perto da origem do problema, determine o que mudou. Isso o ajudará a determinar rapidamente como resolver o problema imediatamente no curto prazo. No exemplo da [Etapa 2](#find-source), a mudança foi que a válvula não estava mais funcionando devido ao hardware degradado causando um curto-circuito.

Exemplos de mudanças comuns em TI são:

1. Taxas de transferência (tráfego)
2. Código (implantação)
3. Recursos (alocação de hardware)
4. Mudanças de dependência upstream ou downstream
5. Volume de dados

Para outros exemplos comuns de problemas que afetam o desempenho, consulte a [matriz de problemas](#problem-matrix) abaixo.

## Use pontos de dados de saúde [#health-data-points]

Conforme mencionado anteriormente, existem três categorias principais de desempenho que impulsionam sua jornada de diagnóstico. Compreender esses pontos de dados de saúde reduzirá significativamente o seu tempo para entender onde está a origem do problema.

<CollapserGroup>
  <Collapser id="output-perf" title="Desempenho de produção">
    <DNT>**This requires**</DNT>: APM

    Desempenho de saída é a capacidade de sua pilha interna de tecnologia entregar as respostas esperadas (saída) ao usuário final. Isso é tradicionalmente chamado de serviços de “backend”.

    Na grande maioria dos cenários, o desempenho da saída é medido simplesmente pela velocidade e pela qualidade da resposta (em outras palavras, é livre de erros). Lembre-se da perspectiva do usuário descrita acima. O usuário final indicará que o serviço está lento, não funciona ou está inacessível.

    O problema mais comum é a capacidade de responder às solicitações do usuário final em tempo hábil <DNT>**and**</DNT> e com êxito.

    Isso é facilmente identificado por uma anomalia de latência ou anomalia de erro nos serviços que suportam a capacidade do produto com problemas.
  </Collapser>

  <Collapser id="input-perf" title="Desempenho de entrada">
    <DNT>**This requires**</DNT>: Sintético

    O desempenho de entrada é simplesmente a capacidade dos seus serviços de receber uma solicitação do cliente. Isso não é o mesmo que a capacidade do cliente de enviar uma solicitação.

    Seu desempenho de saída (serviços backend ) pode estar excedendo os níveis de desempenho esperados. No entanto, algo entre o cliente e seus serviços está interrompendo o ciclo de vida de solicitação-resposta. Pode ser qualquer coisa entre o cliente e seus serviços.
  </Collapser>

  <Collapser id="client-perf" title="Desempenho do cliente">
    <DNT>**This requires**</DNT>: monitoramento de Browser e/ou monitoramento de Mobile

    O desempenho do cliente é a capacidade de um Browser e/ou aplicativo móvel fazer solicitações e renderizar respostas. O Browser e/ou mobile são facilmente identificados como a origem do problema, uma vez descartados o desempenho de saída (backend) e de entrada (Sintético).

    O desempenho de saída e entrada é relativamente fácil de descartar (ou descartar). Devido à profundidade do diagnóstico de entrada e saída, o Browser e o dispositivo móvel serão abordados em um guia de diagnóstico avançado no futuro.
  </Collapser>
</CollapserGroup>

## Matriz do problema [#problem-matrix]

A matriz de problemas é uma folha de referências de problemas comuns categorizados pelos três pontos de dados de saúde.

As fontes dos problemas são organizadas de acordo com sua frequência, sendo as mais comuns na linha superior e à esquerda. Uma análise mais detalhada está listada abaixo. gerenciar um nível de serviço bem feito irá ajudá-lo a descartar rapidamente dois desses três pontos de dados.

Esta tabela é uma matriz de problemas classificada por ponto de dados de saúde:

<table>
  <thead>
    <tr>
      <th>
        Ponto de dados
      </th>

      <th>
        Capacidade New Relic
      </th>

      <th>
        Fontes de problemas comuns
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        Saída
      </td>

      <td>
        APM, infra, log, NPM
      </td>

      <td>
        Aplicativo, fontes de dados, alteração de configuração de hardware, infraestrutura, rede interna, provedor terceirizado (AWS, GCP)
      </td>
    </tr>

    <tr>
      <td>
        Entrada
      </td>

      <td>
        Sintético, registro
      </td>

      <td>
        Roteamento externo (CDN, gateways, etc.), roteamento interno, coisas na internet (ISP, etc.)
      </td>
    </tr>

    <tr>
      <td>
        Cliente
      </td>

      <td>
        Browser, mobile
      </td>

      <td>
        Código do Browser ou mobile
      </td>
    </tr>
  </tbody>
</table>

Os problemas tendem a ser agravados, mas o objetivo é “encontrar a fonte” e então determinar “o que mudou” para restaurar rapidamente o nível de serviço.

### Problemas de exemplo [#example-problem]

Vejamos um exemplo de problema. Digamos que sua empresa implante um novo produto, e o aumento significativo de solicitações provoque um tempo de resposta inaceitável. A origem é descoberta no serviço de middleware de login. O problema é um salto nos tempos de fila TCP.

Aqui está um resumo desta situação:

* <DNT>**Category**</DNT>: desempenho de saída
* <DNT>**Source**</DNT>: middleware de login
* <DNT>**Direct cause**</DNT>: tempos de fila TCP de carga de solicitação adicional
* <DNT>**Solution**</DNT>: aumento do limite de conexão TCP e recursos escalonados
* <DNT>**Root-cause**</DNT>: planejamento de capacidade e testes de garantia de qualidade insuficientes no serviço downstream, afetando o middleware de login

### Outro exemplo de problema [#example-problem-2]

Aqui está outro exemplo de problema:

* Houve um aumento repentino de 500 erros de gateway no login...
* O tempo de resposta da API de login aumentou até o ponto em que o tempo limite começou...
* Os tempos limite foram rastreados para as conexões de banco de dados na camada de middleware...
* Trace da transação revelou aumento significativo no número de consultas ao banco de dados por solicitação de login...
* Foi encontrado um marcador de implantação para uma implantação que ocorreu logo antes do problema.

Aqui está um resumo desta situação:

* <DNT>**Category**</DNT>: degradação do desempenho de saída levando à falha no desempenho de entrada
* <DNT>**Source**</DNT>: banco de dados de chamada de serviço de middleware
* <DNT>**Direct cause**</DNT>: Aumento de 10x na consulta ao banco de dados após implantação do código
* <DNT>**Solution**</DNT>: reversão de implantação
* <DNT>**Root-cause**</DNT>: testes de garantia de qualidade insuficientes

### Matriz do problema por fonte [#problem-matrix-sources]

Aqui está uma tabela com a matriz do problema classificada por fontes.

<table>
  <thead>
    <tr>
      <th>
        <DNT>
          **Source**
        </DNT>
      </th>

      <th>
        <DNT>
          **Common direct causes**
        </DNT>
      </th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>
        Aplicativo
      </td>

      <td>
        1. Implantação recente (código)
        2. Restrições de recursos de hardware
        3. Restrições de banco de dados
        4. Alteração de configuração (hardware, roteamento ou rede)
        5. Dependência de terceiros
      </td>
    </tr>

    <tr>
      <td>
        Fonte de dados
      </td>

      <td>
        1. Restrições de banco de dados
        2. Mudança na lógica de consulta (n+1)
        3. Fila de mensagens (geralmente resultando em mau desempenho do produtor ou consumidor)
      </td>
    </tr>

    <tr>
      <td>
        Rede interna e roteamento
      </td>

      <td>
        1. Balanceadores de carga
        2. Proxies
        3. Gateways de API
        4. Roteadores (raro)
        5. ISP/CDN (raro)
      </td>
    </tr>
  </tbody>
</table>

## Identificando anomalia no padrão de desempenho [#pattern-anomalies]

<Callout variant="tip">
  Ter [um nível de serviço](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide) bem formado em seus limites de serviço em relação à transação principal (capacidades) ajudará você a identificar mais rapidamente em qual fluxo de trabalho de ponta a ponta reside o problema.
</Callout>

Identificar anomalias de padrão melhorará sua capacidade de identificar qual e onde pode estar a causa direta dos problemas.

Há muitas informações excelentes e aulas on-line gratuitas sobre identificação de padrões, mas o conceito geral é bastante simples e pode desbloquear poderosas habilidades de diagnóstico.

A chave para identificar padrões e anomalias nos dados de desempenho é que você não precisa saber como o serviço deveria estar funcionando: você só precisa determinar se o comportamento recente mudou.

Os exemplos fornecidos nesta seção usam tempo de resposta ou latência como métrica, mas você pode aplicar a mesma análise a quase qualquer conjunto de dados, como erros, taxas de transferência, métricas de recursos de hardware, profundidade de fila, etc.

### Normal [#normal]

Abaixo está um exemplo de gráfico de tempo de resposta aparentemente volátil (7 dias) no APM. Olhando mais de perto, você pode ver que o comportamento do tempo de resposta é repetitivo. Em outras palavras, não há mudança radical no comportamento durante um período de 7 dias. Os picos são repetitivos e comuns em comparação com o resto da linha do tempo.

<img alt="normal pattern" title="Normal pattern" src="/images/solutions_screenshot-full_oma-upr-pattern-normal.webp" />

Na verdade, se você alterar a visualização dos dados de <DNT>**average over time**</DNT> para <DNT>**percentiles over time**</DNT>, fica ainda mais claro o quão &quot;regulares&quot; são as alterações no tempo de resposta.

<img alt="normal pattern with percentile" title="Normal pattern with percentile" src="/images/solutions_screenshot-full_normal-percentile-pattern.webp" />

### Anormal [#abnormal]

Este gráfico mostra um tempo de resposta do aplicativo que parece ter aumentado de forma incomum em comparação com o comportamento recente.

<img alt="abnormal pattern" title="Abnormal pattern" src="/images/solutions_screenshot-full_pattern-abnormal.webp" />

Isso pode ser confirmado usando a comparação semana após semana.

<img alt="abnormal pattern week-over-week" title="Abnormal pattern week-over-week comparison" src="/images/solutions_screenshot-full_pattern-abnormal-compare.webp" />

Vemos que o padrão mudou e parece estar piorando em relação à comparação da semana passada.

## Encontrando a fonte [#finding-source]

A seguir, veremos como você encontraria a fonte no New Relic. Observe que esse fluxo de trabalho depende de distributed tracing.

Primeiro, você encontrará um aplicativo relacionado à latência ou aos erros sofridos pelo usuário final. Isso não significa que o aplicativo ou o código seja o problema, mas encontrar qualquer aplicativo dentro do fluxo (*first*) leva você mais rapidamente para perto da fonte. Depois que esse aplicativo for encontrado, você poderá descartar rapidamente componentes como código, host, banco de dados, configuração e rede.

Uma vez identificado o aplicativo, a questão é quais transações dentro desse aplicativo fazem parte do problema. Use o aplicativo que você identificou como tendo um problema de desempenho e identifique quais transações ou transações foram impactadas. Aqui você pode repetir a habilidade de anomalia de padrão de desempenho descrita acima em [Identificar anomalias de padrão de desempenho](#pattern-anomalies), mas desta vez na própria transação.

Os documentos a seguir ajudarão você a usar o New Relic para identificar problemas de transação:

1. [Página de transação: Encontre problemas específicos de desempenho](/docs/apm/apm-ui-pages/monitoring/transactions-page-find-specific-performance-problems/)
2. [Transação lenta na página de resumo do serviço](/whats-new/2021/03/slow-transactions)

Depois que as transações problemáticas forem identificadas, você poderá usar distributed tracing para revisar os componentes de ponta a ponta que dão suporte a essa transação. distributed tracing ajuda a identificar rapidamente onde está a latência e/ou onde os erros estão ocorrendo em toda stack, tudo em uma única visualização.

Os recursos a seguir ajudarão você a aprender como usar distributed tracing para identificar o componente de origem do problema:

1. [Introdução ao distributed tracing](/docs/distributed-tracing/concepts/introduction-distributed-tracing)
2. [Como usar a interface distributed tracing](/docs/distributed-tracing/ui-data/understand-use-distributed-tracing-ui/)
3. [Webinars on-line gratuitos sobre distributed tracing](https://learn.newrelic.com/new-relic-distributed-tracing-tracking-across-your-application-stacks)
4. [Um vídeo sobre como usar distributed tracing para análise de causa direta](https://www.youtube.com/watch?v=r9ImAQ5J5h4)

Aqui está um breve resumo das etapas para encontrar a fonte:

1. Examine um aplicativo relacionado ao desempenho afetado.
2. Identifique quais transações estão contribuindo para o problema.
3. Use distributed tracing para identificar o componente do problema no fluxo de ponta a ponta.

Podemos agora passar para a etapa final, identificando a causa direta.

## Encontrando a causa direta [#finding-direct-cause]

Depois que o componente de origem for encontrado, você poderá começar a determinar a causa direta.

Usando o conhecimento das etapas anteriores, você saberá se o problema é latência, sucesso ou ambos.

Problemas de latência podem ser encontrados usando trace da transação e/ou &quot;span em processamento&quot; dentro do trace distribuído.

Problemas de sucesso A mensagem de erro também pode ser vista no rastreamento, mas os melhores detalhes para problemas de sucesso geralmente são encontrados no log do aplicativo.

De qualquer forma, se você for o respondente de incidentes de primeiro nível ou um SRE, encontrar a causa direta caberia aos especialistas no assunto (SMEs), que normalmente são os desenvolvedores e engenheiros responsáveis pelo componente de origem encontrado.

A próxima etapa mais eficaz após descobrir o componente de origem é entrar em contato com os especialistas no assunto desse componente. Mostre a eles os dados descobertos na triagem e no diagnóstico que você concluiu para ter uma vantagem inicial na resolução de problemas.

<Callout variant="tip">
  Observe que tanto o login no contexto quanto distributed tracing são ativados por padrão com nosso mais novo <InlinePopover type="apm" />agente. (Se você não atualiza seu agente há algum tempo, recomendamos [atualizá-lo](/docs/new-relic-solutions/new-relic-one/install-configure/update-new-relic-agent) regularmente.)

  Log-in-context e distributed tracing são recursos essenciais necessários para reduzir o tempo de triagem, diagnóstico e resolução de problemas a longo prazo.
</Callout>

Agora vá em frente e seja um excelente engenheiro de confiabilidade do site com a New Relic!

## Próximos passos [#next-steps]

Se ainda não o fez, você pode querer ler alguns de nossos [guias de maturidade de observabilidade](/docs/new-relic-solutions/observability-maturity/introduction) relacionados, incluindo:

* [Gerenciamento de qualidade de alerta](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/alert-quality-management-guide)
* [Gerenciamento do nível de serviço](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide)